05/27 09:12:50PM parser.py:28 [INFO] 
05/27 09:12:50PM parser.py:29 [INFO] Parameters:
05/27 09:12:50PM parser.py:31 [INFO] DAG_PATH=results/search_Stage/CIFAR10/SCRATCH_SPEC/EXP-20240527-211250/DAG
05/27 09:12:50PM parser.py:31 [INFO] ALPHA_LR=0.0003
05/27 09:12:50PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
05/27 09:12:50PM parser.py:31 [INFO] BATCH_SIZE=64
05/27 09:12:50PM parser.py:31 [INFO] CHECKPOINT_RESET=False
05/27 09:12:50PM parser.py:31 [INFO] DATA_PATH=../data/
05/27 09:12:50PM parser.py:31 [INFO] DATASET=CIFAR10
05/27 09:12:50PM parser.py:31 [INFO] EPOCHS=50
05/27 09:12:50PM parser.py:31 [INFO] EXP_NAME=EXP-20240527-211250
05/27 09:12:50PM parser.py:31 [INFO] GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('skip_connect', 2)]], normal_concat=[2, 3, 4, 5], reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce_concat=[2, 3, 4, 5])
05/27 09:12:50PM parser.py:31 [INFO] GPUS=[0]
05/27 09:12:50PM parser.py:31 [INFO] INIT_CHANNELS=16
05/27 09:12:50PM parser.py:31 [INFO] LAYERS=20
05/27 09:12:50PM parser.py:31 [INFO] LOCAL_RANK=0
05/27 09:12:50PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/27 09:12:50PM parser.py:31 [INFO] NAME=SCRATCH_SPEC
05/27 09:12:50PM parser.py:31 [INFO] PATH=results/search_Stage/CIFAR10/SCRATCH_SPEC/EXP-20240527-211250
05/27 09:12:50PM parser.py:31 [INFO] PRINT_FREQ=50
05/27 09:12:50PM parser.py:31 [INFO] RESUME_PATH=None
05/27 09:12:50PM parser.py:31 [INFO] SAVE=EXP
05/27 09:12:50PM parser.py:31 [INFO] SEED=1
05/27 09:12:50PM parser.py:31 [INFO] SHARE_STAGE=False
05/27 09:12:50PM parser.py:31 [INFO] TRAIN_PORTION=0.5
05/27 09:12:50PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
05/27 09:12:50PM parser.py:31 [INFO] W_LR=0.025
05/27 09:12:50PM parser.py:31 [INFO] W_LR_MIN=0.001
05/27 09:12:50PM parser.py:31 [INFO] W_MOMENTUM=0.9
05/27 09:12:50PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
05/27 09:12:50PM parser.py:31 [INFO] WORKERS=4
05/27 09:12:50PM parser.py:32 [INFO] 
05/27 09:12:52PM searchStage_trainer.py:103 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2501, 0.2500, 0.2499, 0.2501],
        [0.2500, 0.2501, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2500, 0.2498, 0.2501, 0.2501],
        [0.2501, 0.2500, 0.2498, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2494, 0.2503, 0.2496],
        [0.2498, 0.2496, 0.2501, 0.2504],
        [0.2502, 0.2498, 0.2498, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2495, 0.2503, 0.2501],
        [0.2502, 0.2499, 0.2498, 0.2501],
        [0.2500, 0.2501, 0.2498, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2502, 0.2501, 0.2498],
        [0.2499, 0.2502, 0.2500, 0.2499],
        [0.2497, 0.2503, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2498, 0.2501, 0.2500],
        [0.2495, 0.2498, 0.2503, 0.2503],
        [0.2499, 0.2503, 0.2497, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2499, 0.2501, 0.2502],
        [0.2500, 0.2503, 0.2496, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2500, 0.2498],
        [0.2501, 0.2499, 0.2502, 0.2498],
        [0.2504, 0.2499, 0.2502, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2498, 0.2503, 0.2500],
        [0.2498, 0.2502, 0.2502, 0.2498],
        [0.2502, 0.2502, 0.2502, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2499, 0.2497, 0.2502],
        [0.2500, 0.2498, 0.2499, 0.2503],
        [0.2502, 0.2498, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2500, 0.2501],
        [0.2501, 0.2498, 0.2500, 0.2502],
        [0.2500, 0.2497, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2501, 0.2500, 0.2501],
        [0.2499, 0.2505, 0.2498, 0.2498],
        [0.2502, 0.2500, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2499, 0.2499, 0.2500],
        [0.2501, 0.2502, 0.2499, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2499, 0.2501],
        [0.2499, 0.2504, 0.2494, 0.2504],
        [0.2499, 0.2501, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2503, 0.2498, 0.2499],
        [0.2501, 0.2503, 0.2499, 0.2497],
        [0.2500, 0.2500, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2503, 0.2498],
        [0.2498, 0.2502, 0.2498, 0.2502],
        [0.2499, 0.2500, 0.2505, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2500, 0.2500],
        [0.2499, 0.2499, 0.2500, 0.2501],
        [0.2499, 0.2500, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2500, 0.2500, 0.2503],
        [0.2495, 0.2498, 0.2503, 0.2504],
        [0.2498, 0.2498, 0.2501, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:13:08PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 2.2445 (2.2812)	Prec@(1,5) (16.1%, 66.6%)	
05/27 09:13:24PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 2.0989 (2.2005)	Prec@(1,5) (20.0%, 71.9%)	
05/27 09:13:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 1.6904 (2.1097)	Prec@(1,5) (23.1%, 75.2%)	
05/27 09:13:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 1.6877 (2.0344)	Prec@(1,5) (25.2%, 77.8%)	
05/27 09:14:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 1.7719 (1.9853)	Prec@(1,5) (26.6%, 79.4%)	
05/27 09:14:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 1.9760 (1.9415)	Prec@(1,5) (27.9%, 80.7%)	
05/27 09:14:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 1.6480 (1.9057)	Prec@(1,5) (29.1%, 81.8%)	
05/27 09:14:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 1.4932 (1.8839)	Prec@(1,5) (29.9%, 82.5%)	
05/27 09:14:56PM searchStage_trainer.py:221 [INFO] Train: [  0/49] Final Prec@1 29.8920%
05/27 09:14:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 1.6916	Prec@(1,5) (36.4%, 88.9%)
05/27 09:15:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 1.6843	Prec@(1,5) (36.7%, 88.5%)
05/27 09:15:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 1.6841	Prec@(1,5) (36.7%, 88.5%)
05/27 09:15:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 1.6839	Prec@(1,5) (36.5%, 88.7%)
05/27 09:15:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 1.6871	Prec@(1,5) (36.6%, 88.5%)
05/27 09:15:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 1.6877	Prec@(1,5) (36.8%, 88.5%)
05/27 09:15:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 1.6831	Prec@(1,5) (37.1%, 88.6%)
05/27 09:15:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 1.6782	Prec@(1,5) (37.2%, 88.7%)
05/27 09:15:16PM searchStage_trainer.py:260 [INFO] Valid: [  0/49] Final Prec@1 37.1560%
05/27 09:15:16PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 4)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 5)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 5)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/27 09:15:16PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 37.1560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2489, 0.2546, 0.2513, 0.2453],
        [0.2504, 0.2530, 0.2499, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2525, 0.2498, 0.2465],
        [0.2516, 0.2510, 0.2518, 0.2456],
        [0.2531, 0.2578, 0.2462, 0.2430]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2525, 0.2527, 0.2429],
        [0.2525, 0.2543, 0.2461, 0.2471],
        [0.2523, 0.2556, 0.2465, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2540, 0.2485, 0.2468],
        [0.2518, 0.2536, 0.2478, 0.2469],
        [0.2516, 0.2544, 0.2490, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2531, 0.2490, 0.2473],
        [0.2513, 0.2542, 0.2488, 0.2458],
        [0.2510, 0.2535, 0.2497, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2524, 0.2488, 0.2467],
        [0.2520, 0.2522, 0.2481, 0.2477],
        [0.2512, 0.2525, 0.2471, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2499, 0.2507, 0.2502],
        [0.2498, 0.2511, 0.2505, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2507, 0.2501, 0.2495],
        [0.2504, 0.2505, 0.2504, 0.2487],
        [0.2512, 0.2509, 0.2495, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2503, 0.2507, 0.2491],
        [0.2498, 0.2510, 0.2493, 0.2498],
        [0.2508, 0.2515, 0.2497, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2506, 0.2493, 0.2500],
        [0.2498, 0.2507, 0.2497, 0.2498],
        [0.2505, 0.2504, 0.2500, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2503, 0.2498, 0.2496],
        [0.2501, 0.2506, 0.2501, 0.2491],
        [0.2506, 0.2501, 0.2488, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2507, 0.2489, 0.2505],
        [0.2500, 0.2511, 0.2501, 0.2488],
        [0.2501, 0.2503, 0.2501, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2498, 0.2503, 0.2500],
        [0.2495, 0.2506, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2504, 0.2504, 0.2497],
        [0.2493, 0.2507, 0.2501, 0.2499],
        [0.2492, 0.2501, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2502, 0.2498],
        [0.2486, 0.2506, 0.2510, 0.2498],
        [0.2495, 0.2501, 0.2505, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2497, 0.2501, 0.2506],
        [0.2493, 0.2502, 0.2508, 0.2496],
        [0.2493, 0.2504, 0.2510, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2499, 0.2526, 0.2482],
        [0.2493, 0.2494, 0.2495, 0.2517],
        [0.2493, 0.2488, 0.2520, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2494, 0.2499, 0.2521],
        [0.2496, 0.2500, 0.2501, 0.2503],
        [0.2482, 0.2507, 0.2527, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:15:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 1.4358 (1.6141)	Prec@(1,5) (39.2%, 89.7%)	
05/27 09:15:48PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 1.3883 (1.6227)	Prec@(1,5) (39.3%, 89.5%)	
05/27 09:16:05PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 1.7408 (1.6124)	Prec@(1,5) (39.7%, 89.5%)	
05/27 09:16:22PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 1.7189 (1.5988)	Prec@(1,5) (40.3%, 89.7%)	
05/27 09:16:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 1.6045 (1.5808)	Prec@(1,5) (41.0%, 89.9%)	
05/27 09:16:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 1.4236 (1.5693)	Prec@(1,5) (41.5%, 90.2%)	
05/27 09:17:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 1.4810 (1.5621)	Prec@(1,5) (41.9%, 90.3%)	
05/27 09:17:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 1.6077 (1.5527)	Prec@(1,5) (42.4%, 90.3%)	
05/27 09:17:26PM searchStage_trainer.py:221 [INFO] Train: [  1/49] Final Prec@1 42.4120%
05/27 09:17:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 1.5139	Prec@(1,5) (44.3%, 92.1%)
05/27 09:17:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 1.5119	Prec@(1,5) (43.6%, 92.3%)
05/27 09:17:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 1.5062	Prec@(1,5) (43.8%, 92.1%)
05/27 09:17:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 1.5061	Prec@(1,5) (43.6%, 92.2%)
05/27 09:17:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 1.5006	Prec@(1,5) (43.7%, 92.3%)
05/27 09:17:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 1.5066	Prec@(1,5) (43.7%, 92.2%)
05/27 09:17:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 1.5086	Prec@(1,5) (43.7%, 92.3%)
05/27 09:17:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 1.5073	Prec@(1,5) (43.6%, 92.3%)
05/27 09:17:47PM searchStage_trainer.py:260 [INFO] Valid: [  1/49] Final Prec@1 43.6320%
05/27 09:17:47PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:17:48PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 43.6320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2579, 0.2515, 0.2414],
        [0.2522, 0.2563, 0.2496, 0.2419]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2563, 0.2519, 0.2403],
        [0.2529, 0.2525, 0.2525, 0.2421],
        [0.2556, 0.2650, 0.2419, 0.2375]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2543, 0.2537, 0.2393],
        [0.2551, 0.2584, 0.2444, 0.2421],
        [0.2542, 0.2616, 0.2436, 0.2407]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2572, 0.2463, 0.2436],
        [0.2532, 0.2576, 0.2469, 0.2423],
        [0.2543, 0.2587, 0.2462, 0.2408]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2566, 0.2472, 0.2447],
        [0.2531, 0.2580, 0.2469, 0.2420],
        [0.2530, 0.2575, 0.2479, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2548, 0.2463, 0.2448],
        [0.2537, 0.2552, 0.2475, 0.2435],
        [0.2534, 0.2550, 0.2446, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2503, 0.2511, 0.2496],
        [0.2503, 0.2515, 0.2507, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2511, 0.2502, 0.2493],
        [0.2505, 0.2516, 0.2506, 0.2474],
        [0.2516, 0.2519, 0.2489, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2507, 0.2493],
        [0.2497, 0.2517, 0.2492, 0.2493],
        [0.2509, 0.2524, 0.2501, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2506, 0.2485, 0.2502],
        [0.2501, 0.2512, 0.2495, 0.2492],
        [0.2509, 0.2508, 0.2498, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2510, 0.2492, 0.2491],
        [0.2500, 0.2513, 0.2500, 0.2487],
        [0.2506, 0.2508, 0.2484, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2511, 0.2487, 0.2499],
        [0.2501, 0.2513, 0.2495, 0.2491],
        [0.2503, 0.2506, 0.2500, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2495, 0.2508, 0.2503],
        [0.2489, 0.2510, 0.2508, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2507, 0.2509, 0.2496],
        [0.2487, 0.2512, 0.2508, 0.2493],
        [0.2484, 0.2501, 0.2508, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2502, 0.2504, 0.2499],
        [0.2476, 0.2506, 0.2518, 0.2500],
        [0.2491, 0.2503, 0.2509, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2496, 0.2500, 0.2510],
        [0.2488, 0.2502, 0.2518, 0.2492],
        [0.2488, 0.2507, 0.2513, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2499, 0.2547, 0.2466],
        [0.2487, 0.2490, 0.2491, 0.2531],
        [0.2485, 0.2480, 0.2537, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2487, 0.2497, 0.2537],
        [0.2498, 0.2501, 0.2499, 0.2503],
        [0.2469, 0.2513, 0.2551, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:18:05PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 1.4166 (1.4211)	Prec@(1,5) (48.1%, 92.9%)	
05/27 09:18:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 1.2780 (1.4064)	Prec@(1,5) (48.5%, 92.6%)	
05/27 09:18:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 1.2556 (1.4019)	Prec@(1,5) (48.7%, 92.6%)	
05/27 09:18:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 1.3496 (1.4014)	Prec@(1,5) (48.6%, 92.6%)	
05/27 09:19:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 1.5766 (1.3914)	Prec@(1,5) (49.2%, 92.9%)	
05/27 09:19:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 1.1131 (1.3828)	Prec@(1,5) (49.6%, 93.0%)	
05/27 09:19:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 1.3726 (1.3818)	Prec@(1,5) (49.6%, 92.9%)	
05/27 09:19:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 1.3815 (1.3686)	Prec@(1,5) (50.1%, 93.1%)	
05/27 09:19:58PM searchStage_trainer.py:221 [INFO] Train: [  2/49] Final Prec@1 50.0800%
05/27 09:20:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 1.3740	Prec@(1,5) (50.8%, 93.2%)
05/27 09:20:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 1.3782	Prec@(1,5) (51.0%, 93.2%)
05/27 09:20:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 1.3859	Prec@(1,5) (50.8%, 92.9%)
05/27 09:20:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 1.3697	Prec@(1,5) (51.1%, 93.2%)
05/27 09:20:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 1.3672	Prec@(1,5) (51.2%, 93.3%)
05/27 09:20:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 1.3678	Prec@(1,5) (51.1%, 93.3%)
05/27 09:20:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 1.3606	Prec@(1,5) (51.4%, 93.4%)
05/27 09:20:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 1.3706	Prec@(1,5) (51.0%, 93.3%)
05/27 09:20:19PM searchStage_trainer.py:260 [INFO] Valid: [  2/49] Final Prec@1 51.0440%
05/27 09:20:19PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('avg_pool_3x3', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:20:19PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 51.0440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2629, 0.2515, 0.2364],
        [0.2511, 0.2606, 0.2504, 0.2379]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2608, 0.2520, 0.2353],
        [0.2529, 0.2549, 0.2536, 0.2387],
        [0.2579, 0.2707, 0.2395, 0.2318]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2566, 0.2547, 0.2338],
        [0.2562, 0.2623, 0.2427, 0.2388],
        [0.2566, 0.2654, 0.2407, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2605, 0.2449, 0.2411],
        [0.2542, 0.2609, 0.2470, 0.2379],
        [0.2558, 0.2622, 0.2447, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2611, 0.2451, 0.2416],
        [0.2536, 0.2620, 0.2449, 0.2394],
        [0.2549, 0.2608, 0.2466, 0.2377]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2572, 0.2455, 0.2419],
        [0.2545, 0.2576, 0.2477, 0.2402],
        [0.2554, 0.2581, 0.2410, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2504, 0.2508, 0.2500],
        [0.2507, 0.2524, 0.2509, 0.2460]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2513, 0.2508, 0.2485],
        [0.2505, 0.2517, 0.2504, 0.2474],
        [0.2519, 0.2527, 0.2488, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2507, 0.2510, 0.2486],
        [0.2495, 0.2525, 0.2492, 0.2487],
        [0.2507, 0.2530, 0.2498, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2510, 0.2485, 0.2500],
        [0.2500, 0.2515, 0.2499, 0.2485],
        [0.2509, 0.2510, 0.2495, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2517, 0.2492, 0.2483],
        [0.2502, 0.2516, 0.2500, 0.2482],
        [0.2506, 0.2510, 0.2478, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2516, 0.2493, 0.2492],
        [0.2503, 0.2515, 0.2492, 0.2490],
        [0.2506, 0.2510, 0.2495, 0.2489]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2493, 0.2513, 0.2505],
        [0.2484, 0.2513, 0.2511, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2509, 0.2514, 0.2495],
        [0.2484, 0.2515, 0.2513, 0.2488],
        [0.2479, 0.2501, 0.2510, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2501, 0.2506, 0.2499],
        [0.2466, 0.2508, 0.2526, 0.2500],
        [0.2487, 0.2504, 0.2512, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2493, 0.2498, 0.2516],
        [0.2484, 0.2502, 0.2526, 0.2489],
        [0.2484, 0.2510, 0.2518, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2499, 0.2564, 0.2454],
        [0.2483, 0.2487, 0.2488, 0.2543],
        [0.2479, 0.2473, 0.2552, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2483, 0.2498, 0.2547],
        [0.2497, 0.2502, 0.2497, 0.2503],
        [0.2456, 0.2519, 0.2569, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:20:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 1.3440 (1.2635)	Prec@(1,5) (54.9%, 94.0%)	
05/27 09:20:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 1.4313 (1.2752)	Prec@(1,5) (54.5%, 94.3%)	
05/27 09:21:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 1.0743 (1.2665)	Prec@(1,5) (54.7%, 94.4%)	
05/27 09:21:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 1.1830 (1.2547)	Prec@(1,5) (55.0%, 94.4%)	
05/27 09:21:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 1.3062 (1.2448)	Prec@(1,5) (55.2%, 94.6%)	
05/27 09:21:57PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 1.1016 (1.2436)	Prec@(1,5) (55.5%, 94.4%)	
05/27 09:22:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 1.1075 (1.2372)	Prec@(1,5) (55.7%, 94.5%)	
05/27 09:22:24PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 1.0495 (1.2274)	Prec@(1,5) (56.0%, 94.6%)	
05/27 09:22:25PM searchStage_trainer.py:221 [INFO] Train: [  3/49] Final Prec@1 55.9600%
05/27 09:22:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 1.2369	Prec@(1,5) (56.2%, 95.2%)
05/27 09:22:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 1.2183	Prec@(1,5) (56.7%, 94.9%)
05/27 09:22:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 1.2276	Prec@(1,5) (56.3%, 94.7%)
05/27 09:22:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 1.2342	Prec@(1,5) (56.0%, 94.6%)
05/27 09:22:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 1.2382	Prec@(1,5) (55.8%, 94.5%)
05/27 09:22:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 1.2403	Prec@(1,5) (55.8%, 94.6%)
05/27 09:22:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 1.2404	Prec@(1,5) (55.8%, 94.5%)
05/27 09:22:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 1.2407	Prec@(1,5) (55.9%, 94.5%)
05/27 09:22:45PM searchStage_trainer.py:260 [INFO] Valid: [  3/49] Final Prec@1 55.9440%
05/27 09:22:45PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:22:45PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 55.9440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2514, 0.2658, 0.2519, 0.2308],
        [0.2520, 0.2629, 0.2504, 0.2347]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2628, 0.2520, 0.2327],
        [0.2539, 0.2571, 0.2541, 0.2349],
        [0.2595, 0.2751, 0.2387, 0.2267]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2580, 0.2549, 0.2312],
        [0.2576, 0.2652, 0.2408, 0.2363],
        [0.2587, 0.2689, 0.2391, 0.2333]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2629, 0.2452, 0.2369],
        [0.2555, 0.2626, 0.2452, 0.2367],
        [0.2570, 0.2642, 0.2442, 0.2346]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2639, 0.2443, 0.2392],
        [0.2549, 0.2647, 0.2427, 0.2377],
        [0.2563, 0.2634, 0.2461, 0.2341]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2578, 0.2590, 0.2435, 0.2396],
        [0.2555, 0.2593, 0.2468, 0.2385],
        [0.2563, 0.2610, 0.2398, 0.2429]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2513, 0.2510, 0.2489],
        [0.2499, 0.2527, 0.2512, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2517, 0.2515, 0.2476],
        [0.2506, 0.2520, 0.2504, 0.2471],
        [0.2518, 0.2531, 0.2486, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2509, 0.2508, 0.2486],
        [0.2493, 0.2530, 0.2498, 0.2480],
        [0.2507, 0.2532, 0.2496, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2512, 0.2486, 0.2498],
        [0.2500, 0.2518, 0.2496, 0.2486],
        [0.2508, 0.2513, 0.2497, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2523, 0.2493, 0.2477],
        [0.2502, 0.2518, 0.2503, 0.2477],
        [0.2505, 0.2513, 0.2472, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2519, 0.2493, 0.2488],
        [0.2504, 0.2517, 0.2491, 0.2488],
        [0.2505, 0.2514, 0.2492, 0.2489]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2493, 0.2516, 0.2505],
        [0.2479, 0.2516, 0.2514, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2510, 0.2516, 0.2495],
        [0.2481, 0.2518, 0.2520, 0.2482],
        [0.2477, 0.2500, 0.2511, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2501, 0.2508, 0.2501],
        [0.2459, 0.2509, 0.2532, 0.2500],
        [0.2484, 0.2505, 0.2515, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2492, 0.2497, 0.2519],
        [0.2481, 0.2501, 0.2531, 0.2487],
        [0.2481, 0.2512, 0.2521, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2500, 0.2580, 0.2443],
        [0.2479, 0.2484, 0.2486, 0.2551],
        [0.2473, 0.2468, 0.2563, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2479, 0.2499, 0.2557],
        [0.2497, 0.2503, 0.2495, 0.2504],
        [0.2446, 0.2523, 0.2585, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:23:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 1.0191 (1.1635)	Prec@(1,5) (58.5%, 95.6%)	
05/27 09:23:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 1.0435 (1.1599)	Prec@(1,5) (58.5%, 95.6%)	
05/27 09:23:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 1.3640 (1.1433)	Prec@(1,5) (59.4%, 95.6%)	
05/27 09:23:47PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 1.1046 (1.1443)	Prec@(1,5) (59.3%, 95.6%)	
05/27 09:24:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 1.1183 (1.1396)	Prec@(1,5) (59.5%, 95.7%)	
05/27 09:24:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 1.3318 (1.1340)	Prec@(1,5) (59.7%, 95.7%)	
05/27 09:24:33PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 0.9215 (1.1303)	Prec@(1,5) (59.8%, 95.6%)	
05/27 09:24:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 1.1915 (1.1224)	Prec@(1,5) (60.0%, 95.7%)	
05/27 09:24:46PM searchStage_trainer.py:221 [INFO] Train: [  4/49] Final Prec@1 59.9800%
05/27 09:24:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 1.1280	Prec@(1,5) (60.2%, 95.3%)
05/27 09:24:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 1.1156	Prec@(1,5) (60.6%, 95.5%)
05/27 09:24:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 1.1229	Prec@(1,5) (60.6%, 95.4%)
05/27 09:24:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 1.1242	Prec@(1,5) (60.5%, 95.4%)
05/27 09:24:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 1.1241	Prec@(1,5) (60.2%, 95.4%)
05/27 09:25:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 1.1142	Prec@(1,5) (60.7%, 95.4%)
05/27 09:25:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 1.1147	Prec@(1,5) (60.6%, 95.4%)
05/27 09:25:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 1.1129	Prec@(1,5) (60.7%, 95.4%)
05/27 09:25:06PM searchStage_trainer.py:260 [INFO] Valid: [  4/49] Final Prec@1 60.6800%
05/27 09:25:06PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:25:06PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 60.6800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2508, 0.2680, 0.2531, 0.2281],
        [0.2521, 0.2660, 0.2516, 0.2304]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2641, 0.2524, 0.2299],
        [0.2543, 0.2595, 0.2540, 0.2322],
        [0.2604, 0.2773, 0.2386, 0.2237]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2587, 0.2555, 0.2300],
        [0.2588, 0.2675, 0.2399, 0.2337],
        [0.2597, 0.2707, 0.2385, 0.2311]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2648, 0.2450, 0.2349],
        [0.2557, 0.2638, 0.2444, 0.2361],
        [0.2575, 0.2661, 0.2446, 0.2319]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2657, 0.2436, 0.2372],
        [0.2558, 0.2666, 0.2418, 0.2357],
        [0.2570, 0.2652, 0.2454, 0.2324]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2591, 0.2602, 0.2423, 0.2383],
        [0.2566, 0.2606, 0.2463, 0.2365],
        [0.2571, 0.2625, 0.2390, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2514, 0.2509, 0.2487],
        [0.2502, 0.2529, 0.2511, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2520, 0.2517, 0.2474],
        [0.2506, 0.2521, 0.2505, 0.2468],
        [0.2518, 0.2535, 0.2485, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2509, 0.2485],
        [0.2491, 0.2532, 0.2498, 0.2479],
        [0.2507, 0.2533, 0.2498, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2513, 0.2483, 0.2500],
        [0.2501, 0.2519, 0.2495, 0.2485],
        [0.2508, 0.2514, 0.2499, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2525, 0.2491, 0.2474],
        [0.2503, 0.2520, 0.2506, 0.2472],
        [0.2506, 0.2514, 0.2468, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2521, 0.2489, 0.2489],
        [0.2505, 0.2520, 0.2491, 0.2485],
        [0.2503, 0.2516, 0.2493, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2492, 0.2520, 0.2505],
        [0.2477, 0.2517, 0.2516, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2512, 0.2519, 0.2494],
        [0.2479, 0.2519, 0.2523, 0.2479],
        [0.2473, 0.2500, 0.2513, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2510, 0.2502],
        [0.2453, 0.2510, 0.2536, 0.2501],
        [0.2482, 0.2506, 0.2518, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2491, 0.2497, 0.2522],
        [0.2479, 0.2501, 0.2536, 0.2485],
        [0.2478, 0.2514, 0.2522, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2500, 0.2592, 0.2434],
        [0.2475, 0.2482, 0.2483, 0.2560],
        [0.2469, 0.2465, 0.2572, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2476, 0.2498, 0.2566],
        [0.2497, 0.2505, 0.2494, 0.2504],
        [0.2439, 0.2526, 0.2596, 0.2439]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:25:22PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 1.2561 (1.0713)	Prec@(1,5) (61.8%, 96.1%)	
05/27 09:25:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 1.1949 (1.0596)	Prec@(1,5) (62.7%, 96.3%)	
05/27 09:25:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 0.8035 (1.0558)	Prec@(1,5) (62.8%, 96.2%)	
05/27 09:26:08PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 1.0178 (1.0406)	Prec@(1,5) (63.2%, 96.5%)	
05/27 09:26:23PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 1.0172 (1.0354)	Prec@(1,5) (63.3%, 96.7%)	
05/27 09:26:39PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 0.7650 (1.0341)	Prec@(1,5) (63.4%, 96.6%)	
05/27 09:26:54PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 0.9598 (1.0239)	Prec@(1,5) (63.8%, 96.6%)	
05/27 09:27:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 1.5063 (1.0219)	Prec@(1,5) (63.9%, 96.5%)	
05/27 09:27:07PM searchStage_trainer.py:221 [INFO] Train: [  5/49] Final Prec@1 63.9520%
05/27 09:27:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 1.1159	Prec@(1,5) (61.2%, 95.0%)
05/27 09:27:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 1.1421	Prec@(1,5) (61.0%, 94.8%)
05/27 09:27:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 1.1479	Prec@(1,5) (60.7%, 94.8%)
05/27 09:27:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 1.1513	Prec@(1,5) (60.6%, 94.7%)
05/27 09:27:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 1.1456	Prec@(1,5) (60.9%, 94.7%)
05/27 09:27:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 1.1448	Prec@(1,5) (60.9%, 94.7%)
05/27 09:27:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 1.1468	Prec@(1,5) (60.9%, 94.6%)
05/27 09:27:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 1.1493	Prec@(1,5) (60.8%, 94.7%)
05/27 09:27:26PM searchStage_trainer.py:260 [INFO] Valid: [  5/49] Final Prec@1 60.7680%
05/27 09:27:26PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:27:27PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 60.7680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2514, 0.2701, 0.2527, 0.2258],
        [0.2523, 0.2685, 0.2515, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2550, 0.2651, 0.2513, 0.2287],
        [0.2550, 0.2607, 0.2541, 0.2301],
        [0.2613, 0.2787, 0.2386, 0.2214]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2567, 0.2601, 0.2558, 0.2275],
        [0.2596, 0.2683, 0.2384, 0.2337],
        [0.2601, 0.2719, 0.2387, 0.2293]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2656, 0.2443, 0.2346],
        [0.2564, 0.2649, 0.2438, 0.2349],
        [0.2582, 0.2672, 0.2445, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2671, 0.2428, 0.2360],
        [0.2561, 0.2679, 0.2418, 0.2341],
        [0.2578, 0.2663, 0.2447, 0.2312]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2597, 0.2613, 0.2416, 0.2374],
        [0.2571, 0.2619, 0.2461, 0.2348],
        [0.2576, 0.2636, 0.2383, 0.2404]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2514, 0.2507, 0.2489],
        [0.2505, 0.2529, 0.2512, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2524, 0.2515, 0.2471],
        [0.2505, 0.2523, 0.2506, 0.2467],
        [0.2518, 0.2537, 0.2485, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2509, 0.2509, 0.2481],
        [0.2492, 0.2532, 0.2497, 0.2479],
        [0.2507, 0.2534, 0.2497, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2514, 0.2480, 0.2502],
        [0.2500, 0.2521, 0.2494, 0.2484],
        [0.2508, 0.2515, 0.2499, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2526, 0.2489, 0.2473],
        [0.2503, 0.2521, 0.2507, 0.2469],
        [0.2507, 0.2514, 0.2464, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2522, 0.2486, 0.2493],
        [0.2503, 0.2522, 0.2492, 0.2482],
        [0.2503, 0.2518, 0.2495, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2492, 0.2523, 0.2507],
        [0.2475, 0.2518, 0.2519, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2512, 0.2521, 0.2494],
        [0.2477, 0.2522, 0.2527, 0.2475],
        [0.2471, 0.2500, 0.2514, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2500, 0.2512, 0.2503],
        [0.2449, 0.2510, 0.2540, 0.2500],
        [0.2481, 0.2506, 0.2519, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2490, 0.2497, 0.2524],
        [0.2477, 0.2501, 0.2539, 0.2483],
        [0.2476, 0.2515, 0.2524, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2500, 0.2603, 0.2427],
        [0.2473, 0.2480, 0.2482, 0.2566],
        [0.2466, 0.2462, 0.2578, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2473, 0.2497, 0.2573],
        [0.2498, 0.2504, 0.2493, 0.2504],
        [0.2433, 0.2529, 0.2605, 0.2433]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:27:43PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 0.9607 (0.9314)	Prec@(1,5) (67.3%, 96.7%)	
05/27 09:27:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 1.0642 (0.9351)	Prec@(1,5) (67.4%, 96.9%)	
05/27 09:28:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 1.0321 (0.9442)	Prec@(1,5) (66.8%, 97.0%)	
05/27 09:28:29PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 1.1208 (0.9422)	Prec@(1,5) (66.8%, 97.1%)	
05/27 09:28:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 0.9063 (0.9385)	Prec@(1,5) (67.0%, 97.1%)	
05/27 09:28:59PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 0.8979 (0.9354)	Prec@(1,5) (67.3%, 97.1%)	
05/27 09:29:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 0.9582 (0.9353)	Prec@(1,5) (67.4%, 97.1%)	
05/27 09:29:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 0.9019 (0.9339)	Prec@(1,5) (67.3%, 97.1%)	
05/27 09:29:27PM searchStage_trainer.py:221 [INFO] Train: [  6/49] Final Prec@1 67.3200%
05/27 09:29:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 1.0292	Prec@(1,5) (63.5%, 96.6%)
05/27 09:29:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 1.0601	Prec@(1,5) (62.5%, 96.6%)
05/27 09:29:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 1.0488	Prec@(1,5) (63.0%, 96.5%)
05/27 09:29:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 1.0434	Prec@(1,5) (63.2%, 96.5%)
05/27 09:29:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 1.0441	Prec@(1,5) (63.1%, 96.6%)
05/27 09:29:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 1.0524	Prec@(1,5) (63.0%, 96.5%)
05/27 09:29:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 1.0541	Prec@(1,5) (62.9%, 96.5%)
05/27 09:29:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 1.0524	Prec@(1,5) (63.1%, 96.4%)
05/27 09:29:47PM searchStage_trainer.py:260 [INFO] Valid: [  6/49] Final Prec@1 63.0560%
05/27 09:29:47PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:29:47PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 63.0560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2516, 0.2711, 0.2523, 0.2251],
        [0.2528, 0.2697, 0.2521, 0.2254]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2653, 0.2514, 0.2274],
        [0.2553, 0.2614, 0.2541, 0.2293],
        [0.2621, 0.2797, 0.2383, 0.2199]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2572, 0.2603, 0.2559, 0.2267],
        [0.2603, 0.2685, 0.2375, 0.2337],
        [0.2604, 0.2725, 0.2391, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2662, 0.2438, 0.2342],
        [0.2568, 0.2653, 0.2433, 0.2346],
        [0.2587, 0.2677, 0.2445, 0.2291]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2678, 0.2420, 0.2358],
        [0.2564, 0.2687, 0.2419, 0.2330],
        [0.2583, 0.2670, 0.2445, 0.2303]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2620, 0.2411, 0.2364],
        [0.2574, 0.2622, 0.2460, 0.2345],
        [0.2579, 0.2642, 0.2383, 0.2396]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2517, 0.2509, 0.2486],
        [0.2504, 0.2529, 0.2512, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2525, 0.2516, 0.2471],
        [0.2506, 0.2523, 0.2505, 0.2467],
        [0.2518, 0.2538, 0.2485, 0.2460]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2510, 0.2512, 0.2479],
        [0.2491, 0.2532, 0.2497, 0.2480],
        [0.2507, 0.2535, 0.2496, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2515, 0.2480, 0.2501],
        [0.2500, 0.2522, 0.2495, 0.2483],
        [0.2508, 0.2515, 0.2499, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2529, 0.2491, 0.2470],
        [0.2503, 0.2521, 0.2507, 0.2468],
        [0.2506, 0.2515, 0.2464, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2523, 0.2487, 0.2491],
        [0.2503, 0.2522, 0.2493, 0.2482],
        [0.2503, 0.2518, 0.2494, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2491, 0.2524, 0.2507],
        [0.2473, 0.2519, 0.2520, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2513, 0.2522, 0.2495],
        [0.2475, 0.2523, 0.2530, 0.2473],
        [0.2469, 0.2500, 0.2515, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2500, 0.2513, 0.2503],
        [0.2446, 0.2511, 0.2542, 0.2500],
        [0.2480, 0.2506, 0.2520, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2490, 0.2496, 0.2526],
        [0.2476, 0.2500, 0.2542, 0.2482],
        [0.2475, 0.2516, 0.2525, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2500, 0.2610, 0.2422],
        [0.2471, 0.2478, 0.2481, 0.2570],
        [0.2464, 0.2460, 0.2582, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2454, 0.2471, 0.2498, 0.2578],
        [0.2498, 0.2505, 0.2492, 0.2505],
        [0.2430, 0.2530, 0.2611, 0.2429]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:30:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 0.9628 (0.8750)	Prec@(1,5) (68.9%, 97.6%)	
05/27 09:30:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 0.9314 (0.8854)	Prec@(1,5) (69.3%, 97.4%)	
05/27 09:30:34PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 1.0886 (0.8865)	Prec@(1,5) (69.3%, 97.6%)	
05/27 09:30:50PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 0.7767 (0.8827)	Prec@(1,5) (69.3%, 97.6%)	
05/27 09:31:05PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 1.0072 (0.8771)	Prec@(1,5) (69.6%, 97.6%)	
05/27 09:31:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 0.7122 (0.8732)	Prec@(1,5) (69.7%, 97.5%)	
05/27 09:31:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 0.9325 (0.8763)	Prec@(1,5) (69.6%, 97.5%)	
05/27 09:31:49PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 0.8160 (0.8749)	Prec@(1,5) (69.7%, 97.5%)	
05/27 09:31:49PM searchStage_trainer.py:221 [INFO] Train: [  7/49] Final Prec@1 69.7000%
05/27 09:31:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 0.8856	Prec@(1,5) (69.1%, 97.5%)
05/27 09:31:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 0.8889	Prec@(1,5) (69.1%, 97.4%)
05/27 09:31:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 0.8857	Prec@(1,5) (69.0%, 97.4%)
05/27 09:31:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 0.8917	Prec@(1,5) (68.6%, 97.5%)
05/27 09:32:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 0.8876	Prec@(1,5) (68.7%, 97.5%)
05/27 09:32:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 0.8833	Prec@(1,5) (68.9%, 97.5%)
05/27 09:32:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 0.8803	Prec@(1,5) (68.9%, 97.6%)
05/27 09:32:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 0.8810	Prec@(1,5) (69.0%, 97.5%)
05/27 09:32:09PM searchStage_trainer.py:260 [INFO] Valid: [  7/49] Final Prec@1 68.9600%
05/27 09:32:09PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:32:09PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 68.9600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2516, 0.2711, 0.2523, 0.2250],
        [0.2533, 0.2704, 0.2528, 0.2235]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2660, 0.2520, 0.2264],
        [0.2556, 0.2619, 0.2543, 0.2282],
        [0.2622, 0.2800, 0.2384, 0.2194]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2572, 0.2615, 0.2567, 0.2247],
        [0.2603, 0.2686, 0.2371, 0.2340],
        [0.2608, 0.2727, 0.2390, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2665, 0.2435, 0.2342],
        [0.2569, 0.2656, 0.2432, 0.2344],
        [0.2588, 0.2681, 0.2447, 0.2284]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2684, 0.2418, 0.2347],
        [0.2566, 0.2691, 0.2417, 0.2326],
        [0.2585, 0.2672, 0.2440, 0.2303]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2606, 0.2623, 0.2412, 0.2359],
        [0.2577, 0.2626, 0.2458, 0.2339],
        [0.2581, 0.2646, 0.2379, 0.2394]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2516, 0.2507, 0.2487],
        [0.2504, 0.2530, 0.2512, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2525, 0.2516, 0.2472],
        [0.2507, 0.2523, 0.2505, 0.2465],
        [0.2518, 0.2538, 0.2484, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2512, 0.2514, 0.2476],
        [0.2490, 0.2532, 0.2496, 0.2482],
        [0.2507, 0.2535, 0.2497, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2516, 0.2481, 0.2499],
        [0.2500, 0.2522, 0.2494, 0.2484],
        [0.2508, 0.2515, 0.2498, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2531, 0.2490, 0.2469],
        [0.2504, 0.2521, 0.2506, 0.2469],
        [0.2506, 0.2515, 0.2464, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2523, 0.2486, 0.2492],
        [0.2504, 0.2523, 0.2491, 0.2482],
        [0.2503, 0.2519, 0.2495, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2491, 0.2525, 0.2507],
        [0.2472, 0.2520, 0.2521, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2514, 0.2524, 0.2494],
        [0.2473, 0.2525, 0.2531, 0.2471],
        [0.2468, 0.2500, 0.2515, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2499, 0.2514, 0.2503],
        [0.2444, 0.2511, 0.2544, 0.2500],
        [0.2480, 0.2507, 0.2521, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2489, 0.2496, 0.2528],
        [0.2475, 0.2500, 0.2543, 0.2481],
        [0.2475, 0.2516, 0.2525, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2500, 0.2617, 0.2417],
        [0.2470, 0.2477, 0.2479, 0.2574],
        [0.2463, 0.2458, 0.2585, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2469, 0.2498, 0.2582],
        [0.2498, 0.2505, 0.2492, 0.2505],
        [0.2427, 0.2532, 0.2615, 0.2426]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:32:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 0.9127 (0.7854)	Prec@(1,5) (72.6%, 98.2%)	
05/27 09:32:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 0.6204 (0.8065)	Prec@(1,5) (71.6%, 98.0%)	
05/27 09:32:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 0.5863 (0.8189)	Prec@(1,5) (71.4%, 97.8%)	
05/27 09:33:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 0.8625 (0.8154)	Prec@(1,5) (71.4%, 97.9%)	
05/27 09:33:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 0.9629 (0.8156)	Prec@(1,5) (71.4%, 97.8%)	
05/27 09:33:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 0.6374 (0.8031)	Prec@(1,5) (71.9%, 97.9%)	
05/27 09:33:57PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 0.8895 (0.8039)	Prec@(1,5) (71.7%, 97.9%)	
05/27 09:34:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 0.7766 (0.8055)	Prec@(1,5) (71.6%, 97.9%)	
05/27 09:34:10PM searchStage_trainer.py:221 [INFO] Train: [  8/49] Final Prec@1 71.5760%
05/27 09:34:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 0.8878	Prec@(1,5) (69.8%, 97.5%)
05/27 09:34:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 0.8832	Prec@(1,5) (69.6%, 97.7%)
05/27 09:34:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 0.8809	Prec@(1,5) (69.4%, 97.5%)
05/27 09:34:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 0.8799	Prec@(1,5) (69.5%, 97.5%)
05/27 09:34:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 0.8766	Prec@(1,5) (69.4%, 97.6%)
05/27 09:34:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 0.8775	Prec@(1,5) (69.5%, 97.6%)
05/27 09:34:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 0.8773	Prec@(1,5) (69.5%, 97.6%)
05/27 09:34:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 0.8794	Prec@(1,5) (69.4%, 97.6%)
05/27 09:34:30PM searchStage_trainer.py:260 [INFO] Valid: [  8/49] Final Prec@1 69.3800%
05/27 09:34:30PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:34:30PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 69.3800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2522, 0.2715, 0.2526, 0.2237],
        [0.2532, 0.2704, 0.2526, 0.2238]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2560, 0.2664, 0.2522, 0.2254],
        [0.2560, 0.2621, 0.2541, 0.2279],
        [0.2626, 0.2802, 0.2381, 0.2192]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2579, 0.2619, 0.2571, 0.2231],
        [0.2603, 0.2685, 0.2370, 0.2341],
        [0.2607, 0.2727, 0.2390, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2667, 0.2434, 0.2342],
        [0.2568, 0.2659, 0.2434, 0.2339],
        [0.2587, 0.2681, 0.2447, 0.2285]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2687, 0.2414, 0.2347],
        [0.2566, 0.2694, 0.2419, 0.2321],
        [0.2587, 0.2674, 0.2437, 0.2302]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2624, 0.2414, 0.2354],
        [0.2577, 0.2627, 0.2459, 0.2338],
        [0.2581, 0.2646, 0.2379, 0.2394]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2518, 0.2507, 0.2487],
        [0.2503, 0.2529, 0.2514, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2526, 0.2515, 0.2471],
        [0.2509, 0.2523, 0.2504, 0.2464],
        [0.2517, 0.2539, 0.2484, 0.2460]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2514, 0.2516, 0.2473],
        [0.2490, 0.2532, 0.2495, 0.2483],
        [0.2507, 0.2535, 0.2497, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2517, 0.2481, 0.2498],
        [0.2500, 0.2522, 0.2494, 0.2485],
        [0.2508, 0.2515, 0.2499, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2531, 0.2489, 0.2469],
        [0.2503, 0.2521, 0.2507, 0.2468],
        [0.2506, 0.2514, 0.2464, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2523, 0.2485, 0.2492],
        [0.2504, 0.2522, 0.2492, 0.2482],
        [0.2503, 0.2519, 0.2494, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2491, 0.2526, 0.2508],
        [0.2472, 0.2520, 0.2522, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2515, 0.2525, 0.2493],
        [0.2472, 0.2525, 0.2532, 0.2470],
        [0.2468, 0.2500, 0.2515, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2499, 0.2514, 0.2503],
        [0.2443, 0.2512, 0.2546, 0.2500],
        [0.2479, 0.2507, 0.2521, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2489, 0.2495, 0.2529],
        [0.2474, 0.2500, 0.2545, 0.2480],
        [0.2474, 0.2517, 0.2526, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2500, 0.2621, 0.2414],
        [0.2469, 0.2476, 0.2479, 0.2576],
        [0.2462, 0.2457, 0.2588, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2468, 0.2498, 0.2585],
        [0.2498, 0.2506, 0.2491, 0.2505],
        [0.2425, 0.2533, 0.2618, 0.2424]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:34:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 0.9731 (0.7763)	Prec@(1,5) (72.3%, 98.2%)	
05/27 09:35:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 0.6024 (0.7459)	Prec@(1,5) (73.7%, 98.5%)	
05/27 09:35:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 0.3942 (0.7568)	Prec@(1,5) (73.3%, 98.3%)	
05/27 09:35:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 0.8587 (0.7590)	Prec@(1,5) (73.2%, 98.3%)	
05/27 09:35:47PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 0.8820 (0.7649)	Prec@(1,5) (73.2%, 98.1%)	
05/27 09:36:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 0.6649 (0.7609)	Prec@(1,5) (73.4%, 98.2%)	
05/27 09:36:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 0.7347 (0.7554)	Prec@(1,5) (73.6%, 98.2%)	
05/27 09:36:31PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 0.6516 (0.7585)	Prec@(1,5) (73.5%, 98.2%)	
05/27 09:36:31PM searchStage_trainer.py:221 [INFO] Train: [  9/49] Final Prec@1 73.5400%
05/27 09:36:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 0.7624	Prec@(1,5) (72.8%, 98.1%)
05/27 09:36:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 0.7562	Prec@(1,5) (73.2%, 98.2%)
05/27 09:36:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 0.7613	Prec@(1,5) (72.9%, 98.2%)
05/27 09:36:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 0.7606	Prec@(1,5) (73.0%, 98.1%)
05/27 09:36:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 0.7720	Prec@(1,5) (72.8%, 98.1%)
05/27 09:36:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 0.7678	Prec@(1,5) (73.0%, 98.1%)
05/27 09:36:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 0.7692	Prec@(1,5) (73.0%, 98.0%)
05/27 09:36:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 0.7709	Prec@(1,5) (73.0%, 98.0%)
05/27 09:36:51PM searchStage_trainer.py:260 [INFO] Valid: [  9/49] Final Prec@1 72.9560%
05/27 09:36:51PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:36:51PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 72.9560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2525, 0.2712, 0.2522, 0.2241],
        [0.2534, 0.2705, 0.2531, 0.2230]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2562, 0.2668, 0.2524, 0.2247],
        [0.2559, 0.2625, 0.2546, 0.2270],
        [0.2624, 0.2801, 0.2380, 0.2195]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2585, 0.2622, 0.2576, 0.2217],
        [0.2604, 0.2684, 0.2366, 0.2345],
        [0.2607, 0.2727, 0.2389, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2666, 0.2430, 0.2345],
        [0.2567, 0.2661, 0.2435, 0.2337],
        [0.2587, 0.2681, 0.2447, 0.2284]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2686, 0.2407, 0.2354],
        [0.2565, 0.2696, 0.2423, 0.2316],
        [0.2587, 0.2675, 0.2437, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2625, 0.2415, 0.2351],
        [0.2577, 0.2627, 0.2458, 0.2338],
        [0.2581, 0.2647, 0.2378, 0.2395]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2518, 0.2507, 0.2487],
        [0.2502, 0.2530, 0.2515, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2526, 0.2515, 0.2471],
        [0.2509, 0.2523, 0.2504, 0.2464],
        [0.2517, 0.2539, 0.2485, 0.2460]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2515, 0.2517, 0.2472],
        [0.2490, 0.2532, 0.2494, 0.2484],
        [0.2507, 0.2535, 0.2496, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2517, 0.2482, 0.2497],
        [0.2500, 0.2522, 0.2493, 0.2485],
        [0.2507, 0.2515, 0.2498, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2532, 0.2489, 0.2469],
        [0.2503, 0.2521, 0.2507, 0.2469],
        [0.2506, 0.2514, 0.2464, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2524, 0.2485, 0.2492],
        [0.2503, 0.2522, 0.2492, 0.2482],
        [0.2502, 0.2519, 0.2495, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2490, 0.2526, 0.2508],
        [0.2471, 0.2521, 0.2523, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2516, 0.2526, 0.2493],
        [0.2471, 0.2526, 0.2533, 0.2469],
        [0.2467, 0.2500, 0.2515, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2499, 0.2516, 0.2503],
        [0.2442, 0.2512, 0.2547, 0.2500],
        [0.2479, 0.2507, 0.2522, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2488, 0.2495, 0.2530],
        [0.2474, 0.2500, 0.2546, 0.2480],
        [0.2474, 0.2517, 0.2526, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2500, 0.2626, 0.2411],
        [0.2468, 0.2475, 0.2477, 0.2579],
        [0.2461, 0.2456, 0.2589, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2467, 0.2497, 0.2589],
        [0.2498, 0.2506, 0.2491, 0.2505],
        [0.2424, 0.2534, 0.2620, 0.2423]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:37:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 0.6153 (0.7226)	Prec@(1,5) (73.6%, 98.4%)	
05/27 09:37:22PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 0.7444 (0.7160)	Prec@(1,5) (74.6%, 98.5%)	
05/27 09:37:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 0.6631 (0.7134)	Prec@(1,5) (74.9%, 98.3%)	
05/27 09:37:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 0.4975 (0.7163)	Prec@(1,5) (74.9%, 98.3%)	
05/27 09:38:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 0.4544 (0.7130)	Prec@(1,5) (75.0%, 98.4%)	
05/27 09:38:24PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 0.8234 (0.7070)	Prec@(1,5) (75.3%, 98.4%)	
05/27 09:38:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 0.5076 (0.7072)	Prec@(1,5) (75.6%, 98.4%)	
05/27 09:38:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 0.6504 (0.7057)	Prec@(1,5) (75.6%, 98.4%)	
05/27 09:38:52PM searchStage_trainer.py:221 [INFO] Train: [ 10/49] Final Prec@1 75.5680%
05/27 09:38:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 0.7625	Prec@(1,5) (73.2%, 98.1%)
05/27 09:38:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 0.7375	Prec@(1,5) (74.1%, 98.1%)
05/27 09:39:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 0.7271	Prec@(1,5) (74.5%, 98.3%)
05/27 09:39:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 0.7314	Prec@(1,5) (74.3%, 98.3%)
05/27 09:39:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 0.7299	Prec@(1,5) (74.3%, 98.3%)
05/27 09:39:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 0.7294	Prec@(1,5) (74.3%, 98.3%)
05/27 09:39:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 0.7269	Prec@(1,5) (74.5%, 98.3%)
05/27 09:39:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 0.7298	Prec@(1,5) (74.5%, 98.3%)
05/27 09:39:12PM searchStage_trainer.py:260 [INFO] Valid: [ 10/49] Final Prec@1 74.4720%
05/27 09:39:12PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:39:12PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 74.4720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2525, 0.2713, 0.2521, 0.2241],
        [0.2534, 0.2706, 0.2531, 0.2229]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2670, 0.2527, 0.2245],
        [0.2558, 0.2628, 0.2546, 0.2267],
        [0.2623, 0.2800, 0.2381, 0.2196]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2584, 0.2621, 0.2578, 0.2217],
        [0.2605, 0.2684, 0.2365, 0.2346],
        [0.2607, 0.2726, 0.2390, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2666, 0.2433, 0.2344],
        [0.2566, 0.2660, 0.2434, 0.2340],
        [0.2586, 0.2680, 0.2449, 0.2285]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2688, 0.2411, 0.2350],
        [0.2563, 0.2695, 0.2426, 0.2316],
        [0.2586, 0.2674, 0.2436, 0.2304]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2625, 0.2415, 0.2351],
        [0.2577, 0.2627, 0.2460, 0.2337],
        [0.2581, 0.2645, 0.2378, 0.2396]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2519, 0.2508, 0.2485],
        [0.2501, 0.2529, 0.2514, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2527, 0.2515, 0.2472],
        [0.2509, 0.2523, 0.2504, 0.2464],
        [0.2517, 0.2539, 0.2484, 0.2460]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2514, 0.2518, 0.2473],
        [0.2489, 0.2533, 0.2495, 0.2484],
        [0.2507, 0.2536, 0.2496, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2517, 0.2481, 0.2497],
        [0.2500, 0.2522, 0.2493, 0.2485],
        [0.2507, 0.2515, 0.2499, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2532, 0.2489, 0.2469],
        [0.2503, 0.2521, 0.2506, 0.2469],
        [0.2505, 0.2515, 0.2465, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2524, 0.2485, 0.2492],
        [0.2503, 0.2522, 0.2492, 0.2483],
        [0.2502, 0.2519, 0.2496, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2490, 0.2527, 0.2508],
        [0.2471, 0.2521, 0.2523, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2516, 0.2526, 0.2494],
        [0.2470, 0.2527, 0.2534, 0.2468],
        [0.2467, 0.2500, 0.2515, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2499, 0.2516, 0.2503],
        [0.2441, 0.2512, 0.2548, 0.2500],
        [0.2479, 0.2507, 0.2522, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2488, 0.2495, 0.2531],
        [0.2474, 0.2500, 0.2547, 0.2479],
        [0.2474, 0.2517, 0.2526, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2500, 0.2629, 0.2408],
        [0.2468, 0.2475, 0.2477, 0.2581],
        [0.2461, 0.2456, 0.2590, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2466, 0.2497, 0.2591],
        [0.2498, 0.2506, 0.2490, 0.2506],
        [0.2423, 0.2534, 0.2621, 0.2422]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:39:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 0.7551 (0.6458)	Prec@(1,5) (77.0%, 98.9%)	
05/27 09:39:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 0.6130 (0.6416)	Prec@(1,5) (77.5%, 98.8%)	
05/27 09:39:59PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 0.5623 (0.6499)	Prec@(1,5) (77.3%, 98.9%)	
05/27 09:40:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 0.4704 (0.6611)	Prec@(1,5) (76.7%, 98.7%)	
05/27 09:40:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 0.6678 (0.6628)	Prec@(1,5) (76.8%, 98.7%)	
05/27 09:40:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 0.6154 (0.6631)	Prec@(1,5) (76.8%, 98.7%)	
05/27 09:41:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 0.6984 (0.6625)	Prec@(1,5) (76.9%, 98.7%)	
05/27 09:41:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 0.7200 (0.6635)	Prec@(1,5) (76.9%, 98.6%)	
05/27 09:41:13PM searchStage_trainer.py:221 [INFO] Train: [ 11/49] Final Prec@1 76.8440%
05/27 09:41:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 0.8235	Prec@(1,5) (71.2%, 97.4%)
05/27 09:41:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 0.8176	Prec@(1,5) (72.0%, 97.4%)
05/27 09:41:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 0.8194	Prec@(1,5) (71.9%, 97.5%)
05/27 09:41:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 0.8174	Prec@(1,5) (71.9%, 97.6%)
05/27 09:41:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 0.8194	Prec@(1,5) (71.8%, 97.6%)
05/27 09:41:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 0.8215	Prec@(1,5) (71.7%, 97.6%)
05/27 09:41:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 0.8228	Prec@(1,5) (71.7%, 97.6%)
05/27 09:41:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 0.8248	Prec@(1,5) (71.7%, 97.6%)
05/27 09:41:33PM searchStage_trainer.py:260 [INFO] Valid: [ 11/49] Final Prec@1 71.7200%
05/27 09:41:33PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:41:33PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 74.4720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2524, 0.2713, 0.2522, 0.2242],
        [0.2534, 0.2707, 0.2531, 0.2228]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2561, 0.2669, 0.2523, 0.2247],
        [0.2559, 0.2628, 0.2546, 0.2268],
        [0.2623, 0.2799, 0.2383, 0.2196]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2622, 0.2580, 0.2211],
        [0.2605, 0.2683, 0.2365, 0.2348],
        [0.2606, 0.2725, 0.2390, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2666, 0.2432, 0.2346],
        [0.2566, 0.2659, 0.2434, 0.2341],
        [0.2586, 0.2679, 0.2449, 0.2286]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2687, 0.2409, 0.2352],
        [0.2563, 0.2694, 0.2428, 0.2315],
        [0.2586, 0.2673, 0.2436, 0.2306]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2624, 0.2412, 0.2354],
        [0.2576, 0.2626, 0.2461, 0.2336],
        [0.2580, 0.2644, 0.2379, 0.2397]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2519, 0.2508, 0.2484],
        [0.2501, 0.2528, 0.2514, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2527, 0.2515, 0.2472],
        [0.2510, 0.2522, 0.2503, 0.2465],
        [0.2517, 0.2539, 0.2485, 0.2460]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2515, 0.2518, 0.2472],
        [0.2489, 0.2533, 0.2494, 0.2485],
        [0.2507, 0.2535, 0.2496, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2517, 0.2481, 0.2497],
        [0.2500, 0.2522, 0.2493, 0.2485],
        [0.2507, 0.2515, 0.2499, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2532, 0.2488, 0.2469],
        [0.2503, 0.2521, 0.2506, 0.2469],
        [0.2505, 0.2514, 0.2465, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2524, 0.2485, 0.2493],
        [0.2503, 0.2522, 0.2492, 0.2483],
        [0.2502, 0.2519, 0.2496, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2490, 0.2527, 0.2509],
        [0.2471, 0.2522, 0.2524, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2516, 0.2526, 0.2494],
        [0.2470, 0.2528, 0.2535, 0.2467],
        [0.2467, 0.2500, 0.2515, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2499, 0.2517, 0.2503],
        [0.2440, 0.2512, 0.2549, 0.2500],
        [0.2479, 0.2507, 0.2522, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2488, 0.2494, 0.2532],
        [0.2474, 0.2500, 0.2548, 0.2479],
        [0.2474, 0.2518, 0.2526, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2500, 0.2632, 0.2406],
        [0.2467, 0.2474, 0.2476, 0.2583],
        [0.2461, 0.2455, 0.2591, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2465, 0.2497, 0.2593],
        [0.2498, 0.2506, 0.2490, 0.2506],
        [0.2422, 0.2535, 0.2622, 0.2421]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:41:49PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 0.9410 (0.6147)	Prec@(1,5) (78.2%, 98.9%)	
05/27 09:42:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 0.7195 (0.6194)	Prec@(1,5) (78.6%, 98.8%)	
05/27 09:42:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 0.7557 (0.6227)	Prec@(1,5) (78.2%, 98.8%)	
05/27 09:42:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 0.8256 (0.6279)	Prec@(1,5) (78.0%, 98.8%)	
05/27 09:42:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 0.5579 (0.6244)	Prec@(1,5) (78.0%, 98.8%)	
05/27 09:43:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 0.6390 (0.6276)	Prec@(1,5) (77.9%, 98.9%)	
05/27 09:43:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 0.7310 (0.6272)	Prec@(1,5) (78.0%, 98.8%)	
05/27 09:43:34PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 0.6451 (0.6278)	Prec@(1,5) (78.0%, 98.8%)	
05/27 09:43:34PM searchStage_trainer.py:221 [INFO] Train: [ 12/49] Final Prec@1 77.9600%
05/27 09:43:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 0.7083	Prec@(1,5) (75.8%, 98.2%)
05/27 09:43:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 0.7203	Prec@(1,5) (75.7%, 98.0%)
05/27 09:43:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 0.7281	Prec@(1,5) (75.1%, 98.0%)
05/27 09:43:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 0.7185	Prec@(1,5) (75.5%, 98.1%)
05/27 09:43:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 0.7202	Prec@(1,5) (75.5%, 98.1%)
05/27 09:43:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 0.7204	Prec@(1,5) (75.6%, 98.1%)
05/27 09:43:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 0.7233	Prec@(1,5) (75.5%, 98.0%)
05/27 09:43:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 0.7172	Prec@(1,5) (75.6%, 98.1%)
05/27 09:43:54PM searchStage_trainer.py:260 [INFO] Valid: [ 12/49] Final Prec@1 75.5960%
05/27 09:43:54PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:43:54PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 75.5960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2523, 0.2711, 0.2522, 0.2244],
        [0.2533, 0.2708, 0.2532, 0.2227]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2559, 0.2667, 0.2521, 0.2252],
        [0.2559, 0.2628, 0.2544, 0.2269],
        [0.2622, 0.2798, 0.2385, 0.2195]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2587, 0.2624, 0.2581, 0.2208],
        [0.2605, 0.2681, 0.2365, 0.2349],
        [0.2605, 0.2724, 0.2391, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2663, 0.2429, 0.2350],
        [0.2566, 0.2658, 0.2435, 0.2341],
        [0.2585, 0.2678, 0.2451, 0.2286]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2687, 0.2406, 0.2355],
        [0.2563, 0.2693, 0.2427, 0.2317],
        [0.2586, 0.2671, 0.2438, 0.2306]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2625, 0.2413, 0.2354],
        [0.2575, 0.2625, 0.2462, 0.2338],
        [0.2580, 0.2642, 0.2379, 0.2399]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2519, 0.2507, 0.2484],
        [0.2501, 0.2528, 0.2514, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2526, 0.2514, 0.2474],
        [0.2510, 0.2523, 0.2504, 0.2463],
        [0.2517, 0.2539, 0.2485, 0.2460]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2515, 0.2519, 0.2470],
        [0.2488, 0.2533, 0.2494, 0.2485],
        [0.2507, 0.2535, 0.2497, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2517, 0.2482, 0.2497],
        [0.2500, 0.2522, 0.2493, 0.2485],
        [0.2507, 0.2514, 0.2499, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2532, 0.2487, 0.2470],
        [0.2504, 0.2521, 0.2507, 0.2468],
        [0.2505, 0.2514, 0.2464, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2524, 0.2484, 0.2494],
        [0.2503, 0.2522, 0.2492, 0.2483],
        [0.2502, 0.2519, 0.2496, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2490, 0.2527, 0.2509],
        [0.2471, 0.2522, 0.2524, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2516, 0.2526, 0.2494],
        [0.2469, 0.2529, 0.2536, 0.2466],
        [0.2467, 0.2500, 0.2515, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2499, 0.2517, 0.2503],
        [0.2439, 0.2512, 0.2549, 0.2499],
        [0.2479, 0.2507, 0.2523, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2487, 0.2494, 0.2533],
        [0.2474, 0.2500, 0.2548, 0.2479],
        [0.2474, 0.2518, 0.2526, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2500, 0.2635, 0.2405],
        [0.2467, 0.2474, 0.2475, 0.2584],
        [0.2461, 0.2455, 0.2592, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2444, 0.2464, 0.2497, 0.2595],
        [0.2499, 0.2506, 0.2490, 0.2506],
        [0.2421, 0.2535, 0.2623, 0.2420]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:44:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 0.5109 (0.5762)	Prec@(1,5) (80.5%, 98.7%)	
05/27 09:44:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 0.5834 (0.5743)	Prec@(1,5) (80.3%, 98.8%)	
05/27 09:44:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 0.7496 (0.5968)	Prec@(1,5) (79.4%, 98.8%)	
05/27 09:44:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 0.8153 (0.6000)	Prec@(1,5) (79.2%, 98.8%)	
05/27 09:45:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 0.7376 (0.5963)	Prec@(1,5) (79.2%, 98.8%)	
05/27 09:45:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 0.5870 (0.6000)	Prec@(1,5) (79.1%, 98.8%)	
05/27 09:45:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 0.5279 (0.6044)	Prec@(1,5) (79.0%, 98.8%)	
05/27 09:45:54PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 0.7216 (0.6029)	Prec@(1,5) (79.0%, 98.8%)	
05/27 09:45:55PM searchStage_trainer.py:221 [INFO] Train: [ 13/49] Final Prec@1 79.0280%
05/27 09:45:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 0.6826	Prec@(1,5) (76.4%, 98.6%)
05/27 09:46:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 0.6672	Prec@(1,5) (77.2%, 98.7%)
05/27 09:46:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 0.6709	Prec@(1,5) (77.1%, 98.5%)
05/27 09:46:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 0.6692	Prec@(1,5) (77.2%, 98.5%)
05/27 09:46:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 0.6708	Prec@(1,5) (76.9%, 98.6%)
05/27 09:46:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 0.6788	Prec@(1,5) (76.6%, 98.5%)
05/27 09:46:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 0.6804	Prec@(1,5) (76.5%, 98.5%)
05/27 09:46:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 0.6837	Prec@(1,5) (76.5%, 98.5%)
05/27 09:46:14PM searchStage_trainer.py:260 [INFO] Valid: [ 13/49] Final Prec@1 76.4680%
05/27 09:46:14PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:46:15PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 76.4680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2523, 0.2710, 0.2522, 0.2246],
        [0.2532, 0.2707, 0.2534, 0.2227]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2559, 0.2665, 0.2521, 0.2256],
        [0.2560, 0.2627, 0.2544, 0.2269],
        [0.2622, 0.2797, 0.2386, 0.2195]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2587, 0.2627, 0.2580, 0.2206],
        [0.2604, 0.2680, 0.2363, 0.2353],
        [0.2605, 0.2722, 0.2392, 0.2281]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2663, 0.2430, 0.2351],
        [0.2564, 0.2657, 0.2435, 0.2343],
        [0.2584, 0.2677, 0.2451, 0.2288]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2687, 0.2407, 0.2353],
        [0.2562, 0.2693, 0.2428, 0.2317],
        [0.2585, 0.2670, 0.2437, 0.2308]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2625, 0.2413, 0.2354],
        [0.2574, 0.2624, 0.2463, 0.2340],
        [0.2580, 0.2641, 0.2379, 0.2400]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2519, 0.2508, 0.2484],
        [0.2501, 0.2528, 0.2514, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2527, 0.2514, 0.2473],
        [0.2510, 0.2523, 0.2504, 0.2463],
        [0.2517, 0.2539, 0.2484, 0.2460]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2516, 0.2520, 0.2470],
        [0.2488, 0.2532, 0.2494, 0.2486],
        [0.2507, 0.2535, 0.2497, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2517, 0.2482, 0.2497],
        [0.2500, 0.2522, 0.2493, 0.2486],
        [0.2507, 0.2514, 0.2499, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2521, 0.2507, 0.2469],
        [0.2505, 0.2514, 0.2464, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2524, 0.2484, 0.2494],
        [0.2503, 0.2522, 0.2492, 0.2483],
        [0.2502, 0.2519, 0.2496, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2490, 0.2527, 0.2509],
        [0.2471, 0.2522, 0.2524, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2516, 0.2526, 0.2494],
        [0.2469, 0.2529, 0.2536, 0.2466],
        [0.2467, 0.2499, 0.2515, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2499, 0.2518, 0.2503],
        [0.2439, 0.2512, 0.2550, 0.2499],
        [0.2479, 0.2507, 0.2523, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2487, 0.2494, 0.2533],
        [0.2473, 0.2500, 0.2548, 0.2478],
        [0.2474, 0.2518, 0.2526, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2499, 0.2637, 0.2403],
        [0.2467, 0.2473, 0.2474, 0.2585],
        [0.2461, 0.2454, 0.2592, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2463, 0.2496, 0.2597],
        [0.2499, 0.2506, 0.2489, 0.2506],
        [0.2421, 0.2535, 0.2624, 0.2420]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:46:31PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 0.5441 (0.5314)	Prec@(1,5) (81.8%, 99.2%)	
05/27 09:46:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 0.5118 (0.5395)	Prec@(1,5) (81.4%, 99.2%)	
05/27 09:47:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 0.7150 (0.5487)	Prec@(1,5) (80.8%, 99.1%)	
05/27 09:47:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 0.4814 (0.5601)	Prec@(1,5) (80.6%, 99.1%)	
05/27 09:47:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 0.8119 (0.5588)	Prec@(1,5) (80.6%, 99.0%)	
05/27 09:47:48PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 0.6555 (0.5649)	Prec@(1,5) (80.4%, 99.0%)	
05/27 09:48:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 0.5900 (0.5688)	Prec@(1,5) (80.2%, 99.0%)	
05/27 09:48:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 0.5879 (0.5721)	Prec@(1,5) (80.1%, 99.0%)	
05/27 09:48:16PM searchStage_trainer.py:221 [INFO] Train: [ 14/49] Final Prec@1 80.0720%
05/27 09:48:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 0.6669	Prec@(1,5) (77.0%, 98.4%)
05/27 09:48:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 0.6452	Prec@(1,5) (78.0%, 98.5%)
05/27 09:48:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 0.6498	Prec@(1,5) (77.9%, 98.5%)
05/27 09:48:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 0.6409	Prec@(1,5) (78.1%, 98.6%)
05/27 09:48:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 0.6414	Prec@(1,5) (78.1%, 98.6%)
05/27 09:48:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 0.6425	Prec@(1,5) (78.0%, 98.6%)
05/27 09:48:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 0.6432	Prec@(1,5) (78.0%, 98.6%)
05/27 09:48:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 0.6434	Prec@(1,5) (78.0%, 98.5%)
05/27 09:48:36PM searchStage_trainer.py:260 [INFO] Valid: [ 14/49] Final Prec@1 77.9480%
05/27 09:48:36PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:48:36PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 77.9480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2523, 0.2710, 0.2523, 0.2245],
        [0.2532, 0.2705, 0.2534, 0.2229]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2664, 0.2522, 0.2257],
        [0.2560, 0.2627, 0.2545, 0.2268],
        [0.2622, 0.2796, 0.2386, 0.2197]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2589, 0.2627, 0.2581, 0.2204],
        [0.2604, 0.2679, 0.2365, 0.2352],
        [0.2605, 0.2721, 0.2391, 0.2284]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2662, 0.2429, 0.2352],
        [0.2564, 0.2656, 0.2435, 0.2345],
        [0.2584, 0.2676, 0.2451, 0.2289]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2687, 0.2407, 0.2355],
        [0.2562, 0.2692, 0.2428, 0.2319],
        [0.2585, 0.2668, 0.2438, 0.2309]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2624, 0.2412, 0.2355],
        [0.2575, 0.2623, 0.2463, 0.2340],
        [0.2579, 0.2639, 0.2380, 0.2402]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2519, 0.2507, 0.2485],
        [0.2501, 0.2528, 0.2514, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2515, 0.2473],
        [0.2510, 0.2524, 0.2504, 0.2463],
        [0.2517, 0.2539, 0.2484, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2516, 0.2520, 0.2469],
        [0.2488, 0.2532, 0.2493, 0.2486],
        [0.2507, 0.2535, 0.2497, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2518, 0.2482, 0.2497],
        [0.2499, 0.2522, 0.2493, 0.2486],
        [0.2507, 0.2514, 0.2499, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2521, 0.2507, 0.2468],
        [0.2505, 0.2514, 0.2464, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2524, 0.2484, 0.2494],
        [0.2503, 0.2522, 0.2492, 0.2483],
        [0.2502, 0.2519, 0.2495, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2490, 0.2527, 0.2509],
        [0.2470, 0.2522, 0.2524, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2517, 0.2526, 0.2494],
        [0.2469, 0.2529, 0.2536, 0.2466],
        [0.2467, 0.2499, 0.2515, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2499, 0.2518, 0.2504],
        [0.2438, 0.2512, 0.2550, 0.2499],
        [0.2479, 0.2508, 0.2523, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2487, 0.2493, 0.2534],
        [0.2473, 0.2500, 0.2549, 0.2478],
        [0.2474, 0.2518, 0.2526, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2499, 0.2639, 0.2402],
        [0.2467, 0.2473, 0.2474, 0.2586],
        [0.2461, 0.2454, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2462, 0.2496, 0.2599],
        [0.2499, 0.2506, 0.2489, 0.2506],
        [0.2421, 0.2536, 0.2624, 0.2419]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:48:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 0.4700 (0.5145)	Prec@(1,5) (82.4%, 99.3%)	
05/27 09:49:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 0.7881 (0.5214)	Prec@(1,5) (81.8%, 99.3%)	
05/27 09:49:22PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 0.4623 (0.5362)	Prec@(1,5) (81.2%, 99.2%)	
05/27 09:49:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 0.6678 (0.5343)	Prec@(1,5) (81.3%, 99.2%)	
05/27 09:49:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 0.7502 (0.5372)	Prec@(1,5) (81.3%, 99.2%)	
05/27 09:50:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 0.5561 (0.5425)	Prec@(1,5) (81.2%, 99.2%)	
05/27 09:50:24PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 0.5112 (0.5410)	Prec@(1,5) (81.3%, 99.2%)	
05/27 09:50:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 0.4438 (0.5442)	Prec@(1,5) (81.2%, 99.2%)	
05/27 09:50:37PM searchStage_trainer.py:221 [INFO] Train: [ 15/49] Final Prec@1 81.1920%
05/27 09:50:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 0.6475	Prec@(1,5) (76.8%, 98.8%)
05/27 09:50:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 0.6672	Prec@(1,5) (77.1%, 98.6%)
05/27 09:50:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 0.6670	Prec@(1,5) (77.1%, 98.7%)
05/27 09:50:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 0.6706	Prec@(1,5) (76.9%, 98.6%)
05/27 09:50:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 0.6713	Prec@(1,5) (76.9%, 98.7%)
05/27 09:50:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 0.6717	Prec@(1,5) (76.9%, 98.7%)
05/27 09:50:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 0.6727	Prec@(1,5) (76.9%, 98.6%)
05/27 09:50:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 0.6746	Prec@(1,5) (76.9%, 98.6%)
05/27 09:50:56PM searchStage_trainer.py:260 [INFO] Valid: [ 15/49] Final Prec@1 76.8760%
05/27 09:50:56PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:50:57PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 77.9480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2522, 0.2709, 0.2523, 0.2246],
        [0.2530, 0.2704, 0.2536, 0.2230]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2664, 0.2522, 0.2256],
        [0.2559, 0.2627, 0.2545, 0.2270],
        [0.2621, 0.2794, 0.2387, 0.2198]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2589, 0.2626, 0.2580, 0.2205],
        [0.2604, 0.2677, 0.2365, 0.2354],
        [0.2605, 0.2720, 0.2391, 0.2285]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2662, 0.2429, 0.2353],
        [0.2564, 0.2655, 0.2436, 0.2345],
        [0.2583, 0.2675, 0.2452, 0.2290]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2687, 0.2406, 0.2356],
        [0.2562, 0.2691, 0.2428, 0.2320],
        [0.2584, 0.2667, 0.2438, 0.2310]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2623, 0.2413, 0.2356],
        [0.2574, 0.2622, 0.2462, 0.2342],
        [0.2578, 0.2638, 0.2381, 0.2404]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2519, 0.2508, 0.2484],
        [0.2501, 0.2528, 0.2514, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2527, 0.2514, 0.2473],
        [0.2510, 0.2524, 0.2504, 0.2463],
        [0.2516, 0.2538, 0.2484, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2516, 0.2520, 0.2469],
        [0.2488, 0.2532, 0.2493, 0.2487],
        [0.2507, 0.2535, 0.2497, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2518, 0.2483, 0.2496],
        [0.2499, 0.2522, 0.2493, 0.2486],
        [0.2507, 0.2514, 0.2499, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2521, 0.2507, 0.2468],
        [0.2505, 0.2514, 0.2464, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2522, 0.2492, 0.2483],
        [0.2502, 0.2519, 0.2495, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2490, 0.2527, 0.2509],
        [0.2470, 0.2523, 0.2524, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2517, 0.2526, 0.2494],
        [0.2469, 0.2529, 0.2536, 0.2465],
        [0.2467, 0.2499, 0.2514, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2499, 0.2519, 0.2504],
        [0.2438, 0.2513, 0.2551, 0.2499],
        [0.2479, 0.2508, 0.2523, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2487, 0.2493, 0.2534],
        [0.2473, 0.2500, 0.2549, 0.2478],
        [0.2474, 0.2518, 0.2526, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2499, 0.2640, 0.2401],
        [0.2467, 0.2473, 0.2474, 0.2587],
        [0.2461, 0.2454, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2462, 0.2496, 0.2600],
        [0.2499, 0.2506, 0.2489, 0.2506],
        [0.2421, 0.2536, 0.2625, 0.2419]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:51:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 0.3614 (0.5180)	Prec@(1,5) (81.6%, 99.2%)	
05/27 09:51:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 0.4662 (0.5113)	Prec@(1,5) (82.0%, 99.1%)	
05/27 09:51:43PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 0.4617 (0.5198)	Prec@(1,5) (81.9%, 99.1%)	
05/27 09:51:59PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 0.5924 (0.5216)	Prec@(1,5) (81.8%, 99.1%)	
05/27 09:52:14PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 0.5778 (0.5171)	Prec@(1,5) (82.0%, 99.2%)	
05/27 09:52:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 0.7457 (0.5198)	Prec@(1,5) (81.9%, 99.2%)	
05/27 09:52:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 0.5195 (0.5240)	Prec@(1,5) (81.7%, 99.1%)	
05/27 09:52:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 0.5325 (0.5228)	Prec@(1,5) (81.8%, 99.2%)	
05/27 09:52:58PM searchStage_trainer.py:221 [INFO] Train: [ 16/49] Final Prec@1 81.7760%
05/27 09:53:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 0.6089	Prec@(1,5) (79.0%, 98.8%)
05/27 09:53:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 0.6049	Prec@(1,5) (79.1%, 98.7%)
05/27 09:53:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 0.6053	Prec@(1,5) (79.0%, 98.7%)
05/27 09:53:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 0.5969	Prec@(1,5) (79.3%, 98.7%)
05/27 09:53:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 0.6074	Prec@(1,5) (79.1%, 98.7%)
05/27 09:53:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 0.6120	Prec@(1,5) (78.9%, 98.6%)
05/27 09:53:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 0.6129	Prec@(1,5) (79.0%, 98.6%)
05/27 09:53:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 0.6150	Prec@(1,5) (78.9%, 98.6%)
05/27 09:53:18PM searchStage_trainer.py:260 [INFO] Valid: [ 16/49] Final Prec@1 78.9000%
05/27 09:53:18PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:53:18PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 78.9000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2522, 0.2708, 0.2523, 0.2247],
        [0.2530, 0.2702, 0.2536, 0.2232]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2559, 0.2664, 0.2521, 0.2256],
        [0.2560, 0.2625, 0.2543, 0.2272],
        [0.2621, 0.2793, 0.2387, 0.2199]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2591, 0.2626, 0.2581, 0.2202],
        [0.2604, 0.2676, 0.2365, 0.2355],
        [0.2604, 0.2718, 0.2391, 0.2286]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2660, 0.2429, 0.2355],
        [0.2563, 0.2655, 0.2436, 0.2345],
        [0.2583, 0.2673, 0.2452, 0.2292]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2686, 0.2405, 0.2359],
        [0.2561, 0.2690, 0.2429, 0.2320],
        [0.2584, 0.2666, 0.2438, 0.2312]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2622, 0.2412, 0.2358],
        [0.2574, 0.2621, 0.2462, 0.2342],
        [0.2577, 0.2636, 0.2381, 0.2405]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2518, 0.2507, 0.2485],
        [0.2501, 0.2528, 0.2514, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2473],
        [0.2510, 0.2524, 0.2504, 0.2463],
        [0.2516, 0.2538, 0.2484, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2516, 0.2521, 0.2469],
        [0.2488, 0.2532, 0.2493, 0.2487],
        [0.2507, 0.2535, 0.2497, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2518, 0.2484, 0.2495],
        [0.2499, 0.2521, 0.2493, 0.2486],
        [0.2507, 0.2514, 0.2499, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2521, 0.2508, 0.2468],
        [0.2505, 0.2514, 0.2464, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2522, 0.2492, 0.2483],
        [0.2502, 0.2519, 0.2495, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2490, 0.2527, 0.2509],
        [0.2470, 0.2523, 0.2524, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2517, 0.2526, 0.2494],
        [0.2469, 0.2530, 0.2537, 0.2465],
        [0.2467, 0.2499, 0.2514, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2499, 0.2519, 0.2503],
        [0.2438, 0.2513, 0.2551, 0.2499],
        [0.2479, 0.2508, 0.2523, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2487, 0.2493, 0.2535],
        [0.2473, 0.2500, 0.2549, 0.2478],
        [0.2474, 0.2518, 0.2526, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2499, 0.2642, 0.2400],
        [0.2467, 0.2472, 0.2473, 0.2588],
        [0.2461, 0.2454, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2461, 0.2496, 0.2601],
        [0.2500, 0.2506, 0.2489, 0.2506],
        [0.2421, 0.2536, 0.2625, 0.2418]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:53:34PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 0.4813 (0.4676)	Prec@(1,5) (83.7%, 99.4%)	
05/27 09:53:49PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 0.3973 (0.4970)	Prec@(1,5) (82.8%, 99.2%)	
05/27 09:54:05PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 0.3415 (0.4951)	Prec@(1,5) (83.1%, 99.3%)	
05/27 09:54:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 0.3269 (0.4894)	Prec@(1,5) (83.2%, 99.3%)	
05/27 09:54:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 0.4941 (0.4941)	Prec@(1,5) (83.1%, 99.3%)	
05/27 09:54:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 0.3081 (0.4977)	Prec@(1,5) (82.9%, 99.2%)	
05/27 09:55:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 0.4648 (0.4973)	Prec@(1,5) (83.0%, 99.2%)	
05/27 09:55:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 0.5835 (0.5011)	Prec@(1,5) (82.8%, 99.2%)	
05/27 09:55:18PM searchStage_trainer.py:221 [INFO] Train: [ 17/49] Final Prec@1 82.8160%
05/27 09:55:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 0.6119	Prec@(1,5) (78.2%, 98.7%)
05/27 09:55:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 0.6209	Prec@(1,5) (78.0%, 98.7%)
05/27 09:55:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 0.6268	Prec@(1,5) (78.2%, 98.8%)
05/27 09:55:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 0.6330	Prec@(1,5) (78.2%, 98.7%)
05/27 09:55:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 0.6355	Prec@(1,5) (78.1%, 98.7%)
05/27 09:55:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 0.6327	Prec@(1,5) (78.3%, 98.7%)
05/27 09:55:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 0.6293	Prec@(1,5) (78.5%, 98.7%)
05/27 09:55:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 0.6288	Prec@(1,5) (78.5%, 98.7%)
05/27 09:55:38PM searchStage_trainer.py:260 [INFO] Valid: [ 17/49] Final Prec@1 78.4960%
05/27 09:55:38PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:55:38PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 78.9000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2520, 0.2707, 0.2524, 0.2249],
        [0.2530, 0.2702, 0.2537, 0.2231]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2559, 0.2663, 0.2523, 0.2254],
        [0.2559, 0.2625, 0.2544, 0.2272],
        [0.2620, 0.2792, 0.2387, 0.2201]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2592, 0.2627, 0.2582, 0.2199],
        [0.2604, 0.2675, 0.2364, 0.2357],
        [0.2604, 0.2717, 0.2391, 0.2288]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2659, 0.2427, 0.2358],
        [0.2563, 0.2654, 0.2437, 0.2346],
        [0.2583, 0.2672, 0.2452, 0.2293]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2685, 0.2404, 0.2360],
        [0.2560, 0.2689, 0.2430, 0.2320],
        [0.2584, 0.2665, 0.2438, 0.2313]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2621, 0.2411, 0.2360],
        [0.2574, 0.2620, 0.2462, 0.2344],
        [0.2577, 0.2635, 0.2382, 0.2406]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2518, 0.2507, 0.2485],
        [0.2501, 0.2527, 0.2514, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2527, 0.2514, 0.2473],
        [0.2510, 0.2523, 0.2504, 0.2463],
        [0.2517, 0.2538, 0.2484, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2516, 0.2521, 0.2469],
        [0.2488, 0.2532, 0.2493, 0.2487],
        [0.2506, 0.2535, 0.2497, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2518, 0.2484, 0.2495],
        [0.2499, 0.2521, 0.2493, 0.2486],
        [0.2507, 0.2514, 0.2499, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2521, 0.2508, 0.2469],
        [0.2505, 0.2513, 0.2464, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2522, 0.2492, 0.2483],
        [0.2502, 0.2519, 0.2495, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2490, 0.2527, 0.2509],
        [0.2470, 0.2523, 0.2524, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2517, 0.2526, 0.2495],
        [0.2469, 0.2530, 0.2537, 0.2464],
        [0.2467, 0.2499, 0.2514, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2499, 0.2519, 0.2503],
        [0.2437, 0.2513, 0.2551, 0.2499],
        [0.2479, 0.2508, 0.2523, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2487, 0.2493, 0.2535],
        [0.2473, 0.2500, 0.2549, 0.2478],
        [0.2474, 0.2518, 0.2526, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2499, 0.2643, 0.2399],
        [0.2467, 0.2472, 0.2473, 0.2589],
        [0.2461, 0.2453, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2461, 0.2495, 0.2603],
        [0.2500, 0.2506, 0.2488, 0.2506],
        [0.2421, 0.2536, 0.2625, 0.2418]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:55:54PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 0.5958 (0.4810)	Prec@(1,5) (82.7%, 99.4%)	
05/27 09:56:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 0.4595 (0.4647)	Prec@(1,5) (83.6%, 99.4%)	
05/27 09:56:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 0.3310 (0.4580)	Prec@(1,5) (83.8%, 99.5%)	
05/27 09:56:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 0.5316 (0.4579)	Prec@(1,5) (83.9%, 99.4%)	
05/27 09:56:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 0.4974 (0.4564)	Prec@(1,5) (84.1%, 99.4%)	
05/27 09:57:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 0.5340 (0.4606)	Prec@(1,5) (83.9%, 99.4%)	
05/27 09:57:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 0.5566 (0.4680)	Prec@(1,5) (83.8%, 99.4%)	
05/27 09:57:39PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 0.4819 (0.4706)	Prec@(1,5) (83.7%, 99.4%)	
05/27 09:57:39PM searchStage_trainer.py:221 [INFO] Train: [ 18/49] Final Prec@1 83.7400%
05/27 09:57:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 0.5589	Prec@(1,5) (81.2%, 99.0%)
05/27 09:57:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 0.5653	Prec@(1,5) (80.8%, 98.9%)
05/27 09:57:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 0.5784	Prec@(1,5) (80.3%, 98.9%)
05/27 09:57:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 0.5808	Prec@(1,5) (80.1%, 98.8%)
05/27 09:57:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 0.5776	Prec@(1,5) (80.2%, 98.8%)
05/27 09:57:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 0.5773	Prec@(1,5) (80.1%, 98.8%)
05/27 09:57:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 0.5712	Prec@(1,5) (80.3%, 98.8%)
05/27 09:57:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 0.5699	Prec@(1,5) (80.3%, 98.9%)
05/27 09:57:59PM searchStage_trainer.py:260 [INFO] Valid: [ 18/49] Final Prec@1 80.3160%
05/27 09:57:59PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 09:57:59PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.3160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2520, 0.2706, 0.2524, 0.2250],
        [0.2530, 0.2701, 0.2537, 0.2232]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2663, 0.2524, 0.2255],
        [0.2560, 0.2624, 0.2545, 0.2272],
        [0.2620, 0.2791, 0.2388, 0.2202]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2593, 0.2627, 0.2581, 0.2198],
        [0.2603, 0.2674, 0.2364, 0.2359],
        [0.2604, 0.2716, 0.2392, 0.2289]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2658, 0.2427, 0.2358],
        [0.2563, 0.2653, 0.2437, 0.2347],
        [0.2582, 0.2671, 0.2452, 0.2294]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2685, 0.2404, 0.2359],
        [0.2560, 0.2688, 0.2431, 0.2321],
        [0.2583, 0.2664, 0.2438, 0.2315]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2621, 0.2410, 0.2361],
        [0.2573, 0.2619, 0.2463, 0.2344],
        [0.2576, 0.2634, 0.2382, 0.2407]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2518, 0.2507, 0.2485],
        [0.2501, 0.2527, 0.2514, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2473],
        [0.2510, 0.2524, 0.2504, 0.2463],
        [0.2516, 0.2538, 0.2484, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2515, 0.2520, 0.2471],
        [0.2488, 0.2532, 0.2494, 0.2487],
        [0.2506, 0.2535, 0.2497, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2518, 0.2484, 0.2495],
        [0.2499, 0.2521, 0.2493, 0.2486],
        [0.2507, 0.2513, 0.2499, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2521, 0.2508, 0.2469],
        [0.2505, 0.2513, 0.2464, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2524, 0.2485, 0.2495],
        [0.2503, 0.2522, 0.2492, 0.2483],
        [0.2502, 0.2518, 0.2495, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2489, 0.2527, 0.2509],
        [0.2471, 0.2523, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2517, 0.2526, 0.2495],
        [0.2468, 0.2530, 0.2537, 0.2464],
        [0.2467, 0.2499, 0.2514, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2498, 0.2519, 0.2504],
        [0.2437, 0.2513, 0.2551, 0.2499],
        [0.2479, 0.2508, 0.2523, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2487, 0.2492, 0.2535],
        [0.2473, 0.2500, 0.2549, 0.2478],
        [0.2474, 0.2518, 0.2526, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2499, 0.2645, 0.2398],
        [0.2467, 0.2472, 0.2472, 0.2589],
        [0.2461, 0.2453, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2461, 0.2495, 0.2604],
        [0.2500, 0.2506, 0.2488, 0.2506],
        [0.2421, 0.2536, 0.2625, 0.2418]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 09:58:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 0.5120 (0.4521)	Prec@(1,5) (84.3%, 99.2%)	
05/27 09:58:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 0.5209 (0.4573)	Prec@(1,5) (83.9%, 99.4%)	
05/27 09:58:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 0.3191 (0.4552)	Prec@(1,5) (84.1%, 99.4%)	
05/27 09:59:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 0.4060 (0.4515)	Prec@(1,5) (84.3%, 99.3%)	
05/27 09:59:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 0.4463 (0.4522)	Prec@(1,5) (84.2%, 99.4%)	
05/27 09:59:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 0.2128 (0.4528)	Prec@(1,5) (84.1%, 99.4%)	
05/27 09:59:47PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 0.4024 (0.4549)	Prec@(1,5) (84.0%, 99.4%)	
05/27 10:00:00PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 0.4258 (0.4571)	Prec@(1,5) (84.0%, 99.4%)	
05/27 10:00:00PM searchStage_trainer.py:221 [INFO] Train: [ 19/49] Final Prec@1 84.0200%
05/27 10:00:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 0.6159	Prec@(1,5) (79.8%, 98.7%)
05/27 10:00:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 0.6414	Prec@(1,5) (78.6%, 98.6%)
05/27 10:00:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 0.6427	Prec@(1,5) (78.6%, 98.6%)
05/27 10:00:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 0.6354	Prec@(1,5) (78.7%, 98.6%)
05/27 10:00:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 0.6237	Prec@(1,5) (79.1%, 98.7%)
05/27 10:00:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 0.6305	Prec@(1,5) (78.8%, 98.7%)
05/27 10:00:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 0.6286	Prec@(1,5) (78.8%, 98.7%)
05/27 10:00:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 0.6242	Prec@(1,5) (79.0%, 98.7%)
05/27 10:00:20PM searchStage_trainer.py:260 [INFO] Valid: [ 19/49] Final Prec@1 78.9360%
05/27 10:00:20PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:00:20PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.3160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2519, 0.2705, 0.2525, 0.2251],
        [0.2529, 0.2700, 0.2538, 0.2233]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2662, 0.2525, 0.2255],
        [0.2559, 0.2623, 0.2545, 0.2273],
        [0.2619, 0.2790, 0.2388, 0.2203]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2593, 0.2628, 0.2583, 0.2197],
        [0.2602, 0.2674, 0.2364, 0.2360],
        [0.2603, 0.2715, 0.2392, 0.2290]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2658, 0.2427, 0.2360],
        [0.2562, 0.2652, 0.2438, 0.2348],
        [0.2582, 0.2670, 0.2453, 0.2295]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2684, 0.2405, 0.2360],
        [0.2560, 0.2687, 0.2431, 0.2322],
        [0.2583, 0.2663, 0.2439, 0.2316]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2620, 0.2409, 0.2362],
        [0.2573, 0.2619, 0.2464, 0.2345],
        [0.2576, 0.2633, 0.2383, 0.2408]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2518, 0.2507, 0.2485],
        [0.2501, 0.2527, 0.2514, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2527, 0.2514, 0.2473],
        [0.2510, 0.2523, 0.2504, 0.2463],
        [0.2516, 0.2538, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2515, 0.2520, 0.2471],
        [0.2488, 0.2532, 0.2493, 0.2487],
        [0.2506, 0.2535, 0.2497, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2518, 0.2484, 0.2495],
        [0.2499, 0.2521, 0.2493, 0.2486],
        [0.2507, 0.2513, 0.2499, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2521, 0.2508, 0.2469],
        [0.2505, 0.2513, 0.2464, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2522, 0.2492, 0.2483],
        [0.2502, 0.2518, 0.2495, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2489, 0.2527, 0.2509],
        [0.2471, 0.2523, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2517, 0.2526, 0.2495],
        [0.2468, 0.2530, 0.2537, 0.2464],
        [0.2467, 0.2499, 0.2514, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2498, 0.2520, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2479, 0.2508, 0.2523, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2487, 0.2492, 0.2536],
        [0.2473, 0.2500, 0.2550, 0.2478],
        [0.2474, 0.2518, 0.2525, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2499, 0.2646, 0.2397],
        [0.2467, 0.2472, 0.2472, 0.2590],
        [0.2461, 0.2453, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2440, 0.2460, 0.2495, 0.2604],
        [0.2500, 0.2506, 0.2488, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2418]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:00:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 0.4495 (0.3984)	Prec@(1,5) (86.1%, 99.4%)	
05/27 10:00:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 0.3951 (0.4054)	Prec@(1,5) (85.8%, 99.4%)	
05/27 10:01:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 0.4174 (0.4179)	Prec@(1,5) (85.3%, 99.4%)	
05/27 10:01:22PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 0.4542 (0.4210)	Prec@(1,5) (85.2%, 99.4%)	
05/27 10:01:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 0.3775 (0.4223)	Prec@(1,5) (85.3%, 99.4%)	
05/27 10:01:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 0.3198 (0.4235)	Prec@(1,5) (85.3%, 99.4%)	
05/27 10:02:08PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 0.3779 (0.4312)	Prec@(1,5) (85.0%, 99.4%)	
05/27 10:02:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 0.2726 (0.4324)	Prec@(1,5) (84.9%, 99.5%)	
05/27 10:02:21PM searchStage_trainer.py:221 [INFO] Train: [ 20/49] Final Prec@1 84.9360%
05/27 10:02:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 0.5742	Prec@(1,5) (79.7%, 98.8%)
05/27 10:02:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 0.5619	Prec@(1,5) (80.4%, 99.0%)
05/27 10:02:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 0.5680	Prec@(1,5) (80.4%, 98.9%)
05/27 10:02:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 0.5792	Prec@(1,5) (80.1%, 98.9%)
05/27 10:02:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 0.5794	Prec@(1,5) (80.0%, 98.9%)
05/27 10:02:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 0.5803	Prec@(1,5) (80.0%, 98.8%)
05/27 10:02:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 0.5812	Prec@(1,5) (80.1%, 98.9%)
05/27 10:02:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 0.5820	Prec@(1,5) (80.0%, 98.9%)
05/27 10:02:40PM searchStage_trainer.py:260 [INFO] Valid: [ 20/49] Final Prec@1 80.0560%
05/27 10:02:40PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:02:40PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.3160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2519, 0.2704, 0.2525, 0.2252],
        [0.2529, 0.2699, 0.2538, 0.2234]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2662, 0.2524, 0.2256],
        [0.2558, 0.2622, 0.2546, 0.2274],
        [0.2619, 0.2789, 0.2388, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2593, 0.2627, 0.2583, 0.2197],
        [0.2602, 0.2673, 0.2364, 0.2361],
        [0.2603, 0.2714, 0.2393, 0.2291]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2657, 0.2427, 0.2360],
        [0.2561, 0.2652, 0.2438, 0.2349],
        [0.2582, 0.2669, 0.2453, 0.2296]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2684, 0.2405, 0.2360],
        [0.2559, 0.2686, 0.2432, 0.2323],
        [0.2583, 0.2662, 0.2439, 0.2317]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2619, 0.2410, 0.2363],
        [0.2572, 0.2618, 0.2464, 0.2346],
        [0.2575, 0.2632, 0.2383, 0.2410]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2518, 0.2507, 0.2485],
        [0.2501, 0.2527, 0.2514, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2473],
        [0.2510, 0.2523, 0.2504, 0.2464],
        [0.2516, 0.2538, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2516, 0.2521, 0.2470],
        [0.2488, 0.2532, 0.2494, 0.2487],
        [0.2506, 0.2535, 0.2497, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2518, 0.2484, 0.2495],
        [0.2499, 0.2521, 0.2493, 0.2486],
        [0.2507, 0.2513, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2521, 0.2508, 0.2469],
        [0.2505, 0.2513, 0.2464, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2522, 0.2492, 0.2484],
        [0.2502, 0.2518, 0.2495, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2489, 0.2527, 0.2509],
        [0.2471, 0.2523, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2517, 0.2526, 0.2494],
        [0.2468, 0.2531, 0.2537, 0.2464],
        [0.2467, 0.2499, 0.2514, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2498, 0.2520, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2480, 0.2508, 0.2523, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2487, 0.2492, 0.2536],
        [0.2473, 0.2500, 0.2550, 0.2478],
        [0.2474, 0.2518, 0.2525, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2499, 0.2647, 0.2396],
        [0.2467, 0.2471, 0.2472, 0.2590],
        [0.2461, 0.2453, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2440, 0.2460, 0.2495, 0.2605],
        [0.2500, 0.2506, 0.2488, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2418]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:02:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 0.2284 (0.3936)	Prec@(1,5) (86.2%, 99.4%)	
05/27 10:03:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 0.3430 (0.3957)	Prec@(1,5) (86.1%, 99.5%)	
05/27 10:03:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 0.5636 (0.4020)	Prec@(1,5) (85.9%, 99.5%)	
05/27 10:03:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 0.3767 (0.4095)	Prec@(1,5) (85.6%, 99.5%)	
05/27 10:03:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 0.2915 (0.4122)	Prec@(1,5) (85.6%, 99.4%)	
05/27 10:04:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 0.4859 (0.4113)	Prec@(1,5) (85.5%, 99.4%)	
05/27 10:04:29PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 0.3352 (0.4123)	Prec@(1,5) (85.6%, 99.5%)	
05/27 10:04:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 0.2661 (0.4143)	Prec@(1,5) (85.5%, 99.5%)	
05/27 10:04:41PM searchStage_trainer.py:221 [INFO] Train: [ 21/49] Final Prec@1 85.4760%
05/27 10:04:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 0.5197	Prec@(1,5) (82.3%, 99.1%)
05/27 10:04:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 0.5346	Prec@(1,5) (81.7%, 99.1%)
05/27 10:04:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 0.5389	Prec@(1,5) (81.5%, 99.0%)
05/27 10:04:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 0.5516	Prec@(1,5) (81.2%, 99.0%)
05/27 10:04:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 0.5485	Prec@(1,5) (81.2%, 99.0%)
05/27 10:04:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 0.5438	Prec@(1,5) (81.4%, 99.0%)
05/27 10:04:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 0.5442	Prec@(1,5) (81.4%, 99.0%)
05/27 10:05:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 0.5433	Prec@(1,5) (81.4%, 99.0%)
05/27 10:05:01PM searchStage_trainer.py:260 [INFO] Valid: [ 21/49] Final Prec@1 81.3800%
05/27 10:05:01PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:05:01PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 81.3800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2519, 0.2703, 0.2526, 0.2253],
        [0.2528, 0.2699, 0.2539, 0.2234]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2662, 0.2524, 0.2257],
        [0.2558, 0.2621, 0.2545, 0.2276],
        [0.2619, 0.2788, 0.2389, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2593, 0.2628, 0.2583, 0.2195],
        [0.2602, 0.2672, 0.2364, 0.2363],
        [0.2602, 0.2713, 0.2393, 0.2292]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2657, 0.2427, 0.2361],
        [0.2561, 0.2651, 0.2438, 0.2350],
        [0.2581, 0.2669, 0.2453, 0.2297]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2683, 0.2405, 0.2361],
        [0.2559, 0.2686, 0.2432, 0.2323],
        [0.2582, 0.2661, 0.2439, 0.2318]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2619, 0.2410, 0.2363],
        [0.2572, 0.2617, 0.2464, 0.2347],
        [0.2575, 0.2631, 0.2384, 0.2411]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2518, 0.2508, 0.2485],
        [0.2501, 0.2527, 0.2514, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2474],
        [0.2510, 0.2523, 0.2504, 0.2464],
        [0.2516, 0.2538, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2516, 0.2521, 0.2469],
        [0.2488, 0.2532, 0.2493, 0.2487],
        [0.2506, 0.2535, 0.2497, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2518, 0.2484, 0.2495],
        [0.2499, 0.2521, 0.2493, 0.2487],
        [0.2507, 0.2513, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2521, 0.2508, 0.2469],
        [0.2505, 0.2513, 0.2464, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2522, 0.2492, 0.2484],
        [0.2502, 0.2518, 0.2495, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2489, 0.2527, 0.2509],
        [0.2471, 0.2523, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2517, 0.2526, 0.2494],
        [0.2468, 0.2531, 0.2537, 0.2464],
        [0.2467, 0.2499, 0.2513, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2498, 0.2520, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2480, 0.2508, 0.2523, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2492, 0.2536],
        [0.2473, 0.2500, 0.2550, 0.2477],
        [0.2474, 0.2518, 0.2525, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2499, 0.2648, 0.2395],
        [0.2467, 0.2471, 0.2471, 0.2590],
        [0.2461, 0.2453, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2440, 0.2459, 0.2495, 0.2606],
        [0.2500, 0.2506, 0.2488, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:05:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 0.3715 (0.3683)	Prec@(1,5) (87.9%, 99.8%)	
05/27 10:05:33PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 0.5052 (0.3766)	Prec@(1,5) (86.8%, 99.7%)	
05/27 10:05:48PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 0.4856 (0.3840)	Prec@(1,5) (86.5%, 99.7%)	
05/27 10:06:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 0.3946 (0.3753)	Prec@(1,5) (86.9%, 99.7%)	
05/27 10:06:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 0.2993 (0.3777)	Prec@(1,5) (86.9%, 99.6%)	
05/27 10:06:34PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 0.2890 (0.3833)	Prec@(1,5) (86.7%, 99.6%)	
05/27 10:06:49PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 0.3289 (0.3894)	Prec@(1,5) (86.5%, 99.6%)	
05/27 10:07:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 0.3238 (0.3948)	Prec@(1,5) (86.3%, 99.6%)	
05/27 10:07:02PM searchStage_trainer.py:221 [INFO] Train: [ 22/49] Final Prec@1 86.2960%
05/27 10:07:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 0.6433	Prec@(1,5) (79.4%, 98.8%)
05/27 10:07:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 0.6668	Prec@(1,5) (78.7%, 98.8%)
05/27 10:07:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 0.6633	Prec@(1,5) (78.6%, 98.7%)
05/27 10:07:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 0.6615	Prec@(1,5) (78.8%, 98.7%)
05/27 10:07:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 0.6617	Prec@(1,5) (78.7%, 98.7%)
05/27 10:07:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 0.6594	Prec@(1,5) (78.8%, 98.6%)
05/27 10:07:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 0.6612	Prec@(1,5) (78.7%, 98.6%)
05/27 10:07:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 0.6650	Prec@(1,5) (78.6%, 98.6%)
05/27 10:07:22PM searchStage_trainer.py:260 [INFO] Valid: [ 22/49] Final Prec@1 78.5600%
05/27 10:07:22PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:07:22PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 81.3800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2518, 0.2702, 0.2526, 0.2254],
        [0.2528, 0.2698, 0.2539, 0.2235]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2662, 0.2526, 0.2256],
        [0.2557, 0.2621, 0.2545, 0.2277],
        [0.2618, 0.2787, 0.2389, 0.2205]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2593, 0.2629, 0.2584, 0.2194],
        [0.2601, 0.2671, 0.2364, 0.2364],
        [0.2602, 0.2712, 0.2393, 0.2293]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2657, 0.2427, 0.2361],
        [0.2560, 0.2650, 0.2438, 0.2351],
        [0.2581, 0.2668, 0.2454, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2683, 0.2405, 0.2362],
        [0.2558, 0.2685, 0.2433, 0.2324],
        [0.2582, 0.2660, 0.2439, 0.2319]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2618, 0.2410, 0.2364],
        [0.2572, 0.2616, 0.2464, 0.2348],
        [0.2575, 0.2630, 0.2384, 0.2412]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2518, 0.2508, 0.2485],
        [0.2501, 0.2527, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2474],
        [0.2510, 0.2523, 0.2504, 0.2464],
        [0.2516, 0.2538, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2516, 0.2521, 0.2470],
        [0.2488, 0.2532, 0.2493, 0.2487],
        [0.2506, 0.2535, 0.2497, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2518, 0.2485, 0.2495],
        [0.2499, 0.2521, 0.2493, 0.2487],
        [0.2507, 0.2513, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2513, 0.2464, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2522, 0.2492, 0.2484],
        [0.2501, 0.2518, 0.2495, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2489, 0.2527, 0.2509],
        [0.2471, 0.2523, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2517, 0.2526, 0.2494],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2467, 0.2499, 0.2513, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2498, 0.2520, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2480, 0.2508, 0.2523, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2492, 0.2536],
        [0.2473, 0.2500, 0.2550, 0.2477],
        [0.2475, 0.2518, 0.2525, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2499, 0.2649, 0.2395],
        [0.2467, 0.2471, 0.2471, 0.2591],
        [0.2461, 0.2453, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2440, 0.2459, 0.2494, 0.2607],
        [0.2500, 0.2506, 0.2487, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:07:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 0.3149 (0.3611)	Prec@(1,5) (87.5%, 99.6%)	
05/27 10:07:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 0.3296 (0.3575)	Prec@(1,5) (87.3%, 99.7%)	
05/27 10:08:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 0.4922 (0.3636)	Prec@(1,5) (87.2%, 99.6%)	
05/27 10:08:24PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 0.3391 (0.3706)	Prec@(1,5) (86.9%, 99.6%)	
05/27 10:08:39PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 0.4600 (0.3745)	Prec@(1,5) (86.8%, 99.6%)	
05/27 10:08:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 0.4409 (0.3722)	Prec@(1,5) (86.9%, 99.6%)	
05/27 10:09:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 0.4067 (0.3741)	Prec@(1,5) (86.8%, 99.6%)	
05/27 10:09:23PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 0.3069 (0.3752)	Prec@(1,5) (86.8%, 99.6%)	
05/27 10:09:23PM searchStage_trainer.py:221 [INFO] Train: [ 23/49] Final Prec@1 86.7800%
05/27 10:09:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 0.5191	Prec@(1,5) (82.6%, 98.9%)
05/27 10:09:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 0.5204	Prec@(1,5) (82.4%, 99.0%)
05/27 10:09:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 0.5284	Prec@(1,5) (82.2%, 99.0%)
05/27 10:09:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 0.5360	Prec@(1,5) (82.0%, 99.0%)
05/27 10:09:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 0.5305	Prec@(1,5) (82.2%, 99.0%)
05/27 10:09:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 0.5264	Prec@(1,5) (82.3%, 99.0%)
05/27 10:09:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 0.5248	Prec@(1,5) (82.4%, 99.0%)
05/27 10:09:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 0.5243	Prec@(1,5) (82.4%, 99.0%)
05/27 10:09:43PM searchStage_trainer.py:260 [INFO] Valid: [ 23/49] Final Prec@1 82.4240%
05/27 10:09:43PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:09:43PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.4240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2518, 0.2702, 0.2526, 0.2255],
        [0.2527, 0.2698, 0.2540, 0.2235]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2661, 0.2526, 0.2257],
        [0.2558, 0.2620, 0.2545, 0.2277],
        [0.2618, 0.2787, 0.2389, 0.2206]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2628, 0.2583, 0.2194],
        [0.2601, 0.2671, 0.2364, 0.2364],
        [0.2602, 0.2711, 0.2393, 0.2294]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2656, 0.2427, 0.2361],
        [0.2560, 0.2649, 0.2438, 0.2353],
        [0.2580, 0.2667, 0.2454, 0.2299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2682, 0.2404, 0.2363],
        [0.2558, 0.2684, 0.2433, 0.2324],
        [0.2582, 0.2659, 0.2440, 0.2319]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2607, 0.2618, 0.2411, 0.2364],
        [0.2571, 0.2616, 0.2464, 0.2349],
        [0.2574, 0.2629, 0.2384, 0.2412]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2518, 0.2508, 0.2485],
        [0.2501, 0.2527, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2473],
        [0.2510, 0.2523, 0.2503, 0.2464],
        [0.2516, 0.2538, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2516, 0.2521, 0.2470],
        [0.2487, 0.2532, 0.2494, 0.2487],
        [0.2506, 0.2535, 0.2497, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2484, 0.2495],
        [0.2499, 0.2521, 0.2494, 0.2487],
        [0.2507, 0.2513, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2513, 0.2464, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2522, 0.2492, 0.2484],
        [0.2501, 0.2518, 0.2495, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2489, 0.2527, 0.2509],
        [0.2471, 0.2523, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2518, 0.2526, 0.2494],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2467, 0.2499, 0.2513, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2498, 0.2520, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2480, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2492, 0.2537],
        [0.2473, 0.2500, 0.2550, 0.2477],
        [0.2475, 0.2518, 0.2525, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2499, 0.2649, 0.2394],
        [0.2467, 0.2471, 0.2471, 0.2591],
        [0.2461, 0.2453, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2459, 0.2494, 0.2607],
        [0.2501, 0.2506, 0.2487, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:09:59PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 0.3593 (0.3438)	Prec@(1,5) (88.4%, 99.7%)	
05/27 10:10:14PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 0.4240 (0.3410)	Prec@(1,5) (88.2%, 99.6%)	
05/27 10:10:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 0.4248 (0.3433)	Prec@(1,5) (88.0%, 99.6%)	
05/27 10:10:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 0.5128 (0.3492)	Prec@(1,5) (87.9%, 99.6%)	
05/27 10:11:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 0.4231 (0.3458)	Prec@(1,5) (88.0%, 99.6%)	
05/27 10:11:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 0.3535 (0.3456)	Prec@(1,5) (88.0%, 99.6%)	
05/27 10:11:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 0.3091 (0.3472)	Prec@(1,5) (87.9%, 99.6%)	
05/27 10:11:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 0.4275 (0.3479)	Prec@(1,5) (87.8%, 99.6%)	
05/27 10:11:44PM searchStage_trainer.py:221 [INFO] Train: [ 24/49] Final Prec@1 87.8120%
05/27 10:11:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 0.5541	Prec@(1,5) (82.0%, 98.9%)
05/27 10:11:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 0.5621	Prec@(1,5) (81.5%, 98.9%)
05/27 10:11:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 0.5620	Prec@(1,5) (81.6%, 98.9%)
05/27 10:11:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 0.5671	Prec@(1,5) (81.3%, 98.9%)
05/27 10:11:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 0.5682	Prec@(1,5) (81.3%, 99.0%)
05/27 10:11:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 0.5695	Prec@(1,5) (81.3%, 99.0%)
05/27 10:12:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 0.5685	Prec@(1,5) (81.3%, 99.0%)
05/27 10:12:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 0.5651	Prec@(1,5) (81.5%, 99.0%)
05/27 10:12:04PM searchStage_trainer.py:260 [INFO] Valid: [ 24/49] Final Prec@1 81.5200%
05/27 10:12:04PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:12:04PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.4240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2517, 0.2701, 0.2526, 0.2255],
        [0.2527, 0.2697, 0.2540, 0.2236]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2660, 0.2527, 0.2258],
        [0.2558, 0.2620, 0.2545, 0.2278],
        [0.2618, 0.2786, 0.2390, 0.2207]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2628, 0.2583, 0.2195],
        [0.2601, 0.2670, 0.2364, 0.2365],
        [0.2601, 0.2711, 0.2394, 0.2294]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2656, 0.2428, 0.2361],
        [0.2560, 0.2649, 0.2438, 0.2353],
        [0.2580, 0.2666, 0.2454, 0.2300]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2682, 0.2404, 0.2364],
        [0.2558, 0.2684, 0.2434, 0.2325],
        [0.2581, 0.2658, 0.2440, 0.2320]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2607, 0.2617, 0.2411, 0.2365],
        [0.2571, 0.2615, 0.2464, 0.2350],
        [0.2574, 0.2628, 0.2385, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2518, 0.2508, 0.2485],
        [0.2501, 0.2527, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2474],
        [0.2510, 0.2523, 0.2503, 0.2464],
        [0.2516, 0.2538, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2516, 0.2521, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2499, 0.2521, 0.2494, 0.2487],
        [0.2507, 0.2513, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2513, 0.2464, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2521, 0.2492, 0.2484],
        [0.2501, 0.2518, 0.2495, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2527, 0.2509],
        [0.2471, 0.2523, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2518, 0.2526, 0.2494],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2467, 0.2499, 0.2513, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2498, 0.2520, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2480, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2491, 0.2537],
        [0.2473, 0.2500, 0.2550, 0.2477],
        [0.2475, 0.2518, 0.2525, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2499, 0.2650, 0.2394],
        [0.2467, 0.2471, 0.2471, 0.2591],
        [0.2461, 0.2452, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2459, 0.2494, 0.2608],
        [0.2501, 0.2506, 0.2487, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:12:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 0.3387 (0.3240)	Prec@(1,5) (88.7%, 99.8%)	
05/27 10:12:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 0.2504 (0.3202)	Prec@(1,5) (89.0%, 99.9%)	
05/27 10:12:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 0.2770 (0.3116)	Prec@(1,5) (89.1%, 99.8%)	
05/27 10:13:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 0.4886 (0.3216)	Prec@(1,5) (88.7%, 99.8%)	
05/27 10:13:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 0.3296 (0.3288)	Prec@(1,5) (88.4%, 99.8%)	
05/27 10:13:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 0.2682 (0.3323)	Prec@(1,5) (88.3%, 99.7%)	
05/27 10:13:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 0.3230 (0.3321)	Prec@(1,5) (88.4%, 99.7%)	
05/27 10:14:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 0.5727 (0.3337)	Prec@(1,5) (88.4%, 99.7%)	
05/27 10:14:05PM searchStage_trainer.py:221 [INFO] Train: [ 25/49] Final Prec@1 88.3560%
05/27 10:14:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 0.5570	Prec@(1,5) (82.0%, 98.9%)
05/27 10:14:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 0.5698	Prec@(1,5) (81.5%, 98.9%)
05/27 10:14:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 0.5588	Prec@(1,5) (81.6%, 99.0%)
05/27 10:14:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 0.5673	Prec@(1,5) (81.6%, 99.0%)
05/27 10:14:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 0.5564	Prec@(1,5) (81.7%, 99.1%)
05/27 10:14:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 0.5557	Prec@(1,5) (81.7%, 99.1%)
05/27 10:14:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 0.5533	Prec@(1,5) (81.7%, 99.1%)
05/27 10:14:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 0.5521	Prec@(1,5) (81.8%, 99.1%)
05/27 10:14:24PM searchStage_trainer.py:260 [INFO] Valid: [ 25/49] Final Prec@1 81.8360%
05/27 10:14:24PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:14:24PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.4240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2517, 0.2700, 0.2527, 0.2256],
        [0.2527, 0.2696, 0.2540, 0.2237]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2660, 0.2527, 0.2259],
        [0.2558, 0.2620, 0.2545, 0.2278],
        [0.2617, 0.2785, 0.2390, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2628, 0.2583, 0.2194],
        [0.2601, 0.2669, 0.2364, 0.2366],
        [0.2601, 0.2710, 0.2394, 0.2295]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2655, 0.2428, 0.2362],
        [0.2559, 0.2648, 0.2438, 0.2354],
        [0.2580, 0.2666, 0.2454, 0.2300]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2550, 0.2681, 0.2404, 0.2364],
        [0.2558, 0.2683, 0.2434, 0.2325],
        [0.2581, 0.2658, 0.2440, 0.2321]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2607, 0.2617, 0.2411, 0.2366],
        [0.2571, 0.2614, 0.2464, 0.2351],
        [0.2574, 0.2627, 0.2385, 0.2414]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2518, 0.2508, 0.2485],
        [0.2501, 0.2527, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2474],
        [0.2510, 0.2523, 0.2503, 0.2464],
        [0.2516, 0.2537, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2515, 0.2522, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2484, 0.2495],
        [0.2499, 0.2521, 0.2494, 0.2487],
        [0.2507, 0.2513, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2521, 0.2491, 0.2484],
        [0.2501, 0.2518, 0.2495, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2527, 0.2509],
        [0.2471, 0.2523, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2518, 0.2526, 0.2494],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2513, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2520, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2480, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2491, 0.2537],
        [0.2473, 0.2500, 0.2550, 0.2477],
        [0.2475, 0.2518, 0.2525, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2499, 0.2650, 0.2394],
        [0.2467, 0.2471, 0.2470, 0.2592],
        [0.2462, 0.2452, 0.2593, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2458, 0.2494, 0.2608],
        [0.2501, 0.2506, 0.2487, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:14:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 0.2045 (0.3078)	Prec@(1,5) (89.9%, 99.7%)	
05/27 10:14:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 0.3616 (0.2940)	Prec@(1,5) (90.2%, 99.8%)	
05/27 10:15:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 0.3433 (0.3022)	Prec@(1,5) (89.8%, 99.7%)	
05/27 10:15:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.2430 (0.3070)	Prec@(1,5) (89.5%, 99.8%)	
05/27 10:15:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 0.4328 (0.3142)	Prec@(1,5) (89.2%, 99.7%)	
05/27 10:15:57PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 0.2299 (0.3172)	Prec@(1,5) (89.1%, 99.7%)	
05/27 10:16:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 0.3617 (0.3189)	Prec@(1,5) (89.0%, 99.7%)	
05/27 10:16:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 0.2637 (0.3220)	Prec@(1,5) (88.9%, 99.7%)	
05/27 10:16:25PM searchStage_trainer.py:221 [INFO] Train: [ 26/49] Final Prec@1 88.8920%
05/27 10:16:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 0.5926	Prec@(1,5) (81.4%, 98.7%)
05/27 10:16:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 0.5795	Prec@(1,5) (81.4%, 98.7%)
05/27 10:16:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 0.5702	Prec@(1,5) (81.4%, 98.7%)
05/27 10:16:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 0.5699	Prec@(1,5) (81.4%, 98.9%)
05/27 10:16:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 0.5681	Prec@(1,5) (81.4%, 98.9%)
05/27 10:16:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 0.5639	Prec@(1,5) (81.5%, 99.0%)
05/27 10:16:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 0.5647	Prec@(1,5) (81.5%, 99.0%)
05/27 10:16:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 0.5652	Prec@(1,5) (81.5%, 99.0%)
05/27 10:16:45PM searchStage_trainer.py:260 [INFO] Valid: [ 26/49] Final Prec@1 81.5480%
05/27 10:16:45PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:16:45PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.4240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2516, 0.2700, 0.2527, 0.2257],
        [0.2526, 0.2696, 0.2541, 0.2237]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2659, 0.2527, 0.2259],
        [0.2557, 0.2620, 0.2546, 0.2278],
        [0.2617, 0.2784, 0.2390, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2629, 0.2584, 0.2194],
        [0.2600, 0.2669, 0.2364, 0.2366],
        [0.2601, 0.2709, 0.2394, 0.2296]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2655, 0.2428, 0.2363],
        [0.2559, 0.2647, 0.2438, 0.2355],
        [0.2579, 0.2665, 0.2455, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2550, 0.2681, 0.2404, 0.2365],
        [0.2557, 0.2683, 0.2434, 0.2326],
        [0.2581, 0.2657, 0.2440, 0.2322]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2607, 0.2616, 0.2411, 0.2367],
        [0.2571, 0.2614, 0.2464, 0.2351],
        [0.2573, 0.2627, 0.2385, 0.2415]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2518, 0.2508, 0.2485],
        [0.2501, 0.2526, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2474],
        [0.2510, 0.2523, 0.2503, 0.2464],
        [0.2516, 0.2537, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2515, 0.2522, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2484, 0.2495],
        [0.2499, 0.2521, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2521, 0.2491, 0.2484],
        [0.2501, 0.2518, 0.2495, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2527, 0.2509],
        [0.2471, 0.2524, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2518, 0.2526, 0.2494],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2513, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2480, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2491, 0.2537],
        [0.2473, 0.2500, 0.2550, 0.2477],
        [0.2475, 0.2518, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2499, 0.2651, 0.2393],
        [0.2467, 0.2471, 0.2470, 0.2592],
        [0.2462, 0.2452, 0.2593, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2458, 0.2494, 0.2609],
        [0.2501, 0.2506, 0.2487, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:17:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 0.3819 (0.2875)	Prec@(1,5) (89.9%, 99.7%)	
05/27 10:17:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 0.1199 (0.2804)	Prec@(1,5) (90.2%, 99.7%)	
05/27 10:17:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 0.2599 (0.2864)	Prec@(1,5) (89.8%, 99.7%)	
05/27 10:17:48PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 0.3939 (0.2893)	Prec@(1,5) (89.6%, 99.8%)	
05/27 10:18:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 0.4829 (0.2955)	Prec@(1,5) (89.4%, 99.8%)	
05/27 10:18:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 0.4394 (0.2970)	Prec@(1,5) (89.4%, 99.8%)	
05/27 10:18:34PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 0.3434 (0.2981)	Prec@(1,5) (89.4%, 99.8%)	
05/27 10:18:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 0.2428 (0.2993)	Prec@(1,5) (89.4%, 99.8%)	
05/27 10:18:47PM searchStage_trainer.py:221 [INFO] Train: [ 27/49] Final Prec@1 89.3840%
05/27 10:18:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 0.5114	Prec@(1,5) (83.3%, 99.2%)
05/27 10:18:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 0.5366	Prec@(1,5) (82.9%, 99.0%)
05/27 10:18:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 0.5264	Prec@(1,5) (82.9%, 99.1%)
05/27 10:18:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 0.5319	Prec@(1,5) (82.7%, 99.1%)
05/27 10:18:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 0.5372	Prec@(1,5) (82.5%, 99.1%)
05/27 10:19:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 0.5416	Prec@(1,5) (82.4%, 99.1%)
05/27 10:19:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 0.5446	Prec@(1,5) (82.4%, 99.0%)
05/27 10:19:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 0.5455	Prec@(1,5) (82.5%, 99.0%)
05/27 10:19:06PM searchStage_trainer.py:260 [INFO] Valid: [ 27/49] Final Prec@1 82.5040%
05/27 10:19:06PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:19:07PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.5040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2516, 0.2699, 0.2528, 0.2257],
        [0.2526, 0.2695, 0.2541, 0.2238]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2659, 0.2527, 0.2260],
        [0.2557, 0.2619, 0.2546, 0.2278],
        [0.2617, 0.2784, 0.2391, 0.2209]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2593, 0.2630, 0.2584, 0.2193],
        [0.2600, 0.2668, 0.2364, 0.2367],
        [0.2600, 0.2709, 0.2394, 0.2297]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2654, 0.2428, 0.2364],
        [0.2559, 0.2647, 0.2439, 0.2355],
        [0.2579, 0.2664, 0.2455, 0.2302]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2550, 0.2680, 0.2404, 0.2366],
        [0.2557, 0.2682, 0.2434, 0.2327],
        [0.2580, 0.2656, 0.2441, 0.2322]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2607, 0.2615, 0.2411, 0.2367],
        [0.2570, 0.2613, 0.2465, 0.2352],
        [0.2573, 0.2626, 0.2385, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2485],
        [0.2500, 0.2526, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2515, 0.2474],
        [0.2510, 0.2523, 0.2503, 0.2464],
        [0.2516, 0.2537, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2516, 0.2522, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2484, 0.2495],
        [0.2499, 0.2521, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2521, 0.2492, 0.2484],
        [0.2501, 0.2518, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2527, 0.2509],
        [0.2471, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2518, 0.2526, 0.2494],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2513, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2480, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2491, 0.2537],
        [0.2474, 0.2500, 0.2550, 0.2477],
        [0.2475, 0.2518, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2652, 0.2393],
        [0.2467, 0.2471, 0.2470, 0.2592],
        [0.2462, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2458, 0.2494, 0.2609],
        [0.2501, 0.2506, 0.2487, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:19:23PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 0.1964 (0.2696)	Prec@(1,5) (90.8%, 99.8%)	
05/27 10:19:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 0.3220 (0.2777)	Prec@(1,5) (90.2%, 99.8%)	
05/27 10:19:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 0.3373 (0.2796)	Prec@(1,5) (90.3%, 99.8%)	
05/27 10:20:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.2330 (0.2786)	Prec@(1,5) (90.4%, 99.8%)	
05/27 10:20:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 0.2157 (0.2786)	Prec@(1,5) (90.4%, 99.8%)	
05/27 10:20:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 0.2927 (0.2781)	Prec@(1,5) (90.4%, 99.8%)	
05/27 10:20:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][350/390]	Step 11298	lr 0.01075	Loss 0.3319 (0.2798)	Prec@(1,5) (90.4%, 99.8%)	
05/27 10:21:08PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 0.2491 (0.2822)	Prec@(1,5) (90.3%, 99.8%)	
05/27 10:21:08PM searchStage_trainer.py:221 [INFO] Train: [ 28/49] Final Prec@1 90.3320%
05/27 10:21:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][50/391]	Step 11339	Loss 0.5851	Prec@(1,5) (82.1%, 98.9%)
05/27 10:21:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 0.5692	Prec@(1,5) (82.5%, 99.0%)
05/27 10:21:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][150/391]	Step 11339	Loss 0.5710	Prec@(1,5) (82.4%, 98.9%)
05/27 10:21:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 0.5727	Prec@(1,5) (82.3%, 98.9%)
05/27 10:21:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][250/391]	Step 11339	Loss 0.5765	Prec@(1,5) (82.2%, 98.9%)
05/27 10:21:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 0.5775	Prec@(1,5) (82.1%, 98.9%)
05/27 10:21:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][350/391]	Step 11339	Loss 0.5803	Prec@(1,5) (82.0%, 98.9%)
05/27 10:21:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 0.5782	Prec@(1,5) (82.1%, 98.9%)
05/27 10:21:28PM searchStage_trainer.py:260 [INFO] Valid: [ 28/49] Final Prec@1 82.1000%
05/27 10:21:28PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:21:28PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.5040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2516, 0.2698, 0.2528, 0.2258],
        [0.2526, 0.2695, 0.2541, 0.2238]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2658, 0.2528, 0.2261],
        [0.2556, 0.2618, 0.2546, 0.2279],
        [0.2617, 0.2783, 0.2391, 0.2209]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2629, 0.2584, 0.2192],
        [0.2600, 0.2668, 0.2365, 0.2368],
        [0.2600, 0.2708, 0.2394, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2654, 0.2428, 0.2365],
        [0.2559, 0.2646, 0.2439, 0.2356],
        [0.2579, 0.2664, 0.2455, 0.2302]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2680, 0.2404, 0.2367],
        [0.2557, 0.2682, 0.2435, 0.2327],
        [0.2580, 0.2656, 0.2441, 0.2323]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2607, 0.2615, 0.2411, 0.2368],
        [0.2570, 0.2613, 0.2465, 0.2352],
        [0.2573, 0.2625, 0.2385, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2485],
        [0.2500, 0.2526, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2464],
        [0.2516, 0.2537, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2516, 0.2522, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2518, 0.2485, 0.2495],
        [0.2499, 0.2521, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2521, 0.2492, 0.2484],
        [0.2501, 0.2518, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2527, 0.2509],
        [0.2471, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2513, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2480, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2491, 0.2538],
        [0.2474, 0.2500, 0.2550, 0.2477],
        [0.2475, 0.2518, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2652, 0.2392],
        [0.2467, 0.2470, 0.2470, 0.2592],
        [0.2462, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2458, 0.2494, 0.2610],
        [0.2501, 0.2506, 0.2487, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:21:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][50/390]	Step 11389	lr 0.01002	Loss 0.2992 (0.2649)	Prec@(1,5) (91.0%, 99.9%)	
05/27 10:21:59PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 0.3636 (0.2642)	Prec@(1,5) (91.0%, 99.9%)	
05/27 10:22:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][150/390]	Step 11489	lr 0.01002	Loss 0.2188 (0.2558)	Prec@(1,5) (91.0%, 99.9%)	
05/27 10:22:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.2850 (0.2605)	Prec@(1,5) (90.9%, 99.9%)	
05/27 10:22:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][250/390]	Step 11589	lr 0.01002	Loss 0.3112 (0.2607)	Prec@(1,5) (90.9%, 99.9%)	
05/27 10:23:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 0.1816 (0.2635)	Prec@(1,5) (90.7%, 99.8%)	
05/27 10:23:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][350/390]	Step 11689	lr 0.01002	Loss 0.2241 (0.2662)	Prec@(1,5) (90.7%, 99.8%)	
05/27 10:23:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 0.2037 (0.2662)	Prec@(1,5) (90.7%, 99.8%)	
05/27 10:23:29PM searchStage_trainer.py:221 [INFO] Train: [ 29/49] Final Prec@1 90.7560%
05/27 10:23:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][50/391]	Step 11730	Loss 0.4889	Prec@(1,5) (84.2%, 99.1%)
05/27 10:23:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 0.5360	Prec@(1,5) (82.8%, 99.1%)
05/27 10:23:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][150/391]	Step 11730	Loss 0.5347	Prec@(1,5) (82.9%, 99.1%)
05/27 10:23:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 0.5536	Prec@(1,5) (82.6%, 99.1%)
05/27 10:23:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][250/391]	Step 11730	Loss 0.5524	Prec@(1,5) (82.5%, 99.0%)
05/27 10:23:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 0.5539	Prec@(1,5) (82.5%, 99.0%)
05/27 10:23:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][350/391]	Step 11730	Loss 0.5542	Prec@(1,5) (82.5%, 99.0%)
05/27 10:23:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 0.5573	Prec@(1,5) (82.4%, 99.0%)
05/27 10:23:48PM searchStage_trainer.py:260 [INFO] Valid: [ 29/49] Final Prec@1 82.3920%
05/27 10:23:48PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:23:48PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.5040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2516, 0.2698, 0.2528, 0.2259],
        [0.2526, 0.2694, 0.2542, 0.2239]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2658, 0.2528, 0.2260],
        [0.2556, 0.2618, 0.2546, 0.2280],
        [0.2617, 0.2783, 0.2391, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2629, 0.2584, 0.2192],
        [0.2600, 0.2667, 0.2365, 0.2368],
        [0.2600, 0.2707, 0.2394, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2653, 0.2428, 0.2365],
        [0.2559, 0.2646, 0.2439, 0.2356],
        [0.2579, 0.2663, 0.2455, 0.2303]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2680, 0.2403, 0.2368],
        [0.2557, 0.2681, 0.2435, 0.2327],
        [0.2580, 0.2655, 0.2441, 0.2324]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2606, 0.2615, 0.2411, 0.2368],
        [0.2570, 0.2612, 0.2465, 0.2353],
        [0.2572, 0.2625, 0.2386, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2474],
        [0.2510, 0.2523, 0.2503, 0.2464],
        [0.2516, 0.2537, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2522, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2499, 0.2521, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2486, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2485, 0.2494],
        [0.2503, 0.2521, 0.2492, 0.2484],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2526, 0.2509],
        [0.2471, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2512, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2480, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2491, 0.2538],
        [0.2474, 0.2500, 0.2550, 0.2477],
        [0.2475, 0.2518, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2652, 0.2392],
        [0.2467, 0.2470, 0.2470, 0.2592],
        [0.2462, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2458, 0.2494, 0.2610],
        [0.2501, 0.2506, 0.2487, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:24:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][50/390]	Step 11780	lr 0.00929	Loss 0.2744 (0.2320)	Prec@(1,5) (92.3%, 99.7%)	
05/27 10:24:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 0.2071 (0.2236)	Prec@(1,5) (92.4%, 99.8%)	
05/27 10:24:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][150/390]	Step 11880	lr 0.00929	Loss 0.1797 (0.2240)	Prec@(1,5) (92.4%, 99.9%)	
05/27 10:24:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 0.1542 (0.2315)	Prec@(1,5) (92.0%, 99.9%)	
05/27 10:25:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][250/390]	Step 11980	lr 0.00929	Loss 0.3255 (0.2333)	Prec@(1,5) (91.9%, 99.9%)	
05/27 10:25:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 0.2401 (0.2347)	Prec@(1,5) (91.9%, 99.9%)	
05/27 10:25:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][350/390]	Step 12080	lr 0.00929	Loss 0.2210 (0.2374)	Prec@(1,5) (91.8%, 99.9%)	
05/27 10:25:49PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 0.1404 (0.2372)	Prec@(1,5) (91.8%, 99.9%)	
05/27 10:25:49PM searchStage_trainer.py:221 [INFO] Train: [ 30/49] Final Prec@1 91.8080%
05/27 10:25:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][50/391]	Step 12121	Loss 0.5350	Prec@(1,5) (83.7%, 99.2%)
05/27 10:25:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 0.5329	Prec@(1,5) (83.2%, 99.2%)
05/27 10:25:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][150/391]	Step 12121	Loss 0.5418	Prec@(1,5) (83.3%, 99.1%)
05/27 10:25:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 0.5311	Prec@(1,5) (83.7%, 99.1%)
05/27 10:26:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][250/391]	Step 12121	Loss 0.5310	Prec@(1,5) (83.6%, 99.1%)
05/27 10:26:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 0.5294	Prec@(1,5) (83.5%, 99.1%)
05/27 10:26:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][350/391]	Step 12121	Loss 0.5322	Prec@(1,5) (83.4%, 99.1%)
05/27 10:26:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 0.5356	Prec@(1,5) (83.3%, 99.0%)
05/27 10:26:09PM searchStage_trainer.py:260 [INFO] Valid: [ 30/49] Final Prec@1 83.3160%
05/27 10:26:09PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:26:09PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.3160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2516, 0.2697, 0.2528, 0.2259],
        [0.2525, 0.2694, 0.2542, 0.2239]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2658, 0.2528, 0.2261],
        [0.2556, 0.2617, 0.2546, 0.2280],
        [0.2616, 0.2782, 0.2391, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2629, 0.2584, 0.2192],
        [0.2600, 0.2667, 0.2365, 0.2368],
        [0.2600, 0.2707, 0.2395, 0.2299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2653, 0.2428, 0.2365],
        [0.2558, 0.2645, 0.2439, 0.2357],
        [0.2579, 0.2663, 0.2455, 0.2304]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2679, 0.2404, 0.2368],
        [0.2556, 0.2681, 0.2435, 0.2328],
        [0.2580, 0.2655, 0.2441, 0.2324]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2606, 0.2614, 0.2411, 0.2368],
        [0.2570, 0.2612, 0.2465, 0.2353],
        [0.2572, 0.2624, 0.2386, 0.2418]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2464],
        [0.2516, 0.2537, 0.2484, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2522, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2499, 0.2521, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2486, 0.2494],
        [0.2503, 0.2521, 0.2492, 0.2484],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2526, 0.2509],
        [0.2471, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2512, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2491, 0.2538],
        [0.2474, 0.2500, 0.2550, 0.2477],
        [0.2475, 0.2518, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2653, 0.2392],
        [0.2467, 0.2470, 0.2470, 0.2593],
        [0.2462, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2458, 0.2494, 0.2610],
        [0.2501, 0.2506, 0.2487, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:26:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][50/390]	Step 12171	lr 0.00858	Loss 0.1699 (0.2126)	Prec@(1,5) (92.7%, 99.9%)	
05/27 10:26:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.1201 (0.2182)	Prec@(1,5) (92.5%, 99.9%)	
05/27 10:26:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][150/390]	Step 12271	lr 0.00858	Loss 0.1934 (0.2147)	Prec@(1,5) (92.5%, 99.9%)	
05/27 10:27:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.2036 (0.2133)	Prec@(1,5) (92.6%, 99.9%)	
05/27 10:27:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][250/390]	Step 12371	lr 0.00858	Loss 0.2318 (0.2148)	Prec@(1,5) (92.5%, 99.9%)	
05/27 10:27:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 0.3576 (0.2204)	Prec@(1,5) (92.3%, 99.9%)	
05/27 10:27:57PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][350/390]	Step 12471	lr 0.00858	Loss 0.2530 (0.2218)	Prec@(1,5) (92.2%, 99.9%)	
05/27 10:28:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 0.1975 (0.2241)	Prec@(1,5) (92.1%, 99.9%)	
05/27 10:28:10PM searchStage_trainer.py:221 [INFO] Train: [ 31/49] Final Prec@1 92.1280%
05/27 10:28:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][50/391]	Step 12512	Loss 0.5578	Prec@(1,5) (83.0%, 99.1%)
05/27 10:28:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 0.5545	Prec@(1,5) (83.1%, 99.0%)
05/27 10:28:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][150/391]	Step 12512	Loss 0.5382	Prec@(1,5) (83.6%, 99.1%)
05/27 10:28:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 0.5385	Prec@(1,5) (83.5%, 99.1%)
05/27 10:28:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][250/391]	Step 12512	Loss 0.5461	Prec@(1,5) (83.2%, 99.0%)
05/27 10:28:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 0.5449	Prec@(1,5) (83.3%, 99.0%)
05/27 10:28:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][350/391]	Step 12512	Loss 0.5471	Prec@(1,5) (83.2%, 99.0%)
05/27 10:28:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 0.5498	Prec@(1,5) (83.1%, 99.1%)
05/27 10:28:30PM searchStage_trainer.py:260 [INFO] Valid: [ 31/49] Final Prec@1 83.1040%
05/27 10:28:30PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:28:30PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.3160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2515, 0.2697, 0.2528, 0.2260],
        [0.2525, 0.2693, 0.2542, 0.2240]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2658, 0.2528, 0.2261],
        [0.2556, 0.2617, 0.2546, 0.2280],
        [0.2616, 0.2782, 0.2391, 0.2211]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2629, 0.2584, 0.2192],
        [0.2600, 0.2667, 0.2365, 0.2369],
        [0.2599, 0.2706, 0.2395, 0.2300]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2653, 0.2428, 0.2365],
        [0.2558, 0.2645, 0.2439, 0.2357],
        [0.2578, 0.2662, 0.2455, 0.2304]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2679, 0.2404, 0.2369],
        [0.2556, 0.2680, 0.2435, 0.2328],
        [0.2580, 0.2654, 0.2441, 0.2325]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2606, 0.2614, 0.2412, 0.2369],
        [0.2570, 0.2611, 0.2465, 0.2354],
        [0.2572, 0.2624, 0.2386, 0.2419]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2514, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2464],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2522, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2499, 0.2521, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2486, 0.2494],
        [0.2503, 0.2521, 0.2492, 0.2484],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2526, 0.2509],
        [0.2471, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2512, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2491, 0.2538],
        [0.2474, 0.2500, 0.2550, 0.2477],
        [0.2475, 0.2518, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2653, 0.2392],
        [0.2467, 0.2470, 0.2469, 0.2593],
        [0.2462, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2457, 0.2494, 0.2611],
        [0.2501, 0.2506, 0.2486, 0.2506],
        [0.2421, 0.2537, 0.2625, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:28:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][50/390]	Step 12562	lr 0.00789	Loss 0.1729 (0.1810)	Prec@(1,5) (93.5%, 99.9%)	
05/27 10:29:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.2643 (0.1892)	Prec@(1,5) (93.3%, 99.9%)	
05/27 10:29:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][150/390]	Step 12662	lr 0.00789	Loss 0.1543 (0.1883)	Prec@(1,5) (93.4%, 99.9%)	
05/27 10:29:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.1924 (0.1983)	Prec@(1,5) (93.0%, 99.9%)	
05/27 10:29:47PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][250/390]	Step 12762	lr 0.00789	Loss 0.3279 (0.2005)	Prec@(1,5) (93.0%, 99.9%)	
05/27 10:30:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.1614 (0.2022)	Prec@(1,5) (92.9%, 99.9%)	
05/27 10:30:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][350/390]	Step 12862	lr 0.00789	Loss 0.1622 (0.2053)	Prec@(1,5) (92.7%, 99.9%)	
05/27 10:30:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 0.2877 (0.2065)	Prec@(1,5) (92.8%, 99.9%)	
05/27 10:30:30PM searchStage_trainer.py:221 [INFO] Train: [ 32/49] Final Prec@1 92.7560%
05/27 10:30:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][50/391]	Step 12903	Loss 0.5417	Prec@(1,5) (83.5%, 99.0%)
05/27 10:30:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 0.5209	Prec@(1,5) (84.1%, 99.1%)
05/27 10:30:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][150/391]	Step 12903	Loss 0.5066	Prec@(1,5) (84.5%, 99.2%)
05/27 10:30:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 0.5046	Prec@(1,5) (84.6%, 99.1%)
05/27 10:30:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][250/391]	Step 12903	Loss 0.5111	Prec@(1,5) (84.4%, 99.1%)
05/27 10:30:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 0.5147	Prec@(1,5) (84.2%, 99.1%)
05/27 10:30:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][350/391]	Step 12903	Loss 0.5175	Prec@(1,5) (84.3%, 99.0%)
05/27 10:30:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 0.5186	Prec@(1,5) (84.3%, 99.1%)
05/27 10:30:50PM searchStage_trainer.py:260 [INFO] Valid: [ 32/49] Final Prec@1 84.2600%
05/27 10:30:50PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:30:50PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 84.2600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2515, 0.2696, 0.2528, 0.2260],
        [0.2525, 0.2693, 0.2542, 0.2240]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2657, 0.2529, 0.2261],
        [0.2556, 0.2617, 0.2546, 0.2280],
        [0.2616, 0.2781, 0.2391, 0.2212]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2629, 0.2584, 0.2192],
        [0.2600, 0.2666, 0.2365, 0.2369],
        [0.2599, 0.2706, 0.2395, 0.2300]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2652, 0.2428, 0.2366],
        [0.2558, 0.2645, 0.2440, 0.2358],
        [0.2578, 0.2662, 0.2456, 0.2305]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2679, 0.2404, 0.2370],
        [0.2556, 0.2680, 0.2436, 0.2328],
        [0.2579, 0.2654, 0.2442, 0.2325]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2606, 0.2613, 0.2412, 0.2369],
        [0.2570, 0.2611, 0.2465, 0.2354],
        [0.2572, 0.2623, 0.2386, 0.2419]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2464],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2522, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2499, 0.2521, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2486, 0.2494],
        [0.2503, 0.2521, 0.2491, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2526, 0.2509],
        [0.2472, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2512, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2490, 0.2538],
        [0.2474, 0.2500, 0.2550, 0.2477],
        [0.2476, 0.2518, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2653, 0.2391],
        [0.2468, 0.2470, 0.2469, 0.2593],
        [0.2462, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2457, 0.2493, 0.2611],
        [0.2501, 0.2506, 0.2486, 0.2506],
        [0.2421, 0.2537, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:31:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][50/390]	Step 12953	lr 0.00722	Loss 0.2321 (0.1796)	Prec@(1,5) (93.8%, 100.0%)	
05/27 10:31:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.3412 (0.1749)	Prec@(1,5) (93.9%, 100.0%)	
05/27 10:31:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][150/390]	Step 13053	lr 0.00722	Loss 0.1117 (0.1755)	Prec@(1,5) (93.9%, 99.9%)	
05/27 10:31:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.1786 (0.1768)	Prec@(1,5) (93.7%, 99.9%)	
05/27 10:32:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][250/390]	Step 13153	lr 0.00722	Loss 0.1647 (0.1769)	Prec@(1,5) (93.8%, 99.9%)	
05/27 10:32:23PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.2243 (0.1809)	Prec@(1,5) (93.8%, 99.9%)	
05/27 10:32:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][350/390]	Step 13253	lr 0.00722	Loss 0.1510 (0.1837)	Prec@(1,5) (93.7%, 99.9%)	
05/27 10:32:50PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 0.2728 (0.1860)	Prec@(1,5) (93.6%, 99.9%)	
05/27 10:32:51PM searchStage_trainer.py:221 [INFO] Train: [ 33/49] Final Prec@1 93.5680%
05/27 10:32:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][50/391]	Step 13294	Loss 0.5395	Prec@(1,5) (84.5%, 99.1%)
05/27 10:32:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 0.5412	Prec@(1,5) (84.0%, 99.1%)
05/27 10:32:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][150/391]	Step 13294	Loss 0.5367	Prec@(1,5) (83.7%, 99.2%)
05/27 10:33:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 0.5447	Prec@(1,5) (83.7%, 99.2%)
05/27 10:33:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][250/391]	Step 13294	Loss 0.5465	Prec@(1,5) (83.7%, 99.2%)
05/27 10:33:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 0.5543	Prec@(1,5) (83.4%, 99.2%)
05/27 10:33:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][350/391]	Step 13294	Loss 0.5482	Prec@(1,5) (83.5%, 99.2%)
05/27 10:33:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 0.5518	Prec@(1,5) (83.5%, 99.2%)
05/27 10:33:11PM searchStage_trainer.py:260 [INFO] Valid: [ 33/49] Final Prec@1 83.4800%
05/27 10:33:11PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:33:11PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 84.2600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2515, 0.2696, 0.2529, 0.2260],
        [0.2525, 0.2692, 0.2543, 0.2240]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2657, 0.2529, 0.2262],
        [0.2556, 0.2617, 0.2547, 0.2280],
        [0.2616, 0.2781, 0.2391, 0.2212]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2628, 0.2584, 0.2193],
        [0.2599, 0.2666, 0.2365, 0.2370],
        [0.2599, 0.2705, 0.2395, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2652, 0.2428, 0.2367],
        [0.2558, 0.2644, 0.2440, 0.2358],
        [0.2578, 0.2661, 0.2456, 0.2305]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2679, 0.2404, 0.2370],
        [0.2556, 0.2680, 0.2436, 0.2329],
        [0.2579, 0.2653, 0.2442, 0.2326]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2606, 0.2613, 0.2412, 0.2369],
        [0.2569, 0.2610, 0.2465, 0.2355],
        [0.2572, 0.2622, 0.2386, 0.2420]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2522, 0.2470],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2498, 0.2521, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2486, 0.2494],
        [0.2503, 0.2521, 0.2491, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2526, 0.2509],
        [0.2472, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2512, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2490, 0.2538],
        [0.2474, 0.2499, 0.2550, 0.2477],
        [0.2476, 0.2518, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2654, 0.2391],
        [0.2468, 0.2470, 0.2469, 0.2593],
        [0.2462, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2457, 0.2493, 0.2611],
        [0.2501, 0.2506, 0.2486, 0.2506],
        [0.2421, 0.2537, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:33:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][50/390]	Step 13344	lr 0.00657	Loss 0.2273 (0.1672)	Prec@(1,5) (94.0%, 99.9%)	
05/27 10:33:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.1773 (0.1628)	Prec@(1,5) (94.2%, 99.9%)	
05/27 10:33:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][150/390]	Step 13444	lr 0.00657	Loss 0.3138 (0.1666)	Prec@(1,5) (94.1%, 100.0%)	
05/27 10:34:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.0457 (0.1690)	Prec@(1,5) (94.0%, 100.0%)	
05/27 10:34:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][250/390]	Step 13544	lr 0.00657	Loss 0.0933 (0.1687)	Prec@(1,5) (94.0%, 100.0%)	
05/27 10:34:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.1454 (0.1709)	Prec@(1,5) (94.0%, 99.9%)	
05/27 10:34:59PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][350/390]	Step 13644	lr 0.00657	Loss 0.1258 (0.1724)	Prec@(1,5) (93.9%, 99.9%)	
05/27 10:35:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 0.1964 (0.1718)	Prec@(1,5) (93.9%, 99.9%)	
05/27 10:35:12PM searchStage_trainer.py:221 [INFO] Train: [ 34/49] Final Prec@1 93.9320%
05/27 10:35:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][50/391]	Step 13685	Loss 0.5510	Prec@(1,5) (84.2%, 99.2%)
05/27 10:35:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 0.5712	Prec@(1,5) (84.0%, 99.0%)
05/27 10:35:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][150/391]	Step 13685	Loss 0.5710	Prec@(1,5) (83.9%, 99.0%)
05/27 10:35:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 0.5685	Prec@(1,5) (84.0%, 99.0%)
05/27 10:35:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][250/391]	Step 13685	Loss 0.5614	Prec@(1,5) (84.0%, 99.1%)
05/27 10:35:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 0.5585	Prec@(1,5) (84.1%, 99.1%)
05/27 10:35:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][350/391]	Step 13685	Loss 0.5578	Prec@(1,5) (84.1%, 99.1%)
05/27 10:35:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 0.5517	Prec@(1,5) (84.2%, 99.1%)
05/27 10:35:32PM searchStage_trainer.py:260 [INFO] Valid: [ 34/49] Final Prec@1 84.2360%
05/27 10:35:32PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:35:32PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 84.2600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2515, 0.2696, 0.2529, 0.2261],
        [0.2525, 0.2692, 0.2543, 0.2241]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2657, 0.2529, 0.2262],
        [0.2556, 0.2616, 0.2547, 0.2281],
        [0.2616, 0.2780, 0.2391, 0.2212]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2629, 0.2584, 0.2192],
        [0.2599, 0.2666, 0.2365, 0.2370],
        [0.2599, 0.2705, 0.2395, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2652, 0.2428, 0.2367],
        [0.2558, 0.2644, 0.2440, 0.2359],
        [0.2578, 0.2661, 0.2456, 0.2306]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2678, 0.2404, 0.2371],
        [0.2556, 0.2679, 0.2436, 0.2329],
        [0.2579, 0.2653, 0.2442, 0.2326]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2606, 0.2613, 0.2412, 0.2369],
        [0.2569, 0.2610, 0.2466, 0.2355],
        [0.2571, 0.2622, 0.2386, 0.2420]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2522, 0.2470],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2517, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2486, 0.2495],
        [0.2503, 0.2521, 0.2491, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2526, 0.2509],
        [0.2472, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2512, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2490, 0.2538],
        [0.2474, 0.2500, 0.2550, 0.2477],
        [0.2476, 0.2518, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2654, 0.2391],
        [0.2468, 0.2470, 0.2469, 0.2593],
        [0.2462, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2457, 0.2493, 0.2611],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2537, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:35:48PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][50/390]	Step 13735	lr 0.00595	Loss 0.2137 (0.1560)	Prec@(1,5) (94.6%, 99.9%)	
05/27 10:36:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.0643 (0.1525)	Prec@(1,5) (94.9%, 99.9%)	
05/27 10:36:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][150/390]	Step 13835	lr 0.00595	Loss 0.2060 (0.1503)	Prec@(1,5) (94.8%, 99.9%)	
05/27 10:36:34PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 0.2830 (0.1508)	Prec@(1,5) (94.8%, 100.0%)	
05/27 10:36:50PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][250/390]	Step 13935	lr 0.00595	Loss 0.2568 (0.1523)	Prec@(1,5) (94.7%, 99.9%)	
05/27 10:37:05PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 0.1256 (0.1540)	Prec@(1,5) (94.7%, 100.0%)	
05/27 10:37:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][350/390]	Step 14035	lr 0.00595	Loss 0.1417 (0.1567)	Prec@(1,5) (94.6%, 100.0%)	
05/27 10:37:33PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 0.1660 (0.1588)	Prec@(1,5) (94.5%, 100.0%)	
05/27 10:37:33PM searchStage_trainer.py:221 [INFO] Train: [ 35/49] Final Prec@1 94.5360%
05/27 10:37:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][50/391]	Step 14076	Loss 0.5259	Prec@(1,5) (84.1%, 99.4%)
05/27 10:37:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 0.5399	Prec@(1,5) (84.1%, 99.2%)
05/27 10:37:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][150/391]	Step 14076	Loss 0.5370	Prec@(1,5) (84.2%, 99.2%)
05/27 10:37:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 0.5334	Prec@(1,5) (84.4%, 99.2%)
05/27 10:37:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][250/391]	Step 14076	Loss 0.5289	Prec@(1,5) (84.3%, 99.3%)
05/27 10:37:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 0.5324	Prec@(1,5) (84.3%, 99.2%)
05/27 10:37:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][350/391]	Step 14076	Loss 0.5277	Prec@(1,5) (84.3%, 99.2%)
05/27 10:37:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 0.5255	Prec@(1,5) (84.4%, 99.3%)
05/27 10:37:53PM searchStage_trainer.py:260 [INFO] Valid: [ 35/49] Final Prec@1 84.3600%
05/27 10:37:53PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:37:53PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 84.3600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2515, 0.2695, 0.2529, 0.2261],
        [0.2525, 0.2691, 0.2543, 0.2241]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2656, 0.2529, 0.2263],
        [0.2556, 0.2616, 0.2547, 0.2281],
        [0.2616, 0.2780, 0.2392, 0.2213]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2629, 0.2584, 0.2192],
        [0.2599, 0.2665, 0.2365, 0.2371],
        [0.2599, 0.2705, 0.2395, 0.2302]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2651, 0.2428, 0.2367],
        [0.2558, 0.2644, 0.2440, 0.2359],
        [0.2577, 0.2660, 0.2456, 0.2306]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2678, 0.2404, 0.2371],
        [0.2555, 0.2679, 0.2436, 0.2329],
        [0.2579, 0.2652, 0.2442, 0.2327]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2606, 0.2613, 0.2412, 0.2369],
        [0.2569, 0.2610, 0.2466, 0.2356],
        [0.2571, 0.2622, 0.2386, 0.2421]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2469],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2486, 0.2495],
        [0.2503, 0.2521, 0.2491, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2526, 0.2510],
        [0.2472, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2531, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2512, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2490, 0.2539],
        [0.2474, 0.2500, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2524, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2654, 0.2391],
        [0.2468, 0.2470, 0.2469, 0.2593],
        [0.2462, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2457, 0.2493, 0.2612],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2537, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:38:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][50/390]	Step 14126	lr 0.00535	Loss 0.1428 (0.1295)	Prec@(1,5) (95.5%, 100.0%)	
05/27 10:38:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.1056 (0.1304)	Prec@(1,5) (95.4%, 100.0%)	
05/27 10:38:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][150/390]	Step 14226	lr 0.00535	Loss 0.1538 (0.1325)	Prec@(1,5) (95.4%, 100.0%)	
05/27 10:38:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.1725 (0.1362)	Prec@(1,5) (95.3%, 100.0%)	
05/27 10:39:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][250/390]	Step 14326	lr 0.00535	Loss 0.1676 (0.1358)	Prec@(1,5) (95.3%, 99.9%)	
05/27 10:39:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.2072 (0.1357)	Prec@(1,5) (95.3%, 100.0%)	
05/27 10:39:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][350/390]	Step 14426	lr 0.00535	Loss 0.3129 (0.1377)	Prec@(1,5) (95.2%, 100.0%)	
05/27 10:39:54PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.1192 (0.1380)	Prec@(1,5) (95.2%, 100.0%)	
05/27 10:39:54PM searchStage_trainer.py:221 [INFO] Train: [ 36/49] Final Prec@1 95.1520%
05/27 10:39:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][50/391]	Step 14467	Loss 0.5459	Prec@(1,5) (84.5%, 99.2%)
05/27 10:39:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 0.5494	Prec@(1,5) (84.7%, 99.1%)
05/27 10:40:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][150/391]	Step 14467	Loss 0.5434	Prec@(1,5) (84.7%, 99.1%)
05/27 10:40:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 0.5388	Prec@(1,5) (84.8%, 99.1%)
05/27 10:40:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][250/391]	Step 14467	Loss 0.5363	Prec@(1,5) (84.8%, 99.1%)
05/27 10:40:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 0.5379	Prec@(1,5) (84.7%, 99.1%)
05/27 10:40:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][350/391]	Step 14467	Loss 0.5432	Prec@(1,5) (84.6%, 99.1%)
05/27 10:40:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 0.5457	Prec@(1,5) (84.6%, 99.1%)
05/27 10:40:14PM searchStage_trainer.py:260 [INFO] Valid: [ 36/49] Final Prec@1 84.5840%
05/27 10:40:14PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:40:14PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 84.5840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2514, 0.2695, 0.2529, 0.2262],
        [0.2524, 0.2691, 0.2543, 0.2242]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2656, 0.2529, 0.2263],
        [0.2556, 0.2616, 0.2547, 0.2281],
        [0.2616, 0.2780, 0.2392, 0.2213]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2629, 0.2584, 0.2192],
        [0.2599, 0.2665, 0.2365, 0.2371],
        [0.2599, 0.2704, 0.2395, 0.2302]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2651, 0.2428, 0.2367],
        [0.2557, 0.2643, 0.2440, 0.2359],
        [0.2577, 0.2660, 0.2456, 0.2306]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2678, 0.2404, 0.2372],
        [0.2555, 0.2679, 0.2436, 0.2330],
        [0.2579, 0.2652, 0.2442, 0.2327]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2606, 0.2612, 0.2412, 0.2369],
        [0.2569, 0.2609, 0.2466, 0.2356],
        [0.2571, 0.2621, 0.2387, 0.2421]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2527, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2523, 0.2470],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2470],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2486, 0.2495],
        [0.2503, 0.2521, 0.2491, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2526, 0.2510],
        [0.2472, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2532, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2512, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2490, 0.2539],
        [0.2474, 0.2500, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2655, 0.2390],
        [0.2468, 0.2470, 0.2469, 0.2593],
        [0.2462, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2457, 0.2493, 0.2612],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2537, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:40:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][50/390]	Step 14517	lr 0.00479	Loss 0.0558 (0.1165)	Prec@(1,5) (95.8%, 99.9%)	
05/27 10:40:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.1336 (0.1157)	Prec@(1,5) (96.0%, 100.0%)	
05/27 10:41:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][150/390]	Step 14617	lr 0.00479	Loss 0.1907 (0.1178)	Prec@(1,5) (95.9%, 100.0%)	
05/27 10:41:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.2419 (0.1192)	Prec@(1,5) (95.8%, 100.0%)	
05/27 10:41:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][250/390]	Step 14717	lr 0.00479	Loss 0.2715 (0.1201)	Prec@(1,5) (95.8%, 100.0%)	
05/27 10:41:47PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.0967 (0.1203)	Prec@(1,5) (95.7%, 100.0%)	
05/27 10:42:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][350/390]	Step 14817	lr 0.00479	Loss 0.4081 (0.1233)	Prec@(1,5) (95.7%, 100.0%)	
05/27 10:42:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.1133 (0.1255)	Prec@(1,5) (95.6%, 100.0%)	
05/27 10:42:15PM searchStage_trainer.py:221 [INFO] Train: [ 37/49] Final Prec@1 95.6160%
05/27 10:42:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][50/391]	Step 14858	Loss 0.5355	Prec@(1,5) (85.1%, 99.0%)
05/27 10:42:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 0.5221	Prec@(1,5) (85.3%, 99.0%)
05/27 10:42:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][150/391]	Step 14858	Loss 0.5301	Prec@(1,5) (85.2%, 99.1%)
05/27 10:42:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 0.5296	Prec@(1,5) (85.0%, 99.1%)
05/27 10:42:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][250/391]	Step 14858	Loss 0.5360	Prec@(1,5) (84.9%, 99.1%)
05/27 10:42:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 0.5393	Prec@(1,5) (85.0%, 99.1%)
05/27 10:42:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][350/391]	Step 14858	Loss 0.5368	Prec@(1,5) (85.0%, 99.1%)
05/27 10:42:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 0.5360	Prec@(1,5) (85.1%, 99.1%)
05/27 10:42:35PM searchStage_trainer.py:260 [INFO] Valid: [ 37/49] Final Prec@1 85.0400%
05/27 10:42:35PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:42:35PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.0400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2514, 0.2694, 0.2529, 0.2262],
        [0.2524, 0.2691, 0.2543, 0.2242]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2655, 0.2529, 0.2263],
        [0.2556, 0.2615, 0.2547, 0.2282],
        [0.2615, 0.2779, 0.2392, 0.2214]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2628, 0.2584, 0.2193],
        [0.2599, 0.2665, 0.2365, 0.2371],
        [0.2598, 0.2704, 0.2395, 0.2302]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2651, 0.2429, 0.2367],
        [0.2557, 0.2643, 0.2440, 0.2360],
        [0.2577, 0.2660, 0.2456, 0.2307]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2677, 0.2404, 0.2372],
        [0.2555, 0.2678, 0.2437, 0.2330],
        [0.2579, 0.2652, 0.2442, 0.2328]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2612, 0.2413, 0.2370],
        [0.2569, 0.2609, 0.2466, 0.2356],
        [0.2571, 0.2621, 0.2387, 0.2422]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2517, 0.2523, 0.2470],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2520, 0.2508, 0.2470],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2486, 0.2495],
        [0.2503, 0.2521, 0.2491, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2526, 0.2510],
        [0.2472, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2532, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2512, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2490, 0.2539],
        [0.2474, 0.2500, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2655, 0.2390],
        [0.2468, 0.2470, 0.2469, 0.2594],
        [0.2463, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2457, 0.2493, 0.2612],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2537, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:42:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][50/390]	Step 14908	lr 0.00425	Loss 0.0739 (0.0901)	Prec@(1,5) (96.9%, 100.0%)	
05/27 10:43:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.1975 (0.0916)	Prec@(1,5) (96.9%, 100.0%)	
05/27 10:43:22PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][150/390]	Step 15008	lr 0.00425	Loss 0.1056 (0.0994)	Prec@(1,5) (96.5%, 100.0%)	
05/27 10:43:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.0676 (0.0994)	Prec@(1,5) (96.6%, 100.0%)	
05/27 10:43:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][250/390]	Step 15108	lr 0.00425	Loss 0.0991 (0.1016)	Prec@(1,5) (96.5%, 100.0%)	
05/27 10:44:08PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.0853 (0.1030)	Prec@(1,5) (96.5%, 100.0%)	
05/27 10:44:23PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][350/390]	Step 15208	lr 0.00425	Loss 0.0989 (0.1047)	Prec@(1,5) (96.4%, 100.0%)	
05/27 10:44:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.0326 (0.1049)	Prec@(1,5) (96.4%, 100.0%)	
05/27 10:44:36PM searchStage_trainer.py:221 [INFO] Train: [ 38/49] Final Prec@1 96.3840%
05/27 10:44:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][50/391]	Step 15249	Loss 0.5302	Prec@(1,5) (85.3%, 98.9%)
05/27 10:44:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 0.5302	Prec@(1,5) (85.5%, 99.2%)
05/27 10:44:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][150/391]	Step 15249	Loss 0.5371	Prec@(1,5) (85.5%, 99.1%)
05/27 10:44:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 0.5389	Prec@(1,5) (85.3%, 99.1%)
05/27 10:44:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][250/391]	Step 15249	Loss 0.5363	Prec@(1,5) (85.4%, 99.1%)
05/27 10:44:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 0.5319	Prec@(1,5) (85.4%, 99.2%)
05/27 10:44:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][350/391]	Step 15249	Loss 0.5351	Prec@(1,5) (85.2%, 99.1%)
05/27 10:44:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 0.5398	Prec@(1,5) (85.2%, 99.1%)
05/27 10:44:56PM searchStage_trainer.py:260 [INFO] Valid: [ 38/49] Final Prec@1 85.2360%
05/27 10:44:56PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:44:56PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.2360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2514, 0.2694, 0.2529, 0.2262],
        [0.2524, 0.2690, 0.2543, 0.2242]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2655, 0.2529, 0.2264],
        [0.2556, 0.2615, 0.2547, 0.2282],
        [0.2615, 0.2779, 0.2392, 0.2214]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2628, 0.2585, 0.2192],
        [0.2599, 0.2664, 0.2365, 0.2372],
        [0.2598, 0.2703, 0.2396, 0.2303]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2651, 0.2429, 0.2368],
        [0.2557, 0.2642, 0.2440, 0.2360],
        [0.2577, 0.2659, 0.2457, 0.2307]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2677, 0.2404, 0.2372],
        [0.2555, 0.2678, 0.2437, 0.2330],
        [0.2578, 0.2651, 0.2442, 0.2328]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2612, 0.2413, 0.2370],
        [0.2569, 0.2609, 0.2466, 0.2357],
        [0.2571, 0.2620, 0.2387, 0.2422]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2516, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2519, 0.2508, 0.2470],
        [0.2505, 0.2512, 0.2464, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2524, 0.2486, 0.2495],
        [0.2503, 0.2521, 0.2491, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2526, 0.2510],
        [0.2472, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2532, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2512, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2490, 0.2539],
        [0.2474, 0.2500, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2655, 0.2390],
        [0.2468, 0.2470, 0.2469, 0.2594],
        [0.2463, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2457, 0.2493, 0.2612],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2537, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:45:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][50/390]	Step 15299	lr 0.00375	Loss 0.0744 (0.0957)	Prec@(1,5) (96.7%, 100.0%)	
05/27 10:45:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.1028 (0.0887)	Prec@(1,5) (97.0%, 100.0%)	
05/27 10:45:43PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][150/390]	Step 15399	lr 0.00375	Loss 0.1234 (0.0863)	Prec@(1,5) (97.2%, 100.0%)	
05/27 10:45:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.0840 (0.0879)	Prec@(1,5) (97.1%, 100.0%)	
05/27 10:46:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][250/390]	Step 15499	lr 0.00375	Loss 0.1890 (0.0914)	Prec@(1,5) (97.0%, 100.0%)	
05/27 10:46:29PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.1369 (0.0925)	Prec@(1,5) (96.9%, 100.0%)	
05/27 10:46:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][350/390]	Step 15599	lr 0.00375	Loss 0.0896 (0.0935)	Prec@(1,5) (96.8%, 100.0%)	
05/27 10:46:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.0349 (0.0941)	Prec@(1,5) (96.8%, 100.0%)	
05/27 10:46:56PM searchStage_trainer.py:221 [INFO] Train: [ 39/49] Final Prec@1 96.8080%
05/27 10:46:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][50/391]	Step 15640	Loss 0.5336	Prec@(1,5) (86.2%, 99.3%)
05/27 10:47:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 0.5321	Prec@(1,5) (85.8%, 99.3%)
05/27 10:47:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][150/391]	Step 15640	Loss 0.5222	Prec@(1,5) (85.7%, 99.3%)
05/27 10:47:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 0.5202	Prec@(1,5) (85.9%, 99.3%)
05/27 10:47:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][250/391]	Step 15640	Loss 0.5246	Prec@(1,5) (85.8%, 99.2%)
05/27 10:47:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 0.5340	Prec@(1,5) (85.6%, 99.3%)
05/27 10:47:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][350/391]	Step 15640	Loss 0.5337	Prec@(1,5) (85.6%, 99.3%)
05/27 10:47:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 0.5328	Prec@(1,5) (85.6%, 99.2%)
05/27 10:47:16PM searchStage_trainer.py:260 [INFO] Valid: [ 39/49] Final Prec@1 85.5520%
05/27 10:47:16PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:47:16PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.5520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2514, 0.2694, 0.2530, 0.2263],
        [0.2524, 0.2690, 0.2544, 0.2242]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2655, 0.2529, 0.2264],
        [0.2556, 0.2615, 0.2547, 0.2282],
        [0.2615, 0.2778, 0.2392, 0.2214]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2628, 0.2585, 0.2192],
        [0.2599, 0.2664, 0.2365, 0.2372],
        [0.2598, 0.2703, 0.2396, 0.2303]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2651, 0.2429, 0.2368],
        [0.2557, 0.2642, 0.2440, 0.2361],
        [0.2577, 0.2659, 0.2457, 0.2308]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2677, 0.2404, 0.2372],
        [0.2555, 0.2678, 0.2437, 0.2331],
        [0.2578, 0.2651, 0.2443, 0.2328]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2611, 0.2413, 0.2370],
        [0.2569, 0.2608, 0.2466, 0.2357],
        [0.2571, 0.2620, 0.2387, 0.2423]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2517, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2487],
        [0.2507, 0.2512, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2519, 0.2508, 0.2470],
        [0.2505, 0.2512, 0.2464, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2524, 0.2486, 0.2495],
        [0.2503, 0.2521, 0.2491, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2489, 0.2526, 0.2510],
        [0.2472, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2468, 0.2532, 0.2537, 0.2463],
        [0.2468, 0.2499, 0.2512, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2505],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2490, 0.2539],
        [0.2474, 0.2500, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2655, 0.2390],
        [0.2468, 0.2470, 0.2469, 0.2594],
        [0.2463, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2457, 0.2493, 0.2612],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2537, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:47:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][50/390]	Step 15690	lr 0.00329	Loss 0.0867 (0.0718)	Prec@(1,5) (97.7%, 100.0%)	
05/27 10:47:48PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.0126 (0.0750)	Prec@(1,5) (97.5%, 100.0%)	
05/27 10:48:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][150/390]	Step 15790	lr 0.00329	Loss 0.0237 (0.0752)	Prec@(1,5) (97.6%, 100.0%)	
05/27 10:48:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.1432 (0.0771)	Prec@(1,5) (97.4%, 100.0%)	
05/27 10:48:34PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][250/390]	Step 15890	lr 0.00329	Loss 0.0837 (0.0786)	Prec@(1,5) (97.3%, 100.0%)	
05/27 10:48:50PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.0141 (0.0780)	Prec@(1,5) (97.3%, 100.0%)	
05/27 10:49:05PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][350/390]	Step 15990	lr 0.00329	Loss 0.0464 (0.0781)	Prec@(1,5) (97.3%, 100.0%)	
05/27 10:49:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.0477 (0.0775)	Prec@(1,5) (97.4%, 100.0%)	
05/27 10:49:18PM searchStage_trainer.py:221 [INFO] Train: [ 40/49] Final Prec@1 97.3880%
05/27 10:49:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][50/391]	Step 16031	Loss 0.5453	Prec@(1,5) (85.2%, 99.3%)
05/27 10:49:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 0.5467	Prec@(1,5) (85.5%, 99.1%)
05/27 10:49:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][150/391]	Step 16031	Loss 0.5242	Prec@(1,5) (85.9%, 99.2%)
05/27 10:49:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 0.5283	Prec@(1,5) (86.0%, 99.1%)
05/27 10:49:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][250/391]	Step 16031	Loss 0.5292	Prec@(1,5) (86.0%, 99.2%)
05/27 10:49:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 0.5354	Prec@(1,5) (85.8%, 99.1%)
05/27 10:49:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][350/391]	Step 16031	Loss 0.5385	Prec@(1,5) (85.8%, 99.2%)
05/27 10:49:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 0.5402	Prec@(1,5) (85.7%, 99.1%)
05/27 10:49:37PM searchStage_trainer.py:260 [INFO] Valid: [ 40/49] Final Prec@1 85.7080%
05/27 10:49:37PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:49:38PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.7080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2514, 0.2693, 0.2530, 0.2263],
        [0.2524, 0.2690, 0.2544, 0.2243]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2655, 0.2529, 0.2264],
        [0.2556, 0.2615, 0.2547, 0.2282],
        [0.2615, 0.2778, 0.2392, 0.2215]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2595, 0.2628, 0.2585, 0.2193],
        [0.2599, 0.2664, 0.2365, 0.2373],
        [0.2598, 0.2703, 0.2396, 0.2304]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2650, 0.2429, 0.2368],
        [0.2557, 0.2642, 0.2440, 0.2361],
        [0.2577, 0.2658, 0.2457, 0.2308]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2677, 0.2404, 0.2373],
        [0.2555, 0.2677, 0.2437, 0.2331],
        [0.2578, 0.2650, 0.2443, 0.2329]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2611, 0.2413, 0.2371],
        [0.2568, 0.2608, 0.2466, 0.2358],
        [0.2570, 0.2620, 0.2387, 0.2423]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2516, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2518, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2488],
        [0.2507, 0.2512, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2519, 0.2508, 0.2470],
        [0.2505, 0.2512, 0.2464, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2524, 0.2486, 0.2495],
        [0.2503, 0.2521, 0.2492, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2489, 0.2526, 0.2510],
        [0.2472, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2469, 0.2532, 0.2537, 0.2463],
        [0.2469, 0.2499, 0.2512, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2498, 0.2521, 0.2504],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2490, 0.2539],
        [0.2474, 0.2500, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2655, 0.2390],
        [0.2468, 0.2470, 0.2468, 0.2594],
        [0.2463, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2457, 0.2493, 0.2613],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2537, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:49:54PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][50/390]	Step 16081	lr 0.00287	Loss 0.0350 (0.0672)	Prec@(1,5) (97.7%, 100.0%)	
05/27 10:50:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.1031 (0.0713)	Prec@(1,5) (97.7%, 100.0%)	
05/27 10:50:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][150/390]	Step 16181	lr 0.00287	Loss 0.0177 (0.0729)	Prec@(1,5) (97.5%, 100.0%)	
05/27 10:50:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.0517 (0.0733)	Prec@(1,5) (97.5%, 100.0%)	
05/27 10:50:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][250/390]	Step 16281	lr 0.00287	Loss 0.1076 (0.0726)	Prec@(1,5) (97.5%, 100.0%)	
05/27 10:51:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.0362 (0.0713)	Prec@(1,5) (97.6%, 100.0%)	
05/27 10:51:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][350/390]	Step 16381	lr 0.00287	Loss 0.1205 (0.0698)	Prec@(1,5) (97.6%, 100.0%)	
05/27 10:51:39PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.0558 (0.0695)	Prec@(1,5) (97.6%, 100.0%)	
05/27 10:51:39PM searchStage_trainer.py:221 [INFO] Train: [ 41/49] Final Prec@1 97.6360%
05/27 10:51:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][50/391]	Step 16422	Loss 0.5616	Prec@(1,5) (85.5%, 99.2%)
05/27 10:51:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 0.5546	Prec@(1,5) (85.4%, 99.1%)
05/27 10:51:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][150/391]	Step 16422	Loss 0.5450	Prec@(1,5) (85.6%, 99.2%)
05/27 10:51:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 0.5465	Prec@(1,5) (85.6%, 99.2%)
05/27 10:51:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][250/391]	Step 16422	Loss 0.5483	Prec@(1,5) (85.6%, 99.2%)
05/27 10:51:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 0.5453	Prec@(1,5) (85.7%, 99.2%)
05/27 10:51:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][350/391]	Step 16422	Loss 0.5426	Prec@(1,5) (85.8%, 99.2%)
05/27 10:51:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 0.5394	Prec@(1,5) (85.8%, 99.2%)
05/27 10:51:59PM searchStage_trainer.py:260 [INFO] Valid: [ 41/49] Final Prec@1 85.8280%
05/27 10:51:59PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:51:59PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.8280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2514, 0.2693, 0.2530, 0.2263],
        [0.2524, 0.2689, 0.2544, 0.2243]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2654, 0.2529, 0.2264],
        [0.2555, 0.2614, 0.2547, 0.2283],
        [0.2615, 0.2778, 0.2392, 0.2215]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2628, 0.2585, 0.2193],
        [0.2598, 0.2663, 0.2365, 0.2373],
        [0.2598, 0.2702, 0.2396, 0.2304]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2650, 0.2429, 0.2369],
        [0.2557, 0.2642, 0.2440, 0.2361],
        [0.2577, 0.2658, 0.2457, 0.2308]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2676, 0.2404, 0.2373],
        [0.2555, 0.2677, 0.2437, 0.2331],
        [0.2578, 0.2650, 0.2443, 0.2329]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2611, 0.2413, 0.2371],
        [0.2568, 0.2608, 0.2466, 0.2358],
        [0.2570, 0.2619, 0.2387, 0.2423]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2503, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2516, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2517, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2488],
        [0.2507, 0.2512, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2519, 0.2508, 0.2470],
        [0.2505, 0.2511, 0.2464, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2524, 0.2485, 0.2495],
        [0.2503, 0.2521, 0.2492, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2489, 0.2526, 0.2510],
        [0.2472, 0.2524, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2469, 0.2532, 0.2537, 0.2463],
        [0.2469, 0.2499, 0.2511, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2498, 0.2521, 0.2505],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2490, 0.2539],
        [0.2474, 0.2500, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2656, 0.2390],
        [0.2468, 0.2470, 0.2468, 0.2594],
        [0.2463, 0.2452, 0.2592, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2456, 0.2493, 0.2613],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2537, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:52:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][50/390]	Step 16472	lr 0.00248	Loss 0.0526 (0.0530)	Prec@(1,5) (98.3%, 100.0%)	
05/27 10:52:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.0471 (0.0596)	Prec@(1,5) (98.1%, 100.0%)	
05/27 10:52:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][150/390]	Step 16572	lr 0.00248	Loss 0.0640 (0.0601)	Prec@(1,5) (98.1%, 100.0%)	
05/27 10:53:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.0426 (0.0592)	Prec@(1,5) (98.1%, 100.0%)	
05/27 10:53:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][250/390]	Step 16672	lr 0.00248	Loss 0.0768 (0.0580)	Prec@(1,5) (98.1%, 100.0%)	
05/27 10:53:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.0695 (0.0599)	Prec@(1,5) (98.0%, 100.0%)	
05/27 10:53:47PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][350/390]	Step 16772	lr 0.00248	Loss 0.0368 (0.0602)	Prec@(1,5) (98.1%, 100.0%)	
05/27 10:54:00PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.0852 (0.0611)	Prec@(1,5) (98.0%, 100.0%)	
05/27 10:54:00PM searchStage_trainer.py:221 [INFO] Train: [ 42/49] Final Prec@1 98.0040%
05/27 10:54:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][50/391]	Step 16813	Loss 0.5669	Prec@(1,5) (85.2%, 99.4%)
05/27 10:54:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 0.5580	Prec@(1,5) (85.2%, 99.3%)
05/27 10:54:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][150/391]	Step 16813	Loss 0.5628	Prec@(1,5) (85.7%, 99.2%)
05/27 10:54:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 0.5601	Prec@(1,5) (85.8%, 99.3%)
05/27 10:54:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][250/391]	Step 16813	Loss 0.5505	Prec@(1,5) (86.0%, 99.3%)
05/27 10:54:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 0.5396	Prec@(1,5) (86.2%, 99.3%)
05/27 10:54:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][350/391]	Step 16813	Loss 0.5455	Prec@(1,5) (86.1%, 99.2%)
05/27 10:54:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 0.5457	Prec@(1,5) (86.0%, 99.3%)
05/27 10:54:20PM searchStage_trainer.py:260 [INFO] Valid: [ 42/49] Final Prec@1 86.0400%
05/27 10:54:20PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:54:20PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.0400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2514, 0.2693, 0.2530, 0.2264],
        [0.2524, 0.2689, 0.2544, 0.2243]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2654, 0.2529, 0.2264],
        [0.2555, 0.2614, 0.2548, 0.2283],
        [0.2615, 0.2777, 0.2392, 0.2215]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2628, 0.2585, 0.2193],
        [0.2598, 0.2663, 0.2365, 0.2373],
        [0.2598, 0.2702, 0.2396, 0.2304]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2650, 0.2429, 0.2369],
        [0.2557, 0.2641, 0.2440, 0.2362],
        [0.2576, 0.2658, 0.2457, 0.2309]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2676, 0.2404, 0.2373],
        [0.2555, 0.2677, 0.2437, 0.2332],
        [0.2578, 0.2650, 0.2443, 0.2330]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2611, 0.2413, 0.2371],
        [0.2568, 0.2607, 0.2466, 0.2358],
        [0.2570, 0.2619, 0.2387, 0.2424]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2504, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2516, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2534, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2517, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2488],
        [0.2507, 0.2512, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2519, 0.2508, 0.2470],
        [0.2505, 0.2511, 0.2464, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2524, 0.2485, 0.2495],
        [0.2503, 0.2521, 0.2492, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2489, 0.2526, 0.2510],
        [0.2472, 0.2524, 0.2522, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2518, 0.2526, 0.2495],
        [0.2469, 0.2532, 0.2537, 0.2463],
        [0.2469, 0.2499, 0.2511, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2498, 0.2521, 0.2505],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2522, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2489, 0.2539],
        [0.2474, 0.2500, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2656, 0.2389],
        [0.2468, 0.2470, 0.2468, 0.2594],
        [0.2463, 0.2452, 0.2591, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2456, 0.2493, 0.2613],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2538, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:54:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][50/390]	Step 16863	lr 0.00214	Loss 0.0862 (0.0561)	Prec@(1,5) (98.3%, 99.9%)	
05/27 10:54:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.0692 (0.0493)	Prec@(1,5) (98.4%, 100.0%)	
05/27 10:55:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][150/390]	Step 16963	lr 0.00214	Loss 0.1077 (0.0492)	Prec@(1,5) (98.5%, 100.0%)	
05/27 10:55:22PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.0334 (0.0501)	Prec@(1,5) (98.4%, 100.0%)	
05/27 10:55:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][250/390]	Step 17063	lr 0.00214	Loss 0.1928 (0.0512)	Prec@(1,5) (98.4%, 100.0%)	
05/27 10:55:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.0297 (0.0505)	Prec@(1,5) (98.3%, 100.0%)	
05/27 10:56:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][350/390]	Step 17163	lr 0.00214	Loss 0.0426 (0.0508)	Prec@(1,5) (98.3%, 100.0%)	
05/27 10:56:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.0335 (0.0517)	Prec@(1,5) (98.3%, 100.0%)	
05/27 10:56:21PM searchStage_trainer.py:221 [INFO] Train: [ 43/49] Final Prec@1 98.2720%
05/27 10:56:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][50/391]	Step 17204	Loss 0.5405	Prec@(1,5) (86.4%, 99.2%)
05/27 10:56:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 0.5474	Prec@(1,5) (86.2%, 99.3%)
05/27 10:56:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][150/391]	Step 17204	Loss 0.5519	Prec@(1,5) (85.8%, 99.3%)
05/27 10:56:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 0.5411	Prec@(1,5) (86.1%, 99.3%)
05/27 10:56:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][250/391]	Step 17204	Loss 0.5484	Prec@(1,5) (85.9%, 99.3%)
05/27 10:56:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 0.5497	Prec@(1,5) (85.9%, 99.3%)
05/27 10:56:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][350/391]	Step 17204	Loss 0.5514	Prec@(1,5) (86.0%, 99.2%)
05/27 10:56:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 0.5496	Prec@(1,5) (86.0%, 99.2%)
05/27 10:56:41PM searchStage_trainer.py:260 [INFO] Valid: [ 43/49] Final Prec@1 86.0080%
05/27 10:56:41PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:56:41PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.0400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2513, 0.2692, 0.2530, 0.2264],
        [0.2523, 0.2689, 0.2544, 0.2244]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2654, 0.2529, 0.2265],
        [0.2555, 0.2614, 0.2548, 0.2283],
        [0.2615, 0.2777, 0.2393, 0.2216]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2627, 0.2585, 0.2194],
        [0.2598, 0.2663, 0.2365, 0.2373],
        [0.2598, 0.2702, 0.2396, 0.2305]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2650, 0.2429, 0.2369],
        [0.2557, 0.2641, 0.2440, 0.2362],
        [0.2576, 0.2658, 0.2457, 0.2309]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2676, 0.2404, 0.2374],
        [0.2554, 0.2676, 0.2437, 0.2332],
        [0.2578, 0.2650, 0.2443, 0.2330]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2611, 0.2413, 0.2372],
        [0.2568, 0.2607, 0.2466, 0.2358],
        [0.2570, 0.2618, 0.2387, 0.2424]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2526, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2504, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2517, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2488],
        [0.2506, 0.2533, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2517, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2488],
        [0.2507, 0.2511, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2469],
        [0.2503, 0.2519, 0.2508, 0.2470],
        [0.2505, 0.2511, 0.2464, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2524, 0.2485, 0.2495],
        [0.2503, 0.2521, 0.2492, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2489, 0.2526, 0.2510],
        [0.2472, 0.2524, 0.2522, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2518, 0.2526, 0.2495],
        [0.2469, 0.2532, 0.2537, 0.2463],
        [0.2469, 0.2499, 0.2511, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2498, 0.2521, 0.2505],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2521, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2489, 0.2539],
        [0.2474, 0.2500, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2656, 0.2389],
        [0.2468, 0.2470, 0.2468, 0.2594],
        [0.2463, 0.2451, 0.2591, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2456, 0.2493, 0.2613],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2538, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:56:57PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][50/390]	Step 17254	lr 0.00184	Loss 0.0481 (0.0469)	Prec@(1,5) (98.5%, 100.0%)	
05/27 10:57:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.1241 (0.0503)	Prec@(1,5) (98.5%, 100.0%)	
05/27 10:57:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][150/390]	Step 17354	lr 0.00184	Loss 0.0189 (0.0495)	Prec@(1,5) (98.5%, 100.0%)	
05/27 10:57:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.0121 (0.0473)	Prec@(1,5) (98.6%, 100.0%)	
05/27 10:57:59PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][250/390]	Step 17454	lr 0.00184	Loss 0.1192 (0.0480)	Prec@(1,5) (98.6%, 100.0%)	
05/27 10:58:14PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.1039 (0.0490)	Prec@(1,5) (98.5%, 100.0%)	
05/27 10:58:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][350/390]	Step 17554	lr 0.00184	Loss 0.1044 (0.0490)	Prec@(1,5) (98.5%, 100.0%)	
05/27 10:58:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.0772 (0.0491)	Prec@(1,5) (98.5%, 100.0%)	
05/27 10:58:42PM searchStage_trainer.py:221 [INFO] Train: [ 44/49] Final Prec@1 98.5120%
05/27 10:58:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][50/391]	Step 17595	Loss 0.5517	Prec@(1,5) (86.1%, 99.1%)
05/27 10:58:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 0.5619	Prec@(1,5) (86.0%, 99.1%)
05/27 10:58:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][150/391]	Step 17595	Loss 0.5565	Prec@(1,5) (86.1%, 99.1%)
05/27 10:58:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 0.5486	Prec@(1,5) (86.0%, 99.2%)
05/27 10:58:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][250/391]	Step 17595	Loss 0.5530	Prec@(1,5) (85.9%, 99.1%)
05/27 10:58:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 0.5456	Prec@(1,5) (86.1%, 99.2%)
05/27 10:59:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][350/391]	Step 17595	Loss 0.5456	Prec@(1,5) (86.1%, 99.2%)
05/27 10:59:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 0.5506	Prec@(1,5) (86.1%, 99.2%)
05/27 10:59:02PM searchStage_trainer.py:260 [INFO] Valid: [ 44/49] Final Prec@1 86.1040%
05/27 10:59:02PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 10:59:02PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.1040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2513, 0.2692, 0.2530, 0.2264],
        [0.2523, 0.2689, 0.2544, 0.2244]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2654, 0.2529, 0.2265],
        [0.2555, 0.2614, 0.2548, 0.2283],
        [0.2615, 0.2777, 0.2393, 0.2216]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2628, 0.2585, 0.2193],
        [0.2598, 0.2663, 0.2365, 0.2373],
        [0.2598, 0.2701, 0.2396, 0.2305]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2650, 0.2429, 0.2369],
        [0.2557, 0.2641, 0.2441, 0.2362],
        [0.2576, 0.2657, 0.2457, 0.2309]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2676, 0.2404, 0.2374],
        [0.2554, 0.2676, 0.2437, 0.2332],
        [0.2578, 0.2649, 0.2443, 0.2330]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2610, 0.2413, 0.2372],
        [0.2568, 0.2607, 0.2466, 0.2359],
        [0.2570, 0.2618, 0.2387, 0.2425]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2525, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2504, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2517, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2489],
        [0.2506, 0.2533, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2517, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2488],
        [0.2507, 0.2511, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2534, 0.2487, 0.2469],
        [0.2503, 0.2519, 0.2508, 0.2470],
        [0.2505, 0.2511, 0.2464, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2524, 0.2485, 0.2495],
        [0.2503, 0.2520, 0.2492, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2489, 0.2525, 0.2510],
        [0.2472, 0.2524, 0.2522, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2518, 0.2526, 0.2495],
        [0.2469, 0.2532, 0.2537, 0.2463],
        [0.2469, 0.2499, 0.2511, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2498, 0.2521, 0.2505],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2521, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2489, 0.2539],
        [0.2474, 0.2500, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2656, 0.2389],
        [0.2468, 0.2470, 0.2468, 0.2594],
        [0.2463, 0.2451, 0.2591, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2456, 0.2493, 0.2613],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2538, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 10:59:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][50/390]	Step 17645	lr 0.00159	Loss 0.0839 (0.0397)	Prec@(1,5) (98.9%, 100.0%)	
05/27 10:59:33PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.0211 (0.0404)	Prec@(1,5) (99.0%, 100.0%)	
05/27 10:59:49PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][150/390]	Step 17745	lr 0.00159	Loss 0.0485 (0.0424)	Prec@(1,5) (98.8%, 100.0%)	
05/27 11:00:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.0447 (0.0415)	Prec@(1,5) (98.8%, 100.0%)	
05/27 11:00:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][250/390]	Step 17845	lr 0.00159	Loss 0.0332 (0.0419)	Prec@(1,5) (98.8%, 100.0%)	
05/27 11:00:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.0473 (0.0423)	Prec@(1,5) (98.8%, 100.0%)	
05/27 11:00:50PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][350/390]	Step 17945	lr 0.00159	Loss 0.0096 (0.0420)	Prec@(1,5) (98.8%, 100.0%)	
05/27 11:01:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.0310 (0.0414)	Prec@(1,5) (98.8%, 100.0%)	
05/27 11:01:03PM searchStage_trainer.py:221 [INFO] Train: [ 45/49] Final Prec@1 98.8080%
05/27 11:01:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][50/391]	Step 17986	Loss 0.5231	Prec@(1,5) (87.0%, 99.5%)
05/27 11:01:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 0.5452	Prec@(1,5) (87.0%, 99.3%)
05/27 11:01:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][150/391]	Step 17986	Loss 0.5582	Prec@(1,5) (86.4%, 99.2%)
05/27 11:01:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 0.5547	Prec@(1,5) (86.4%, 99.2%)
05/27 11:01:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][250/391]	Step 17986	Loss 0.5491	Prec@(1,5) (86.4%, 99.2%)
05/27 11:01:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 0.5559	Prec@(1,5) (86.3%, 99.2%)
05/27 11:01:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][350/391]	Step 17986	Loss 0.5503	Prec@(1,5) (86.3%, 99.2%)
05/27 11:01:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 0.5478	Prec@(1,5) (86.4%, 99.2%)
05/27 11:01:23PM searchStage_trainer.py:260 [INFO] Valid: [ 45/49] Final Prec@1 86.3880%
05/27 11:01:23PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 11:01:23PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.3880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2513, 0.2692, 0.2530, 0.2265],
        [0.2523, 0.2688, 0.2545, 0.2244]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2653, 0.2530, 0.2265],
        [0.2555, 0.2614, 0.2548, 0.2284],
        [0.2615, 0.2776, 0.2393, 0.2216]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2628, 0.2585, 0.2193],
        [0.2598, 0.2663, 0.2366, 0.2374],
        [0.2597, 0.2701, 0.2396, 0.2306]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2649, 0.2429, 0.2370],
        [0.2556, 0.2641, 0.2441, 0.2362],
        [0.2576, 0.2657, 0.2457, 0.2310]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2675, 0.2404, 0.2374],
        [0.2554, 0.2676, 0.2437, 0.2332],
        [0.2578, 0.2649, 0.2443, 0.2330]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2610, 0.2413, 0.2372],
        [0.2568, 0.2607, 0.2467, 0.2359],
        [0.2570, 0.2618, 0.2387, 0.2425]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2525, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2504, 0.2465],
        [0.2516, 0.2537, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2517, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2489],
        [0.2506, 0.2533, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2517, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2488],
        [0.2507, 0.2511, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2519, 0.2508, 0.2470],
        [0.2505, 0.2511, 0.2464, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2524, 0.2485, 0.2495],
        [0.2503, 0.2521, 0.2492, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2489, 0.2525, 0.2510],
        [0.2472, 0.2524, 0.2522, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2518, 0.2526, 0.2495],
        [0.2469, 0.2532, 0.2537, 0.2463],
        [0.2469, 0.2499, 0.2511, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2498, 0.2521, 0.2505],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2521, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2486, 0.2489, 0.2540],
        [0.2474, 0.2499, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2657, 0.2389],
        [0.2468, 0.2470, 0.2468, 0.2594],
        [0.2463, 0.2451, 0.2591, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2456, 0.2493, 0.2613],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2538, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 11:01:39PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][50/390]	Step 18036	lr 0.00138	Loss 0.0145 (0.0326)	Prec@(1,5) (99.1%, 100.0%)	
05/27 11:01:54PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.0325 (0.0315)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:02:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][150/390]	Step 18136	lr 0.00138	Loss 0.0551 (0.0333)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:02:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.0438 (0.0355)	Prec@(1,5) (98.9%, 100.0%)	
05/27 11:02:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][250/390]	Step 18236	lr 0.00138	Loss 0.0254 (0.0349)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:02:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.0869 (0.0347)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:03:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][350/390]	Step 18336	lr 0.00138	Loss 0.0169 (0.0354)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:03:23PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.0426 (0.0349)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:03:23PM searchStage_trainer.py:221 [INFO] Train: [ 46/49] Final Prec@1 98.9960%
05/27 11:03:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][50/391]	Step 18377	Loss 0.5731	Prec@(1,5) (85.4%, 99.1%)
05/27 11:03:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 0.5661	Prec@(1,5) (85.6%, 99.2%)
05/27 11:03:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][150/391]	Step 18377	Loss 0.5581	Prec@(1,5) (86.0%, 99.2%)
05/27 11:03:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 0.5602	Prec@(1,5) (86.1%, 99.1%)
05/27 11:03:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][250/391]	Step 18377	Loss 0.5647	Prec@(1,5) (86.0%, 99.2%)
05/27 11:03:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 0.5611	Prec@(1,5) (86.1%, 99.2%)
05/27 11:03:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][350/391]	Step 18377	Loss 0.5583	Prec@(1,5) (86.1%, 99.2%)
05/27 11:03:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 0.5557	Prec@(1,5) (86.2%, 99.2%)
05/27 11:03:43PM searchStage_trainer.py:260 [INFO] Valid: [ 46/49] Final Prec@1 86.2080%
05/27 11:03:43PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 11:03:43PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.3880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2513, 0.2692, 0.2531, 0.2265],
        [0.2523, 0.2688, 0.2545, 0.2244]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2653, 0.2530, 0.2265],
        [0.2555, 0.2613, 0.2548, 0.2284],
        [0.2615, 0.2776, 0.2393, 0.2216]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2628, 0.2585, 0.2194],
        [0.2598, 0.2662, 0.2366, 0.2374],
        [0.2597, 0.2701, 0.2396, 0.2306]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2649, 0.2429, 0.2370],
        [0.2556, 0.2640, 0.2441, 0.2363],
        [0.2576, 0.2657, 0.2457, 0.2310]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2675, 0.2404, 0.2374],
        [0.2554, 0.2676, 0.2438, 0.2333],
        [0.2577, 0.2649, 0.2443, 0.2331]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2610, 0.2413, 0.2372],
        [0.2568, 0.2607, 0.2467, 0.2359],
        [0.2570, 0.2617, 0.2388, 0.2425]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2525, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2504, 0.2465],
        [0.2516, 0.2536, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2517, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2489],
        [0.2506, 0.2533, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2517, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2488],
        [0.2507, 0.2511, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2519, 0.2508, 0.2470],
        [0.2505, 0.2511, 0.2464, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2524, 0.2485, 0.2495],
        [0.2503, 0.2520, 0.2492, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2489, 0.2525, 0.2510],
        [0.2472, 0.2524, 0.2522, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2518, 0.2526, 0.2495],
        [0.2469, 0.2532, 0.2537, 0.2463],
        [0.2469, 0.2499, 0.2511, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2498, 0.2521, 0.2505],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2481, 0.2508, 0.2521, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2489, 0.2540],
        [0.2474, 0.2499, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2657, 0.2389],
        [0.2468, 0.2470, 0.2468, 0.2594],
        [0.2463, 0.2451, 0.2591, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2456, 0.2493, 0.2613],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2538, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 11:03:59PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][50/390]	Step 18427	lr 0.00121	Loss 0.0372 (0.0326)	Prec@(1,5) (99.1%, 100.0%)	
05/27 11:04:14PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.0138 (0.0343)	Prec@(1,5) (98.9%, 100.0%)	
05/27 11:04:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][150/390]	Step 18527	lr 0.00121	Loss 0.0104 (0.0346)	Prec@(1,5) (98.9%, 100.0%)	
05/27 11:04:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.1081 (0.0339)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:05:00PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][250/390]	Step 18627	lr 0.00121	Loss 0.0507 (0.0338)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:05:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.0418 (0.0335)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:05:31PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][350/390]	Step 18727	lr 0.00121	Loss 0.0723 (0.0336)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:05:43PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.0057 (0.0335)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:05:43PM searchStage_trainer.py:221 [INFO] Train: [ 47/49] Final Prec@1 98.9840%
05/27 11:05:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][50/391]	Step 18768	Loss 0.5479	Prec@(1,5) (86.9%, 99.3%)
05/27 11:05:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 0.5595	Prec@(1,5) (86.5%, 99.3%)
05/27 11:05:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][150/391]	Step 18768	Loss 0.5631	Prec@(1,5) (86.6%, 99.2%)
05/27 11:05:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 0.5625	Prec@(1,5) (86.6%, 99.3%)
05/27 11:05:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][250/391]	Step 18768	Loss 0.5603	Prec@(1,5) (86.6%, 99.3%)
05/27 11:05:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 0.5714	Prec@(1,5) (86.4%, 99.2%)
05/27 11:06:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][350/391]	Step 18768	Loss 0.5658	Prec@(1,5) (86.4%, 99.2%)
05/27 11:06:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 0.5760	Prec@(1,5) (86.2%, 99.2%)
05/27 11:06:03PM searchStage_trainer.py:260 [INFO] Valid: [ 47/49] Final Prec@1 86.2160%
05/27 11:06:03PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 11:06:03PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.3880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2513, 0.2691, 0.2531, 0.2265],
        [0.2523, 0.2688, 0.2545, 0.2245]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2653, 0.2530, 0.2265],
        [0.2555, 0.2613, 0.2548, 0.2284],
        [0.2614, 0.2776, 0.2393, 0.2217]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2627, 0.2585, 0.2194],
        [0.2598, 0.2662, 0.2366, 0.2374],
        [0.2597, 0.2701, 0.2396, 0.2306]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2649, 0.2429, 0.2370],
        [0.2556, 0.2640, 0.2441, 0.2363],
        [0.2576, 0.2656, 0.2458, 0.2310]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2675, 0.2404, 0.2375],
        [0.2554, 0.2675, 0.2438, 0.2333],
        [0.2577, 0.2648, 0.2443, 0.2331]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2610, 0.2413, 0.2373],
        [0.2568, 0.2606, 0.2467, 0.2359],
        [0.2570, 0.2617, 0.2388, 0.2426]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2525, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2504, 0.2465],
        [0.2516, 0.2536, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2517, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2489],
        [0.2506, 0.2533, 0.2497, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2517, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2488],
        [0.2507, 0.2511, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2519, 0.2508, 0.2470],
        [0.2505, 0.2511, 0.2464, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2524, 0.2485, 0.2495],
        [0.2503, 0.2520, 0.2492, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2489, 0.2525, 0.2510],
        [0.2472, 0.2524, 0.2522, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2518, 0.2526, 0.2495],
        [0.2469, 0.2532, 0.2537, 0.2463],
        [0.2469, 0.2499, 0.2511, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2498, 0.2521, 0.2505],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2482, 0.2508, 0.2521, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2489, 0.2540],
        [0.2474, 0.2499, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2657, 0.2389],
        [0.2468, 0.2470, 0.2468, 0.2594],
        [0.2463, 0.2451, 0.2591, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2456, 0.2493, 0.2613],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2538, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 11:06:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][50/390]	Step 18818	lr 0.00109	Loss 0.0104 (0.0316)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:06:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.0281 (0.0298)	Prec@(1,5) (99.1%, 100.0%)	
05/27 11:06:50PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][150/390]	Step 18918	lr 0.00109	Loss 0.0238 (0.0336)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:07:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.0701 (0.0327)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:07:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][250/390]	Step 19018	lr 0.00109	Loss 0.0282 (0.0325)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:07:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.0218 (0.0329)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:07:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][350/390]	Step 19118	lr 0.00109	Loss 0.0167 (0.0323)	Prec@(1,5) (99.1%, 100.0%)	
05/27 11:08:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.0377 (0.0330)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:08:04PM searchStage_trainer.py:221 [INFO] Train: [ 48/49] Final Prec@1 99.0040%
05/27 11:08:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][50/391]	Step 19159	Loss 0.5727	Prec@(1,5) (86.2%, 99.4%)
05/27 11:08:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 0.5925	Prec@(1,5) (86.3%, 99.3%)
05/27 11:08:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][150/391]	Step 19159	Loss 0.5735	Prec@(1,5) (86.7%, 99.3%)
05/27 11:08:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 0.5729	Prec@(1,5) (86.7%, 99.3%)
05/27 11:08:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][250/391]	Step 19159	Loss 0.5670	Prec@(1,5) (86.8%, 99.3%)
05/27 11:08:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 0.5766	Prec@(1,5) (86.7%, 99.2%)
05/27 11:08:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][350/391]	Step 19159	Loss 0.5767	Prec@(1,5) (86.6%, 99.3%)
05/27 11:08:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 0.5773	Prec@(1,5) (86.6%, 99.2%)
05/27 11:08:24PM searchStage_trainer.py:260 [INFO] Valid: [ 48/49] Final Prec@1 86.5840%
05/27 11:08:24PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 11:08:25PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.5840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2513, 0.2691, 0.2531, 0.2265],
        [0.2523, 0.2688, 0.2545, 0.2245]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2653, 0.2530, 0.2266],
        [0.2555, 0.2613, 0.2548, 0.2284],
        [0.2614, 0.2776, 0.2393, 0.2217]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2627, 0.2585, 0.2194],
        [0.2598, 0.2662, 0.2366, 0.2375],
        [0.2597, 0.2700, 0.2396, 0.2306]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2649, 0.2429, 0.2370],
        [0.2556, 0.2640, 0.2441, 0.2363],
        [0.2576, 0.2656, 0.2458, 0.2311]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2675, 0.2404, 0.2375],
        [0.2554, 0.2675, 0.2438, 0.2333],
        [0.2577, 0.2648, 0.2443, 0.2331]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2610, 0.2413, 0.2373],
        [0.2568, 0.2606, 0.2467, 0.2360],
        [0.2569, 0.2617, 0.2388, 0.2426]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2517, 0.2508, 0.2486],
        [0.2500, 0.2525, 0.2515, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2526, 0.2515, 0.2474],
        [0.2510, 0.2522, 0.2504, 0.2465],
        [0.2516, 0.2536, 0.2484, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2517, 0.2523, 0.2471],
        [0.2487, 0.2531, 0.2494, 0.2489],
        [0.2506, 0.2533, 0.2497, 0.2464]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2517, 0.2485, 0.2495],
        [0.2498, 0.2520, 0.2494, 0.2488],
        [0.2507, 0.2511, 0.2499, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2533, 0.2487, 0.2470],
        [0.2503, 0.2519, 0.2508, 0.2470],
        [0.2505, 0.2511, 0.2464, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2524, 0.2485, 0.2495],
        [0.2503, 0.2520, 0.2492, 0.2485],
        [0.2501, 0.2517, 0.2496, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2489, 0.2525, 0.2510],
        [0.2472, 0.2524, 0.2522, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2518, 0.2525, 0.2495],
        [0.2469, 0.2532, 0.2537, 0.2463],
        [0.2469, 0.2499, 0.2511, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2498, 0.2521, 0.2505],
        [0.2437, 0.2513, 0.2552, 0.2498],
        [0.2482, 0.2508, 0.2521, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2489, 0.2540],
        [0.2474, 0.2500, 0.2549, 0.2477],
        [0.2476, 0.2518, 0.2523, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2498, 0.2657, 0.2389],
        [0.2468, 0.2470, 0.2468, 0.2594],
        [0.2463, 0.2451, 0.2591, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.2456, 0.2493, 0.2614],
        [0.2502, 0.2506, 0.2486, 0.2506],
        [0.2422, 0.2538, 0.2624, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 11:08:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][50/390]	Step 19209	lr 0.00102	Loss 0.0128 (0.0329)	Prec@(1,5) (98.9%, 100.0%)	
05/27 11:08:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.0053 (0.0317)	Prec@(1,5) (99.0%, 100.0%)	
05/27 11:09:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][150/390]	Step 19309	lr 0.00102	Loss 0.0834 (0.0306)	Prec@(1,5) (99.1%, 100.0%)	
05/27 11:09:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.0035 (0.0304)	Prec@(1,5) (99.2%, 100.0%)	
05/27 11:09:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][250/390]	Step 19409	lr 0.00102	Loss 0.0280 (0.0307)	Prec@(1,5) (99.1%, 100.0%)	
05/27 11:09:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.0376 (0.0305)	Prec@(1,5) (99.1%, 100.0%)	
05/27 11:10:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][350/390]	Step 19509	lr 0.00102	Loss 0.0236 (0.0299)	Prec@(1,5) (99.1%, 100.0%)	
05/27 11:10:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.0186 (0.0300)	Prec@(1,5) (99.1%, 100.0%)	
05/27 11:10:26PM searchStage_trainer.py:221 [INFO] Train: [ 49/49] Final Prec@1 99.1160%
05/27 11:10:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][50/391]	Step 19550	Loss 0.5763	Prec@(1,5) (86.0%, 99.2%)
05/27 11:10:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 0.5869	Prec@(1,5) (86.2%, 99.2%)
05/27 11:10:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][150/391]	Step 19550	Loss 0.5761	Prec@(1,5) (86.5%, 99.2%)
05/27 11:10:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 0.5664	Prec@(1,5) (86.6%, 99.2%)
05/27 11:10:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][250/391]	Step 19550	Loss 0.5748	Prec@(1,5) (86.4%, 99.2%)
05/27 11:10:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 0.5802	Prec@(1,5) (86.3%, 99.2%)
05/27 11:10:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][350/391]	Step 19550	Loss 0.5741	Prec@(1,5) (86.2%, 99.2%)
05/27 11:10:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 0.5787	Prec@(1,5) (86.2%, 99.2%)
05/27 11:10:45PM searchStage_trainer.py:260 [INFO] Valid: [ 49/49] Final Prec@1 86.2240%
05/27 11:10:45PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 11:10:45PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.5840%
05/27 11:10:46PM searchStage_main.py:73 [INFO] Final best Prec@1 = 86.5840%
05/27 11:10:46PM searchStage_main.py:74 [INFO] Final Best Genotype = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 1)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('avg_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
