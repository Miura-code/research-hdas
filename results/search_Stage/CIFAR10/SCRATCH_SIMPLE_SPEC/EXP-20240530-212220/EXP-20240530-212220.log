05/30 09:22:20PM parser.py:28 [INFO] 
05/30 09:22:20PM parser.py:29 [INFO] Parameters:
05/30 09:22:20PM parser.py:31 [INFO] DAG_PATH=results/search_Stage/CIFAR10/SCRATCH_SIMPLE_SPEC/EXP-20240530-212220/DAG
05/30 09:22:20PM parser.py:31 [INFO] ALPHA_LR=0.0003
05/30 09:22:20PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
05/30 09:22:20PM parser.py:31 [INFO] BATCH_SIZE=64
05/30 09:22:20PM parser.py:31 [INFO] CHECKPOINT_RESET=False
05/30 09:22:20PM parser.py:31 [INFO] DATA_PATH=../data/
05/30 09:22:20PM parser.py:31 [INFO] DATASET=CIFAR10
05/30 09:22:20PM parser.py:31 [INFO] EPOCHS=50
05/30 09:22:20PM parser.py:31 [INFO] EXP_NAME=EXP-20240530-212220
05/30 09:22:20PM parser.py:31 [INFO] GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 3)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('none', 1), ('avg_pool_3x3', 2)], [('skip_connect', 2), ('sep_conv_5x5', 3)], [('none', 3), ('avg_pool_3x3', 4)]], reduce_concat=range(2, 6))
05/30 09:22:20PM parser.py:31 [INFO] GPUS=[1]
05/30 09:22:20PM parser.py:31 [INFO] INIT_CHANNELS=16
05/30 09:22:20PM parser.py:31 [INFO] LAYERS=20
05/30 09:22:20PM parser.py:31 [INFO] LOCAL_RANK=0
05/30 09:22:20PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/30 09:22:20PM parser.py:31 [INFO] NAME=SCRATCH_SIMPLE_SPEC
05/30 09:22:20PM parser.py:31 [INFO] PATH=results/search_Stage/CIFAR10/SCRATCH_SIMPLE_SPEC/EXP-20240530-212220
05/30 09:22:20PM parser.py:31 [INFO] PRINT_FREQ=50
05/30 09:22:20PM parser.py:31 [INFO] RESUME_PATH=None
05/30 09:22:20PM parser.py:31 [INFO] SAVE=EXP
05/30 09:22:20PM parser.py:31 [INFO] SEED=1
05/30 09:22:20PM parser.py:31 [INFO] SHARE_STAGE=False
05/30 09:22:20PM parser.py:31 [INFO] SPEC_CELL=False
05/30 09:22:20PM parser.py:31 [INFO] TRAIN_PORTION=0.5
05/30 09:22:20PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
05/30 09:22:20PM parser.py:31 [INFO] W_LR=0.025
05/30 09:22:20PM parser.py:31 [INFO] W_LR_MIN=0.001
05/30 09:22:20PM parser.py:31 [INFO] W_MOMENTUM=0.9
05/30 09:22:20PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
05/30 09:22:20PM parser.py:31 [INFO] WORKERS=4
05/30 09:22:20PM parser.py:32 [INFO] 
05/30 09:22:22PM searchStage_trainer.py:103 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2501, 0.2500, 0.2499, 0.2501],
        [0.2500, 0.2501, 0.2497, 0.2502]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2499, 0.2501],
        [0.2500, 0.2498, 0.2501, 0.2501],
        [0.2501, 0.2500, 0.2498, 0.2501]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2494, 0.2503, 0.2496],
        [0.2498, 0.2496, 0.2501, 0.2504],
        [0.2502, 0.2498, 0.2498, 0.2502]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2495, 0.2503, 0.2501],
        [0.2502, 0.2499, 0.2498, 0.2501],
        [0.2500, 0.2501, 0.2498, 0.2501]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2502, 0.2501, 0.2498],
        [0.2499, 0.2502, 0.2500, 0.2499],
        [0.2497, 0.2503, 0.2501, 0.2499]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2498, 0.2501, 0.2500],
        [0.2495, 0.2498, 0.2503, 0.2503],
        [0.2499, 0.2503, 0.2497, 0.2501]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2499, 0.2501, 0.2502],
        [0.2500, 0.2503, 0.2496, 0.2501]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2500, 0.2498],
        [0.2501, 0.2499, 0.2502, 0.2498],
        [0.2504, 0.2499, 0.2502, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2498, 0.2503, 0.2500],
        [0.2498, 0.2502, 0.2502, 0.2498],
        [0.2502, 0.2502, 0.2502, 0.2494]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2499, 0.2497, 0.2502],
        [0.2500, 0.2498, 0.2499, 0.2503],
        [0.2502, 0.2498, 0.2503, 0.2497]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2500, 0.2501],
        [0.2501, 0.2498, 0.2500, 0.2502],
        [0.2500, 0.2497, 0.2503, 0.2500]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2501, 0.2500, 0.2501],
        [0.2499, 0.2505, 0.2498, 0.2498],
        [0.2502, 0.2500, 0.2500, 0.2499]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2499, 0.2499, 0.2500],
        [0.2501, 0.2502, 0.2499, 0.2497]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2499, 0.2501],
        [0.2499, 0.2504, 0.2494, 0.2504],
        [0.2499, 0.2501, 0.2501, 0.2499]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2503, 0.2498, 0.2499],
        [0.2501, 0.2503, 0.2499, 0.2497],
        [0.2500, 0.2500, 0.2499, 0.2500]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2503, 0.2498],
        [0.2498, 0.2502, 0.2498, 0.2502],
        [0.2499, 0.2500, 0.2505, 0.2497]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2500, 0.2500],
        [0.2499, 0.2499, 0.2500, 0.2501],
        [0.2499, 0.2500, 0.2500, 0.2501]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2500, 0.2500, 0.2503],
        [0.2495, 0.2498, 0.2503, 0.2504],
        [0.2498, 0.2498, 0.2501, 0.2503]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:22:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 2.2157 (2.2955)	Prec@(1,5) (16.1%, 64.0%)	
05/30 09:23:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 2.1002 (2.2189)	Prec@(1,5) (18.8%, 69.4%)	
05/30 09:23:48PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 1.9663 (2.1399)	Prec@(1,5) (20.9%, 72.9%)	
05/30 09:24:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 1.7989 (2.0868)	Prec@(1,5) (22.8%, 75.1%)	
05/30 09:24:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 1.8667 (2.0344)	Prec@(1,5) (24.4%, 77.1%)	
05/30 09:25:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 1.7302 (1.9975)	Prec@(1,5) (25.6%, 78.6%)	
05/30 09:25:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 1.5659 (1.9542)	Prec@(1,5) (27.0%, 80.0%)	
05/30 09:26:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 1.5980 (1.9271)	Prec@(1,5) (28.0%, 80.9%)	
05/30 09:26:04PM searchStage_trainer.py:221 [INFO] Train: [  0/49] Final Prec@1 28.0120%
05/30 09:26:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 1.7517	Prec@(1,5) (32.9%, 87.0%)
05/30 09:26:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 1.7622	Prec@(1,5) (32.9%, 86.8%)
05/30 09:26:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 1.7566	Prec@(1,5) (33.3%, 86.7%)
05/30 09:26:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 1.7550	Prec@(1,5) (33.5%, 86.7%)
05/30 09:26:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 1.7533	Prec@(1,5) (33.6%, 86.7%)
05/30 09:26:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 1.7537	Prec@(1,5) (33.7%, 86.6%)
05/30 09:26:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 1.7529	Prec@(1,5) (33.6%, 86.7%)
05/30 09:26:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 1.7555	Prec@(1,5) (33.5%, 86.6%)
05/30 09:26:41PM searchStage_trainer.py:260 [INFO] Valid: [  0/49] Final Prec@1 33.4320%
05/30 09:26:41PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 4)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('max_pool_3x3', 4), ('max_pool_3x3', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 5)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:26:41PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 33.4320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2486, 0.2522, 0.2533, 0.2459],
        [0.2503, 0.2527, 0.2509, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2524, 0.2540, 0.2472],
        [0.2518, 0.2553, 0.2533, 0.2395],
        [0.2508, 0.2586, 0.2433, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2526, 0.2520, 0.2426],
        [0.2535, 0.2581, 0.2502, 0.2382],
        [0.2533, 0.2561, 0.2419, 0.2487]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2551, 0.2484, 0.2431],
        [0.2507, 0.2581, 0.2458, 0.2453],
        [0.2526, 0.2574, 0.2463, 0.2438]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2542, 0.2432, 0.2471],
        [0.2546, 0.2580, 0.2464, 0.2410],
        [0.2549, 0.2601, 0.2449, 0.2401]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2555, 0.2490, 0.2429],
        [0.2499, 0.2562, 0.2462, 0.2478],
        [0.2517, 0.2572, 0.2456, 0.2456]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2512, 0.2498],
        [0.2496, 0.2511, 0.2504, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2497, 0.2505, 0.2493],
        [0.2520, 0.2500, 0.2505, 0.2475],
        [0.2510, 0.2511, 0.2487, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2501, 0.2506, 0.2496],
        [0.2503, 0.2519, 0.2503, 0.2475],
        [0.2508, 0.2510, 0.2484, 0.2498]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2502, 0.2493, 0.2494],
        [0.2507, 0.2504, 0.2494, 0.2495],
        [0.2508, 0.2501, 0.2499, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2503, 0.2495, 0.2488],
        [0.2513, 0.2503, 0.2492, 0.2492],
        [0.2507, 0.2499, 0.2486, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2506, 0.2498, 0.2504],
        [0.2503, 0.2507, 0.2494, 0.2495],
        [0.2502, 0.2507, 0.2501, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2499, 0.2501, 0.2502],
        [0.2495, 0.2507, 0.2503, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2508, 0.2501, 0.2495],
        [0.2493, 0.2505, 0.2491, 0.2511],
        [0.2492, 0.2508, 0.2503, 0.2497]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2499, 0.2500],
        [0.2494, 0.2505, 0.2496, 0.2504],
        [0.2490, 0.2512, 0.2508, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2501, 0.2505, 0.2497],
        [0.2497, 0.2504, 0.2493, 0.2506],
        [0.2497, 0.2505, 0.2507, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2487, 0.2494, 0.2525],
        [0.2484, 0.2510, 0.2521, 0.2486],
        [0.2481, 0.2507, 0.2520, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2521, 0.2540, 0.2489],
        [0.2443, 0.2521, 0.2536, 0.2500],
        [0.2463, 0.2503, 0.2515, 0.2520]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:27:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 2.0065 (1.6991)	Prec@(1,5) (36.7%, 88.1%)	
05/30 09:27:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 1.5054 (1.6685)	Prec@(1,5) (38.0%, 88.7%)	
05/30 09:28:06午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 1.5256 (1.6503)	Prec@(1,5) (38.7%, 88.9%)	
05/30 09:28:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 1.3575 (1.6366)	Prec@(1,5) (39.3%, 89.0%)	
05/30 09:29:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 1.3524 (1.6210)	Prec@(1,5) (39.9%, 89.4%)	
05/30 09:29:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 1.4510 (1.6066)	Prec@(1,5) (40.6%, 89.6%)	
05/30 09:29:58午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 1.5536 (1.5944)	Prec@(1,5) (41.0%, 90.0%)	
05/30 09:30:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 1.3312 (1.5868)	Prec@(1,5) (41.4%, 90.1%)	
05/30 09:30:21午後 searchStage_trainer.py:221 [INFO] Train: [  1/49] Final Prec@1 41.4040%
05/30 09:30:26午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 1.6243	Prec@(1,5) (38.7%, 89.7%)
05/30 09:30:30午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 1.6352	Prec@(1,5) (38.0%, 89.5%)
05/30 09:30:35午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 1.6316	Prec@(1,5) (38.4%, 89.3%)
05/30 09:30:40午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 1.6254	Prec@(1,5) (39.0%, 89.5%)
05/30 09:30:44午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 1.6265	Prec@(1,5) (39.1%, 89.4%)
05/30 09:30:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 1.6287	Prec@(1,5) (39.0%, 89.4%)
05/30 09:30:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 1.6312	Prec@(1,5) (38.7%, 89.4%)
05/30 09:30:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 1.6303	Prec@(1,5) (38.8%, 89.3%)
05/30 09:30:57午後 searchStage_trainer.py:260 [INFO] Valid: [  1/49] Final Prec@1 38.8240%
05/30 09:30:57午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:30:58午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 38.8240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2500, 0.2550, 0.2522, 0.2429],
        [0.2555, 0.2570, 0.2486, 0.2389]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2440, 0.2607, 0.2532, 0.2421],
        [0.2508, 0.2592, 0.2562, 0.2338],
        [0.2504, 0.2704, 0.2392, 0.2399]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2570, 0.2547, 0.2357],
        [0.2560, 0.2697, 0.2490, 0.2253],
        [0.2557, 0.2639, 0.2347, 0.2457]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2641, 0.2466, 0.2357],
        [0.2541, 0.2657, 0.2414, 0.2388],
        [0.2544, 0.2670, 0.2418, 0.2368]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2575, 0.2616, 0.2400, 0.2409],
        [0.2598, 0.2654, 0.2437, 0.2311],
        [0.2581, 0.2678, 0.2392, 0.2349]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2580, 0.2607, 0.2450, 0.2364],
        [0.2531, 0.2624, 0.2422, 0.2423],
        [0.2548, 0.2621, 0.2421, 0.2410]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2504, 0.2520, 0.2494],
        [0.2497, 0.2517, 0.2504, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2493, 0.2510, 0.2489],
        [0.2529, 0.2510, 0.2512, 0.2449],
        [0.2517, 0.2521, 0.2472, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2501, 0.2511, 0.2494],
        [0.2513, 0.2528, 0.2489, 0.2469],
        [0.2517, 0.2518, 0.2482, 0.2483]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2510, 0.2495, 0.2489],
        [0.2512, 0.2512, 0.2493, 0.2482],
        [0.2515, 0.2507, 0.2488, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2512, 0.2496, 0.2477],
        [0.2524, 0.2508, 0.2479, 0.2488],
        [0.2516, 0.2501, 0.2476, 0.2508]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2509, 0.2488, 0.2506],
        [0.2509, 0.2511, 0.2490, 0.2490],
        [0.2508, 0.2513, 0.2495, 0.2484]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2499, 0.2501, 0.2505],
        [0.2488, 0.2514, 0.2505, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2514, 0.2505, 0.2490],
        [0.2490, 0.2504, 0.2492, 0.2514],
        [0.2485, 0.2511, 0.2505, 0.2498]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2509, 0.2500, 0.2504],
        [0.2486, 0.2509, 0.2497, 0.2509],
        [0.2479, 0.2522, 0.2518, 0.2480]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2504, 0.2508, 0.2495],
        [0.2494, 0.2505, 0.2491, 0.2510],
        [0.2495, 0.2508, 0.2509, 0.2488]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2477, 0.2488, 0.2547],
        [0.2468, 0.2520, 0.2539, 0.2472],
        [0.2469, 0.2512, 0.2536, 0.2483]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2408, 0.2539, 0.2575, 0.2478],
        [0.2400, 0.2539, 0.2564, 0.2498],
        [0.2432, 0.2507, 0.2528, 0.2533]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:31:26午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 1.4064 (1.4923)	Prec@(1,5) (44.5%, 92.2%)	
05/30 09:31:54午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 1.5943 (1.4703)	Prec@(1,5) (45.7%, 92.1%)	
05/30 09:32:22午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 1.3847 (1.4492)	Prec@(1,5) (46.6%, 92.3%)	
05/30 09:32:50午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 1.3227 (1.4359)	Prec@(1,5) (47.3%, 92.5%)	
05/30 09:33:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 1.2280 (1.4225)	Prec@(1,5) (48.0%, 92.5%)	
05/30 09:33:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 1.4705 (1.4147)	Prec@(1,5) (48.5%, 92.7%)	
05/30 09:34:14午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 1.4210 (1.4031)	Prec@(1,5) (48.9%, 92.8%)	
05/30 09:34:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 1.4256 (1.3994)	Prec@(1,5) (49.0%, 92.9%)	
05/30 09:34:37午後 searchStage_trainer.py:221 [INFO] Train: [  2/49] Final Prec@1 48.9440%
05/30 09:34:42午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 1.3478	Prec@(1,5) (50.5%, 93.9%)
05/30 09:34:46午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 1.3615	Prec@(1,5) (50.2%, 93.5%)
05/30 09:34:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 1.3663	Prec@(1,5) (50.0%, 93.5%)
05/30 09:34:56午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 1.3599	Prec@(1,5) (50.5%, 93.6%)
05/30 09:35:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 1.3607	Prec@(1,5) (50.2%, 93.6%)
05/30 09:35:05午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 1.3632	Prec@(1,5) (50.2%, 93.5%)
05/30 09:35:09午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 1.3638	Prec@(1,5) (50.3%, 93.5%)
05/30 09:35:13午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 1.3606	Prec@(1,5) (50.5%, 93.4%)
05/30 09:35:13午後 searchStage_trainer.py:260 [INFO] Valid: [  2/49] Final Prec@1 50.5360%
05/30 09:35:13午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:35:14午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 50.5360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2500, 0.2634, 0.2542, 0.2324],
        [0.2569, 0.2600, 0.2465, 0.2366]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2429, 0.2658, 0.2540, 0.2373],
        [0.2534, 0.2646, 0.2548, 0.2273],
        [0.2517, 0.2824, 0.2354, 0.2305]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2617, 0.2588, 0.2277],
        [0.2554, 0.2822, 0.2457, 0.2167],
        [0.2573, 0.2753, 0.2302, 0.2372]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2566, 0.2736, 0.2419, 0.2279],
        [0.2554, 0.2736, 0.2368, 0.2343],
        [0.2567, 0.2777, 0.2392, 0.2263]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2617, 0.2691, 0.2358, 0.2334],
        [0.2642, 0.2743, 0.2385, 0.2230],
        [0.2614, 0.2771, 0.2342, 0.2273]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2602, 0.2679, 0.2417, 0.2302],
        [0.2556, 0.2703, 0.2370, 0.2371],
        [0.2553, 0.2703, 0.2394, 0.2350]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2517, 0.2535, 0.2472],
        [0.2497, 0.2519, 0.2492, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2491, 0.2518, 0.2490],
        [0.2534, 0.2513, 0.2516, 0.2436],
        [0.2514, 0.2530, 0.2476, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2498, 0.2509, 0.2496],
        [0.2518, 0.2537, 0.2489, 0.2456],
        [0.2516, 0.2524, 0.2481, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2515, 0.2490, 0.2485],
        [0.2512, 0.2519, 0.2496, 0.2474],
        [0.2516, 0.2510, 0.2485, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2514, 0.2496, 0.2470],
        [0.2527, 0.2518, 0.2476, 0.2479],
        [0.2521, 0.2502, 0.2468, 0.2510]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2515, 0.2483, 0.2505],
        [0.2512, 0.2512, 0.2487, 0.2489],
        [0.2509, 0.2518, 0.2496, 0.2477]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2501, 0.2502, 0.2506],
        [0.2484, 0.2517, 0.2508, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2521, 0.2509, 0.2484],
        [0.2485, 0.2503, 0.2493, 0.2518],
        [0.2479, 0.2515, 0.2506, 0.2500]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2514, 0.2502, 0.2503],
        [0.2480, 0.2512, 0.2496, 0.2512],
        [0.2472, 0.2529, 0.2524, 0.2475]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2505, 0.2511, 0.2494],
        [0.2492, 0.2506, 0.2489, 0.2514],
        [0.2492, 0.2511, 0.2511, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2469, 0.2483, 0.2565],
        [0.2456, 0.2528, 0.2556, 0.2460],
        [0.2457, 0.2516, 0.2549, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2374, 0.2554, 0.2604, 0.2468],
        [0.2363, 0.2554, 0.2587, 0.2496],
        [0.2406, 0.2511, 0.2539, 0.2544]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:35:42午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 1.2873 (1.2835)	Prec@(1,5) (52.6%, 94.5%)	
05/30 09:36:10午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 1.3210 (1.2761)	Prec@(1,5) (53.0%, 94.3%)	
05/30 09:36:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 1.3330 (1.2723)	Prec@(1,5) (53.7%, 94.1%)	
05/30 09:37:05午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 1.5550 (1.2696)	Prec@(1,5) (53.6%, 94.3%)	
05/30 09:37:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 1.1888 (1.2667)	Prec@(1,5) (53.8%, 94.3%)	
05/30 09:38:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 1.1319 (1.2579)	Prec@(1,5) (54.3%, 94.3%)	
05/30 09:38:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 1.1937 (1.2517)	Prec@(1,5) (54.5%, 94.4%)	
05/30 09:38:52午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 1.2155 (1.2438)	Prec@(1,5) (55.0%, 94.5%)	
05/30 09:38:53午後 searchStage_trainer.py:221 [INFO] Train: [  3/49] Final Prec@1 54.9600%
05/30 09:38:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 1.2736	Prec@(1,5) (54.8%, 93.6%)
05/30 09:39:02午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 1.2787	Prec@(1,5) (54.2%, 93.8%)
05/30 09:39:07午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 1.2819	Prec@(1,5) (54.1%, 94.1%)
05/30 09:39:11午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 1.2882	Prec@(1,5) (54.1%, 93.8%)
05/30 09:39:16午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 1.2886	Prec@(1,5) (54.0%, 93.7%)
05/30 09:39:20午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 1.2914	Prec@(1,5) (53.8%, 93.8%)
05/30 09:39:25午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 1.2929	Prec@(1,5) (53.8%, 93.8%)
05/30 09:39:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 1.2932	Prec@(1,5) (53.8%, 93.8%)
05/30 09:39:29午後 searchStage_trainer.py:260 [INFO] Valid: [  3/49] Final Prec@1 53.8320%
05/30 09:39:29午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)]], DAG3_concat=range(6, 8))
05/30 09:39:29午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 53.8320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2723, 0.2518, 0.2258],
        [0.2558, 0.2688, 0.2477, 0.2277]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2401, 0.2756, 0.2547, 0.2295],
        [0.2510, 0.2721, 0.2560, 0.2209],
        [0.2530, 0.2981, 0.2285, 0.2204]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2655, 0.2595, 0.2230],
        [0.2570, 0.2941, 0.2415, 0.2074],
        [0.2623, 0.2857, 0.2264, 0.2256]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2570, 0.2839, 0.2396, 0.2195],
        [0.2570, 0.2814, 0.2331, 0.2285],
        [0.2592, 0.2863, 0.2373, 0.2172]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2775, 0.2313, 0.2277],
        [0.2681, 0.2835, 0.2342, 0.2141],
        [0.2640, 0.2860, 0.2312, 0.2188]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2748, 0.2382, 0.2244],
        [0.2587, 0.2777, 0.2327, 0.2309],
        [0.2568, 0.2781, 0.2366, 0.2285]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2513, 0.2525, 0.2484],
        [0.2503, 0.2526, 0.2497, 0.2474]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2493, 0.2514, 0.2490],
        [0.2543, 0.2522, 0.2519, 0.2417],
        [0.2519, 0.2534, 0.2467, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2504, 0.2508, 0.2494],
        [0.2518, 0.2541, 0.2490, 0.2452],
        [0.2520, 0.2528, 0.2477, 0.2475]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2518, 0.2494, 0.2479],
        [0.2514, 0.2522, 0.2495, 0.2470],
        [0.2519, 0.2513, 0.2479, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2518, 0.2494, 0.2461],
        [0.2528, 0.2522, 0.2469, 0.2481],
        [0.2522, 0.2504, 0.2467, 0.2506]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2517, 0.2482, 0.2503],
        [0.2514, 0.2515, 0.2483, 0.2487],
        [0.2512, 0.2522, 0.2492, 0.2475]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2502, 0.2503, 0.2507],
        [0.2479, 0.2521, 0.2509, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2525, 0.2512, 0.2480],
        [0.2483, 0.2502, 0.2493, 0.2522],
        [0.2474, 0.2518, 0.2508, 0.2499]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2517, 0.2503, 0.2503],
        [0.2475, 0.2515, 0.2496, 0.2515],
        [0.2464, 0.2534, 0.2530, 0.2471]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2507, 0.2514, 0.2493],
        [0.2490, 0.2507, 0.2488, 0.2515],
        [0.2490, 0.2514, 0.2512, 0.2484]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2463, 0.2479, 0.2580],
        [0.2447, 0.2535, 0.2568, 0.2451],
        [0.2448, 0.2519, 0.2559, 0.2474]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2344, 0.2566, 0.2630, 0.2460],
        [0.2334, 0.2566, 0.2606, 0.2494],
        [0.2385, 0.2514, 0.2548, 0.2553]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:39:58午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 0.9871 (1.1406)	Prec@(1,5) (58.9%, 95.3%)	
05/30 09:40:26午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 1.0814 (1.1353)	Prec@(1,5) (58.7%, 95.6%)	
05/30 09:40:53午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 1.3273 (1.1367)	Prec@(1,5) (58.8%, 95.5%)	
05/30 09:41:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 0.9381 (1.1332)	Prec@(1,5) (59.2%, 95.5%)	
05/30 09:41:49午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 1.0980 (1.1221)	Prec@(1,5) (59.7%, 95.7%)	
05/30 09:42:17午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 1.0264 (1.1228)	Prec@(1,5) (59.7%, 95.7%)	
05/30 09:42:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 1.2905 (1.1186)	Prec@(1,5) (59.9%, 95.8%)	
05/30 09:43:08午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 0.9448 (1.1139)	Prec@(1,5) (60.2%, 95.8%)	
05/30 09:43:08午後 searchStage_trainer.py:221 [INFO] Train: [  4/49] Final Prec@1 60.1800%
05/30 09:43:13午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 1.0894	Prec@(1,5) (61.2%, 95.5%)
05/30 09:43:18午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 1.0943	Prec@(1,5) (60.7%, 95.6%)
05/30 09:43:22午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 1.0980	Prec@(1,5) (60.7%, 95.6%)
05/30 09:43:27午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 1.0989	Prec@(1,5) (60.8%, 95.6%)
05/30 09:43:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 1.0975	Prec@(1,5) (61.0%, 95.7%)
05/30 09:43:36午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 1.0974	Prec@(1,5) (61.0%, 95.7%)
05/30 09:43:41午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 1.1035	Prec@(1,5) (60.8%, 95.7%)
05/30 09:43:45午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 1.1011	Prec@(1,5) (60.9%, 95.7%)
05/30 09:43:45午後 searchStage_trainer.py:260 [INFO] Valid: [  4/49] Final Prec@1 60.9000%
05/30 09:43:45午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)]], DAG3_concat=range(6, 8))
05/30 09:43:45午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 60.9000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2537, 0.2796, 0.2507, 0.2160],
        [0.2582, 0.2754, 0.2460, 0.2204]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2384, 0.2846, 0.2553, 0.2217],
        [0.2509, 0.2807, 0.2575, 0.2109],
        [0.2543, 0.3113, 0.2222, 0.2122]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2712, 0.2603, 0.2158],
        [0.2602, 0.3056, 0.2367, 0.1974],
        [0.2654, 0.2947, 0.2223, 0.2176]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2597, 0.2908, 0.2361, 0.2134],
        [0.2594, 0.2894, 0.2304, 0.2208],
        [0.2617, 0.2945, 0.2347, 0.2090]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2672, 0.2861, 0.2291, 0.2176],
        [0.2723, 0.2912, 0.2296, 0.2068],
        [0.2669, 0.2942, 0.2261, 0.2128]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.2819, 0.2347, 0.2176],
        [0.2620, 0.2854, 0.2282, 0.2244],
        [0.2585, 0.2854, 0.2332, 0.2230]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2519, 0.2521, 0.2485],
        [0.2498, 0.2529, 0.2506, 0.2467]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2519, 0.2477],
        [0.2544, 0.2522, 0.2517, 0.2418],
        [0.2519, 0.2535, 0.2465, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2503, 0.2506, 0.2498],
        [0.2521, 0.2545, 0.2488, 0.2446],
        [0.2521, 0.2530, 0.2476, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2522, 0.2494, 0.2477],
        [0.2513, 0.2524, 0.2493, 0.2470],
        [0.2520, 0.2514, 0.2479, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2518, 0.2492, 0.2460],
        [0.2534, 0.2522, 0.2463, 0.2481],
        [0.2526, 0.2505, 0.2466, 0.2503]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2520, 0.2478, 0.2502],
        [0.2514, 0.2518, 0.2485, 0.2484],
        [0.2513, 0.2523, 0.2489, 0.2475]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2504, 0.2504, 0.2507],
        [0.2473, 0.2525, 0.2512, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2529, 0.2513, 0.2478],
        [0.2479, 0.2502, 0.2494, 0.2525],
        [0.2471, 0.2521, 0.2510, 0.2498]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2520, 0.2503, 0.2503],
        [0.2470, 0.2517, 0.2496, 0.2517],
        [0.2460, 0.2538, 0.2533, 0.2469]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2508, 0.2515, 0.2492],
        [0.2489, 0.2507, 0.2488, 0.2516],
        [0.2488, 0.2515, 0.2513, 0.2484]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2458, 0.2476, 0.2591],
        [0.2439, 0.2540, 0.2577, 0.2445],
        [0.2442, 0.2521, 0.2565, 0.2472]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2321, 0.2576, 0.2650, 0.2453],
        [0.2311, 0.2576, 0.2621, 0.2492],
        [0.2369, 0.2517, 0.2555, 0.2559]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:44:14午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 0.9716 (1.0050)	Prec@(1,5) (64.2%, 96.6%)	
05/30 09:44:42午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 0.8737 (1.0340)	Prec@(1,5) (63.3%, 96.6%)	
05/30 09:45:09午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 1.1642 (1.0339)	Prec@(1,5) (63.3%, 96.5%)	
05/30 09:45:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 1.1800 (1.0367)	Prec@(1,5) (63.1%, 96.4%)	
05/30 09:46:05午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 1.1516 (1.0447)	Prec@(1,5) (62.9%, 96.3%)	
05/30 09:46:33午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 1.0310 (1.0361)	Prec@(1,5) (63.2%, 96.4%)	
05/30 09:47:01午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 1.0207 (1.0331)	Prec@(1,5) (63.4%, 96.4%)	
05/30 09:47:24午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 0.8419 (1.0285)	Prec@(1,5) (63.4%, 96.5%)	
05/30 09:47:24午後 searchStage_trainer.py:221 [INFO] Train: [  5/49] Final Prec@1 63.4000%
05/30 09:47:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 1.1261	Prec@(1,5) (60.8%, 95.8%)
05/30 09:47:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 1.1230	Prec@(1,5) (60.5%, 96.1%)
05/30 09:47:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 1.1185	Prec@(1,5) (60.5%, 95.9%)
05/30 09:47:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 1.1159	Prec@(1,5) (60.6%, 95.9%)
05/30 09:47:47午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 1.1146	Prec@(1,5) (60.6%, 95.9%)
05/30 09:47:52午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 1.1093	Prec@(1,5) (60.7%, 95.9%)
05/30 09:47:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 1.1097	Prec@(1,5) (60.7%, 95.9%)
05/30 09:48:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 1.1098	Prec@(1,5) (60.7%, 95.9%)
05/30 09:48:00午後 searchStage_trainer.py:260 [INFO] Valid: [  5/49] Final Prec@1 60.6680%
05/30 09:48:01午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 6)]], DAG3_concat=range(6, 8))
05/30 09:48:01午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 60.9000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2557, 0.2886, 0.2460, 0.2098],
        [0.2620, 0.2837, 0.2461, 0.2082]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2384, 0.2954, 0.2566, 0.2097],
        [0.2525, 0.2875, 0.2566, 0.2034],
        [0.2595, 0.3235, 0.2123, 0.2047]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2777, 0.2639, 0.2066],
        [0.2625, 0.3151, 0.2319, 0.1906],
        [0.2675, 0.3045, 0.2187, 0.2093]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2616, 0.2979, 0.2327, 0.2079],
        [0.2637, 0.2961, 0.2272, 0.2129],
        [0.2651, 0.3007, 0.2318, 0.2024]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2702, 0.2949, 0.2256, 0.2094],
        [0.2743, 0.2981, 0.2274, 0.2002],
        [0.2694, 0.3017, 0.2228, 0.2062]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2676, 0.2890, 0.2322, 0.2112],
        [0.2662, 0.2921, 0.2241, 0.2176],
        [0.2610, 0.2920, 0.2288, 0.2182]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2520, 0.2523, 0.2487],
        [0.2496, 0.2535, 0.2508, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2499, 0.2521, 0.2473],
        [0.2543, 0.2523, 0.2520, 0.2413],
        [0.2520, 0.2537, 0.2462, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2508, 0.2508, 0.2490],
        [0.2517, 0.2546, 0.2491, 0.2446],
        [0.2521, 0.2530, 0.2476, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2523, 0.2494, 0.2476],
        [0.2514, 0.2525, 0.2494, 0.2467],
        [0.2521, 0.2514, 0.2478, 0.2487]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2519, 0.2493, 0.2457],
        [0.2535, 0.2523, 0.2461, 0.2480],
        [0.2529, 0.2506, 0.2462, 0.2503]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2521, 0.2478, 0.2505],
        [0.2515, 0.2517, 0.2484, 0.2484],
        [0.2515, 0.2525, 0.2490, 0.2471]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2504, 0.2504, 0.2508],
        [0.2470, 0.2528, 0.2514, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2532, 0.2515, 0.2476],
        [0.2476, 0.2502, 0.2494, 0.2528],
        [0.2468, 0.2523, 0.2512, 0.2497]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2522, 0.2504, 0.2504],
        [0.2467, 0.2518, 0.2496, 0.2519],
        [0.2456, 0.2541, 0.2536, 0.2467]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2509, 0.2516, 0.2492],
        [0.2488, 0.2507, 0.2487, 0.2518],
        [0.2488, 0.2516, 0.2514, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2454, 0.2474, 0.2600],
        [0.2433, 0.2543, 0.2584, 0.2440],
        [0.2436, 0.2523, 0.2571, 0.2470]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2302, 0.2583, 0.2667, 0.2447],
        [0.2294, 0.2583, 0.2633, 0.2490],
        [0.2358, 0.2518, 0.2560, 0.2564]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:48:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 0.9324 (0.9726)	Prec@(1,5) (65.2%, 97.2%)	
05/30 09:48:57午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 1.0182 (0.9654)	Prec@(1,5) (65.8%, 97.1%)	
05/30 09:49:25午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 0.9829 (0.9628)	Prec@(1,5) (66.0%, 97.0%)	
05/30 09:49:53午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 0.6078 (0.9618)	Prec@(1,5) (65.9%, 96.9%)	
05/30 09:50:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 1.1133 (0.9657)	Prec@(1,5) (65.8%, 97.0%)	
05/30 09:50:49午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 1.0547 (0.9589)	Prec@(1,5) (66.0%, 97.1%)	
05/30 09:51:17午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 1.0843 (0.9602)	Prec@(1,5) (66.0%, 97.0%)	
05/30 09:51:40午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 0.8154 (0.9564)	Prec@(1,5) (66.2%, 97.0%)	
05/30 09:51:40午後 searchStage_trainer.py:221 [INFO] Train: [  6/49] Final Prec@1 66.2400%
05/30 09:51:45午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 0.9886	Prec@(1,5) (64.2%, 97.0%)
05/30 09:51:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 0.9995	Prec@(1,5) (64.4%, 97.0%)
05/30 09:51:54午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 1.0000	Prec@(1,5) (64.1%, 97.1%)
05/30 09:51:59午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 0.9975	Prec@(1,5) (64.3%, 97.0%)
05/30 09:52:03午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 0.9956	Prec@(1,5) (64.4%, 97.0%)
05/30 09:52:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 0.9961	Prec@(1,5) (64.5%, 97.0%)
05/30 09:52:12午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 0.9935	Prec@(1,5) (64.6%, 97.0%)
05/30 09:52:16午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 0.9962	Prec@(1,5) (64.6%, 97.0%)
05/30 09:52:16午後 searchStage_trainer.py:260 [INFO] Valid: [  6/49] Final Prec@1 64.5920%
05/30 09:52:16午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:52:17午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 64.5920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2574, 0.2964, 0.2451, 0.2011],
        [0.2662, 0.2915, 0.2437, 0.1985]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2393, 0.3026, 0.2564, 0.2017],
        [0.2550, 0.2927, 0.2548, 0.1976],
        [0.2628, 0.3345, 0.2084, 0.1943]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2820, 0.2640, 0.2003],
        [0.2663, 0.3208, 0.2292, 0.1837],
        [0.2709, 0.3108, 0.2164, 0.2019]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2635, 0.3032, 0.2316, 0.2018],
        [0.2657, 0.3014, 0.2249, 0.2081],
        [0.2670, 0.3063, 0.2300, 0.1967]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2732, 0.3018, 0.2222, 0.2028],
        [0.2762, 0.3038, 0.2249, 0.1951],
        [0.2725, 0.3074, 0.2206, 0.1995]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2710, 0.2942, 0.2297, 0.2050],
        [0.2697, 0.2962, 0.2211, 0.2131],
        [0.2635, 0.2970, 0.2264, 0.2131]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2521, 0.2522, 0.2485],
        [0.2493, 0.2535, 0.2510, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2499, 0.2517, 0.2474],
        [0.2542, 0.2526, 0.2523, 0.2409],
        [0.2520, 0.2538, 0.2461, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2509, 0.2506, 0.2489],
        [0.2515, 0.2547, 0.2493, 0.2445],
        [0.2521, 0.2531, 0.2474, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2524, 0.2496, 0.2473],
        [0.2514, 0.2526, 0.2495, 0.2465],
        [0.2521, 0.2515, 0.2476, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2519, 0.2492, 0.2456],
        [0.2537, 0.2524, 0.2459, 0.2481],
        [0.2530, 0.2506, 0.2462, 0.2502]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2522, 0.2476, 0.2505],
        [0.2516, 0.2516, 0.2483, 0.2485],
        [0.2515, 0.2526, 0.2491, 0.2468]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2504, 0.2506, 0.2508],
        [0.2468, 0.2530, 0.2515, 0.2488]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2535, 0.2516, 0.2474],
        [0.2474, 0.2502, 0.2494, 0.2530],
        [0.2466, 0.2524, 0.2513, 0.2497]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2523, 0.2504, 0.2505],
        [0.2466, 0.2519, 0.2495, 0.2520],
        [0.2454, 0.2543, 0.2538, 0.2465]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2509, 0.2517, 0.2492],
        [0.2487, 0.2508, 0.2487, 0.2518],
        [0.2487, 0.2517, 0.2514, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2451, 0.2472, 0.2608],
        [0.2429, 0.2546, 0.2590, 0.2436],
        [0.2433, 0.2524, 0.2575, 0.2468]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2287, 0.2589, 0.2680, 0.2443],
        [0.2280, 0.2588, 0.2642, 0.2489],
        [0.2350, 0.2520, 0.2563, 0.2567]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:52:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 0.7730 (0.8880)	Prec@(1,5) (68.9%, 97.6%)	
05/30 09:53:14午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 0.5074 (0.8891)	Prec@(1,5) (68.4%, 97.5%)	
05/30 09:53:41午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 1.0003 (0.8953)	Prec@(1,5) (68.2%, 97.4%)	
05/30 09:54:09午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 0.7876 (0.8999)	Prec@(1,5) (67.9%, 97.4%)	
05/30 09:54:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 0.9735 (0.8874)	Prec@(1,5) (68.5%, 97.5%)	
05/30 09:55:05午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 0.7229 (0.8895)	Prec@(1,5) (68.6%, 97.5%)	
05/30 09:55:33午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 0.8609 (0.8873)	Prec@(1,5) (68.8%, 97.4%)	
05/30 09:55:56午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 0.8334 (0.8872)	Prec@(1,5) (68.9%, 97.4%)	
05/30 09:55:56午後 searchStage_trainer.py:221 [INFO] Train: [  7/49] Final Prec@1 68.9000%
05/30 09:56:01午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 0.9001	Prec@(1,5) (68.2%, 97.0%)
05/30 09:56:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 0.9206	Prec@(1,5) (67.5%, 96.7%)
05/30 09:56:10午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 0.9221	Prec@(1,5) (67.4%, 96.9%)
05/30 09:56:15午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 0.9269	Prec@(1,5) (67.5%, 96.9%)
05/30 09:56:19午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 0.9286	Prec@(1,5) (67.4%, 97.0%)
05/30 09:56:24午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 0.9233	Prec@(1,5) (67.5%, 97.0%)
05/30 09:56:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 0.9261	Prec@(1,5) (67.5%, 97.0%)
05/30 09:56:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 0.9271	Prec@(1,5) (67.5%, 97.0%)
05/30 09:56:32午後 searchStage_trainer.py:260 [INFO] Valid: [  7/49] Final Prec@1 67.4600%
05/30 09:56:32午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:56:33午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 67.4600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2596, 0.3033, 0.2442, 0.1929],
        [0.2687, 0.2984, 0.2420, 0.1909]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2410, 0.3085, 0.2556, 0.1949],
        [0.2555, 0.2975, 0.2551, 0.1918],
        [0.2649, 0.3414, 0.2069, 0.1868]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2562, 0.2865, 0.2643, 0.1931],
        [0.2681, 0.3261, 0.2281, 0.1777],
        [0.2717, 0.3149, 0.2150, 0.1984]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2651, 0.3081, 0.2295, 0.1973],
        [0.2680, 0.3048, 0.2225, 0.2047],
        [0.2694, 0.3098, 0.2289, 0.1919]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2744, 0.3067, 0.2207, 0.1982],
        [0.2780, 0.3090, 0.2231, 0.1898],
        [0.2746, 0.3118, 0.2188, 0.1948]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2738, 0.2985, 0.2271, 0.2006],
        [0.2715, 0.3000, 0.2205, 0.2079],
        [0.2654, 0.3008, 0.2240, 0.2098]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2522, 0.2524, 0.2484],
        [0.2491, 0.2535, 0.2511, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2500, 0.2520, 0.2469],
        [0.2544, 0.2524, 0.2525, 0.2408],
        [0.2519, 0.2538, 0.2459, 0.2484]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2508, 0.2511, 0.2488],
        [0.2514, 0.2548, 0.2494, 0.2444],
        [0.2521, 0.2531, 0.2474, 0.2474]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2524, 0.2496, 0.2475],
        [0.2514, 0.2527, 0.2495, 0.2464],
        [0.2522, 0.2514, 0.2476, 0.2488]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2519, 0.2492, 0.2455],
        [0.2537, 0.2523, 0.2459, 0.2481],
        [0.2531, 0.2506, 0.2461, 0.2502]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2522, 0.2476, 0.2505],
        [0.2516, 0.2517, 0.2482, 0.2485],
        [0.2516, 0.2526, 0.2491, 0.2468]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2504, 0.2507, 0.2509],
        [0.2466, 0.2531, 0.2516, 0.2487]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2537, 0.2516, 0.2473],
        [0.2473, 0.2503, 0.2494, 0.2531],
        [0.2464, 0.2525, 0.2514, 0.2497]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2525, 0.2504, 0.2505],
        [0.2464, 0.2520, 0.2495, 0.2521],
        [0.2453, 0.2544, 0.2539, 0.2465]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2510, 0.2517, 0.2491],
        [0.2487, 0.2508, 0.2486, 0.2519],
        [0.2487, 0.2517, 0.2514, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2448, 0.2471, 0.2614],
        [0.2425, 0.2548, 0.2594, 0.2432],
        [0.2430, 0.2525, 0.2578, 0.2467]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2275, 0.2594, 0.2692, 0.2439],
        [0.2270, 0.2592, 0.2649, 0.2488],
        [0.2344, 0.2521, 0.2566, 0.2569]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:57:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 1.0716 (0.8134)	Prec@(1,5) (71.6%, 97.9%)	
05/30 09:57:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 1.0047 (0.8057)	Prec@(1,5) (71.6%, 97.9%)	
05/30 09:57:57午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 0.7174 (0.8165)	Prec@(1,5) (71.3%, 97.9%)	
05/30 09:58:25午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 0.7132 (0.8134)	Prec@(1,5) (71.4%, 98.0%)	
05/30 09:58:53午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 0.8701 (0.8167)	Prec@(1,5) (71.4%, 97.9%)	
05/30 09:59:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 1.0308 (0.8251)	Prec@(1,5) (71.2%, 97.8%)	
05/30 09:59:49午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 0.8436 (0.8269)	Prec@(1,5) (71.2%, 97.8%)	
05/30 10:00:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 0.8049 (0.8260)	Prec@(1,5) (71.2%, 97.8%)	
05/30 10:00:12午後 searchStage_trainer.py:221 [INFO] Train: [  8/49] Final Prec@1 71.2120%
05/30 10:00:17午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 0.8817	Prec@(1,5) (68.8%, 97.7%)
05/30 10:00:22午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 0.8755	Prec@(1,5) (69.2%, 97.6%)
05/30 10:00:26午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 0.8833	Prec@(1,5) (68.8%, 97.7%)
05/30 10:00:31午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 0.8782	Prec@(1,5) (68.9%, 97.7%)
05/30 10:00:35午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 0.8821	Prec@(1,5) (68.8%, 97.6%)
05/30 10:00:40午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 0.8810	Prec@(1,5) (68.8%, 97.6%)
05/30 10:00:45午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 0.8856	Prec@(1,5) (68.7%, 97.5%)
05/30 10:00:48午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 0.8851	Prec@(1,5) (68.8%, 97.5%)
05/30 10:00:48午後 searchStage_trainer.py:260 [INFO] Valid: [  8/49] Final Prec@1 68.7680%
05/30 10:00:48午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:00:49午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 68.7680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2610, 0.3092, 0.2438, 0.1860],
        [0.2719, 0.3036, 0.2401, 0.1843]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2416, 0.3135, 0.2539, 0.1910],
        [0.2560, 0.3020, 0.2565, 0.1855],
        [0.2668, 0.3459, 0.2062, 0.1811]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2561, 0.2883, 0.2644, 0.1912],
        [0.2701, 0.3293, 0.2278, 0.1728],
        [0.2733, 0.3174, 0.2150, 0.1943]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2673, 0.3103, 0.2268, 0.1956],
        [0.2700, 0.3075, 0.2219, 0.2006],
        [0.2707, 0.3119, 0.2291, 0.1883]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2762, 0.3108, 0.2200, 0.1930],
        [0.2796, 0.3114, 0.2217, 0.1873],
        [0.2761, 0.3142, 0.2184, 0.1913]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2750, 0.3022, 0.2251, 0.1977],
        [0.2734, 0.3027, 0.2196, 0.2043],
        [0.2667, 0.3032, 0.2235, 0.2066]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2521, 0.2525, 0.2484],
        [0.2491, 0.2535, 0.2511, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2498, 0.2521, 0.2469],
        [0.2547, 0.2521, 0.2525, 0.2407],
        [0.2519, 0.2537, 0.2459, 0.2484]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2512, 0.2513, 0.2485],
        [0.2514, 0.2548, 0.2493, 0.2445],
        [0.2521, 0.2531, 0.2475, 0.2474]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2524, 0.2496, 0.2475],
        [0.2513, 0.2527, 0.2494, 0.2466],
        [0.2522, 0.2515, 0.2476, 0.2487]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2519, 0.2491, 0.2455],
        [0.2538, 0.2524, 0.2459, 0.2480],
        [0.2531, 0.2505, 0.2461, 0.2503]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2523, 0.2477, 0.2505],
        [0.2515, 0.2517, 0.2483, 0.2485],
        [0.2515, 0.2526, 0.2490, 0.2468]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2504, 0.2507, 0.2510],
        [0.2464, 0.2533, 0.2516, 0.2487]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2539, 0.2517, 0.2472],
        [0.2472, 0.2503, 0.2494, 0.2531],
        [0.2464, 0.2526, 0.2514, 0.2496]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2526, 0.2504, 0.2505],
        [0.2463, 0.2520, 0.2495, 0.2522],
        [0.2452, 0.2545, 0.2539, 0.2465]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2510, 0.2518, 0.2491],
        [0.2487, 0.2508, 0.2486, 0.2520],
        [0.2487, 0.2517, 0.2514, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2445, 0.2469, 0.2620],
        [0.2423, 0.2550, 0.2597, 0.2430],
        [0.2428, 0.2526, 0.2580, 0.2466]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2265, 0.2598, 0.2701, 0.2436],
        [0.2262, 0.2596, 0.2654, 0.2488],
        [0.2340, 0.2522, 0.2567, 0.2571]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:01:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 0.8714 (0.7726)	Prec@(1,5) (73.4%, 97.9%)	
05/30 10:01:45午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 0.8482 (0.7544)	Prec@(1,5) (73.7%, 98.0%)	
05/30 10:02:13午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 1.2376 (0.7621)	Prec@(1,5) (73.7%, 97.9%)	
05/30 10:02:41午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 0.6351 (0.7802)	Prec@(1,5) (73.1%, 97.9%)	
05/30 10:03:09午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 0.9451 (0.7767)	Prec@(1,5) (73.2%, 97.8%)	
05/30 10:03:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 0.7941 (0.7760)	Prec@(1,5) (73.2%, 97.9%)	
05/30 10:04:05午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 0.6434 (0.7763)	Prec@(1,5) (73.2%, 97.9%)	
05/30 10:04:28午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 0.7479 (0.7756)	Prec@(1,5) (73.3%, 97.9%)	
05/30 10:04:28午後 searchStage_trainer.py:221 [INFO] Train: [  9/49] Final Prec@1 73.2680%
05/30 10:04:33午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 0.8033	Prec@(1,5) (72.4%, 97.4%)
05/30 10:04:37午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 0.8256	Prec@(1,5) (71.5%, 97.3%)
05/30 10:04:42午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 0.8243	Prec@(1,5) (71.6%, 97.4%)
05/30 10:04:47午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 0.8270	Prec@(1,5) (71.5%, 97.5%)
05/30 10:04:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 0.8370	Prec@(1,5) (71.1%, 97.5%)
05/30 10:04:56午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 0.8347	Prec@(1,5) (71.2%, 97.5%)
05/30 10:05:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 0.8356	Prec@(1,5) (71.0%, 97.5%)
05/30 10:05:04午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 0.8337	Prec@(1,5) (71.1%, 97.5%)
05/30 10:05:04午後 searchStage_trainer.py:260 [INFO] Valid: [  9/49] Final Prec@1 71.0760%
05/30 10:05:04午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:05:05午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 71.0760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2634, 0.3120, 0.2436, 0.1811],
        [0.2735, 0.3071, 0.2402, 0.1792]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2427, 0.3181, 0.2544, 0.1849],
        [0.2569, 0.3052, 0.2564, 0.1815],
        [0.2672, 0.3486, 0.2059, 0.1783]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2569, 0.2901, 0.2651, 0.1879],
        [0.2722, 0.3315, 0.2267, 0.1696],
        [0.2747, 0.3188, 0.2145, 0.1921]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2689, 0.3126, 0.2264, 0.1921],
        [0.2710, 0.3094, 0.2208, 0.1988],
        [0.2715, 0.3131, 0.2287, 0.1867]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2780, 0.3135, 0.2184, 0.1901],
        [0.2810, 0.3136, 0.2213, 0.1841],
        [0.2767, 0.3158, 0.2180, 0.1895]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2754, 0.3048, 0.2251, 0.1948],
        [0.2747, 0.3041, 0.2183, 0.2028],
        [0.2679, 0.3047, 0.2232, 0.2042]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2519, 0.2523, 0.2485],
        [0.2492, 0.2536, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2499, 0.2520, 0.2469],
        [0.2547, 0.2522, 0.2524, 0.2407],
        [0.2519, 0.2537, 0.2459, 0.2484]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2512, 0.2514, 0.2485],
        [0.2514, 0.2549, 0.2493, 0.2444],
        [0.2521, 0.2531, 0.2474, 0.2475]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2524, 0.2496, 0.2474],
        [0.2513, 0.2526, 0.2495, 0.2465],
        [0.2523, 0.2514, 0.2475, 0.2488]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2520, 0.2491, 0.2454],
        [0.2537, 0.2524, 0.2459, 0.2480],
        [0.2531, 0.2505, 0.2461, 0.2504]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2523, 0.2476, 0.2506],
        [0.2516, 0.2517, 0.2483, 0.2484],
        [0.2516, 0.2526, 0.2490, 0.2469]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2504, 0.2507, 0.2510],
        [0.2463, 0.2534, 0.2517, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2540, 0.2517, 0.2471],
        [0.2471, 0.2503, 0.2494, 0.2532],
        [0.2463, 0.2526, 0.2515, 0.2496]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2526, 0.2503, 0.2506],
        [0.2462, 0.2520, 0.2494, 0.2523],
        [0.2451, 0.2545, 0.2540, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2510, 0.2518, 0.2491],
        [0.2487, 0.2507, 0.2485, 0.2520],
        [0.2487, 0.2517, 0.2514, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2443, 0.2468, 0.2624],
        [0.2421, 0.2551, 0.2600, 0.2428],
        [0.2427, 0.2526, 0.2581, 0.2465]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2256, 0.2602, 0.2709, 0.2433],
        [0.2256, 0.2598, 0.2658, 0.2488],
        [0.2337, 0.2522, 0.2568, 0.2572]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:05:33午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 0.8272 (0.7066)	Prec@(1,5) (74.7%, 98.7%)	
05/30 10:06:01午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 0.8789 (0.7331)	Prec@(1,5) (73.9%, 98.4%)	
05/30 10:06:28午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 0.7666 (0.7485)	Prec@(1,5) (73.6%, 98.3%)	
05/30 10:06:56午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 0.7383 (0.7475)	Prec@(1,5) (73.6%, 98.2%)	
05/30 10:07:25午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 0.8616 (0.7448)	Prec@(1,5) (73.7%, 98.2%)	
05/30 10:07:53午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 0.6653 (0.7438)	Prec@(1,5) (73.8%, 98.2%)	
05/30 10:08:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 0.7879 (0.7390)	Prec@(1,5) (74.1%, 98.3%)	
05/30 10:08:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 0.7463 (0.7397)	Prec@(1,5) (74.0%, 98.2%)	
05/30 10:08:44午後 searchStage_trainer.py:221 [INFO] Train: [ 10/49] Final Prec@1 74.0160%
05/30 10:08:48午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 0.9323	Prec@(1,5) (67.7%, 96.7%)
05/30 10:08:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 0.9343	Prec@(1,5) (67.8%, 96.7%)
05/30 10:08:58午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 0.9367	Prec@(1,5) (67.7%, 96.9%)
05/30 10:09:02午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 0.9398	Prec@(1,5) (67.4%, 96.9%)
05/30 10:09:07午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 0.9408	Prec@(1,5) (67.4%, 96.8%)
05/30 10:09:11午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 0.9400	Prec@(1,5) (67.7%, 96.7%)
05/30 10:09:16午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 0.9392	Prec@(1,5) (67.7%, 96.7%)
05/30 10:09:20午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 0.9390	Prec@(1,5) (67.7%, 96.7%)
05/30 10:09:20午後 searchStage_trainer.py:260 [INFO] Valid: [ 10/49] Final Prec@1 67.7320%
05/30 10:09:20午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:09:20午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 71.0760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2646, 0.3143, 0.2435, 0.1777],
        [0.2749, 0.3094, 0.2399, 0.1758]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2427, 0.3229, 0.2557, 0.1787],
        [0.2565, 0.3077, 0.2583, 0.1775],
        [0.2674, 0.3496, 0.2051, 0.1779]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2568, 0.2919, 0.2662, 0.1851],
        [0.2729, 0.3331, 0.2269, 0.1671],
        [0.2751, 0.3192, 0.2143, 0.1913]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2699, 0.3138, 0.2252, 0.1911],
        [0.2719, 0.3104, 0.2211, 0.1966],
        [0.2720, 0.3136, 0.2284, 0.1860]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2792, 0.3155, 0.2183, 0.1870],
        [0.2819, 0.3148, 0.2207, 0.1827],
        [0.2773, 0.3163, 0.2179, 0.1885]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2764, 0.3059, 0.2250, 0.1927],
        [0.2756, 0.3051, 0.2179, 0.2015],
        [0.2686, 0.3051, 0.2229, 0.2033]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2518, 0.2523, 0.2487],
        [0.2494, 0.2535, 0.2510, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2497, 0.2521, 0.2471],
        [0.2549, 0.2522, 0.2524, 0.2405],
        [0.2520, 0.2537, 0.2459, 0.2484]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2513, 0.2515, 0.2481],
        [0.2514, 0.2549, 0.2492, 0.2445],
        [0.2520, 0.2530, 0.2474, 0.2475]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2496, 0.2473],
        [0.2513, 0.2526, 0.2494, 0.2467],
        [0.2523, 0.2514, 0.2475, 0.2488]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2520, 0.2492, 0.2452],
        [0.2537, 0.2524, 0.2459, 0.2480],
        [0.2531, 0.2505, 0.2459, 0.2505]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2522, 0.2475, 0.2506],
        [0.2516, 0.2517, 0.2483, 0.2484],
        [0.2516, 0.2526, 0.2489, 0.2469]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2504, 0.2508, 0.2510],
        [0.2462, 0.2534, 0.2517, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2541, 0.2517, 0.2471],
        [0.2471, 0.2503, 0.2494, 0.2532],
        [0.2463, 0.2526, 0.2515, 0.2496]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2527, 0.2503, 0.2507],
        [0.2462, 0.2521, 0.2494, 0.2524],
        [0.2451, 0.2545, 0.2540, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2510, 0.2518, 0.2491],
        [0.2487, 0.2507, 0.2485, 0.2521],
        [0.2488, 0.2517, 0.2514, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2442, 0.2468, 0.2627],
        [0.2420, 0.2552, 0.2602, 0.2426],
        [0.2426, 0.2527, 0.2582, 0.2465]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2249, 0.2605, 0.2716, 0.2430],
        [0.2251, 0.2600, 0.2662, 0.2487],
        [0.2335, 0.2522, 0.2569, 0.2573]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:09:49午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 0.6258 (0.6758)	Prec@(1,5) (76.2%, 98.6%)	
05/30 10:10:17午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 0.7302 (0.6850)	Prec@(1,5) (75.3%, 98.5%)	
05/30 10:10:44午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 0.6999 (0.7002)	Prec@(1,5) (75.3%, 98.2%)	
05/30 10:11:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 0.8546 (0.7051)	Prec@(1,5) (75.0%, 98.3%)	
05/30 10:11:40午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 0.5691 (0.7039)	Prec@(1,5) (75.1%, 98.4%)	
05/30 10:12:08午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 0.7415 (0.7067)	Prec@(1,5) (75.2%, 98.3%)	
05/30 10:12:36午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 0.4567 (0.7025)	Prec@(1,5) (75.3%, 98.3%)	
05/30 10:12:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 0.6448 (0.7032)	Prec@(1,5) (75.4%, 98.3%)	
05/30 10:12:59午後 searchStage_trainer.py:221 [INFO] Train: [ 11/49] Final Prec@1 75.3800%
05/30 10:13:04午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 0.8032	Prec@(1,5) (72.2%, 97.9%)
05/30 10:13:09午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 0.8143	Prec@(1,5) (71.5%, 97.8%)
05/30 10:13:13午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 0.8182	Prec@(1,5) (71.3%, 97.9%)
05/30 10:13:18午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 0.8185	Prec@(1,5) (71.4%, 98.0%)
05/30 10:13:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 0.8114	Prec@(1,5) (71.6%, 98.0%)
05/30 10:13:27午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 0.8058	Prec@(1,5) (71.9%, 98.0%)
05/30 10:13:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 0.8057	Prec@(1,5) (71.9%, 98.0%)
05/30 10:13:35午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 0.8052	Prec@(1,5) (72.0%, 98.0%)
05/30 10:13:36午後 searchStage_trainer.py:260 [INFO] Valid: [ 11/49] Final Prec@1 71.9560%
05/30 10:13:36午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:13:36午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 71.9560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2651, 0.3166, 0.2439, 0.1744],
        [0.2745, 0.3104, 0.2404, 0.1748]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2432, 0.3244, 0.2560, 0.1764],
        [0.2560, 0.3102, 0.2592, 0.1746],
        [0.2678, 0.3502, 0.2052, 0.1768]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2575, 0.2925, 0.2671, 0.1829],
        [0.2736, 0.3336, 0.2263, 0.1665],
        [0.2752, 0.3197, 0.2148, 0.1904]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2703, 0.3146, 0.2263, 0.1888],
        [0.2723, 0.3107, 0.2206, 0.1964],
        [0.2720, 0.3136, 0.2285, 0.1860]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2800, 0.3165, 0.2176, 0.1859],
        [0.2827, 0.3154, 0.2206, 0.1813],
        [0.2774, 0.3165, 0.2181, 0.1880]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2767, 0.3068, 0.2245, 0.1920],
        [0.2760, 0.3052, 0.2175, 0.2013],
        [0.2689, 0.3053, 0.2235, 0.2023]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2518, 0.2524, 0.2487],
        [0.2494, 0.2534, 0.2510, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2497, 0.2520, 0.2471],
        [0.2550, 0.2522, 0.2524, 0.2405],
        [0.2519, 0.2537, 0.2459, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2514, 0.2517, 0.2479],
        [0.2514, 0.2549, 0.2492, 0.2446],
        [0.2520, 0.2530, 0.2474, 0.2475]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2524, 0.2496, 0.2473],
        [0.2513, 0.2526, 0.2494, 0.2467],
        [0.2523, 0.2514, 0.2475, 0.2488]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2520, 0.2492, 0.2452],
        [0.2537, 0.2524, 0.2459, 0.2480],
        [0.2531, 0.2504, 0.2459, 0.2506]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2522, 0.2475, 0.2506],
        [0.2516, 0.2517, 0.2483, 0.2484],
        [0.2516, 0.2525, 0.2489, 0.2470]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2504, 0.2508, 0.2510],
        [0.2462, 0.2534, 0.2518, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2542, 0.2517, 0.2470],
        [0.2470, 0.2503, 0.2494, 0.2533],
        [0.2463, 0.2527, 0.2515, 0.2496]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2527, 0.2503, 0.2507],
        [0.2462, 0.2521, 0.2493, 0.2524],
        [0.2451, 0.2546, 0.2540, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2511, 0.2518, 0.2491],
        [0.2488, 0.2507, 0.2484, 0.2521],
        [0.2488, 0.2517, 0.2513, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2440, 0.2467, 0.2630],
        [0.2419, 0.2553, 0.2604, 0.2425],
        [0.2426, 0.2527, 0.2583, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2243, 0.2607, 0.2721, 0.2428],
        [0.2247, 0.2602, 0.2665, 0.2487],
        [0.2334, 0.2522, 0.2570, 0.2574]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:14:05午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 0.5434 (0.6609)	Prec@(1,5) (76.9%, 98.8%)	
05/30 10:14:33午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 0.7960 (0.6703)	Prec@(1,5) (76.5%, 98.5%)	
05/30 10:14:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 0.6469 (0.6599)	Prec@(1,5) (77.1%, 98.5%)	
05/30 10:15:28午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 0.6544 (0.6725)	Prec@(1,5) (76.6%, 98.6%)	
05/30 10:15:56午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 0.4989 (0.6725)	Prec@(1,5) (76.6%, 98.6%)	
05/30 10:16:24午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 0.7558 (0.6712)	Prec@(1,5) (76.8%, 98.6%)	
05/30 10:16:52午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 0.7483 (0.6730)	Prec@(1,5) (76.6%, 98.5%)	
05/30 10:17:14午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 0.6771 (0.6705)	Prec@(1,5) (76.7%, 98.6%)	
05/30 10:17:15午後 searchStage_trainer.py:221 [INFO] Train: [ 12/49] Final Prec@1 76.6640%
05/30 10:17:20午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 0.8567	Prec@(1,5) (71.7%, 96.8%)
05/30 10:17:24午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 0.8477	Prec@(1,5) (71.8%, 97.0%)
05/30 10:17:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 0.8683	Prec@(1,5) (70.9%, 97.0%)
05/30 10:17:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 0.8657	Prec@(1,5) (70.9%, 97.0%)
05/30 10:17:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 0.8663	Prec@(1,5) (70.9%, 96.9%)
05/30 10:17:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 0.8684	Prec@(1,5) (70.9%, 96.9%)
05/30 10:17:47午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 0.8647	Prec@(1,5) (71.0%, 96.9%)
05/30 10:17:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 0.8678	Prec@(1,5) (70.8%, 96.9%)
05/30 10:17:51午後 searchStage_trainer.py:260 [INFO] Valid: [ 12/49] Final Prec@1 70.8160%
05/30 10:17:51午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:17:51午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 71.9560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2658, 0.3170, 0.2441, 0.1731],
        [0.2751, 0.3105, 0.2407, 0.1737]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2431, 0.3255, 0.2548, 0.1766],
        [0.2553, 0.3118, 0.2603, 0.1726],
        [0.2681, 0.3504, 0.2058, 0.1756]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2579, 0.2936, 0.2678, 0.1808],
        [0.2739, 0.3337, 0.2260, 0.1664],
        [0.2753, 0.3196, 0.2149, 0.1901]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2710, 0.3152, 0.2258, 0.1880],
        [0.2727, 0.3103, 0.2200, 0.1971],
        [0.2722, 0.3135, 0.2288, 0.1855]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2808, 0.3171, 0.2167, 0.1853],
        [0.2829, 0.3156, 0.2205, 0.1810],
        [0.2777, 0.3164, 0.2184, 0.1876]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2772, 0.3074, 0.2236, 0.1918],
        [0.2763, 0.3054, 0.2180, 0.2003],
        [0.2689, 0.3051, 0.2234, 0.2025]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2517, 0.2524, 0.2488],
        [0.2494, 0.2535, 0.2510, 0.2460]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2497, 0.2520, 0.2471],
        [0.2550, 0.2523, 0.2524, 0.2403],
        [0.2519, 0.2536, 0.2459, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2514, 0.2516, 0.2480],
        [0.2514, 0.2549, 0.2493, 0.2445],
        [0.2520, 0.2530, 0.2474, 0.2476]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2496, 0.2473],
        [0.2513, 0.2525, 0.2495, 0.2467],
        [0.2523, 0.2514, 0.2475, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2520, 0.2491, 0.2452],
        [0.2537, 0.2524, 0.2459, 0.2480],
        [0.2531, 0.2504, 0.2459, 0.2506]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2521, 0.2474, 0.2507],
        [0.2516, 0.2516, 0.2484, 0.2484],
        [0.2515, 0.2525, 0.2490, 0.2470]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2504, 0.2508, 0.2510],
        [0.2462, 0.2535, 0.2518, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2543, 0.2517, 0.2469],
        [0.2470, 0.2503, 0.2494, 0.2534],
        [0.2463, 0.2527, 0.2515, 0.2496]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2528, 0.2503, 0.2507],
        [0.2462, 0.2521, 0.2493, 0.2525],
        [0.2451, 0.2546, 0.2540, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2511, 0.2518, 0.2491],
        [0.2488, 0.2507, 0.2483, 0.2521],
        [0.2489, 0.2517, 0.2513, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2439, 0.2466, 0.2633],
        [0.2418, 0.2553, 0.2605, 0.2424],
        [0.2425, 0.2527, 0.2584, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2238, 0.2609, 0.2726, 0.2427],
        [0.2244, 0.2603, 0.2667, 0.2487],
        [0.2333, 0.2523, 0.2570, 0.2575]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:18:20午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 0.7637 (0.5958)	Prec@(1,5) (79.7%, 98.9%)	
05/30 10:18:48午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 0.5542 (0.6105)	Prec@(1,5) (79.2%, 98.8%)	
05/30 10:19:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 0.3900 (0.6211)	Prec@(1,5) (78.8%, 98.7%)	
05/30 10:19:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 0.5023 (0.6280)	Prec@(1,5) (78.7%, 98.6%)	
05/30 10:20:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 0.5944 (0.6331)	Prec@(1,5) (78.4%, 98.6%)	
05/30 10:20:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 0.5457 (0.6308)	Prec@(1,5) (78.4%, 98.6%)	
05/30 10:21:08午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 0.7498 (0.6336)	Prec@(1,5) (78.3%, 98.6%)	
05/30 10:21:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 0.6004 (0.6365)	Prec@(1,5) (78.1%, 98.6%)	
05/30 10:21:30午後 searchStage_trainer.py:221 [INFO] Train: [ 13/49] Final Prec@1 78.1120%
05/30 10:21:35午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 0.8090	Prec@(1,5) (72.7%, 98.1%)
05/30 10:21:40午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 0.7907	Prec@(1,5) (72.7%, 98.2%)
05/30 10:21:44午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 0.7885	Prec@(1,5) (72.8%, 98.2%)
05/30 10:21:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 0.7883	Prec@(1,5) (72.7%, 98.2%)
05/30 10:21:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 0.7892	Prec@(1,5) (72.8%, 98.1%)
05/30 10:21:58午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 0.7874	Prec@(1,5) (72.7%, 98.0%)
05/30 10:22:03午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 0.7865	Prec@(1,5) (72.7%, 98.0%)
05/30 10:22:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 0.7866	Prec@(1,5) (72.7%, 98.0%)
05/30 10:22:07午後 searchStage_trainer.py:260 [INFO] Valid: [ 13/49] Final Prec@1 72.7280%
05/30 10:22:07午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:22:07午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 72.7280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2658, 0.3176, 0.2445, 0.1720],
        [0.2749, 0.3107, 0.2409, 0.1735]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2432, 0.3262, 0.2549, 0.1757],
        [0.2555, 0.3128, 0.2599, 0.1719],
        [0.2681, 0.3504, 0.2062, 0.1753]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2944, 0.2676, 0.1804],
        [0.2741, 0.3338, 0.2260, 0.1661],
        [0.2754, 0.3194, 0.2152, 0.1900]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2714, 0.3153, 0.2255, 0.1878],
        [0.2730, 0.3100, 0.2200, 0.1970],
        [0.2721, 0.3132, 0.2291, 0.1856]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2812, 0.3178, 0.2161, 0.1850],
        [0.2830, 0.3157, 0.2207, 0.1805],
        [0.2776, 0.3162, 0.2184, 0.1878]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2775, 0.3076, 0.2235, 0.1914],
        [0.2763, 0.3052, 0.2178, 0.2007],
        [0.2691, 0.3048, 0.2237, 0.2024]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2516, 0.2524, 0.2488],
        [0.2494, 0.2535, 0.2511, 0.2460]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2498, 0.2520, 0.2470],
        [0.2551, 0.2523, 0.2524, 0.2402],
        [0.2519, 0.2536, 0.2459, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2514, 0.2516, 0.2480],
        [0.2513, 0.2549, 0.2493, 0.2445],
        [0.2520, 0.2530, 0.2474, 0.2476]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2525, 0.2496, 0.2473],
        [0.2513, 0.2525, 0.2495, 0.2467],
        [0.2523, 0.2513, 0.2475, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2519, 0.2491, 0.2452],
        [0.2537, 0.2523, 0.2459, 0.2480],
        [0.2531, 0.2504, 0.2459, 0.2506]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2521, 0.2474, 0.2507],
        [0.2516, 0.2516, 0.2483, 0.2484],
        [0.2515, 0.2525, 0.2490, 0.2470]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2504, 0.2508, 0.2510],
        [0.2462, 0.2535, 0.2517, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2543, 0.2517, 0.2469],
        [0.2470, 0.2503, 0.2494, 0.2534],
        [0.2463, 0.2527, 0.2515, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2528, 0.2503, 0.2507],
        [0.2462, 0.2520, 0.2493, 0.2525],
        [0.2451, 0.2546, 0.2540, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2511, 0.2518, 0.2491],
        [0.2489, 0.2507, 0.2483, 0.2522],
        [0.2489, 0.2517, 0.2513, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2437, 0.2465, 0.2636],
        [0.2417, 0.2554, 0.2606, 0.2423],
        [0.2425, 0.2527, 0.2584, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2234, 0.2611, 0.2731, 0.2425],
        [0.2241, 0.2604, 0.2669, 0.2486],
        [0.2332, 0.2523, 0.2570, 0.2575]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:22:36午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 0.6826 (0.5861)	Prec@(1,5) (79.5%, 99.0%)	
05/30 10:23:04午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 0.6350 (0.6070)	Prec@(1,5) (79.0%, 98.8%)	
05/30 10:23:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 0.6999 (0.6031)	Prec@(1,5) (79.1%, 98.9%)	
05/30 10:23:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 0.4723 (0.5950)	Prec@(1,5) (79.1%, 99.0%)	
05/30 10:24:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 0.7940 (0.6042)	Prec@(1,5) (79.0%, 98.9%)	
05/30 10:24:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 0.5363 (0.6063)	Prec@(1,5) (78.9%, 98.9%)	
05/30 10:25:23午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 0.6839 (0.6089)	Prec@(1,5) (78.8%, 98.8%)	
05/30 10:25:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 0.5009 (0.6123)	Prec@(1,5) (78.6%, 98.8%)	
05/30 10:25:46午後 searchStage_trainer.py:221 [INFO] Train: [ 14/49] Final Prec@1 78.6520%
05/30 10:25:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 0.7300	Prec@(1,5) (74.5%, 98.0%)
05/30 10:25:55午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 0.7160	Prec@(1,5) (74.8%, 97.9%)
05/30 10:26:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 0.7183	Prec@(1,5) (74.8%, 98.0%)
05/30 10:26:05午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 0.7154	Prec@(1,5) (74.9%, 98.1%)
05/30 10:26:09午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 0.7164	Prec@(1,5) (74.9%, 98.1%)
05/30 10:26:14午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 0.7250	Prec@(1,5) (74.5%, 98.1%)
05/30 10:26:18午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 0.7253	Prec@(1,5) (74.7%, 98.1%)
05/30 10:26:22午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 0.7241	Prec@(1,5) (74.7%, 98.1%)
05/30 10:26:22午後 searchStage_trainer.py:260 [INFO] Valid: [ 14/49] Final Prec@1 74.6640%
05/30 10:26:22午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:26:23午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 74.6640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2660, 0.3174, 0.2451, 0.1716],
        [0.2747, 0.3109, 0.2409, 0.1735]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2437, 0.3264, 0.2548, 0.1751],
        [0.2560, 0.3128, 0.2600, 0.1712],
        [0.2682, 0.3501, 0.2063, 0.1753]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2580, 0.2950, 0.2679, 0.1791],
        [0.2742, 0.3335, 0.2262, 0.1661],
        [0.2755, 0.3189, 0.2153, 0.1904]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2712, 0.3154, 0.2254, 0.1879],
        [0.2729, 0.3099, 0.2202, 0.1970],
        [0.2720, 0.3127, 0.2293, 0.1859]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2814, 0.3183, 0.2159, 0.1845],
        [0.2830, 0.3155, 0.2208, 0.1807],
        [0.2774, 0.3159, 0.2187, 0.1880]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2776, 0.3076, 0.2233, 0.1915],
        [0.2763, 0.3049, 0.2178, 0.2010],
        [0.2691, 0.3044, 0.2240, 0.2025]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2516, 0.2524, 0.2489],
        [0.2495, 0.2535, 0.2510, 0.2460]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2497, 0.2519, 0.2469],
        [0.2553, 0.2523, 0.2524, 0.2401],
        [0.2519, 0.2536, 0.2459, 0.2487]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2515, 0.2516, 0.2478],
        [0.2513, 0.2549, 0.2493, 0.2446],
        [0.2520, 0.2530, 0.2474, 0.2476]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2525, 0.2497, 0.2472],
        [0.2513, 0.2525, 0.2495, 0.2467],
        [0.2523, 0.2513, 0.2475, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2519, 0.2492, 0.2452],
        [0.2537, 0.2523, 0.2459, 0.2481],
        [0.2531, 0.2504, 0.2459, 0.2507]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2521, 0.2474, 0.2507],
        [0.2516, 0.2516, 0.2483, 0.2485],
        [0.2515, 0.2525, 0.2490, 0.2470]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2504, 0.2507, 0.2511],
        [0.2462, 0.2535, 0.2518, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2544, 0.2518, 0.2469],
        [0.2469, 0.2502, 0.2494, 0.2535],
        [0.2463, 0.2527, 0.2515, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2529, 0.2502, 0.2508],
        [0.2462, 0.2520, 0.2492, 0.2526],
        [0.2451, 0.2546, 0.2540, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2511, 0.2518, 0.2490],
        [0.2489, 0.2507, 0.2483, 0.2522],
        [0.2489, 0.2517, 0.2513, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2436, 0.2464, 0.2638],
        [0.2417, 0.2554, 0.2607, 0.2422],
        [0.2425, 0.2527, 0.2585, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2230, 0.2612, 0.2735, 0.2424],
        [0.2239, 0.2605, 0.2671, 0.2486],
        [0.2331, 0.2523, 0.2571, 0.2576]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:26:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 0.5929 (0.5482)	Prec@(1,5) (81.5%, 98.9%)	
05/30 10:27:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 0.6383 (0.5592)	Prec@(1,5) (80.9%, 99.0%)	
05/30 10:27:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 0.5325 (0.5654)	Prec@(1,5) (80.4%, 98.9%)	
05/30 10:28:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 0.5615 (0.5705)	Prec@(1,5) (80.4%, 98.8%)	
05/30 10:28:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 0.5853 (0.5717)	Prec@(1,5) (80.2%, 99.0%)	
05/30 10:29:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 0.7213 (0.5798)	Prec@(1,5) (79.9%, 99.0%)	
05/30 10:29:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 0.5041 (0.5831)	Prec@(1,5) (79.7%, 98.9%)	
05/30 10:30:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 0.5498 (0.5844)	Prec@(1,5) (79.7%, 98.9%)	
05/30 10:30:02午後 searchStage_trainer.py:221 [INFO] Train: [ 15/49] Final Prec@1 79.6720%
05/30 10:30:07午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 0.7267	Prec@(1,5) (74.2%, 98.2%)
05/30 10:30:11午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 0.7265	Prec@(1,5) (74.5%, 98.3%)
05/30 10:30:16午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 0.7264	Prec@(1,5) (74.6%, 98.2%)
05/30 10:30:21午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 0.7273	Prec@(1,5) (74.5%, 98.3%)
05/30 10:30:25午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 0.7292	Prec@(1,5) (74.4%, 98.2%)
05/30 10:30:30午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 0.7354	Prec@(1,5) (74.2%, 98.2%)
05/30 10:30:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 0.7378	Prec@(1,5) (74.1%, 98.2%)
05/30 10:30:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 0.7388	Prec@(1,5) (74.1%, 98.2%)
05/30 10:30:38午後 searchStage_trainer.py:260 [INFO] Valid: [ 15/49] Final Prec@1 74.0880%
05/30 10:30:38午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:30:39午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 74.6640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2657, 0.3174, 0.2457, 0.1712],
        [0.2746, 0.3107, 0.2409, 0.1739]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2440, 0.3267, 0.2551, 0.1743],
        [0.2560, 0.3127, 0.2603, 0.1710],
        [0.2682, 0.3498, 0.2064, 0.1757]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2951, 0.2685, 0.1789],
        [0.2741, 0.3332, 0.2264, 0.1663],
        [0.2755, 0.3184, 0.2155, 0.1906]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2711, 0.3154, 0.2257, 0.1879],
        [0.2728, 0.3097, 0.2205, 0.1971],
        [0.2718, 0.3122, 0.2295, 0.1864]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2817, 0.3183, 0.2159, 0.1841],
        [0.2830, 0.3152, 0.2210, 0.1808],
        [0.2773, 0.3155, 0.2188, 0.1884]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2778, 0.3076, 0.2232, 0.1914],
        [0.2762, 0.3045, 0.2179, 0.2013],
        [0.2690, 0.3039, 0.2242, 0.2030]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2516, 0.2525, 0.2488],
        [0.2495, 0.2534, 0.2510, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2496, 0.2519, 0.2470],
        [0.2553, 0.2523, 0.2524, 0.2400],
        [0.2519, 0.2535, 0.2459, 0.2487]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2515, 0.2518, 0.2477],
        [0.2513, 0.2549, 0.2493, 0.2446],
        [0.2520, 0.2530, 0.2474, 0.2477]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2497, 0.2471],
        [0.2513, 0.2525, 0.2495, 0.2467],
        [0.2522, 0.2513, 0.2475, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2519, 0.2491, 0.2453],
        [0.2537, 0.2523, 0.2459, 0.2481],
        [0.2531, 0.2504, 0.2459, 0.2507]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2520, 0.2474, 0.2507],
        [0.2516, 0.2516, 0.2483, 0.2485],
        [0.2515, 0.2525, 0.2490, 0.2471]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2504, 0.2507, 0.2511],
        [0.2462, 0.2535, 0.2517, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2544, 0.2517, 0.2468],
        [0.2470, 0.2502, 0.2493, 0.2535],
        [0.2463, 0.2527, 0.2515, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2529, 0.2502, 0.2508],
        [0.2462, 0.2520, 0.2492, 0.2526],
        [0.2451, 0.2546, 0.2540, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2511, 0.2518, 0.2490],
        [0.2489, 0.2506, 0.2482, 0.2522],
        [0.2490, 0.2516, 0.2512, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2435, 0.2464, 0.2640],
        [0.2417, 0.2554, 0.2608, 0.2421],
        [0.2425, 0.2527, 0.2585, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2226, 0.2613, 0.2738, 0.2422],
        [0.2237, 0.2606, 0.2672, 0.2486],
        [0.2331, 0.2523, 0.2571, 0.2576]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:31:08午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 0.5003 (0.5343)	Prec@(1,5) (80.9%, 99.1%)	
05/30 10:31:35午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 0.4809 (0.5505)	Prec@(1,5) (80.5%, 99.0%)	
05/30 10:32:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 0.3966 (0.5542)	Prec@(1,5) (80.6%, 98.9%)	
05/30 10:32:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 0.4606 (0.5578)	Prec@(1,5) (80.5%, 98.9%)	
05/30 10:32:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 0.4862 (0.5599)	Prec@(1,5) (80.5%, 99.0%)	
05/30 10:33:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 0.4578 (0.5588)	Prec@(1,5) (80.5%, 99.0%)	
05/30 10:33:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 0.7561 (0.5613)	Prec@(1,5) (80.5%, 99.0%)	
05/30 10:34:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 0.4853 (0.5622)	Prec@(1,5) (80.4%, 99.0%)	
05/30 10:34:18午後 searchStage_trainer.py:221 [INFO] Train: [ 16/49] Final Prec@1 80.4320%
05/30 10:34:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 0.6182	Prec@(1,5) (78.8%, 99.0%)
05/30 10:34:27午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 0.6418	Prec@(1,5) (78.1%, 98.8%)
05/30 10:34:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 0.6523	Prec@(1,5) (77.7%, 98.7%)
05/30 10:34:37午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 0.6511	Prec@(1,5) (77.7%, 98.7%)
05/30 10:34:41午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 0.6522	Prec@(1,5) (77.6%, 98.6%)
05/30 10:34:46午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 0.6560	Prec@(1,5) (77.5%, 98.6%)
05/30 10:34:50午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 0.6559	Prec@(1,5) (77.5%, 98.6%)
05/30 10:34:54午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 0.6539	Prec@(1,5) (77.6%, 98.6%)
05/30 10:34:54午後 searchStage_trainer.py:260 [INFO] Valid: [ 16/49] Final Prec@1 77.5480%
05/30 10:34:54午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:34:55午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.5480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2655, 0.3171, 0.2461, 0.1713],
        [0.2744, 0.3103, 0.2413, 0.1740]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.3265, 0.2554, 0.1739],
        [0.2562, 0.3126, 0.2603, 0.1709],
        [0.2682, 0.3494, 0.2065, 0.1760]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2956, 0.2691, 0.1777],
        [0.2741, 0.3329, 0.2265, 0.1665],
        [0.2754, 0.3179, 0.2156, 0.1911]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2711, 0.3153, 0.2259, 0.1877],
        [0.2727, 0.3093, 0.2205, 0.1975],
        [0.2718, 0.3118, 0.2296, 0.1868]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2819, 0.3181, 0.2155, 0.1845],
        [0.2831, 0.3148, 0.2212, 0.1808],
        [0.2773, 0.3150, 0.2189, 0.1888]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2780, 0.3073, 0.2231, 0.1916],
        [0.2762, 0.3041, 0.2179, 0.2018],
        [0.2689, 0.3034, 0.2245, 0.2033]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2515, 0.2525, 0.2488],
        [0.2495, 0.2533, 0.2510, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2496, 0.2519, 0.2470],
        [0.2552, 0.2523, 0.2525, 0.2400],
        [0.2519, 0.2535, 0.2459, 0.2488]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2516, 0.2518, 0.2476],
        [0.2513, 0.2548, 0.2493, 0.2446],
        [0.2520, 0.2529, 0.2474, 0.2477]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2525, 0.2497, 0.2471],
        [0.2513, 0.2524, 0.2495, 0.2468],
        [0.2522, 0.2513, 0.2475, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2519, 0.2491, 0.2452],
        [0.2537, 0.2523, 0.2459, 0.2481],
        [0.2531, 0.2503, 0.2459, 0.2507]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2520, 0.2475, 0.2507],
        [0.2516, 0.2515, 0.2484, 0.2485],
        [0.2515, 0.2525, 0.2490, 0.2471]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2504, 0.2507, 0.2511],
        [0.2462, 0.2535, 0.2517, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2544, 0.2517, 0.2468],
        [0.2470, 0.2502, 0.2493, 0.2535],
        [0.2463, 0.2527, 0.2515, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2529, 0.2502, 0.2509],
        [0.2462, 0.2520, 0.2491, 0.2526],
        [0.2451, 0.2546, 0.2539, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2511, 0.2517, 0.2490],
        [0.2490, 0.2506, 0.2482, 0.2522],
        [0.2490, 0.2516, 0.2512, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2434, 0.2463, 0.2641],
        [0.2416, 0.2555, 0.2609, 0.2420],
        [0.2425, 0.2527, 0.2585, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2224, 0.2614, 0.2741, 0.2421],
        [0.2235, 0.2606, 0.2673, 0.2486],
        [0.2331, 0.2523, 0.2571, 0.2576]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:35:24午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 0.6075 (0.5303)	Prec@(1,5) (81.3%, 99.2%)	
05/30 10:35:52午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 0.6578 (0.5261)	Prec@(1,5) (81.7%, 99.1%)	
05/30 10:36:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 0.4715 (0.5280)	Prec@(1,5) (81.7%, 99.1%)	
05/30 10:36:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 0.5169 (0.5340)	Prec@(1,5) (81.6%, 99.1%)	
05/30 10:37:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 0.3210 (0.5344)	Prec@(1,5) (81.6%, 99.1%)	
05/30 10:37:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 0.5701 (0.5385)	Prec@(1,5) (81.4%, 99.1%)	
05/30 10:38:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 0.7857 (0.5399)	Prec@(1,5) (81.3%, 99.1%)	
05/30 10:38:33午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 0.7249 (0.5397)	Prec@(1,5) (81.4%, 99.1%)	
05/30 10:38:34午後 searchStage_trainer.py:221 [INFO] Train: [ 17/49] Final Prec@1 81.4320%
05/30 10:38:39午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 0.6585	Prec@(1,5) (77.9%, 98.1%)
05/30 10:38:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 0.6670	Prec@(1,5) (77.3%, 98.2%)
05/30 10:38:48午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 0.6662	Prec@(1,5) (77.4%, 98.1%)
05/30 10:38:52午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 0.6676	Prec@(1,5) (77.1%, 98.2%)
05/30 10:38:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 0.6612	Prec@(1,5) (77.3%, 98.2%)
05/30 10:39:02午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 0.6579	Prec@(1,5) (77.3%, 98.3%)
05/30 10:39:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 0.6582	Prec@(1,5) (77.2%, 98.4%)
05/30 10:39:10午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 0.6615	Prec@(1,5) (77.2%, 98.4%)
05/30 10:39:10午後 searchStage_trainer.py:260 [INFO] Valid: [ 17/49] Final Prec@1 77.2240%
05/30 10:39:10午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:39:10午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.5480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2656, 0.3168, 0.2463, 0.1714],
        [0.2744, 0.3098, 0.2415, 0.1744]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.3263, 0.2554, 0.1743],
        [0.2559, 0.3128, 0.2610, 0.1703],
        [0.2681, 0.3489, 0.2067, 0.1763]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2570, 0.2960, 0.2694, 0.1776],
        [0.2742, 0.3325, 0.2264, 0.1668],
        [0.2754, 0.3174, 0.2158, 0.1914]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2711, 0.3153, 0.2262, 0.1874],
        [0.2726, 0.3089, 0.2206, 0.1979],
        [0.2716, 0.3113, 0.2297, 0.1874]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2821, 0.3181, 0.2156, 0.1843],
        [0.2831, 0.3145, 0.2212, 0.1812],
        [0.2772, 0.3146, 0.2190, 0.1892]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2781, 0.3071, 0.2231, 0.1917],
        [0.2762, 0.3037, 0.2178, 0.2024],
        [0.2687, 0.3029, 0.2247, 0.2037]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2515, 0.2525, 0.2488],
        [0.2496, 0.2533, 0.2510, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2496, 0.2519, 0.2470],
        [0.2552, 0.2523, 0.2525, 0.2400],
        [0.2519, 0.2535, 0.2459, 0.2488]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2516, 0.2518, 0.2476],
        [0.2513, 0.2548, 0.2493, 0.2446],
        [0.2519, 0.2529, 0.2474, 0.2477]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2525, 0.2497, 0.2471],
        [0.2513, 0.2524, 0.2495, 0.2467],
        [0.2522, 0.2513, 0.2475, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2518, 0.2491, 0.2453],
        [0.2537, 0.2523, 0.2459, 0.2481],
        [0.2531, 0.2503, 0.2459, 0.2507]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2520, 0.2475, 0.2507],
        [0.2516, 0.2515, 0.2484, 0.2485],
        [0.2514, 0.2525, 0.2490, 0.2471]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2504, 0.2507, 0.2511],
        [0.2462, 0.2535, 0.2517, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2545, 0.2517, 0.2468],
        [0.2470, 0.2502, 0.2493, 0.2535],
        [0.2464, 0.2527, 0.2514, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2530, 0.2501, 0.2509],
        [0.2462, 0.2520, 0.2491, 0.2527],
        [0.2452, 0.2546, 0.2539, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2511, 0.2517, 0.2490],
        [0.2490, 0.2506, 0.2482, 0.2522],
        [0.2491, 0.2516, 0.2512, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2433, 0.2462, 0.2643],
        [0.2416, 0.2555, 0.2609, 0.2420],
        [0.2425, 0.2527, 0.2585, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.2615, 0.2743, 0.2421],
        [0.2234, 0.2607, 0.2674, 0.2486],
        [0.2330, 0.2522, 0.2571, 0.2576]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:39:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 0.6027 (0.4916)	Prec@(1,5) (82.5%, 99.1%)	
05/30 10:40:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 0.5358 (0.4977)	Prec@(1,5) (82.5%, 99.2%)	
05/30 10:40:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 0.4155 (0.5085)	Prec@(1,5) (82.2%, 99.2%)	
05/30 10:41:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 0.4491 (0.5095)	Prec@(1,5) (82.1%, 99.3%)	
05/30 10:41:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 0.4524 (0.5147)	Prec@(1,5) (81.8%, 99.2%)	
05/30 10:41:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 0.4963 (0.5139)	Prec@(1,5) (81.9%, 99.2%)	
05/30 10:42:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 0.4161 (0.5134)	Prec@(1,5) (82.0%, 99.2%)	
05/30 10:42:49午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 0.4969 (0.5152)	Prec@(1,5) (81.9%, 99.2%)	
05/30 10:42:50午後 searchStage_trainer.py:221 [INFO] Train: [ 18/49] Final Prec@1 81.8840%
05/30 10:42:54午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 0.6546	Prec@(1,5) (78.1%, 98.4%)
05/30 10:42:59午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 0.6453	Prec@(1,5) (78.3%, 98.5%)
05/30 10:43:04午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 0.6579	Prec@(1,5) (78.0%, 98.5%)
05/30 10:43:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 0.6563	Prec@(1,5) (77.9%, 98.5%)
05/30 10:43:13午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 0.6566	Prec@(1,5) (77.9%, 98.5%)
05/30 10:43:18午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 0.6633	Prec@(1,5) (77.7%, 98.5%)
05/30 10:43:22午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 0.6641	Prec@(1,5) (77.7%, 98.5%)
05/30 10:43:26午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 0.6650	Prec@(1,5) (77.6%, 98.5%)
05/30 10:43:26午後 searchStage_trainer.py:260 [INFO] Valid: [ 18/49] Final Prec@1 77.5960%
05/30 10:43:26午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:43:26午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.5960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2655, 0.3162, 0.2464, 0.1718],
        [0.2744, 0.3095, 0.2416, 0.1745]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.3263, 0.2553, 0.1746],
        [0.2560, 0.3128, 0.2613, 0.1699],
        [0.2680, 0.3484, 0.2069, 0.1767]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2570, 0.2960, 0.2697, 0.1773],
        [0.2742, 0.3321, 0.2264, 0.1673],
        [0.2754, 0.3169, 0.2160, 0.1917]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2713, 0.3151, 0.2262, 0.1874],
        [0.2726, 0.3084, 0.2207, 0.1983],
        [0.2715, 0.3109, 0.2299, 0.1878]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2822, 0.3179, 0.2155, 0.1844],
        [0.2831, 0.3142, 0.2213, 0.1814],
        [0.2770, 0.3142, 0.2192, 0.1896]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2780, 0.3069, 0.2232, 0.1918],
        [0.2761, 0.3032, 0.2179, 0.2028],
        [0.2685, 0.3024, 0.2250, 0.2041]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2515, 0.2525, 0.2488],
        [0.2496, 0.2533, 0.2510, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2496, 0.2519, 0.2469],
        [0.2553, 0.2522, 0.2525, 0.2400],
        [0.2519, 0.2535, 0.2459, 0.2488]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2518, 0.2476],
        [0.2513, 0.2548, 0.2493, 0.2446],
        [0.2519, 0.2529, 0.2474, 0.2477]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2525, 0.2497, 0.2471],
        [0.2513, 0.2525, 0.2496, 0.2467],
        [0.2522, 0.2512, 0.2475, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2518, 0.2491, 0.2453],
        [0.2537, 0.2522, 0.2459, 0.2481],
        [0.2531, 0.2503, 0.2459, 0.2507]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2520, 0.2475, 0.2507],
        [0.2516, 0.2515, 0.2483, 0.2485],
        [0.2514, 0.2524, 0.2490, 0.2471]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2503, 0.2507, 0.2511],
        [0.2462, 0.2535, 0.2517, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2545, 0.2517, 0.2467],
        [0.2470, 0.2502, 0.2493, 0.2536],
        [0.2464, 0.2527, 0.2514, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2530, 0.2501, 0.2509],
        [0.2462, 0.2520, 0.2491, 0.2527],
        [0.2452, 0.2546, 0.2539, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2511, 0.2517, 0.2490],
        [0.2490, 0.2506, 0.2481, 0.2523],
        [0.2491, 0.2516, 0.2512, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2433, 0.2462, 0.2644],
        [0.2416, 0.2555, 0.2610, 0.2419],
        [0.2425, 0.2527, 0.2585, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2218, 0.2616, 0.2746, 0.2420],
        [0.2233, 0.2607, 0.2675, 0.2485],
        [0.2330, 0.2522, 0.2571, 0.2577]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:43:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 0.4310 (0.4948)	Prec@(1,5) (82.8%, 99.3%)	
05/30 10:44:23午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 0.3809 (0.4762)	Prec@(1,5) (83.4%, 99.4%)	
05/30 10:44:50午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 0.3450 (0.4870)	Prec@(1,5) (83.1%, 99.3%)	
05/30 10:45:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 0.4240 (0.4953)	Prec@(1,5) (82.7%, 99.3%)	
05/30 10:45:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 0.5164 (0.4952)	Prec@(1,5) (82.7%, 99.3%)	
05/30 10:46:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 0.3792 (0.4987)	Prec@(1,5) (82.6%, 99.3%)	
05/30 10:46:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 0.4672 (0.4996)	Prec@(1,5) (82.7%, 99.3%)	
05/30 10:47:05午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 0.4745 (0.5026)	Prec@(1,5) (82.6%, 99.2%)	
05/30 10:47:06午後 searchStage_trainer.py:221 [INFO] Train: [ 19/49] Final Prec@1 82.6000%
05/30 10:47:11午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 0.6809	Prec@(1,5) (76.5%, 98.1%)
05/30 10:47:15午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 0.6795	Prec@(1,5) (76.9%, 98.0%)
05/30 10:47:20午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 0.6814	Prec@(1,5) (76.4%, 98.2%)
05/30 10:47:24午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 0.6761	Prec@(1,5) (76.7%, 98.2%)
05/30 10:47:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 0.6747	Prec@(1,5) (76.8%, 98.3%)
05/30 10:47:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 0.6694	Prec@(1,5) (76.9%, 98.3%)
05/30 10:47:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 0.6713	Prec@(1,5) (76.8%, 98.4%)
05/30 10:47:42午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 0.6738	Prec@(1,5) (76.7%, 98.3%)
05/30 10:47:42午後 searchStage_trainer.py:260 [INFO] Valid: [ 19/49] Final Prec@1 76.6480%
05/30 10:47:42午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:47:42午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.5960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2654, 0.3159, 0.2468, 0.1720],
        [0.2742, 0.3092, 0.2418, 0.1748]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2438, 0.3263, 0.2555, 0.1744],
        [0.2561, 0.3127, 0.2612, 0.1700],
        [0.2678, 0.3480, 0.2071, 0.1771]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2569, 0.2962, 0.2700, 0.1768],
        [0.2740, 0.3318, 0.2265, 0.1677],
        [0.2754, 0.3165, 0.2161, 0.1920]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2712, 0.3148, 0.2262, 0.1877],
        [0.2725, 0.3081, 0.2208, 0.1986],
        [0.2714, 0.3104, 0.2300, 0.1881]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2823, 0.3177, 0.2153, 0.1847],
        [0.2830, 0.3138, 0.2214, 0.1818],
        [0.2770, 0.3138, 0.2193, 0.1899]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2779, 0.3066, 0.2233, 0.1922],
        [0.2760, 0.3029, 0.2179, 0.2032],
        [0.2684, 0.3019, 0.2252, 0.2045]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2515, 0.2525, 0.2489],
        [0.2496, 0.2533, 0.2510, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2519, 0.2469],
        [0.2553, 0.2522, 0.2525, 0.2400],
        [0.2519, 0.2534, 0.2459, 0.2488]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2518, 0.2475],
        [0.2513, 0.2548, 0.2493, 0.2446],
        [0.2519, 0.2529, 0.2474, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2525, 0.2497, 0.2471],
        [0.2513, 0.2524, 0.2496, 0.2467],
        [0.2522, 0.2512, 0.2475, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2518, 0.2490, 0.2454],
        [0.2537, 0.2522, 0.2459, 0.2481],
        [0.2531, 0.2503, 0.2459, 0.2507]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2507],
        [0.2517, 0.2515, 0.2483, 0.2485],
        [0.2514, 0.2524, 0.2490, 0.2471]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2503, 0.2507, 0.2511],
        [0.2463, 0.2535, 0.2517, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2545, 0.2517, 0.2467],
        [0.2470, 0.2502, 0.2493, 0.2536],
        [0.2464, 0.2526, 0.2514, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2530, 0.2501, 0.2509],
        [0.2462, 0.2520, 0.2490, 0.2527],
        [0.2452, 0.2545, 0.2539, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2510, 0.2517, 0.2490],
        [0.2491, 0.2506, 0.2481, 0.2523],
        [0.2491, 0.2516, 0.2511, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2432, 0.2462, 0.2645],
        [0.2416, 0.2555, 0.2610, 0.2419],
        [0.2425, 0.2527, 0.2586, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2216, 0.2617, 0.2748, 0.2419],
        [0.2232, 0.2607, 0.2676, 0.2485],
        [0.2330, 0.2522, 0.2571, 0.2577]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:48:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 0.4625 (0.4756)	Prec@(1,5) (83.6%, 99.1%)	
05/30 10:48:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 0.5718 (0.4547)	Prec@(1,5) (84.2%, 99.2%)	
05/30 10:49:06午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 0.4677 (0.4633)	Prec@(1,5) (84.1%, 99.3%)	
05/30 10:49:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 0.5376 (0.4685)	Prec@(1,5) (83.9%, 99.4%)	
05/30 10:50:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 0.3651 (0.4715)	Prec@(1,5) (83.7%, 99.3%)	
05/30 10:50:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 0.5155 (0.4745)	Prec@(1,5) (83.6%, 99.4%)	
05/30 10:50:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 0.4658 (0.4781)	Prec@(1,5) (83.5%, 99.3%)	
05/30 10:51:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 0.4953 (0.4785)	Prec@(1,5) (83.5%, 99.3%)	
05/30 10:51:22午後 searchStage_trainer.py:221 [INFO] Train: [ 20/49] Final Prec@1 83.4600%
05/30 10:51:26午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 0.6259	Prec@(1,5) (78.8%, 98.6%)
05/30 10:51:31午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 0.6360	Prec@(1,5) (78.4%, 98.5%)
05/30 10:51:36午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 0.6367	Prec@(1,5) (78.4%, 98.6%)
05/30 10:51:40午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 0.6374	Prec@(1,5) (78.4%, 98.6%)
05/30 10:51:45午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 0.6492	Prec@(1,5) (77.9%, 98.6%)
05/30 10:51:50午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 0.6493	Prec@(1,5) (78.1%, 98.5%)
05/30 10:51:54午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 0.6485	Prec@(1,5) (78.1%, 98.6%)
05/30 10:51:58午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 0.6452	Prec@(1,5) (78.2%, 98.6%)
05/30 10:51:58午後 searchStage_trainer.py:260 [INFO] Valid: [ 20/49] Final Prec@1 78.1680%
05/30 10:51:58午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:51:58午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 78.1680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2651, 0.3156, 0.2471, 0.1722],
        [0.2738, 0.3089, 0.2422, 0.1751]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2436, 0.3262, 0.2558, 0.1743],
        [0.2561, 0.3127, 0.2615, 0.1698],
        [0.2676, 0.3477, 0.2072, 0.1775]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2568, 0.2963, 0.2704, 0.1765],
        [0.2739, 0.3315, 0.2266, 0.1680],
        [0.2753, 0.3161, 0.2162, 0.1924]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2712, 0.3147, 0.2264, 0.1877],
        [0.2725, 0.3077, 0.2209, 0.1989],
        [0.2713, 0.3100, 0.2301, 0.1886]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2821, 0.3175, 0.2153, 0.1850],
        [0.2829, 0.3136, 0.2216, 0.1819],
        [0.2768, 0.3134, 0.2195, 0.1903]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2778, 0.3065, 0.2234, 0.1923],
        [0.2759, 0.3026, 0.2181, 0.2035],
        [0.2682, 0.3015, 0.2253, 0.2049]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2514, 0.2525, 0.2489],
        [0.2496, 0.2533, 0.2510, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2520, 0.2469],
        [0.2553, 0.2522, 0.2525, 0.2400],
        [0.2519, 0.2534, 0.2459, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2518, 0.2475],
        [0.2513, 0.2548, 0.2493, 0.2446],
        [0.2519, 0.2529, 0.2474, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2525, 0.2497, 0.2471],
        [0.2513, 0.2524, 0.2496, 0.2467],
        [0.2522, 0.2512, 0.2475, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2518, 0.2490, 0.2454],
        [0.2538, 0.2522, 0.2459, 0.2481],
        [0.2531, 0.2503, 0.2459, 0.2507]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2507],
        [0.2517, 0.2515, 0.2484, 0.2485],
        [0.2514, 0.2524, 0.2490, 0.2472]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2503, 0.2507, 0.2511],
        [0.2463, 0.2535, 0.2517, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2545, 0.2517, 0.2467],
        [0.2470, 0.2502, 0.2492, 0.2536],
        [0.2464, 0.2526, 0.2514, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2530, 0.2501, 0.2510],
        [0.2463, 0.2520, 0.2490, 0.2527],
        [0.2452, 0.2545, 0.2539, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2510, 0.2517, 0.2490],
        [0.2491, 0.2506, 0.2481, 0.2523],
        [0.2492, 0.2516, 0.2511, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2431, 0.2461, 0.2646],
        [0.2416, 0.2555, 0.2610, 0.2418],
        [0.2425, 0.2527, 0.2586, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2214, 0.2618, 0.2749, 0.2418],
        [0.2231, 0.2608, 0.2676, 0.2485],
        [0.2330, 0.2522, 0.2571, 0.2577]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:52:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 0.5582 (0.4563)	Prec@(1,5) (84.3%, 99.4%)	
05/30 10:52:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 0.4531 (0.4562)	Prec@(1,5) (84.2%, 99.3%)	
05/30 10:53:22午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 0.4147 (0.4472)	Prec@(1,5) (84.7%, 99.4%)	
05/30 10:53:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 0.5878 (0.4520)	Prec@(1,5) (84.6%, 99.4%)	
05/30 10:54:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 0.6485 (0.4574)	Prec@(1,5) (84.3%, 99.4%)	
05/30 10:54:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 0.4166 (0.4607)	Prec@(1,5) (84.2%, 99.4%)	
05/30 10:55:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 0.3729 (0.4638)	Prec@(1,5) (84.1%, 99.3%)	
05/30 10:55:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 0.7083 (0.4637)	Prec@(1,5) (84.1%, 99.3%)	
05/30 10:55:38午後 searchStage_trainer.py:221 [INFO] Train: [ 21/49] Final Prec@1 84.0960%
05/30 10:55:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 0.6434	Prec@(1,5) (77.6%, 98.5%)
05/30 10:55:47午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 0.6392	Prec@(1,5) (77.9%, 98.5%)
05/30 10:55:52午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 0.6352	Prec@(1,5) (78.4%, 98.6%)
05/30 10:55:56午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 0.6270	Prec@(1,5) (78.7%, 98.6%)
05/30 10:56:01午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 0.6280	Prec@(1,5) (78.6%, 98.7%)
05/30 10:56:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 0.6328	Prec@(1,5) (78.5%, 98.7%)
05/30 10:56:10午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 0.6339	Prec@(1,5) (78.4%, 98.7%)
05/30 10:56:14午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 0.6327	Prec@(1,5) (78.4%, 98.7%)
05/30 10:56:14午後 searchStage_trainer.py:260 [INFO] Valid: [ 21/49] Final Prec@1 78.4200%
05/30 10:56:14午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:56:15午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 78.4200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2649, 0.3153, 0.2473, 0.1725],
        [0.2737, 0.3087, 0.2423, 0.1753]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2434, 0.3261, 0.2559, 0.1746],
        [0.2561, 0.3127, 0.2615, 0.1697],
        [0.2675, 0.3473, 0.2073, 0.1779]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2570, 0.2964, 0.2703, 0.1764],
        [0.2738, 0.3313, 0.2267, 0.1682],
        [0.2752, 0.3157, 0.2163, 0.1927]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2710, 0.3146, 0.2266, 0.1878],
        [0.2724, 0.3073, 0.2210, 0.1993],
        [0.2712, 0.3097, 0.2303, 0.1889]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2823, 0.3174, 0.2153, 0.1851],
        [0.2828, 0.3133, 0.2217, 0.1822],
        [0.2768, 0.3130, 0.2195, 0.1907]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2779, 0.3063, 0.2235, 0.1924],
        [0.2758, 0.3023, 0.2181, 0.2039],
        [0.2681, 0.3011, 0.2255, 0.2053]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2514, 0.2525, 0.2489],
        [0.2495, 0.2533, 0.2510, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2496, 0.2520, 0.2469],
        [0.2553, 0.2522, 0.2525, 0.2400],
        [0.2519, 0.2534, 0.2459, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2518, 0.2475],
        [0.2513, 0.2548, 0.2493, 0.2446],
        [0.2519, 0.2529, 0.2474, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2525, 0.2497, 0.2471],
        [0.2513, 0.2524, 0.2496, 0.2468],
        [0.2522, 0.2512, 0.2475, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2518, 0.2490, 0.2453],
        [0.2537, 0.2522, 0.2459, 0.2482],
        [0.2531, 0.2503, 0.2459, 0.2508]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2507],
        [0.2516, 0.2515, 0.2484, 0.2485],
        [0.2514, 0.2524, 0.2490, 0.2472]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2503, 0.2506, 0.2511],
        [0.2463, 0.2535, 0.2517, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2545, 0.2517, 0.2467],
        [0.2470, 0.2501, 0.2492, 0.2536],
        [0.2465, 0.2526, 0.2514, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2530, 0.2501, 0.2510],
        [0.2463, 0.2520, 0.2490, 0.2528],
        [0.2453, 0.2545, 0.2539, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2510, 0.2517, 0.2490],
        [0.2491, 0.2505, 0.2480, 0.2523],
        [0.2492, 0.2515, 0.2511, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2431, 0.2461, 0.2647],
        [0.2416, 0.2555, 0.2611, 0.2418],
        [0.2425, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2213, 0.2618, 0.2751, 0.2418],
        [0.2230, 0.2608, 0.2677, 0.2485],
        [0.2330, 0.2522, 0.2571, 0.2577]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:56:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 0.4936 (0.4211)	Prec@(1,5) (85.4%, 99.4%)	
05/30 10:57:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 0.5199 (0.4184)	Prec@(1,5) (85.5%, 99.4%)	
05/30 10:57:38午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 0.4993 (0.4254)	Prec@(1,5) (85.1%, 99.4%)	
05/30 10:58:06午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 0.3861 (0.4228)	Prec@(1,5) (85.2%, 99.4%)	
05/30 10:58:35午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 0.3738 (0.4230)	Prec@(1,5) (85.2%, 99.4%)	
05/30 10:59:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 0.5384 (0.4274)	Prec@(1,5) (85.1%, 99.4%)	
05/30 10:59:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 0.5651 (0.4311)	Prec@(1,5) (84.9%, 99.4%)	
05/30 10:59:53午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 0.5443 (0.4306)	Prec@(1,5) (85.0%, 99.4%)	
05/30 10:59:54午後 searchStage_trainer.py:221 [INFO] Train: [ 22/49] Final Prec@1 84.9360%
05/30 10:59:59午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 0.6133	Prec@(1,5) (80.3%, 98.8%)
05/30 11:00:03午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 0.6196	Prec@(1,5) (79.3%, 98.6%)
05/30 11:00:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 0.6139	Prec@(1,5) (79.7%, 98.6%)
05/30 11:00:13午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 0.6104	Prec@(1,5) (79.6%, 98.7%)
05/30 11:00:17午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 0.6063	Prec@(1,5) (79.7%, 98.7%)
05/30 11:00:22午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 0.6084	Prec@(1,5) (79.6%, 98.7%)
05/30 11:00:26午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 0.6059	Prec@(1,5) (79.6%, 98.7%)
05/30 11:00:30午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 0.6087	Prec@(1,5) (79.5%, 98.7%)
05/30 11:00:30午後 searchStage_trainer.py:260 [INFO] Valid: [ 22/49] Final Prec@1 79.5400%
05/30 11:00:30午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:00:31午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 79.5400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2647, 0.3150, 0.2476, 0.1728],
        [0.2736, 0.3084, 0.2424, 0.1756]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2433, 0.3260, 0.2561, 0.1746],
        [0.2558, 0.3127, 0.2618, 0.1697],
        [0.2674, 0.3469, 0.2074, 0.1782]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2569, 0.2963, 0.2703, 0.1764],
        [0.2737, 0.3311, 0.2267, 0.1685],
        [0.2751, 0.3154, 0.2165, 0.1930]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2709, 0.3144, 0.2267, 0.1880],
        [0.2723, 0.3070, 0.2211, 0.1996],
        [0.2711, 0.3093, 0.2304, 0.1892]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2824, 0.3171, 0.2152, 0.1852],
        [0.2827, 0.3130, 0.2219, 0.1824],
        [0.2767, 0.3127, 0.2196, 0.1910]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2779, 0.3061, 0.2236, 0.1925],
        [0.2757, 0.3020, 0.2181, 0.2042],
        [0.2680, 0.3007, 0.2256, 0.2057]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2514, 0.2525, 0.2490],
        [0.2495, 0.2533, 0.2510, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2520, 0.2468],
        [0.2553, 0.2522, 0.2525, 0.2400],
        [0.2518, 0.2534, 0.2459, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2516, 0.2518, 0.2475],
        [0.2512, 0.2548, 0.2493, 0.2446],
        [0.2519, 0.2529, 0.2475, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2525, 0.2497, 0.2471],
        [0.2513, 0.2524, 0.2496, 0.2468],
        [0.2522, 0.2512, 0.2475, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2518, 0.2490, 0.2454],
        [0.2537, 0.2522, 0.2459, 0.2482],
        [0.2531, 0.2503, 0.2459, 0.2508]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2507],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2514, 0.2524, 0.2490, 0.2472]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2503, 0.2506, 0.2511],
        [0.2463, 0.2535, 0.2517, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2545, 0.2517, 0.2467],
        [0.2470, 0.2501, 0.2492, 0.2536],
        [0.2465, 0.2526, 0.2514, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2530, 0.2500, 0.2510],
        [0.2463, 0.2520, 0.2490, 0.2528],
        [0.2453, 0.2545, 0.2538, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2510, 0.2517, 0.2490],
        [0.2491, 0.2505, 0.2480, 0.2523],
        [0.2492, 0.2515, 0.2511, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2430, 0.2460, 0.2648],
        [0.2416, 0.2555, 0.2611, 0.2418],
        [0.2426, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2211, 0.2619, 0.2753, 0.2417],
        [0.2229, 0.2608, 0.2677, 0.2485],
        [0.2330, 0.2522, 0.2571, 0.2577]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:01:00午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 0.2771 (0.3879)	Prec@(1,5) (86.2%, 99.7%)	
05/30 11:01:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 0.5493 (0.3988)	Prec@(1,5) (86.0%, 99.7%)	
05/30 11:01:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 0.6234 (0.3989)	Prec@(1,5) (86.0%, 99.6%)	
05/30 11:02:23午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 0.3665 (0.3985)	Prec@(1,5) (85.8%, 99.6%)	
05/30 11:02:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 0.8399 (0.4026)	Prec@(1,5) (85.6%, 99.6%)	
05/30 11:03:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 0.3923 (0.4056)	Prec@(1,5) (85.6%, 99.6%)	
05/30 11:03:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 0.3697 (0.4081)	Prec@(1,5) (85.6%, 99.6%)	
05/30 11:04:10午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 0.3286 (0.4100)	Prec@(1,5) (85.6%, 99.5%)	
05/30 11:04:10午後 searchStage_trainer.py:221 [INFO] Train: [ 23/49] Final Prec@1 85.6040%
05/30 11:04:15午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 0.6081	Prec@(1,5) (79.9%, 98.7%)
05/30 11:04:19午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 0.6082	Prec@(1,5) (80.1%, 98.8%)
05/30 11:04:24午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 0.6069	Prec@(1,5) (80.2%, 98.8%)
05/30 11:04:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 0.6173	Prec@(1,5) (79.9%, 98.7%)
05/30 11:04:33午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 0.6223	Prec@(1,5) (79.8%, 98.6%)
05/30 11:04:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 0.6201	Prec@(1,5) (79.8%, 98.6%)
05/30 11:04:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 0.6207	Prec@(1,5) (79.8%, 98.6%)
05/30 11:04:46午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 0.6215	Prec@(1,5) (79.7%, 98.6%)
05/30 11:04:46午後 searchStage_trainer.py:260 [INFO] Valid: [ 23/49] Final Prec@1 79.7520%
05/30 11:04:46午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:04:47午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 79.7520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2646, 0.3147, 0.2477, 0.1730],
        [0.2735, 0.3082, 0.2425, 0.1758]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2430, 0.3260, 0.2563, 0.1747],
        [0.2558, 0.3126, 0.2620, 0.1697],
        [0.2674, 0.3467, 0.2075, 0.1785]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2570, 0.2962, 0.2702, 0.1767],
        [0.2737, 0.3308, 0.2268, 0.1687],
        [0.2751, 0.3151, 0.2165, 0.1933]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2709, 0.3142, 0.2267, 0.1881],
        [0.2723, 0.3067, 0.2211, 0.1999],
        [0.2710, 0.3090, 0.2305, 0.1895]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2823, 0.3169, 0.2152, 0.1856],
        [0.2827, 0.3127, 0.2221, 0.1825],
        [0.2766, 0.3124, 0.2197, 0.1913]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2778, 0.3059, 0.2237, 0.1926],
        [0.2756, 0.3017, 0.2181, 0.2045],
        [0.2679, 0.3004, 0.2257, 0.2061]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2514, 0.2525, 0.2490],
        [0.2495, 0.2533, 0.2510, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2520, 0.2469],
        [0.2553, 0.2522, 0.2525, 0.2400],
        [0.2518, 0.2534, 0.2459, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2516, 0.2518, 0.2476],
        [0.2512, 0.2548, 0.2493, 0.2446],
        [0.2519, 0.2528, 0.2475, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2525, 0.2498, 0.2470],
        [0.2513, 0.2524, 0.2495, 0.2468],
        [0.2522, 0.2512, 0.2475, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2518, 0.2490, 0.2453],
        [0.2538, 0.2522, 0.2459, 0.2482],
        [0.2531, 0.2502, 0.2459, 0.2508]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2507],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2514, 0.2524, 0.2490, 0.2472]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2503, 0.2506, 0.2511],
        [0.2463, 0.2535, 0.2516, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2546, 0.2517, 0.2467],
        [0.2470, 0.2501, 0.2492, 0.2537],
        [0.2465, 0.2526, 0.2514, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2500, 0.2510],
        [0.2463, 0.2520, 0.2489, 0.2528],
        [0.2453, 0.2545, 0.2538, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2510, 0.2517, 0.2490],
        [0.2492, 0.2505, 0.2480, 0.2523],
        [0.2493, 0.2515, 0.2511, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2430, 0.2460, 0.2649],
        [0.2416, 0.2555, 0.2611, 0.2418],
        [0.2426, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2210, 0.2619, 0.2754, 0.2417],
        [0.2229, 0.2608, 0.2678, 0.2485],
        [0.2330, 0.2522, 0.2570, 0.2577]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:05:16午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 0.2390 (0.3704)	Prec@(1,5) (86.8%, 99.5%)	
05/30 11:05:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 0.3066 (0.3645)	Prec@(1,5) (87.0%, 99.6%)	
05/30 11:06:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 0.5005 (0.3828)	Prec@(1,5) (86.4%, 99.6%)	
05/30 11:06:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 0.3023 (0.3899)	Prec@(1,5) (86.2%, 99.6%)	
05/30 11:07:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 0.4723 (0.3935)	Prec@(1,5) (86.2%, 99.6%)	
05/30 11:07:35午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 0.7235 (0.3924)	Prec@(1,5) (86.4%, 99.6%)	
05/30 11:08:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 0.3522 (0.3965)	Prec@(1,5) (86.2%, 99.5%)	
05/30 11:08:26午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 0.4409 (0.4000)	Prec@(1,5) (86.2%, 99.5%)	
05/30 11:08:26午後 searchStage_trainer.py:221 [INFO] Train: [ 24/49] Final Prec@1 86.1680%
05/30 11:08:31午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 0.5718	Prec@(1,5) (80.8%, 98.8%)
05/30 11:08:36午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 0.5751	Prec@(1,5) (80.5%, 98.8%)
05/30 11:08:40午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 0.5737	Prec@(1,5) (80.6%, 98.8%)
05/30 11:08:45午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 0.5792	Prec@(1,5) (80.5%, 98.8%)
05/30 11:08:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 0.5817	Prec@(1,5) (80.5%, 98.8%)
05/30 11:08:54午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 0.5807	Prec@(1,5) (80.5%, 98.8%)
05/30 11:08:59午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 0.5844	Prec@(1,5) (80.4%, 98.8%)
05/30 11:09:02午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 0.5799	Prec@(1,5) (80.6%, 98.8%)
05/30 11:09:02午後 searchStage_trainer.py:260 [INFO] Valid: [ 24/49] Final Prec@1 80.5680%
05/30 11:09:02午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:09:03午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.5680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2645, 0.3144, 0.2479, 0.1732],
        [0.2734, 0.3079, 0.2426, 0.1760]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2429, 0.3258, 0.2564, 0.1749],
        [0.2556, 0.3126, 0.2621, 0.1697],
        [0.2673, 0.3463, 0.2076, 0.1787]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2570, 0.2961, 0.2702, 0.1768],
        [0.2736, 0.3306, 0.2269, 0.1689],
        [0.2750, 0.3148, 0.2166, 0.1936]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2709, 0.3140, 0.2268, 0.1883],
        [0.2722, 0.3065, 0.2212, 0.2001],
        [0.2709, 0.3087, 0.2306, 0.1898]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2823, 0.3167, 0.2153, 0.1857],
        [0.2826, 0.3125, 0.2222, 0.1827],
        [0.2765, 0.3121, 0.2198, 0.1916]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2778, 0.3056, 0.2238, 0.1928],
        [0.2756, 0.3014, 0.2182, 0.2048],
        [0.2678, 0.3000, 0.2258, 0.2064]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2514, 0.2525, 0.2490],
        [0.2495, 0.2533, 0.2510, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2520, 0.2468],
        [0.2553, 0.2522, 0.2525, 0.2400],
        [0.2518, 0.2534, 0.2459, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2515, 0.2518, 0.2476],
        [0.2512, 0.2548, 0.2494, 0.2446],
        [0.2519, 0.2528, 0.2475, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2525, 0.2498, 0.2470],
        [0.2513, 0.2524, 0.2495, 0.2468],
        [0.2522, 0.2512, 0.2475, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2518, 0.2490, 0.2453],
        [0.2537, 0.2522, 0.2459, 0.2482],
        [0.2531, 0.2502, 0.2459, 0.2508]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2507],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2514, 0.2524, 0.2490, 0.2472]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2503, 0.2506, 0.2511],
        [0.2463, 0.2535, 0.2516, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2546, 0.2517, 0.2466],
        [0.2470, 0.2501, 0.2492, 0.2537],
        [0.2465, 0.2526, 0.2513, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2500, 0.2510],
        [0.2463, 0.2519, 0.2489, 0.2528],
        [0.2453, 0.2545, 0.2538, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2510, 0.2517, 0.2490],
        [0.2492, 0.2505, 0.2480, 0.2523],
        [0.2493, 0.2515, 0.2510, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2429, 0.2460, 0.2650],
        [0.2416, 0.2556, 0.2611, 0.2417],
        [0.2426, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2209, 0.2620, 0.2755, 0.2416],
        [0.2228, 0.2609, 0.2678, 0.2485],
        [0.2330, 0.2522, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:09:32午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 0.4346 (0.3630)	Prec@(1,5) (87.5%, 99.6%)	
05/30 11:09:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 0.6119 (0.3717)	Prec@(1,5) (87.2%, 99.6%)	
05/30 11:10:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 0.5831 (0.3674)	Prec@(1,5) (87.4%, 99.6%)	
05/30 11:10:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 0.4504 (0.3740)	Prec@(1,5) (87.0%, 99.6%)	
05/30 11:11:23午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 0.4431 (0.3752)	Prec@(1,5) (87.0%, 99.6%)	
05/30 11:11:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 0.4410 (0.3765)	Prec@(1,5) (86.9%, 99.6%)	
05/30 11:12:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 0.3784 (0.3772)	Prec@(1,5) (86.8%, 99.6%)	
05/30 11:12:42午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 0.3513 (0.3787)	Prec@(1,5) (86.8%, 99.6%)	
05/30 11:12:42午後 searchStage_trainer.py:221 [INFO] Train: [ 25/49] Final Prec@1 86.7560%
05/30 11:12:47午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 0.6352	Prec@(1,5) (78.3%, 98.7%)
05/30 11:12:52午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 0.6473	Prec@(1,5) (78.3%, 98.7%)
05/30 11:12:56午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 0.6450	Prec@(1,5) (78.4%, 98.8%)
05/30 11:13:01午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 0.6405	Prec@(1,5) (78.7%, 98.7%)
05/30 11:13:05午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 0.6404	Prec@(1,5) (78.8%, 98.7%)
05/30 11:13:10午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 0.6436	Prec@(1,5) (78.5%, 98.7%)
05/30 11:13:15午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 0.6411	Prec@(1,5) (78.6%, 98.7%)
05/30 11:13:18午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 0.6450	Prec@(1,5) (78.5%, 98.7%)
05/30 11:13:18午後 searchStage_trainer.py:260 [INFO] Valid: [ 25/49] Final Prec@1 78.4920%
05/30 11:13:18午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:13:19午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.5680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2644, 0.3141, 0.2480, 0.1735],
        [0.2733, 0.3077, 0.2428, 0.1762]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2427, 0.3258, 0.2564, 0.1750],
        [0.2554, 0.3125, 0.2624, 0.1698],
        [0.2672, 0.3461, 0.2077, 0.1790]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2565, 0.2963, 0.2704, 0.1767],
        [0.2735, 0.3304, 0.2269, 0.1691],
        [0.2749, 0.3145, 0.2167, 0.1939]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3139, 0.2269, 0.1884],
        [0.2722, 0.3062, 0.2213, 0.2003],
        [0.2708, 0.3084, 0.2307, 0.1901]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2823, 0.3165, 0.2154, 0.1858],
        [0.2826, 0.3122, 0.2222, 0.1830],
        [0.2765, 0.3118, 0.2199, 0.1919]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2778, 0.3054, 0.2238, 0.1930],
        [0.2755, 0.3012, 0.2183, 0.2050],
        [0.2677, 0.2997, 0.2259, 0.2068]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2514, 0.2526, 0.2490],
        [0.2495, 0.2533, 0.2510, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2496, 0.2520, 0.2468],
        [0.2553, 0.2521, 0.2525, 0.2400],
        [0.2518, 0.2534, 0.2459, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2515, 0.2519, 0.2476],
        [0.2512, 0.2548, 0.2494, 0.2446],
        [0.2519, 0.2528, 0.2475, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2498, 0.2470],
        [0.2513, 0.2524, 0.2496, 0.2468],
        [0.2522, 0.2512, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2518, 0.2490, 0.2454],
        [0.2538, 0.2522, 0.2459, 0.2482],
        [0.2531, 0.2502, 0.2459, 0.2508]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2507],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2514, 0.2524, 0.2490, 0.2472]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2503, 0.2506, 0.2511],
        [0.2464, 0.2535, 0.2516, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2546, 0.2517, 0.2466],
        [0.2471, 0.2501, 0.2492, 0.2537],
        [0.2465, 0.2526, 0.2513, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2500, 0.2510],
        [0.2463, 0.2519, 0.2489, 0.2528],
        [0.2454, 0.2545, 0.2538, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2510, 0.2517, 0.2490],
        [0.2492, 0.2505, 0.2480, 0.2523],
        [0.2493, 0.2515, 0.2510, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2429, 0.2460, 0.2651],
        [0.2416, 0.2556, 0.2612, 0.2417],
        [0.2426, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2207, 0.2620, 0.2756, 0.2416],
        [0.2228, 0.2609, 0.2678, 0.2485],
        [0.2330, 0.2522, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:13:48午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 0.2320 (0.3488)	Prec@(1,5) (87.7%, 99.4%)	
05/30 11:14:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 0.4096 (0.3488)	Prec@(1,5) (88.0%, 99.5%)	
05/30 11:14:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 0.4714 (0.3475)	Prec@(1,5) (88.0%, 99.6%)	
05/30 11:15:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.3672 (0.3533)	Prec@(1,5) (87.7%, 99.6%)	
05/30 11:15:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 0.5738 (0.3565)	Prec@(1,5) (87.5%, 99.6%)	
05/30 11:16:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 0.2787 (0.3552)	Prec@(1,5) (87.6%, 99.6%)	
05/30 11:16:35午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 0.4534 (0.3577)	Prec@(1,5) (87.4%, 99.6%)	
05/30 11:16:58午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 0.2585 (0.3602)	Prec@(1,5) (87.3%, 99.6%)	
05/30 11:16:58午後 searchStage_trainer.py:221 [INFO] Train: [ 26/49] Final Prec@1 87.3280%
05/30 11:17:03午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 0.6656	Prec@(1,5) (79.2%, 98.5%)
05/30 11:17:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 0.6550	Prec@(1,5) (79.7%, 98.6%)
05/30 11:17:12午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 0.6518	Prec@(1,5) (79.7%, 98.7%)
05/30 11:17:17午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 0.6445	Prec@(1,5) (79.9%, 98.7%)
05/30 11:17:21午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 0.6428	Prec@(1,5) (80.0%, 98.7%)
05/30 11:17:26午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 0.6417	Prec@(1,5) (80.0%, 98.7%)
05/30 11:17:31午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 0.6415	Prec@(1,5) (80.0%, 98.7%)
05/30 11:17:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 0.6370	Prec@(1,5) (80.0%, 98.7%)
05/30 11:17:34午後 searchStage_trainer.py:260 [INFO] Valid: [ 26/49] Final Prec@1 80.0600%
05/30 11:17:34午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:17:35午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.5680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2643, 0.3138, 0.2482, 0.1736],
        [0.2732, 0.3074, 0.2429, 0.1765]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2428, 0.3257, 0.2565, 0.1750],
        [0.2553, 0.3123, 0.2626, 0.1698],
        [0.2671, 0.3458, 0.2078, 0.1793]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2566, 0.2963, 0.2706, 0.1765],
        [0.2735, 0.3302, 0.2270, 0.1693],
        [0.2748, 0.3142, 0.2168, 0.1942]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3138, 0.2269, 0.1885],
        [0.2721, 0.3060, 0.2214, 0.2005],
        [0.2707, 0.3081, 0.2308, 0.1904]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2823, 0.3164, 0.2154, 0.1859],
        [0.2825, 0.3120, 0.2223, 0.1832],
        [0.2764, 0.3115, 0.2200, 0.1922]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2778, 0.3052, 0.2238, 0.1932],
        [0.2755, 0.3009, 0.2184, 0.2052],
        [0.2676, 0.2994, 0.2260, 0.2071]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2514, 0.2526, 0.2490],
        [0.2495, 0.2532, 0.2510, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2496, 0.2521, 0.2468],
        [0.2554, 0.2521, 0.2525, 0.2401],
        [0.2518, 0.2534, 0.2459, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2515, 0.2519, 0.2476],
        [0.2512, 0.2548, 0.2494, 0.2446],
        [0.2519, 0.2528, 0.2475, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2498, 0.2470],
        [0.2513, 0.2524, 0.2496, 0.2468],
        [0.2522, 0.2511, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2522, 0.2459, 0.2482],
        [0.2531, 0.2502, 0.2459, 0.2508]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2507],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2514, 0.2524, 0.2490, 0.2472]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2503, 0.2506, 0.2511],
        [0.2464, 0.2535, 0.2516, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2546, 0.2517, 0.2466],
        [0.2471, 0.2501, 0.2491, 0.2537],
        [0.2466, 0.2526, 0.2513, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2500, 0.2511],
        [0.2464, 0.2519, 0.2489, 0.2528],
        [0.2454, 0.2545, 0.2538, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2510, 0.2517, 0.2490],
        [0.2492, 0.2505, 0.2479, 0.2523],
        [0.2494, 0.2515, 0.2510, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2429, 0.2459, 0.2651],
        [0.2416, 0.2556, 0.2612, 0.2417],
        [0.2426, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2206, 0.2621, 0.2757, 0.2416],
        [0.2228, 0.2609, 0.2679, 0.2485],
        [0.2330, 0.2522, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:18:04午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 0.2076 (0.3121)	Prec@(1,5) (89.2%, 99.6%)	
05/30 11:18:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 0.3193 (0.3136)	Prec@(1,5) (89.2%, 99.7%)	
05/30 11:18:58午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 0.5968 (0.3187)	Prec@(1,5) (88.9%, 99.6%)	
05/30 11:19:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 0.4351 (0.3232)	Prec@(1,5) (88.7%, 99.6%)	
05/30 11:19:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 0.3341 (0.3285)	Prec@(1,5) (88.4%, 99.6%)	
05/30 11:20:23午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 0.3061 (0.3293)	Prec@(1,5) (88.4%, 99.6%)	
05/30 11:20:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 0.4057 (0.3334)	Prec@(1,5) (88.4%, 99.6%)	
05/30 11:21:13午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 0.2983 (0.3380)	Prec@(1,5) (88.2%, 99.6%)	
05/30 11:21:14午後 searchStage_trainer.py:221 [INFO] Train: [ 27/49] Final Prec@1 88.2320%
05/30 11:21:19午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 0.5882	Prec@(1,5) (80.8%, 98.7%)
05/30 11:21:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 0.5949	Prec@(1,5) (80.7%, 98.8%)
05/30 11:21:28午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 0.5834	Prec@(1,5) (80.9%, 98.9%)
05/30 11:21:33午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 0.5858	Prec@(1,5) (80.8%, 98.8%)
05/30 11:21:37午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 0.5843	Prec@(1,5) (80.9%, 98.8%)
05/30 11:21:42午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 0.5856	Prec@(1,5) (80.8%, 98.8%)
05/30 11:21:46午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 0.5822	Prec@(1,5) (80.8%, 98.9%)
05/30 11:21:50午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 0.5869	Prec@(1,5) (80.7%, 98.8%)
05/30 11:21:50午後 searchStage_trainer.py:260 [INFO] Valid: [ 27/49] Final Prec@1 80.7080%
05/30 11:21:50午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:21:51午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.7080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2641, 0.3136, 0.2484, 0.1738],
        [0.2731, 0.3072, 0.2430, 0.1767]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2425, 0.3255, 0.2568, 0.1752],
        [0.2553, 0.3122, 0.2627, 0.1699],
        [0.2670, 0.3455, 0.2080, 0.1795]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2565, 0.2961, 0.2707, 0.1767],
        [0.2734, 0.3300, 0.2270, 0.1695],
        [0.2747, 0.3140, 0.2169, 0.1944]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3137, 0.2270, 0.1886],
        [0.2721, 0.3058, 0.2214, 0.2007],
        [0.2707, 0.3079, 0.2308, 0.1906]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2823, 0.3162, 0.2155, 0.1861],
        [0.2825, 0.3118, 0.2224, 0.1834],
        [0.2763, 0.3112, 0.2201, 0.1924]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2778, 0.3050, 0.2239, 0.1933],
        [0.2754, 0.3007, 0.2185, 0.2054],
        [0.2675, 0.2991, 0.2261, 0.2074]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2490],
        [0.2495, 0.2532, 0.2510, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2496, 0.2521, 0.2468],
        [0.2554, 0.2521, 0.2525, 0.2400],
        [0.2518, 0.2534, 0.2459, 0.2489]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2515, 0.2518, 0.2476],
        [0.2512, 0.2548, 0.2494, 0.2446],
        [0.2519, 0.2528, 0.2475, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2498, 0.2470],
        [0.2512, 0.2524, 0.2496, 0.2468],
        [0.2522, 0.2511, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2482],
        [0.2531, 0.2502, 0.2459, 0.2508]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2507],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2514, 0.2523, 0.2490, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2502, 0.2506, 0.2511],
        [0.2464, 0.2535, 0.2516, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2546, 0.2516, 0.2466],
        [0.2471, 0.2501, 0.2491, 0.2537],
        [0.2466, 0.2526, 0.2513, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2499, 0.2511],
        [0.2464, 0.2519, 0.2489, 0.2529],
        [0.2454, 0.2545, 0.2538, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2510, 0.2517, 0.2490],
        [0.2493, 0.2505, 0.2479, 0.2523],
        [0.2494, 0.2515, 0.2510, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2428, 0.2459, 0.2652],
        [0.2416, 0.2556, 0.2612, 0.2417],
        [0.2426, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2205, 0.2621, 0.2758, 0.2415],
        [0.2227, 0.2609, 0.2679, 0.2485],
        [0.2330, 0.2522, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:22:20午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 0.3091 (0.2940)	Prec@(1,5) (89.6%, 99.7%)	
05/30 11:22:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 0.2883 (0.3033)	Prec@(1,5) (89.5%, 99.7%)	
05/30 11:23:14午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 0.2351 (0.3056)	Prec@(1,5) (89.3%, 99.7%)	
05/30 11:23:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.1514 (0.3072)	Prec@(1,5) (89.2%, 99.7%)	
05/30 11:24:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 0.4542 (0.3085)	Prec@(1,5) (89.2%, 99.7%)	
05/30 11:24:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 0.2471 (0.3161)	Prec@(1,5) (89.1%, 99.7%)	
05/30 11:25:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][350/390]	Step 11298	lr 0.01075	Loss 0.3519 (0.3256)	Prec@(1,5) (88.7%, 99.7%)	
05/30 11:25:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 0.3858 (0.3292)	Prec@(1,5) (88.6%, 99.7%)	
05/30 11:25:30午後 searchStage_trainer.py:221 [INFO] Train: [ 28/49] Final Prec@1 88.6120%
05/30 11:25:35午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][50/391]	Step 11339	Loss 0.5547	Prec@(1,5) (81.2%, 98.7%)
05/30 11:25:39午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 0.5616	Prec@(1,5) (81.0%, 98.7%)
05/30 11:25:44午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][150/391]	Step 11339	Loss 0.5564	Prec@(1,5) (81.4%, 98.8%)
05/30 11:25:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 0.5514	Prec@(1,5) (81.6%, 98.8%)
05/30 11:25:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][250/391]	Step 11339	Loss 0.5508	Prec@(1,5) (81.7%, 98.7%)
05/30 11:25:58午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 0.5515	Prec@(1,5) (81.7%, 98.8%)
05/30 11:26:03午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][350/391]	Step 11339	Loss 0.5527	Prec@(1,5) (81.6%, 98.8%)
05/30 11:26:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 0.5528	Prec@(1,5) (81.6%, 98.8%)
05/30 11:26:06午後 searchStage_trainer.py:260 [INFO] Valid: [ 28/49] Final Prec@1 81.6040%
05/30 11:26:06午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:26:07午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.6040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2641, 0.3135, 0.2485, 0.1740],
        [0.2730, 0.3070, 0.2431, 0.1769]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2424, 0.3254, 0.2568, 0.1754],
        [0.2553, 0.3121, 0.2628, 0.1698],
        [0.2669, 0.3453, 0.2080, 0.1797]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2561, 0.2961, 0.2709, 0.1768],
        [0.2734, 0.3299, 0.2271, 0.1696],
        [0.2746, 0.3137, 0.2170, 0.1946]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2707, 0.3136, 0.2270, 0.1886],
        [0.2720, 0.3056, 0.2214, 0.2010],
        [0.2706, 0.3076, 0.2309, 0.1908]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2822, 0.3160, 0.2155, 0.1862],
        [0.2824, 0.3116, 0.2224, 0.1836],
        [0.2763, 0.3110, 0.2201, 0.1926]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2776, 0.3049, 0.2240, 0.1935],
        [0.2753, 0.3005, 0.2185, 0.2056],
        [0.2674, 0.2988, 0.2261, 0.2076]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2490],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2496, 0.2521, 0.2468],
        [0.2553, 0.2521, 0.2525, 0.2401],
        [0.2518, 0.2534, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2514, 0.2518, 0.2476],
        [0.2512, 0.2548, 0.2494, 0.2446],
        [0.2519, 0.2528, 0.2475, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2498, 0.2470],
        [0.2512, 0.2524, 0.2496, 0.2468],
        [0.2522, 0.2511, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2482],
        [0.2530, 0.2502, 0.2459, 0.2508]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2507],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2514, 0.2523, 0.2490, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2502, 0.2505, 0.2511],
        [0.2464, 0.2535, 0.2516, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2546, 0.2516, 0.2466],
        [0.2471, 0.2501, 0.2491, 0.2537],
        [0.2466, 0.2526, 0.2513, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2499, 0.2511],
        [0.2464, 0.2519, 0.2488, 0.2529],
        [0.2454, 0.2545, 0.2538, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2510, 0.2517, 0.2490],
        [0.2493, 0.2505, 0.2479, 0.2523],
        [0.2494, 0.2515, 0.2510, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2428, 0.2459, 0.2652],
        [0.2416, 0.2556, 0.2612, 0.2417],
        [0.2426, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2205, 0.2621, 0.2759, 0.2415],
        [0.2227, 0.2609, 0.2679, 0.2485],
        [0.2330, 0.2522, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:26:36午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][50/390]	Step 11389	lr 0.01002	Loss 0.3371 (0.2806)	Prec@(1,5) (90.2%, 99.8%)	
05/30 11:27:04午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 0.2940 (0.2744)	Prec@(1,5) (90.4%, 99.8%)	
05/30 11:27:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][150/390]	Step 11489	lr 0.01002	Loss 0.2761 (0.2828)	Prec@(1,5) (90.0%, 99.7%)	
05/30 11:27:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.5042 (0.2953)	Prec@(1,5) (89.6%, 99.7%)	
05/30 11:28:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][250/390]	Step 11589	lr 0.01002	Loss 0.1907 (0.2954)	Prec@(1,5) (89.6%, 99.7%)	
05/30 11:28:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 0.2178 (0.2997)	Prec@(1,5) (89.5%, 99.7%)	
05/30 11:29:23午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][350/390]	Step 11689	lr 0.01002	Loss 0.1920 (0.3015)	Prec@(1,5) (89.4%, 99.7%)	
05/30 11:29:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 0.5126 (0.3028)	Prec@(1,5) (89.4%, 99.7%)	
05/30 11:29:46午後 searchStage_trainer.py:221 [INFO] Train: [ 29/49] Final Prec@1 89.3800%
05/30 11:29:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][50/391]	Step 11730	Loss 0.5635	Prec@(1,5) (82.1%, 98.6%)
05/30 11:29:56午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 0.5613	Prec@(1,5) (82.1%, 98.8%)
05/30 11:30:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][150/391]	Step 11730	Loss 0.5656	Prec@(1,5) (82.0%, 98.8%)
05/30 11:30:05午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 0.5533	Prec@(1,5) (82.3%, 99.0%)
05/30 11:30:09午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][250/391]	Step 11730	Loss 0.5399	Prec@(1,5) (82.7%, 99.0%)
05/30 11:30:14午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 0.5368	Prec@(1,5) (82.7%, 99.0%)
05/30 11:30:19午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][350/391]	Step 11730	Loss 0.5381	Prec@(1,5) (82.7%, 99.0%)
05/30 11:30:22午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 0.5399	Prec@(1,5) (82.7%, 99.0%)
05/30 11:30:22午後 searchStage_trainer.py:260 [INFO] Valid: [ 29/49] Final Prec@1 82.6480%
05/30 11:30:23午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:30:23午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 82.6480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2640, 0.3132, 0.2487, 0.1742],
        [0.2730, 0.3067, 0.2432, 0.1771]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2423, 0.3253, 0.2569, 0.1755],
        [0.2552, 0.3120, 0.2628, 0.1700],
        [0.2669, 0.3451, 0.2081, 0.1799]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2560, 0.2962, 0.2709, 0.1769],
        [0.2733, 0.3297, 0.2272, 0.1697],
        [0.2746, 0.3135, 0.2171, 0.1949]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2707, 0.3135, 0.2271, 0.1888],
        [0.2720, 0.3054, 0.2215, 0.2011],
        [0.2705, 0.3074, 0.2310, 0.1911]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2821, 0.3159, 0.2156, 0.1863],
        [0.2824, 0.3114, 0.2225, 0.1837],
        [0.2762, 0.3107, 0.2202, 0.1928]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2775, 0.3047, 0.2240, 0.1938],
        [0.2753, 0.3003, 0.2186, 0.2058],
        [0.2673, 0.2986, 0.2262, 0.2079]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2490],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2521, 0.2468],
        [0.2553, 0.2521, 0.2525, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2514, 0.2518, 0.2477],
        [0.2512, 0.2548, 0.2494, 0.2446],
        [0.2519, 0.2528, 0.2475, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2498, 0.2470],
        [0.2512, 0.2524, 0.2496, 0.2468],
        [0.2522, 0.2511, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2482],
        [0.2530, 0.2502, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2507],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2502, 0.2505, 0.2511],
        [0.2464, 0.2535, 0.2516, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2546, 0.2516, 0.2466],
        [0.2471, 0.2501, 0.2491, 0.2537],
        [0.2466, 0.2526, 0.2513, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2499, 0.2511],
        [0.2464, 0.2519, 0.2488, 0.2529],
        [0.2454, 0.2545, 0.2537, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2510, 0.2516, 0.2490],
        [0.2493, 0.2505, 0.2479, 0.2523],
        [0.2494, 0.2514, 0.2510, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2428, 0.2459, 0.2653],
        [0.2416, 0.2556, 0.2612, 0.2416],
        [0.2426, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2204, 0.2621, 0.2760, 0.2415],
        [0.2227, 0.2609, 0.2679, 0.2485],
        [0.2330, 0.2522, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:30:52午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][50/390]	Step 11780	lr 0.00929	Loss 0.3859 (0.2855)	Prec@(1,5) (90.0%, 99.8%)	
05/30 11:31:20午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 0.2585 (0.2629)	Prec@(1,5) (90.9%, 99.8%)	
05/30 11:31:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][150/390]	Step 11880	lr 0.00929	Loss 0.2613 (0.2743)	Prec@(1,5) (90.4%, 99.8%)	
05/30 11:32:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 0.3296 (0.2772)	Prec@(1,5) (90.3%, 99.8%)	
05/30 11:32:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][250/390]	Step 11980	lr 0.00929	Loss 0.3193 (0.2787)	Prec@(1,5) (90.2%, 99.8%)	
05/30 11:33:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 0.2064 (0.2783)	Prec@(1,5) (90.3%, 99.8%)	
05/30 11:33:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][350/390]	Step 12080	lr 0.00929	Loss 0.2107 (0.2758)	Prec@(1,5) (90.3%, 99.8%)	
05/30 11:34:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 0.3405 (0.2793)	Prec@(1,5) (90.2%, 99.8%)	
05/30 11:34:02午後 searchStage_trainer.py:221 [INFO] Train: [ 30/49] Final Prec@1 90.1640%
05/30 11:34:07午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][50/391]	Step 12121	Loss 0.5541	Prec@(1,5) (82.2%, 98.9%)
05/30 11:34:12午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 0.5710	Prec@(1,5) (81.6%, 98.8%)
05/30 11:34:16午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][150/391]	Step 12121	Loss 0.5760	Prec@(1,5) (81.4%, 98.8%)
05/30 11:34:21午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 0.5676	Prec@(1,5) (81.6%, 98.8%)
05/30 11:34:25午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][250/391]	Step 12121	Loss 0.5656	Prec@(1,5) (81.7%, 98.8%)
05/30 11:34:30午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 0.5712	Prec@(1,5) (81.6%, 98.8%)
05/30 11:34:35午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][350/391]	Step 12121	Loss 0.5686	Prec@(1,5) (81.6%, 98.9%)
05/30 11:34:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 0.5693	Prec@(1,5) (81.7%, 98.8%)
05/30 11:34:38午後 searchStage_trainer.py:260 [INFO] Valid: [ 30/49] Final Prec@1 81.6880%
05/30 11:34:38午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:34:39午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 82.6480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2639, 0.3130, 0.2488, 0.1743],
        [0.2729, 0.3065, 0.2433, 0.1773]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2422, 0.3252, 0.2569, 0.1757],
        [0.2552, 0.3119, 0.2629, 0.1701],
        [0.2668, 0.3449, 0.2082, 0.1801]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2564, 0.2962, 0.2708, 0.1766],
        [0.2733, 0.3295, 0.2273, 0.1699],
        [0.2745, 0.3132, 0.2171, 0.1951]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2707, 0.3133, 0.2271, 0.1889],
        [0.2720, 0.3052, 0.2216, 0.2012],
        [0.2705, 0.3072, 0.2311, 0.1913]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2822, 0.3157, 0.2156, 0.1865],
        [0.2823, 0.3112, 0.2226, 0.1839],
        [0.2761, 0.3105, 0.2203, 0.1930]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2775, 0.3045, 0.2240, 0.1939],
        [0.2752, 0.3001, 0.2186, 0.2060],
        [0.2672, 0.2983, 0.2263, 0.2081]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2490],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2521, 0.2468],
        [0.2553, 0.2521, 0.2525, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2514, 0.2518, 0.2477],
        [0.2512, 0.2548, 0.2494, 0.2446],
        [0.2519, 0.2528, 0.2475, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2499, 0.2470],
        [0.2512, 0.2524, 0.2496, 0.2468],
        [0.2522, 0.2511, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2482],
        [0.2530, 0.2502, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2519, 0.2475, 0.2508],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2502, 0.2505, 0.2511],
        [0.2465, 0.2535, 0.2516, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2546, 0.2516, 0.2466],
        [0.2471, 0.2500, 0.2491, 0.2537],
        [0.2466, 0.2526, 0.2513, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2499, 0.2511],
        [0.2464, 0.2519, 0.2488, 0.2529],
        [0.2454, 0.2545, 0.2537, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2510, 0.2516, 0.2490],
        [0.2493, 0.2505, 0.2479, 0.2524],
        [0.2494, 0.2514, 0.2510, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2427, 0.2458, 0.2653],
        [0.2416, 0.2556, 0.2612, 0.2416],
        [0.2426, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2203, 0.2622, 0.2761, 0.2414],
        [0.2226, 0.2609, 0.2680, 0.2485],
        [0.2330, 0.2522, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:35:08午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][50/390]	Step 12171	lr 0.00858	Loss 0.3108 (0.2480)	Prec@(1,5) (91.5%, 99.8%)	
05/30 11:35:35午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.2729 (0.2618)	Prec@(1,5) (91.0%, 99.7%)	
05/30 11:36:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][150/390]	Step 12271	lr 0.00858	Loss 0.2674 (0.2557)	Prec@(1,5) (91.2%, 99.8%)	
05/30 11:36:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.2459 (0.2586)	Prec@(1,5) (91.2%, 99.8%)	
05/30 11:36:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][250/390]	Step 12371	lr 0.00858	Loss 0.3175 (0.2599)	Prec@(1,5) (91.1%, 99.7%)	
05/30 11:37:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 0.3040 (0.2588)	Prec@(1,5) (91.2%, 99.7%)	
05/30 11:37:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][350/390]	Step 12471	lr 0.00858	Loss 0.1596 (0.2602)	Prec@(1,5) (91.1%, 99.7%)	
05/30 11:38:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 0.3001 (0.2644)	Prec@(1,5) (90.9%, 99.7%)	
05/30 11:38:18午後 searchStage_trainer.py:221 [INFO] Train: [ 31/49] Final Prec@1 90.8680%
05/30 11:38:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][50/391]	Step 12512	Loss 0.5795	Prec@(1,5) (80.8%, 98.5%)
05/30 11:38:27午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 0.5844	Prec@(1,5) (81.2%, 98.7%)
05/30 11:38:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][150/391]	Step 12512	Loss 0.5887	Prec@(1,5) (81.2%, 98.8%)
05/30 11:38:37午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 0.5880	Prec@(1,5) (81.3%, 98.8%)
05/30 11:38:41午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][250/391]	Step 12512	Loss 0.5831	Prec@(1,5) (81.4%, 98.8%)
05/30 11:38:46午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 0.5843	Prec@(1,5) (81.4%, 98.8%)
05/30 11:38:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][350/391]	Step 12512	Loss 0.5810	Prec@(1,5) (81.5%, 98.8%)
05/30 11:38:54午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 0.5771	Prec@(1,5) (81.7%, 98.8%)
05/30 11:38:54午後 searchStage_trainer.py:260 [INFO] Valid: [ 31/49] Final Prec@1 81.7000%
05/30 11:38:54午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:38:55午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 82.6480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2639, 0.3128, 0.2488, 0.1745],
        [0.2728, 0.3063, 0.2434, 0.1774]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2421, 0.3250, 0.2571, 0.1758],
        [0.2551, 0.3118, 0.2630, 0.1701],
        [0.2668, 0.3447, 0.2083, 0.1803]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2562, 0.2962, 0.2708, 0.1767],
        [0.2732, 0.3294, 0.2273, 0.1701],
        [0.2745, 0.3130, 0.2172, 0.1953]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.3133, 0.2271, 0.1890],
        [0.2719, 0.3050, 0.2216, 0.2014],
        [0.2704, 0.3070, 0.2311, 0.1915]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2822, 0.3156, 0.2157, 0.1865],
        [0.2823, 0.3110, 0.2226, 0.1841],
        [0.2761, 0.3103, 0.2204, 0.1932]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2775, 0.3044, 0.2240, 0.1940],
        [0.2752, 0.3000, 0.2187, 0.2061],
        [0.2672, 0.2981, 0.2264, 0.2084]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2490],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2521, 0.2468],
        [0.2553, 0.2521, 0.2525, 0.2400],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2514, 0.2518, 0.2477],
        [0.2512, 0.2548, 0.2494, 0.2446],
        [0.2519, 0.2528, 0.2475, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2498, 0.2470],
        [0.2512, 0.2524, 0.2496, 0.2468],
        [0.2521, 0.2511, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2482],
        [0.2530, 0.2502, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2502, 0.2505, 0.2511],
        [0.2465, 0.2535, 0.2516, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2546, 0.2516, 0.2466],
        [0.2471, 0.2500, 0.2491, 0.2537],
        [0.2467, 0.2525, 0.2513, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2499, 0.2511],
        [0.2464, 0.2519, 0.2488, 0.2529],
        [0.2455, 0.2544, 0.2537, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2510, 0.2516, 0.2490],
        [0.2493, 0.2504, 0.2479, 0.2524],
        [0.2495, 0.2514, 0.2509, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2427, 0.2458, 0.2654],
        [0.2416, 0.2556, 0.2612, 0.2416],
        [0.2427, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2202, 0.2622, 0.2762, 0.2414],
        [0.2226, 0.2609, 0.2680, 0.2485],
        [0.2330, 0.2522, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:39:24午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][50/390]	Step 12562	lr 0.00789	Loss 0.1584 (0.2512)	Prec@(1,5) (91.2%, 99.8%)	
05/30 11:39:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.1832 (0.2514)	Prec@(1,5) (91.1%, 99.8%)	
05/30 11:40:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][150/390]	Step 12662	lr 0.00789	Loss 0.2884 (0.2524)	Prec@(1,5) (90.9%, 99.8%)	
05/30 11:40:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.0609 (0.2485)	Prec@(1,5) (91.3%, 99.8%)	
05/30 11:41:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][250/390]	Step 12762	lr 0.00789	Loss 0.2477 (0.2502)	Prec@(1,5) (91.2%, 99.8%)	
05/30 11:41:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.3286 (0.2486)	Prec@(1,5) (91.3%, 99.8%)	
05/30 11:42:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][350/390]	Step 12862	lr 0.00789	Loss 0.2079 (0.2481)	Prec@(1,5) (91.4%, 99.8%)	
05/30 11:42:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 0.3094 (0.2485)	Prec@(1,5) (91.3%, 99.8%)	
05/30 11:42:34午後 searchStage_trainer.py:221 [INFO] Train: [ 32/49] Final Prec@1 91.3240%
05/30 11:42:39午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][50/391]	Step 12903	Loss 0.5678	Prec@(1,5) (81.4%, 99.1%)
05/30 11:42:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 0.5412	Prec@(1,5) (82.6%, 99.0%)
05/30 11:42:48午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][150/391]	Step 12903	Loss 0.5416	Prec@(1,5) (82.5%, 99.1%)
05/30 11:42:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 0.5347	Prec@(1,5) (82.8%, 99.1%)
05/30 11:42:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][250/391]	Step 12903	Loss 0.5429	Prec@(1,5) (82.5%, 99.1%)
05/30 11:43:02午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 0.5413	Prec@(1,5) (82.5%, 99.1%)
05/30 11:43:07午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][350/391]	Step 12903	Loss 0.5451	Prec@(1,5) (82.5%, 99.1%)
05/30 11:43:10午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 0.5448	Prec@(1,5) (82.6%, 99.1%)
05/30 11:43:10午後 searchStage_trainer.py:260 [INFO] Valid: [ 32/49] Final Prec@1 82.5800%
05/30 11:43:10午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:43:11午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 82.6480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2638, 0.3126, 0.2489, 0.1746],
        [0.2728, 0.3061, 0.2435, 0.1776]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2421, 0.3249, 0.2572, 0.1758],
        [0.2550, 0.3117, 0.2630, 0.1702],
        [0.2667, 0.3445, 0.2083, 0.1805]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2562, 0.2962, 0.2709, 0.1766],
        [0.2732, 0.3293, 0.2274, 0.1702],
        [0.2744, 0.3128, 0.2173, 0.1955]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.3131, 0.2270, 0.1892],
        [0.2719, 0.3049, 0.2217, 0.2015],
        [0.2704, 0.3068, 0.2312, 0.1917]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2823, 0.3155, 0.2157, 0.1866],
        [0.2823, 0.3108, 0.2226, 0.1843],
        [0.2760, 0.3101, 0.2204, 0.1934]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2775, 0.3043, 0.2241, 0.1942],
        [0.2752, 0.2998, 0.2187, 0.2063],
        [0.2671, 0.2979, 0.2264, 0.2086]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2490],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2521, 0.2468],
        [0.2554, 0.2521, 0.2525, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2518, 0.2477],
        [0.2512, 0.2548, 0.2494, 0.2446],
        [0.2519, 0.2528, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2499, 0.2470],
        [0.2512, 0.2524, 0.2496, 0.2468],
        [0.2521, 0.2511, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2483],
        [0.2530, 0.2502, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2502, 0.2505, 0.2511],
        [0.2465, 0.2535, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2546, 0.2516, 0.2466],
        [0.2472, 0.2500, 0.2491, 0.2537],
        [0.2467, 0.2525, 0.2513, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2499, 0.2512],
        [0.2464, 0.2519, 0.2488, 0.2529],
        [0.2455, 0.2544, 0.2537, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2510, 0.2516, 0.2490],
        [0.2493, 0.2504, 0.2479, 0.2524],
        [0.2495, 0.2514, 0.2509, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2427, 0.2458, 0.2654],
        [0.2416, 0.2556, 0.2612, 0.2416],
        [0.2427, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2202, 0.2622, 0.2762, 0.2414],
        [0.2226, 0.2609, 0.2680, 0.2485],
        [0.2330, 0.2522, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:43:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][50/390]	Step 12953	lr 0.00722	Loss 0.2525 (0.2136)	Prec@(1,5) (92.7%, 99.7%)	
05/30 11:44:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.1547 (0.2131)	Prec@(1,5) (92.7%, 99.8%)	
05/30 11:44:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][150/390]	Step 13053	lr 0.00722	Loss 0.1416 (0.2115)	Prec@(1,5) (92.7%, 99.8%)	
05/30 11:45:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.3100 (0.2144)	Prec@(1,5) (92.5%, 99.8%)	
05/30 11:45:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][250/390]	Step 13153	lr 0.00722	Loss 0.1198 (0.2202)	Prec@(1,5) (92.4%, 99.8%)	
05/30 11:45:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.3142 (0.2259)	Prec@(1,5) (92.2%, 99.8%)	
05/30 11:46:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][350/390]	Step 13253	lr 0.00722	Loss 0.2047 (0.2284)	Prec@(1,5) (92.1%, 99.8%)	
05/30 11:46:49午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 0.2932 (0.2280)	Prec@(1,5) (92.1%, 99.8%)	
05/30 11:46:50午後 searchStage_trainer.py:221 [INFO] Train: [ 33/49] Final Prec@1 92.1320%
05/30 11:46:55午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][50/391]	Step 13294	Loss 0.5325	Prec@(1,5) (84.0%, 98.9%)
05/30 11:46:59午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 0.5315	Prec@(1,5) (83.6%, 99.0%)
05/30 11:47:04午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][150/391]	Step 13294	Loss 0.5344	Prec@(1,5) (83.7%, 98.9%)
05/30 11:47:09午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 0.5420	Prec@(1,5) (83.3%, 98.9%)
05/30 11:47:13午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][250/391]	Step 13294	Loss 0.5392	Prec@(1,5) (83.4%, 98.9%)
05/30 11:47:18午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 0.5475	Prec@(1,5) (83.2%, 98.9%)
05/30 11:47:22午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][350/391]	Step 13294	Loss 0.5387	Prec@(1,5) (83.4%, 98.9%)
05/30 11:47:26午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 0.5373	Prec@(1,5) (83.3%, 98.9%)
05/30 11:47:26午後 searchStage_trainer.py:260 [INFO] Valid: [ 33/49] Final Prec@1 83.3200%
05/30 11:47:26午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:47:27午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.3200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2638, 0.3124, 0.2490, 0.1748],
        [0.2728, 0.3060, 0.2436, 0.1777]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2421, 0.3249, 0.2572, 0.1758],
        [0.2550, 0.3115, 0.2631, 0.1704],
        [0.2666, 0.3443, 0.2084, 0.1806]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2561, 0.2962, 0.2711, 0.1766],
        [0.2731, 0.3291, 0.2274, 0.1703],
        [0.2743, 0.3127, 0.2173, 0.1957]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.3130, 0.2270, 0.1894],
        [0.2718, 0.3047, 0.2217, 0.2017],
        [0.2703, 0.3066, 0.2313, 0.1918]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2822, 0.3153, 0.2157, 0.1867],
        [0.2822, 0.3107, 0.2227, 0.1844],
        [0.2760, 0.3099, 0.2205, 0.1936]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2775, 0.3042, 0.2241, 0.1943],
        [0.2751, 0.2996, 0.2188, 0.2065],
        [0.2670, 0.2977, 0.2265, 0.2088]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2490],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2521, 0.2468],
        [0.2554, 0.2521, 0.2525, 0.2400],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2518, 0.2477],
        [0.2512, 0.2547, 0.2494, 0.2446],
        [0.2518, 0.2528, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2499, 0.2470],
        [0.2512, 0.2523, 0.2496, 0.2468],
        [0.2521, 0.2511, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2483],
        [0.2530, 0.2502, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2514, 0.2484, 0.2486],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2502, 0.2505, 0.2511],
        [0.2465, 0.2535, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2546, 0.2516, 0.2466],
        [0.2472, 0.2500, 0.2491, 0.2538],
        [0.2467, 0.2525, 0.2512, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2499, 0.2512],
        [0.2465, 0.2519, 0.2488, 0.2529],
        [0.2455, 0.2544, 0.2537, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2510, 0.2516, 0.2490],
        [0.2494, 0.2504, 0.2478, 0.2524],
        [0.2495, 0.2514, 0.2509, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2427, 0.2458, 0.2655],
        [0.2416, 0.2556, 0.2612, 0.2416],
        [0.2427, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2201, 0.2622, 0.2763, 0.2414],
        [0.2226, 0.2609, 0.2680, 0.2485],
        [0.2330, 0.2522, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:47:56午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][50/390]	Step 13344	lr 0.00657	Loss 0.1841 (0.1850)	Prec@(1,5) (93.6%, 100.0%)	
05/30 11:48:24午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.1495 (0.1913)	Prec@(1,5) (93.5%, 100.0%)	
05/30 11:48:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][150/390]	Step 13444	lr 0.00657	Loss 0.1062 (0.1959)	Prec@(1,5) (93.4%, 99.9%)	
05/30 11:49:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.3864 (0.2036)	Prec@(1,5) (93.1%, 99.9%)	
05/30 11:49:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][250/390]	Step 13544	lr 0.00657	Loss 0.3191 (0.2042)	Prec@(1,5) (93.0%, 99.9%)	
05/30 11:50:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.1828 (0.2039)	Prec@(1,5) (93.0%, 99.9%)	
05/30 11:50:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][350/390]	Step 13644	lr 0.00657	Loss 0.2596 (0.2070)	Prec@(1,5) (92.9%, 99.9%)	
05/30 11:51:06午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 0.3883 (0.2103)	Prec@(1,5) (92.7%, 99.9%)	
05/30 11:51:06午後 searchStage_trainer.py:221 [INFO] Train: [ 34/49] Final Prec@1 92.7080%
05/30 11:51:11午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][50/391]	Step 13685	Loss 0.6011	Prec@(1,5) (81.6%, 99.1%)
05/30 11:51:16午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 0.5926	Prec@(1,5) (82.3%, 98.9%)
05/30 11:51:20午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][150/391]	Step 13685	Loss 0.5897	Prec@(1,5) (82.2%, 99.0%)
05/30 11:51:25午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 0.5828	Prec@(1,5) (82.5%, 99.0%)
05/30 11:51:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][250/391]	Step 13685	Loss 0.5839	Prec@(1,5) (82.4%, 99.0%)
05/30 11:51:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 0.5897	Prec@(1,5) (82.3%, 98.9%)
05/30 11:51:39午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][350/391]	Step 13685	Loss 0.5862	Prec@(1,5) (82.3%, 99.0%)
05/30 11:51:42午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 0.5837	Prec@(1,5) (82.3%, 99.0%)
05/30 11:51:42午後 searchStage_trainer.py:260 [INFO] Valid: [ 34/49] Final Prec@1 82.3160%
05/30 11:51:42午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:51:43午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.3200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2638, 0.3123, 0.2491, 0.1749],
        [0.2727, 0.3058, 0.2437, 0.1778]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2420, 0.3248, 0.2573, 0.1759],
        [0.2549, 0.3114, 0.2632, 0.1705],
        [0.2666, 0.3442, 0.2085, 0.1808]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2561, 0.2962, 0.2711, 0.1766],
        [0.2731, 0.3290, 0.2275, 0.1705],
        [0.2743, 0.3125, 0.2174, 0.1958]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.3129, 0.2270, 0.1895],
        [0.2718, 0.3046, 0.2218, 0.2018],
        [0.2703, 0.3064, 0.2313, 0.1920]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2822, 0.3153, 0.2158, 0.1868],
        [0.2822, 0.3105, 0.2228, 0.1846],
        [0.2759, 0.3098, 0.2205, 0.1938]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2775, 0.3041, 0.2241, 0.1944],
        [0.2751, 0.2995, 0.2188, 0.2066],
        [0.2670, 0.2975, 0.2266, 0.2090]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2490],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2521, 0.2468],
        [0.2553, 0.2521, 0.2525, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2518, 0.2477],
        [0.2512, 0.2547, 0.2494, 0.2446],
        [0.2518, 0.2528, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2499, 0.2470],
        [0.2512, 0.2523, 0.2496, 0.2468],
        [0.2521, 0.2511, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2483],
        [0.2530, 0.2502, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2513, 0.2484, 0.2486],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2502, 0.2505, 0.2511],
        [0.2465, 0.2535, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2546, 0.2516, 0.2466],
        [0.2472, 0.2500, 0.2490, 0.2538],
        [0.2467, 0.2525, 0.2512, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2498, 0.2512],
        [0.2465, 0.2519, 0.2488, 0.2529],
        [0.2455, 0.2544, 0.2537, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2510, 0.2516, 0.2490],
        [0.2494, 0.2504, 0.2478, 0.2524],
        [0.2495, 0.2514, 0.2509, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2426, 0.2458, 0.2655],
        [0.2416, 0.2556, 0.2612, 0.2416],
        [0.2427, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2200, 0.2623, 0.2764, 0.2413],
        [0.2226, 0.2609, 0.2680, 0.2485],
        [0.2330, 0.2521, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:52:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][50/390]	Step 13735	lr 0.00595	Loss 0.1375 (0.1809)	Prec@(1,5) (94.1%, 100.0%)	
05/30 11:52:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.2637 (0.1785)	Prec@(1,5) (94.1%, 99.9%)	
05/30 11:53:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][150/390]	Step 13835	lr 0.00595	Loss 0.3090 (0.1824)	Prec@(1,5) (93.9%, 99.9%)	
05/30 11:53:35午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 0.1299 (0.1846)	Prec@(1,5) (93.7%, 99.9%)	
05/30 11:54:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][250/390]	Step 13935	lr 0.00595	Loss 0.1773 (0.1813)	Prec@(1,5) (93.7%, 99.9%)	
05/30 11:54:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 0.1935 (0.1854)	Prec@(1,5) (93.6%, 99.9%)	
05/30 11:54:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][350/390]	Step 14035	lr 0.00595	Loss 0.1790 (0.1892)	Prec@(1,5) (93.4%, 99.9%)	
05/30 11:55:22午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 0.2218 (0.1907)	Prec@(1,5) (93.4%, 99.9%)	
05/30 11:55:22午後 searchStage_trainer.py:221 [INFO] Train: [ 35/49] Final Prec@1 93.3800%
05/30 11:55:27午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][50/391]	Step 14076	Loss 0.5723	Prec@(1,5) (83.2%, 99.0%)
05/30 11:55:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 0.5505	Prec@(1,5) (83.6%, 99.2%)
05/30 11:55:36午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][150/391]	Step 14076	Loss 0.5473	Prec@(1,5) (83.7%, 99.1%)
05/30 11:55:41午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 0.5396	Prec@(1,5) (83.8%, 99.1%)
05/30 11:55:45午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][250/391]	Step 14076	Loss 0.5437	Prec@(1,5) (83.7%, 99.0%)
05/30 11:55:50午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 0.5464	Prec@(1,5) (83.6%, 99.0%)
05/30 11:55:55午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][350/391]	Step 14076	Loss 0.5392	Prec@(1,5) (83.8%, 99.0%)
05/30 11:55:58午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 0.5430	Prec@(1,5) (83.7%, 99.0%)
05/30 11:55:58午後 searchStage_trainer.py:260 [INFO] Valid: [ 35/49] Final Prec@1 83.6960%
05/30 11:55:58午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:55:59午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.6960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2637, 0.3121, 0.2492, 0.1750],
        [0.2727, 0.3056, 0.2437, 0.1780]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2420, 0.3246, 0.2573, 0.1760],
        [0.2548, 0.3113, 0.2633, 0.1706],
        [0.2666, 0.3440, 0.2085, 0.1809]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2961, 0.2712, 0.1769],
        [0.2730, 0.3288, 0.2275, 0.1706],
        [0.2743, 0.3123, 0.2175, 0.1960]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3128, 0.2270, 0.1896],
        [0.2718, 0.3044, 0.2218, 0.2020],
        [0.2702, 0.3062, 0.2314, 0.1921]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2821, 0.3151, 0.2158, 0.1870],
        [0.2821, 0.3104, 0.2228, 0.1847],
        [0.2759, 0.3096, 0.2206, 0.1939]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2774, 0.3040, 0.2241, 0.1945],
        [0.2750, 0.2993, 0.2189, 0.2068],
        [0.2669, 0.2973, 0.2266, 0.2091]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2491],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2521, 0.2525, 0.2400],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2518, 0.2477],
        [0.2512, 0.2547, 0.2494, 0.2446],
        [0.2518, 0.2528, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2499, 0.2470],
        [0.2512, 0.2523, 0.2496, 0.2468],
        [0.2521, 0.2511, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2483],
        [0.2530, 0.2502, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2513, 0.2484, 0.2486],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2502, 0.2505, 0.2512],
        [0.2465, 0.2535, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2546, 0.2516, 0.2466],
        [0.2472, 0.2500, 0.2490, 0.2538],
        [0.2467, 0.2525, 0.2512, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2498, 0.2512],
        [0.2465, 0.2519, 0.2487, 0.2529],
        [0.2455, 0.2544, 0.2537, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2510, 0.2516, 0.2490],
        [0.2494, 0.2504, 0.2478, 0.2524],
        [0.2495, 0.2514, 0.2509, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2426, 0.2458, 0.2655],
        [0.2416, 0.2556, 0.2612, 0.2416],
        [0.2427, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2200, 0.2623, 0.2764, 0.2413],
        [0.2225, 0.2610, 0.2680, 0.2485],
        [0.2331, 0.2521, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:56:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][50/390]	Step 14126	lr 0.00535	Loss 0.0995 (0.1759)	Prec@(1,5) (93.9%, 99.9%)	
05/30 11:56:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.2545 (0.1691)	Prec@(1,5) (94.0%, 99.9%)	
05/30 11:57:23午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][150/390]	Step 14226	lr 0.00535	Loss 0.0892 (0.1630)	Prec@(1,5) (94.2%, 99.9%)	
05/30 11:57:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.1038 (0.1639)	Prec@(1,5) (94.2%, 99.9%)	
05/30 11:58:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][250/390]	Step 14326	lr 0.00535	Loss 0.3623 (0.1666)	Prec@(1,5) (94.1%, 99.9%)	
05/30 11:58:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.2202 (0.1665)	Prec@(1,5) (94.1%, 99.9%)	
05/30 11:59:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][350/390]	Step 14426	lr 0.00535	Loss 0.1152 (0.1682)	Prec@(1,5) (94.0%, 99.9%)	
05/30 11:59:38午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.0917 (0.1698)	Prec@(1,5) (94.0%, 99.9%)	
05/30 11:59:38午後 searchStage_trainer.py:221 [INFO] Train: [ 36/49] Final Prec@1 94.0200%
05/30 11:59:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][50/391]	Step 14467	Loss 0.5112	Prec@(1,5) (85.3%, 99.3%)
05/30 11:59:48午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 0.5046	Prec@(1,5) (85.1%, 99.2%)
05/30 11:59:52午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][150/391]	Step 14467	Loss 0.5103	Prec@(1,5) (85.1%, 99.2%)
05/30 11:59:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 0.5108	Prec@(1,5) (85.1%, 99.2%)
05/31 12:00:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][250/391]	Step 14467	Loss 0.5231	Prec@(1,5) (84.8%, 99.1%)
05/31 12:00:06午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 0.5288	Prec@(1,5) (84.7%, 99.1%)
05/31 12:00:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][350/391]	Step 14467	Loss 0.5337	Prec@(1,5) (84.5%, 99.1%)
05/31 12:00:14午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 0.5360	Prec@(1,5) (84.5%, 99.1%)
05/31 12:00:14午前 searchStage_trainer.py:260 [INFO] Valid: [ 36/49] Final Prec@1 84.4600%
05/31 12:00:15午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:00:15午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.4600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2636, 0.3120, 0.2492, 0.1752],
        [0.2726, 0.3055, 0.2438, 0.1781]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2420, 0.3245, 0.2574, 0.1761],
        [0.2548, 0.3112, 0.2633, 0.1707],
        [0.2665, 0.3438, 0.2086, 0.1810]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2960, 0.2713, 0.1770],
        [0.2730, 0.3287, 0.2276, 0.1707],
        [0.2742, 0.3121, 0.2175, 0.1961]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3127, 0.2270, 0.1898],
        [0.2718, 0.3043, 0.2219, 0.2021],
        [0.2702, 0.3061, 0.2315, 0.1923]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2821, 0.3150, 0.2157, 0.1871],
        [0.2821, 0.3102, 0.2229, 0.1848],
        [0.2759, 0.3094, 0.2206, 0.1941]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2774, 0.3039, 0.2241, 0.1946],
        [0.2750, 0.2992, 0.2189, 0.2069],
        [0.2668, 0.2971, 0.2267, 0.2093]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2491],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2521, 0.2525, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2519, 0.2476],
        [0.2512, 0.2547, 0.2494, 0.2446],
        [0.2518, 0.2528, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2499, 0.2470],
        [0.2512, 0.2523, 0.2496, 0.2468],
        [0.2521, 0.2511, 0.2475, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2513, 0.2484, 0.2486],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2502, 0.2505, 0.2512],
        [0.2465, 0.2535, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2546, 0.2516, 0.2466],
        [0.2472, 0.2500, 0.2490, 0.2538],
        [0.2467, 0.2525, 0.2512, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2498, 0.2512],
        [0.2465, 0.2519, 0.2487, 0.2529],
        [0.2455, 0.2544, 0.2537, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2510, 0.2516, 0.2490],
        [0.2494, 0.2504, 0.2478, 0.2524],
        [0.2496, 0.2514, 0.2509, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2426, 0.2458, 0.2656],
        [0.2416, 0.2556, 0.2612, 0.2416],
        [0.2427, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2199, 0.2623, 0.2765, 0.2413],
        [0.2225, 0.2610, 0.2680, 0.2485],
        [0.2331, 0.2521, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:00:44午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][50/390]	Step 14517	lr 0.00479	Loss 0.0641 (0.1599)	Prec@(1,5) (94.5%, 100.0%)	
05/31 12:01:12午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.1020 (0.1535)	Prec@(1,5) (94.6%, 100.0%)	
05/31 12:01:39午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][150/390]	Step 14617	lr 0.00479	Loss 0.1693 (0.1522)	Prec@(1,5) (94.7%, 100.0%)	
05/31 12:02:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.1325 (0.1506)	Prec@(1,5) (94.8%, 100.0%)	
05/31 12:02:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][250/390]	Step 14717	lr 0.00479	Loss 0.2353 (0.1529)	Prec@(1,5) (94.7%, 100.0%)	
05/31 12:03:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.1747 (0.1525)	Prec@(1,5) (94.7%, 100.0%)	
05/31 12:03:31午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][350/390]	Step 14817	lr 0.00479	Loss 0.2533 (0.1542)	Prec@(1,5) (94.7%, 100.0%)	
05/31 12:03:54午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.2704 (0.1556)	Prec@(1,5) (94.7%, 100.0%)	
05/31 12:03:54午前 searchStage_trainer.py:221 [INFO] Train: [ 37/49] Final Prec@1 94.6520%
05/31 12:03:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][50/391]	Step 14858	Loss 0.5792	Prec@(1,5) (83.4%, 99.3%)
05/31 12:04:04午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 0.5604	Prec@(1,5) (83.8%, 99.2%)
05/31 12:04:08午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][150/391]	Step 14858	Loss 0.5505	Prec@(1,5) (83.9%, 99.2%)
05/31 12:04:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 0.5487	Prec@(1,5) (83.9%, 99.2%)
05/31 12:04:18午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][250/391]	Step 14858	Loss 0.5448	Prec@(1,5) (84.1%, 99.1%)
05/31 12:04:22午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 0.5474	Prec@(1,5) (84.2%, 99.1%)
05/31 12:04:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][350/391]	Step 14858	Loss 0.5465	Prec@(1,5) (84.2%, 99.1%)
05/31 12:04:30午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 0.5476	Prec@(1,5) (84.2%, 99.1%)
05/31 12:04:31午前 searchStage_trainer.py:260 [INFO] Valid: [ 37/49] Final Prec@1 84.1960%
05/31 12:04:31午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:04:31午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.4600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2636, 0.3118, 0.2493, 0.1753],
        [0.2726, 0.3053, 0.2439, 0.1782]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.3245, 0.2574, 0.1762],
        [0.2547, 0.3111, 0.2634, 0.1708],
        [0.2665, 0.3437, 0.2087, 0.1812]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2960, 0.2714, 0.1769],
        [0.2730, 0.3286, 0.2276, 0.1708],
        [0.2742, 0.3120, 0.2176, 0.1963]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3126, 0.2270, 0.1899],
        [0.2717, 0.3041, 0.2219, 0.2022],
        [0.2701, 0.3059, 0.2315, 0.1924]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2821, 0.3149, 0.2158, 0.1872],
        [0.2821, 0.3101, 0.2229, 0.1849],
        [0.2758, 0.3093, 0.2207, 0.1942]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2774, 0.3038, 0.2241, 0.1948],
        [0.2749, 0.2990, 0.2190, 0.2071],
        [0.2668, 0.2970, 0.2267, 0.2095]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2491],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2521, 0.2526, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2519, 0.2476],
        [0.2512, 0.2547, 0.2494, 0.2446],
        [0.2518, 0.2528, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2499, 0.2470],
        [0.2512, 0.2523, 0.2496, 0.2468],
        [0.2521, 0.2511, 0.2476, 0.2492]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2502, 0.2505, 0.2512],
        [0.2466, 0.2535, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2546, 0.2516, 0.2466],
        [0.2472, 0.2500, 0.2490, 0.2538],
        [0.2467, 0.2525, 0.2512, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2498, 0.2512],
        [0.2465, 0.2519, 0.2487, 0.2529],
        [0.2456, 0.2544, 0.2537, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2490],
        [0.2494, 0.2504, 0.2478, 0.2524],
        [0.2496, 0.2514, 0.2509, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2426, 0.2457, 0.2656],
        [0.2416, 0.2556, 0.2612, 0.2416],
        [0.2427, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2199, 0.2623, 0.2765, 0.2413],
        [0.2225, 0.2610, 0.2681, 0.2485],
        [0.2331, 0.2521, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:05:00午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][50/390]	Step 14908	lr 0.00425	Loss 0.1184 (0.1282)	Prec@(1,5) (95.3%, 99.9%)	
05/31 12:05:27午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.0928 (0.1362)	Prec@(1,5) (94.9%, 100.0%)	
05/31 12:05:50午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][150/390]	Step 15008	lr 0.00425	Loss 0.1872 (0.1404)	Prec@(1,5) (94.8%, 100.0%)	
05/31 12:06:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.0377 (0.1390)	Prec@(1,5) (94.9%, 100.0%)	
05/31 12:06:43午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][250/390]	Step 15108	lr 0.00425	Loss 0.3346 (0.1416)	Prec@(1,5) (94.8%, 100.0%)	
05/31 12:07:11午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.1129 (0.1396)	Prec@(1,5) (94.9%, 100.0%)	
05/31 12:07:39午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][350/390]	Step 15208	lr 0.00425	Loss 0.1748 (0.1418)	Prec@(1,5) (94.8%, 100.0%)	
05/31 12:08:02午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.1323 (0.1429)	Prec@(1,5) (94.8%, 99.9%)	
05/31 12:08:03午前 searchStage_trainer.py:221 [INFO] Train: [ 38/49] Final Prec@1 94.7920%
05/31 12:08:07午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][50/391]	Step 15249	Loss 0.5470	Prec@(1,5) (84.6%, 99.1%)
05/31 12:08:12午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 0.5466	Prec@(1,5) (84.1%, 99.1%)
05/31 12:08:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][150/391]	Step 15249	Loss 0.5375	Prec@(1,5) (84.4%, 99.0%)
05/31 12:08:21午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 0.5375	Prec@(1,5) (84.4%, 99.1%)
05/31 12:08:26午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][250/391]	Step 15249	Loss 0.5410	Prec@(1,5) (84.3%, 99.1%)
05/31 12:08:30午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 0.5448	Prec@(1,5) (84.3%, 99.1%)
05/31 12:08:35午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][350/391]	Step 15249	Loss 0.5428	Prec@(1,5) (84.3%, 99.1%)
05/31 12:08:39午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 0.5441	Prec@(1,5) (84.3%, 99.1%)
05/31 12:08:39午前 searchStage_trainer.py:260 [INFO] Valid: [ 38/49] Final Prec@1 84.2640%
05/31 12:08:39午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:08:39午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.4600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2636, 0.3117, 0.2494, 0.1754],
        [0.2726, 0.3052, 0.2440, 0.1783]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.3244, 0.2575, 0.1762],
        [0.2547, 0.3110, 0.2634, 0.1709],
        [0.2664, 0.3436, 0.2087, 0.1813]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2960, 0.2714, 0.1770],
        [0.2730, 0.3285, 0.2277, 0.1709],
        [0.2741, 0.3118, 0.2176, 0.1964]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3125, 0.2270, 0.1900],
        [0.2717, 0.3040, 0.2219, 0.2023],
        [0.2701, 0.3058, 0.2316, 0.1926]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2820, 0.3149, 0.2159, 0.1873],
        [0.2820, 0.3100, 0.2229, 0.1850],
        [0.2758, 0.3091, 0.2207, 0.1943]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2774, 0.3036, 0.2242, 0.1949],
        [0.2749, 0.2989, 0.2190, 0.2072],
        [0.2667, 0.2968, 0.2268, 0.2096]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2491],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2521, 0.2525, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2519, 0.2476],
        [0.2512, 0.2547, 0.2494, 0.2446],
        [0.2518, 0.2528, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2525, 0.2499, 0.2470],
        [0.2512, 0.2523, 0.2496, 0.2469],
        [0.2521, 0.2511, 0.2476, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2502, 0.2504, 0.2512],
        [0.2466, 0.2535, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2546, 0.2515, 0.2466],
        [0.2472, 0.2500, 0.2490, 0.2538],
        [0.2468, 0.2525, 0.2512, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2498, 0.2512],
        [0.2465, 0.2519, 0.2487, 0.2529],
        [0.2456, 0.2544, 0.2536, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2490],
        [0.2494, 0.2504, 0.2478, 0.2524],
        [0.2496, 0.2514, 0.2509, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2426, 0.2457, 0.2656],
        [0.2416, 0.2556, 0.2612, 0.2416],
        [0.2427, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2198, 0.2623, 0.2766, 0.2413],
        [0.2225, 0.2610, 0.2681, 0.2485],
        [0.2331, 0.2521, 0.2570, 0.2578]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:09:08午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][50/390]	Step 15299	lr 0.00375	Loss 0.1011 (0.1195)	Prec@(1,5) (96.0%, 100.0%)	
05/31 12:09:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.0664 (0.1190)	Prec@(1,5) (96.1%, 100.0%)	
05/31 12:10:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][150/390]	Step 15399	lr 0.00375	Loss 0.1338 (0.1242)	Prec@(1,5) (95.9%, 99.9%)	
05/31 12:10:30午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.1182 (0.1209)	Prec@(1,5) (96.0%, 100.0%)	
05/31 12:10:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][250/390]	Step 15499	lr 0.00375	Loss 0.0815 (0.1179)	Prec@(1,5) (96.0%, 100.0%)	
05/31 12:11:27午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.1161 (0.1223)	Prec@(1,5) (95.8%, 100.0%)	
05/31 12:11:55午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][350/390]	Step 15599	lr 0.00375	Loss 0.1573 (0.1223)	Prec@(1,5) (95.8%, 100.0%)	
05/31 12:12:17午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.0843 (0.1212)	Prec@(1,5) (95.8%, 100.0%)	
05/31 12:12:18午前 searchStage_trainer.py:221 [INFO] Train: [ 39/49] Final Prec@1 95.8360%
05/31 12:12:22午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][50/391]	Step 15640	Loss 0.5291	Prec@(1,5) (85.2%, 99.0%)
05/31 12:12:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 0.5492	Prec@(1,5) (84.7%, 99.1%)
05/31 12:12:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][150/391]	Step 15640	Loss 0.5532	Prec@(1,5) (84.6%, 99.1%)
05/31 12:12:36午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 0.5463	Prec@(1,5) (84.9%, 99.1%)
05/31 12:12:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][250/391]	Step 15640	Loss 0.5529	Prec@(1,5) (84.6%, 99.0%)
05/31 12:12:45午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 0.5502	Prec@(1,5) (84.7%, 99.0%)
05/31 12:12:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][350/391]	Step 15640	Loss 0.5499	Prec@(1,5) (84.6%, 99.0%)
05/31 12:12:54午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 0.5505	Prec@(1,5) (84.7%, 99.0%)
05/31 12:12:54午前 searchStage_trainer.py:260 [INFO] Valid: [ 39/49] Final Prec@1 84.6760%
05/31 12:12:54午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:12:54午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.6760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2635, 0.3115, 0.2495, 0.1755],
        [0.2725, 0.3051, 0.2440, 0.1784]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.3243, 0.2575, 0.1764],
        [0.2547, 0.3108, 0.2635, 0.1710],
        [0.2664, 0.3434, 0.2088, 0.1814]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2961, 0.2715, 0.1770],
        [0.2729, 0.3284, 0.2277, 0.1710],
        [0.2741, 0.3117, 0.2177, 0.1965]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3124, 0.2270, 0.1901],
        [0.2717, 0.3039, 0.2220, 0.2024],
        [0.2701, 0.3056, 0.2316, 0.1927]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2819, 0.3148, 0.2159, 0.1873],
        [0.2820, 0.3099, 0.2230, 0.1852],
        [0.2757, 0.3090, 0.2208, 0.1945]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2773, 0.3035, 0.2242, 0.1950],
        [0.2749, 0.2988, 0.2190, 0.2073],
        [0.2667, 0.2967, 0.2268, 0.2098]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2491],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2521, 0.2526, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2519, 0.2476],
        [0.2512, 0.2547, 0.2494, 0.2446],
        [0.2518, 0.2527, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2525, 0.2499, 0.2471],
        [0.2512, 0.2523, 0.2496, 0.2468],
        [0.2521, 0.2511, 0.2476, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2502, 0.2504, 0.2512],
        [0.2466, 0.2534, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2546, 0.2515, 0.2466],
        [0.2472, 0.2500, 0.2490, 0.2538],
        [0.2468, 0.2525, 0.2512, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2498, 0.2512],
        [0.2465, 0.2519, 0.2487, 0.2529],
        [0.2456, 0.2544, 0.2536, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2489],
        [0.2495, 0.2504, 0.2478, 0.2524],
        [0.2496, 0.2514, 0.2509, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2426, 0.2457, 0.2656],
        [0.2416, 0.2556, 0.2612, 0.2415],
        [0.2427, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2198, 0.2623, 0.2766, 0.2412],
        [0.2225, 0.2610, 0.2681, 0.2485],
        [0.2331, 0.2521, 0.2569, 0.2579]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:13:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][50/390]	Step 15690	lr 0.00329	Loss 0.1366 (0.1092)	Prec@(1,5) (96.7%, 99.9%)	
05/31 12:13:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.0325 (0.1065)	Prec@(1,5) (96.6%, 99.9%)	
05/31 12:14:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][150/390]	Step 15790	lr 0.00329	Loss 0.1272 (0.1106)	Prec@(1,5) (96.2%, 99.9%)	
05/31 12:14:46午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.0241 (0.1125)	Prec@(1,5) (96.2%, 100.0%)	
05/31 12:15:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][250/390]	Step 15890	lr 0.00329	Loss 0.0945 (0.1133)	Prec@(1,5) (96.1%, 100.0%)	
05/31 12:15:43午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.0889 (0.1120)	Prec@(1,5) (96.2%, 100.0%)	
05/31 12:16:11午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][350/390]	Step 15990	lr 0.00329	Loss 0.0834 (0.1107)	Prec@(1,5) (96.2%, 100.0%)	
05/31 12:16:33午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.1242 (0.1124)	Prec@(1,5) (96.1%, 100.0%)	
05/31 12:16:34午前 searchStage_trainer.py:221 [INFO] Train: [ 40/49] Final Prec@1 96.0760%
05/31 12:16:39午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][50/391]	Step 16031	Loss 0.5064	Prec@(1,5) (85.3%, 99.6%)
05/31 12:16:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 0.5382	Prec@(1,5) (85.2%, 99.2%)
05/31 12:16:48午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][150/391]	Step 16031	Loss 0.5282	Prec@(1,5) (85.3%, 99.2%)
05/31 12:16:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 0.5422	Prec@(1,5) (84.9%, 99.2%)
05/31 12:16:57午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][250/391]	Step 16031	Loss 0.5440	Prec@(1,5) (85.0%, 99.1%)
05/31 12:17:02午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 0.5469	Prec@(1,5) (84.9%, 99.1%)
05/31 12:17:06午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][350/391]	Step 16031	Loss 0.5531	Prec@(1,5) (84.7%, 99.1%)
05/31 12:17:10午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 0.5548	Prec@(1,5) (84.7%, 99.1%)
05/31 12:17:10午前 searchStage_trainer.py:260 [INFO] Valid: [ 40/49] Final Prec@1 84.6840%
05/31 12:17:10午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:17:11午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.6840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2635, 0.3114, 0.2495, 0.1756],
        [0.2725, 0.3049, 0.2441, 0.1785]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.3242, 0.2575, 0.1764],
        [0.2547, 0.3107, 0.2635, 0.1711],
        [0.2664, 0.3433, 0.2088, 0.1815]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2961, 0.2715, 0.1770],
        [0.2729, 0.3283, 0.2277, 0.1711],
        [0.2741, 0.3115, 0.2177, 0.1967]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3123, 0.2270, 0.1902],
        [0.2717, 0.3038, 0.2220, 0.2025],
        [0.2700, 0.3055, 0.2317, 0.1928]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2819, 0.3147, 0.2160, 0.1874],
        [0.2820, 0.3098, 0.2230, 0.1853],
        [0.2757, 0.3089, 0.2208, 0.1946]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2773, 0.3034, 0.2242, 0.1951],
        [0.2748, 0.2987, 0.2191, 0.2074],
        [0.2667, 0.2965, 0.2269, 0.2099]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2491],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2520, 0.2526, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2519, 0.2476],
        [0.2512, 0.2547, 0.2494, 0.2447],
        [0.2518, 0.2527, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2525, 0.2499, 0.2471],
        [0.2512, 0.2523, 0.2496, 0.2469],
        [0.2521, 0.2511, 0.2476, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2502, 0.2504, 0.2512],
        [0.2466, 0.2534, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2546, 0.2515, 0.2466],
        [0.2472, 0.2500, 0.2490, 0.2538],
        [0.2468, 0.2525, 0.2512, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2498, 0.2512],
        [0.2465, 0.2518, 0.2487, 0.2529],
        [0.2456, 0.2544, 0.2536, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2489],
        [0.2495, 0.2504, 0.2478, 0.2524],
        [0.2496, 0.2514, 0.2509, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2425, 0.2457, 0.2657],
        [0.2417, 0.2556, 0.2612, 0.2415],
        [0.2427, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2198, 0.2624, 0.2766, 0.2412],
        [0.2225, 0.2610, 0.2681, 0.2485],
        [0.2331, 0.2521, 0.2569, 0.2579]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:17:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][50/390]	Step 16081	lr 0.00287	Loss 0.0647 (0.0833)	Prec@(1,5) (97.0%, 100.0%)	
05/31 12:18:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.0565 (0.0858)	Prec@(1,5) (97.0%, 100.0%)	
05/31 12:18:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][150/390]	Step 16181	lr 0.00287	Loss 0.1544 (0.0908)	Prec@(1,5) (96.9%, 100.0%)	
05/31 12:19:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.1258 (0.0932)	Prec@(1,5) (96.8%, 100.0%)	
05/31 12:19:31午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][250/390]	Step 16281	lr 0.00287	Loss 0.0753 (0.0942)	Prec@(1,5) (96.8%, 100.0%)	
05/31 12:19:59午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.0765 (0.0935)	Prec@(1,5) (96.8%, 100.0%)	
05/31 12:20:27午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][350/390]	Step 16381	lr 0.00287	Loss 0.0753 (0.0950)	Prec@(1,5) (96.8%, 100.0%)	
05/31 12:20:50午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.0435 (0.0971)	Prec@(1,5) (96.7%, 100.0%)	
05/31 12:20:50午前 searchStage_trainer.py:221 [INFO] Train: [ 41/49] Final Prec@1 96.6720%
05/31 12:20:55午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][50/391]	Step 16422	Loss 0.5245	Prec@(1,5) (85.1%, 99.3%)
05/31 12:20:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 0.5369	Prec@(1,5) (85.3%, 99.3%)
05/31 12:21:04午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][150/391]	Step 16422	Loss 0.5210	Prec@(1,5) (85.8%, 99.2%)
05/31 12:21:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 0.5376	Prec@(1,5) (85.6%, 99.2%)
05/31 12:21:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][250/391]	Step 16422	Loss 0.5422	Prec@(1,5) (85.5%, 99.1%)
05/31 12:21:18午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 0.5434	Prec@(1,5) (85.5%, 99.2%)
05/31 12:21:22午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][350/391]	Step 16422	Loss 0.5472	Prec@(1,5) (85.4%, 99.2%)
05/31 12:21:26午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 0.5518	Prec@(1,5) (85.2%, 99.2%)
05/31 12:21:26午前 searchStage_trainer.py:260 [INFO] Valid: [ 41/49] Final Prec@1 85.2360%
05/31 12:21:26午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:21:27午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.2360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2634, 0.3113, 0.2496, 0.1757],
        [0.2724, 0.3048, 0.2442, 0.1786]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.3241, 0.2576, 0.1765],
        [0.2546, 0.3106, 0.2635, 0.1712],
        [0.2663, 0.3432, 0.2089, 0.1816]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2960, 0.2715, 0.1771],
        [0.2729, 0.3282, 0.2278, 0.1712],
        [0.2740, 0.3114, 0.2178, 0.1968]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3123, 0.2270, 0.1903],
        [0.2716, 0.3037, 0.2221, 0.2026],
        [0.2700, 0.3054, 0.2317, 0.1929]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2819, 0.3146, 0.2160, 0.1875],
        [0.2819, 0.3096, 0.2230, 0.1854],
        [0.2757, 0.3087, 0.2209, 0.1947]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2773, 0.3033, 0.2243, 0.1952],
        [0.2748, 0.2986, 0.2191, 0.2076],
        [0.2666, 0.2964, 0.2269, 0.2101]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2513, 0.2526, 0.2491],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2520, 0.2526, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2519, 0.2476],
        [0.2512, 0.2547, 0.2494, 0.2447],
        [0.2518, 0.2527, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2525, 0.2499, 0.2471],
        [0.2512, 0.2523, 0.2496, 0.2469],
        [0.2521, 0.2511, 0.2476, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2521, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2509]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2502, 0.2504, 0.2512],
        [0.2466, 0.2534, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2546, 0.2515, 0.2466],
        [0.2473, 0.2500, 0.2490, 0.2538],
        [0.2468, 0.2525, 0.2512, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2498, 0.2512],
        [0.2465, 0.2518, 0.2487, 0.2529],
        [0.2456, 0.2544, 0.2536, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2489],
        [0.2495, 0.2504, 0.2478, 0.2524],
        [0.2496, 0.2513, 0.2508, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2425, 0.2457, 0.2657],
        [0.2417, 0.2556, 0.2612, 0.2415],
        [0.2427, 0.2526, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2197, 0.2624, 0.2767, 0.2412],
        [0.2225, 0.2610, 0.2681, 0.2484],
        [0.2331, 0.2521, 0.2569, 0.2579]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:21:56午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][50/390]	Step 16472	lr 0.00248	Loss 0.1258 (0.0857)	Prec@(1,5) (97.2%, 100.0%)	
05/31 12:22:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.1057 (0.0851)	Prec@(1,5) (97.1%, 100.0%)	
05/31 12:22:52午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][150/390]	Step 16572	lr 0.00248	Loss 0.1098 (0.0859)	Prec@(1,5) (97.1%, 100.0%)	
05/31 12:23:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.1486 (0.0851)	Prec@(1,5) (97.1%, 100.0%)	
05/31 12:23:47午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][250/390]	Step 16672	lr 0.00248	Loss 0.0767 (0.0869)	Prec@(1,5) (97.0%, 100.0%)	
05/31 12:24:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.0878 (0.0866)	Prec@(1,5) (97.1%, 100.0%)	
05/31 12:24:43午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][350/390]	Step 16772	lr 0.00248	Loss 0.1038 (0.0864)	Prec@(1,5) (97.1%, 100.0%)	
05/31 12:25:06午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.0537 (0.0873)	Prec@(1,5) (97.1%, 100.0%)	
05/31 12:25:06午前 searchStage_trainer.py:221 [INFO] Train: [ 42/49] Final Prec@1 97.0480%
05/31 12:25:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][50/391]	Step 16813	Loss 0.5686	Prec@(1,5) (84.4%, 99.1%)
05/31 12:25:16午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 0.5373	Prec@(1,5) (85.1%, 99.1%)
05/31 12:25:20午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][150/391]	Step 16813	Loss 0.5559	Prec@(1,5) (84.8%, 99.1%)
05/31 12:25:25午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 0.5510	Prec@(1,5) (85.0%, 99.1%)
05/31 12:25:30午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][250/391]	Step 16813	Loss 0.5464	Prec@(1,5) (85.1%, 99.2%)
05/31 12:25:34午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 0.5427	Prec@(1,5) (85.1%, 99.2%)
05/31 12:25:39午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][350/391]	Step 16813	Loss 0.5448	Prec@(1,5) (85.0%, 99.2%)
05/31 12:25:42午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 0.5375	Prec@(1,5) (85.2%, 99.2%)
05/31 12:25:43午前 searchStage_trainer.py:260 [INFO] Valid: [ 42/49] Final Prec@1 85.1960%
05/31 12:25:43午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:25:43午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.2360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2634, 0.3112, 0.2497, 0.1758],
        [0.2724, 0.3047, 0.2442, 0.1787]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.3240, 0.2577, 0.1766],
        [0.2546, 0.3106, 0.2636, 0.1712],
        [0.2663, 0.3430, 0.2090, 0.1817]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2960, 0.2715, 0.1771],
        [0.2728, 0.3281, 0.2278, 0.1712],
        [0.2740, 0.3113, 0.2178, 0.1969]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3122, 0.2270, 0.1903],
        [0.2716, 0.3036, 0.2221, 0.2027],
        [0.2700, 0.3052, 0.2318, 0.1930]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2819, 0.3145, 0.2160, 0.1875],
        [0.2819, 0.3095, 0.2231, 0.1855],
        [0.2756, 0.3086, 0.2209, 0.1948]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2773, 0.3033, 0.2243, 0.1951],
        [0.2748, 0.2984, 0.2191, 0.2077],
        [0.2666, 0.2963, 0.2270, 0.2102]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2512, 0.2526, 0.2491],
        [0.2495, 0.2532, 0.2511, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2520, 0.2526, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2519, 0.2476],
        [0.2512, 0.2547, 0.2494, 0.2447],
        [0.2518, 0.2527, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2525, 0.2499, 0.2471],
        [0.2512, 0.2523, 0.2496, 0.2469],
        [0.2521, 0.2511, 0.2476, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2537, 0.2520, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2510]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2501, 0.2504, 0.2512],
        [0.2466, 0.2534, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2546, 0.2515, 0.2466],
        [0.2473, 0.2500, 0.2490, 0.2538],
        [0.2468, 0.2525, 0.2512, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2498, 0.2512],
        [0.2465, 0.2518, 0.2487, 0.2529],
        [0.2456, 0.2544, 0.2536, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2489],
        [0.2495, 0.2504, 0.2478, 0.2524],
        [0.2496, 0.2513, 0.2508, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2425, 0.2457, 0.2657],
        [0.2417, 0.2555, 0.2612, 0.2415],
        [0.2427, 0.2525, 0.2586, 0.2462]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2197, 0.2624, 0.2767, 0.2412],
        [0.2225, 0.2610, 0.2681, 0.2484],
        [0.2331, 0.2521, 0.2569, 0.2579]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:26:12午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][50/390]	Step 16863	lr 0.00214	Loss 0.0638 (0.0732)	Prec@(1,5) (97.7%, 100.0%)	
05/31 12:26:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.0758 (0.0742)	Prec@(1,5) (97.6%, 100.0%)	
05/31 12:27:08午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][150/390]	Step 16963	lr 0.00214	Loss 0.0596 (0.0741)	Prec@(1,5) (97.6%, 100.0%)	
05/31 12:27:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.0279 (0.0709)	Prec@(1,5) (97.7%, 100.0%)	
05/31 12:28:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][250/390]	Step 17063	lr 0.00214	Loss 0.0230 (0.0715)	Prec@(1,5) (97.7%, 100.0%)	
05/31 12:28:31午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.0760 (0.0710)	Prec@(1,5) (97.7%, 100.0%)	
05/31 12:28:59午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][350/390]	Step 17163	lr 0.00214	Loss 0.0420 (0.0703)	Prec@(1,5) (97.8%, 100.0%)	
05/31 12:29:22午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.0570 (0.0698)	Prec@(1,5) (97.8%, 100.0%)	
05/31 12:29:22午前 searchStage_trainer.py:221 [INFO] Train: [ 43/49] Final Prec@1 97.7560%
05/31 12:29:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][50/391]	Step 17204	Loss 0.5710	Prec@(1,5) (85.7%, 99.0%)
05/31 12:29:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 0.5693	Prec@(1,5) (85.0%, 99.1%)
05/31 12:29:36午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][150/391]	Step 17204	Loss 0.5745	Prec@(1,5) (85.3%, 99.0%)
05/31 12:29:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 0.5667	Prec@(1,5) (85.4%, 99.1%)
05/31 12:29:45午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][250/391]	Step 17204	Loss 0.5644	Prec@(1,5) (85.5%, 99.1%)
05/31 12:29:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 0.5657	Prec@(1,5) (85.6%, 99.1%)
05/31 12:29:55午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][350/391]	Step 17204	Loss 0.5693	Prec@(1,5) (85.5%, 99.1%)
05/31 12:29:58午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 0.5714	Prec@(1,5) (85.5%, 99.1%)
05/31 12:29:58午前 searchStage_trainer.py:260 [INFO] Valid: [ 43/49] Final Prec@1 85.5200%
05/31 12:29:59午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:29:59午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.5200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2634, 0.3111, 0.2497, 0.1759],
        [0.2724, 0.3046, 0.2443, 0.1788]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.3239, 0.2577, 0.1766],
        [0.2546, 0.3105, 0.2636, 0.1713],
        [0.2663, 0.3429, 0.2090, 0.1818]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2960, 0.2714, 0.1771],
        [0.2728, 0.3280, 0.2278, 0.1713],
        [0.2740, 0.3111, 0.2179, 0.1970]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2704, 0.3121, 0.2271, 0.1904],
        [0.2716, 0.3035, 0.2221, 0.2028],
        [0.2699, 0.3051, 0.2318, 0.1932]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2819, 0.3145, 0.2161, 0.1876],
        [0.2819, 0.3094, 0.2231, 0.1856],
        [0.2756, 0.3085, 0.2209, 0.1949]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2772, 0.3032, 0.2244, 0.1952],
        [0.2747, 0.2983, 0.2191, 0.2078],
        [0.2665, 0.2961, 0.2270, 0.2103]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2512, 0.2526, 0.2491],
        [0.2495, 0.2532, 0.2511, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2520, 0.2526, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2519, 0.2476],
        [0.2512, 0.2547, 0.2494, 0.2447],
        [0.2518, 0.2527, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2525, 0.2499, 0.2471],
        [0.2512, 0.2523, 0.2496, 0.2469],
        [0.2521, 0.2511, 0.2476, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2537, 0.2520, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2510]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2501, 0.2504, 0.2512],
        [0.2466, 0.2534, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2546, 0.2515, 0.2466],
        [0.2473, 0.2500, 0.2490, 0.2538],
        [0.2468, 0.2525, 0.2512, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2498, 0.2513],
        [0.2466, 0.2518, 0.2487, 0.2529],
        [0.2456, 0.2544, 0.2536, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2489],
        [0.2495, 0.2504, 0.2477, 0.2524],
        [0.2497, 0.2513, 0.2508, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2425, 0.2457, 0.2657],
        [0.2417, 0.2555, 0.2612, 0.2415],
        [0.2428, 0.2525, 0.2585, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2197, 0.2624, 0.2767, 0.2412],
        [0.2225, 0.2610, 0.2681, 0.2484],
        [0.2331, 0.2521, 0.2569, 0.2579]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:30:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][50/390]	Step 17254	lr 0.00184	Loss 0.0595 (0.0597)	Prec@(1,5) (98.1%, 100.0%)	
05/31 12:30:56午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.1217 (0.0610)	Prec@(1,5) (98.0%, 100.0%)	
05/31 12:31:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][150/390]	Step 17354	lr 0.00184	Loss 0.0248 (0.0608)	Prec@(1,5) (98.1%, 100.0%)	
05/31 12:31:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.0681 (0.0583)	Prec@(1,5) (98.2%, 100.0%)	
05/31 12:32:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][250/390]	Step 17454	lr 0.00184	Loss 0.0888 (0.0603)	Prec@(1,5) (98.1%, 100.0%)	
05/31 12:32:47午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.0560 (0.0614)	Prec@(1,5) (98.0%, 100.0%)	
05/31 12:33:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][350/390]	Step 17554	lr 0.00184	Loss 0.0946 (0.0612)	Prec@(1,5) (98.0%, 100.0%)	
05/31 12:33:38午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.0400 (0.0615)	Prec@(1,5) (98.0%, 100.0%)	
05/31 12:33:38午前 searchStage_trainer.py:221 [INFO] Train: [ 44/49] Final Prec@1 98.0040%
05/31 12:33:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][50/391]	Step 17595	Loss 0.5925	Prec@(1,5) (85.2%, 99.4%)
05/31 12:33:48午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 0.5801	Prec@(1,5) (85.6%, 99.1%)
05/31 12:33:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][150/391]	Step 17595	Loss 0.5728	Prec@(1,5) (85.7%, 99.1%)
05/31 12:33:57午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 0.5807	Prec@(1,5) (85.6%, 99.1%)
05/31 12:34:02午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][250/391]	Step 17595	Loss 0.5701	Prec@(1,5) (85.7%, 99.1%)
05/31 12:34:06午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 0.5726	Prec@(1,5) (85.6%, 99.1%)
05/31 12:34:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][350/391]	Step 17595	Loss 0.5672	Prec@(1,5) (85.6%, 99.1%)
05/31 12:34:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 0.5639	Prec@(1,5) (85.7%, 99.1%)
05/31 12:34:15午前 searchStage_trainer.py:260 [INFO] Valid: [ 44/49] Final Prec@1 85.7040%
05/31 12:34:15午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:34:15午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.7040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2633, 0.3109, 0.2498, 0.1760],
        [0.2723, 0.3045, 0.2443, 0.1789]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2417, 0.3238, 0.2578, 0.1767],
        [0.2545, 0.3104, 0.2636, 0.1714],
        [0.2662, 0.3428, 0.2090, 0.1819]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2959, 0.2714, 0.1772],
        [0.2728, 0.3279, 0.2279, 0.1714],
        [0.2739, 0.3110, 0.2179, 0.1971]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2704, 0.3121, 0.2271, 0.1905],
        [0.2715, 0.3034, 0.2222, 0.2029],
        [0.2699, 0.3050, 0.2319, 0.1933]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2819, 0.3144, 0.2161, 0.1876],
        [0.2819, 0.3093, 0.2231, 0.1857],
        [0.2756, 0.3084, 0.2210, 0.1951]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2772, 0.3031, 0.2244, 0.1953],
        [0.2747, 0.2982, 0.2192, 0.2079],
        [0.2665, 0.2960, 0.2271, 0.2105]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2512, 0.2526, 0.2491],
        [0.2495, 0.2531, 0.2511, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2520, 0.2526, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2519, 0.2475],
        [0.2512, 0.2547, 0.2494, 0.2447],
        [0.2518, 0.2527, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2525, 0.2499, 0.2471],
        [0.2512, 0.2523, 0.2496, 0.2469],
        [0.2521, 0.2511, 0.2476, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2537, 0.2520, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2510]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2508],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2501, 0.2504, 0.2512],
        [0.2466, 0.2534, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2546, 0.2515, 0.2466],
        [0.2473, 0.2500, 0.2490, 0.2538],
        [0.2468, 0.2525, 0.2511, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2498, 0.2513],
        [0.2466, 0.2518, 0.2487, 0.2529],
        [0.2456, 0.2544, 0.2536, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2489],
        [0.2495, 0.2504, 0.2477, 0.2524],
        [0.2497, 0.2513, 0.2508, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2425, 0.2457, 0.2657],
        [0.2417, 0.2555, 0.2612, 0.2415],
        [0.2428, 0.2525, 0.2585, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2196, 0.2624, 0.2767, 0.2412],
        [0.2225, 0.2610, 0.2681, 0.2484],
        [0.2331, 0.2521, 0.2569, 0.2579]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:34:44午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][50/390]	Step 17645	lr 0.00159	Loss 0.0212 (0.0567)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:35:12午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.1078 (0.0530)	Prec@(1,5) (98.5%, 100.0%)	
05/31 12:35:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][150/390]	Step 17745	lr 0.00159	Loss 0.0717 (0.0549)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:36:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.0616 (0.0562)	Prec@(1,5) (98.2%, 100.0%)	
05/31 12:36:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][250/390]	Step 17845	lr 0.00159	Loss 0.0516 (0.0561)	Prec@(1,5) (98.2%, 100.0%)	
05/31 12:37:04午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.0447 (0.0575)	Prec@(1,5) (98.1%, 100.0%)	
05/31 12:37:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][350/390]	Step 17945	lr 0.00159	Loss 0.0133 (0.0573)	Prec@(1,5) (98.1%, 100.0%)	
05/31 12:37:54午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.0892 (0.0572)	Prec@(1,5) (98.2%, 100.0%)	
05/31 12:37:55午前 searchStage_trainer.py:221 [INFO] Train: [ 45/49] Final Prec@1 98.1600%
05/31 12:37:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][50/391]	Step 17986	Loss 0.5634	Prec@(1,5) (86.1%, 99.0%)
05/31 12:38:04午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 0.5710	Prec@(1,5) (85.8%, 99.0%)
05/31 12:38:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][150/391]	Step 17986	Loss 0.5684	Prec@(1,5) (85.8%, 99.0%)
05/31 12:38:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 0.5619	Prec@(1,5) (86.1%, 99.0%)
05/31 12:38:18午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][250/391]	Step 17986	Loss 0.5605	Prec@(1,5) (86.0%, 99.0%)
05/31 12:38:22午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 0.5592	Prec@(1,5) (85.9%, 99.1%)
05/31 12:38:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][350/391]	Step 17986	Loss 0.5693	Prec@(1,5) (85.7%, 99.1%)
05/31 12:38:31午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 0.5733	Prec@(1,5) (85.6%, 99.1%)
05/31 12:38:31午前 searchStage_trainer.py:260 [INFO] Valid: [ 45/49] Final Prec@1 85.5960%
05/31 12:38:31午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:38:31午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.7040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2633, 0.3108, 0.2498, 0.1761],
        [0.2723, 0.3044, 0.2444, 0.1789]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2417, 0.3237, 0.2578, 0.1767],
        [0.2545, 0.3104, 0.2637, 0.1715],
        [0.2662, 0.3427, 0.2091, 0.1820]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2959, 0.2715, 0.1772],
        [0.2728, 0.3279, 0.2279, 0.1715],
        [0.2739, 0.3109, 0.2179, 0.1972]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2703, 0.3120, 0.2271, 0.1906],
        [0.2715, 0.3033, 0.2222, 0.2030],
        [0.2699, 0.3049, 0.2319, 0.1934]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2819, 0.3143, 0.2161, 0.1877],
        [0.2818, 0.3093, 0.2232, 0.1857],
        [0.2756, 0.3083, 0.2210, 0.1952]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2772, 0.3030, 0.2244, 0.1954],
        [0.2747, 0.2981, 0.2192, 0.2080],
        [0.2664, 0.2959, 0.2271, 0.2106]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2512, 0.2526, 0.2491],
        [0.2495, 0.2531, 0.2511, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2520, 0.2526, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2513, 0.2519, 0.2475],
        [0.2512, 0.2547, 0.2494, 0.2447],
        [0.2518, 0.2527, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2525, 0.2499, 0.2471],
        [0.2512, 0.2523, 0.2496, 0.2469],
        [0.2521, 0.2510, 0.2476, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2538, 0.2520, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2510]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2509],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2501, 0.2504, 0.2512],
        [0.2466, 0.2534, 0.2515, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2546, 0.2515, 0.2466],
        [0.2473, 0.2500, 0.2490, 0.2538],
        [0.2468, 0.2525, 0.2511, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2497, 0.2513],
        [0.2466, 0.2518, 0.2486, 0.2530],
        [0.2457, 0.2544, 0.2536, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2489],
        [0.2495, 0.2504, 0.2477, 0.2524],
        [0.2497, 0.2513, 0.2508, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2425, 0.2457, 0.2658],
        [0.2417, 0.2555, 0.2612, 0.2415],
        [0.2428, 0.2525, 0.2585, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2196, 0.2624, 0.2768, 0.2412],
        [0.2225, 0.2610, 0.2681, 0.2484],
        [0.2331, 0.2521, 0.2569, 0.2579]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:39:00午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][50/390]	Step 18036	lr 0.00138	Loss 0.0304 (0.0509)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:39:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.0631 (0.0544)	Prec@(1,5) (98.2%, 100.0%)	
05/31 12:39:56午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][150/390]	Step 18136	lr 0.00138	Loss 0.0469 (0.0560)	Prec@(1,5) (98.2%, 100.0%)	
05/31 12:40:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.0395 (0.0560)	Prec@(1,5) (98.2%, 100.0%)	
05/31 12:40:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][250/390]	Step 18236	lr 0.00138	Loss 0.0171 (0.0552)	Prec@(1,5) (98.2%, 100.0%)	
05/31 12:41:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.0420 (0.0534)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:41:48午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][350/390]	Step 18336	lr 0.00138	Loss 0.0602 (0.0527)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:42:10午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.0436 (0.0523)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:42:11午前 searchStage_trainer.py:221 [INFO] Train: [ 46/49] Final Prec@1 98.2720%
05/31 12:42:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][50/391]	Step 18377	Loss 0.5937	Prec@(1,5) (85.3%, 99.1%)
05/31 12:42:20午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 0.5652	Prec@(1,5) (85.9%, 99.1%)
05/31 12:42:24午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][150/391]	Step 18377	Loss 0.5888	Prec@(1,5) (85.3%, 99.0%)
05/31 12:42:29午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 0.5802	Prec@(1,5) (85.5%, 99.1%)
05/31 12:42:34午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][250/391]	Step 18377	Loss 0.5798	Prec@(1,5) (85.6%, 99.1%)
05/31 12:42:38午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 0.5857	Prec@(1,5) (85.5%, 99.1%)
05/31 12:42:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][350/391]	Step 18377	Loss 0.5796	Prec@(1,5) (85.6%, 99.1%)
05/31 12:42:47午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 0.5743	Prec@(1,5) (85.7%, 99.1%)
05/31 12:42:47午前 searchStage_trainer.py:260 [INFO] Valid: [ 46/49] Final Prec@1 85.6920%
05/31 12:42:47午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:42:47午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.7040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2633, 0.3107, 0.2499, 0.1761],
        [0.2723, 0.3043, 0.2444, 0.1790]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2417, 0.3237, 0.2578, 0.1768],
        [0.2544, 0.3103, 0.2638, 0.1715],
        [0.2662, 0.3426, 0.2091, 0.1821]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2959, 0.2715, 0.1771],
        [0.2728, 0.3278, 0.2279, 0.1715],
        [0.2739, 0.3108, 0.2180, 0.1973]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2703, 0.3119, 0.2271, 0.1907],
        [0.2715, 0.3032, 0.2222, 0.2031],
        [0.2698, 0.3048, 0.2319, 0.1934]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2819, 0.3142, 0.2161, 0.1878],
        [0.2818, 0.3092, 0.2232, 0.1858],
        [0.2755, 0.3082, 0.2211, 0.1952]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2772, 0.3029, 0.2244, 0.1955],
        [0.2747, 0.2980, 0.2192, 0.2081],
        [0.2664, 0.2958, 0.2271, 0.2107]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2512, 0.2526, 0.2491],
        [0.2495, 0.2531, 0.2511, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2520, 0.2526, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2519, 0.2476],
        [0.2512, 0.2547, 0.2494, 0.2447],
        [0.2518, 0.2527, 0.2475, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2525, 0.2499, 0.2471],
        [0.2512, 0.2523, 0.2496, 0.2469],
        [0.2521, 0.2510, 0.2476, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2537, 0.2520, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2510]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2509],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2501, 0.2504, 0.2512],
        [0.2467, 0.2534, 0.2514, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2546, 0.2515, 0.2466],
        [0.2473, 0.2500, 0.2489, 0.2538],
        [0.2469, 0.2525, 0.2511, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2497, 0.2513],
        [0.2466, 0.2518, 0.2486, 0.2530],
        [0.2457, 0.2544, 0.2536, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2489],
        [0.2495, 0.2504, 0.2477, 0.2524],
        [0.2497, 0.2513, 0.2508, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2425, 0.2457, 0.2658],
        [0.2417, 0.2555, 0.2612, 0.2415],
        [0.2428, 0.2525, 0.2585, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2195, 0.2624, 0.2769, 0.2412],
        [0.2224, 0.2610, 0.2681, 0.2484],
        [0.2331, 0.2521, 0.2569, 0.2579]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:43:16午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][50/390]	Step 18427	lr 0.00121	Loss 0.0688 (0.0415)	Prec@(1,5) (98.6%, 100.0%)	
05/31 12:43:44午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.2012 (0.0486)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:44:12午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][150/390]	Step 18527	lr 0.00121	Loss 0.0288 (0.0463)	Prec@(1,5) (98.5%, 100.0%)	
05/31 12:44:39午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.1062 (0.0496)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:45:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][250/390]	Step 18627	lr 0.00121	Loss 0.0746 (0.0489)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:45:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.1337 (0.0496)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:46:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][350/390]	Step 18727	lr 0.00121	Loss 0.1508 (0.0500)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:46:26午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.0511 (0.0493)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:46:26午前 searchStage_trainer.py:221 [INFO] Train: [ 47/49] Final Prec@1 98.4240%
05/31 12:46:31午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][50/391]	Step 18768	Loss 0.5633	Prec@(1,5) (85.6%, 99.1%)
05/31 12:46:36午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 0.5786	Prec@(1,5) (85.5%, 99.2%)
05/31 12:46:40午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][150/391]	Step 18768	Loss 0.5811	Prec@(1,5) (85.4%, 99.2%)
05/31 12:46:45午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 0.5888	Prec@(1,5) (85.5%, 99.1%)
05/31 12:46:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][250/391]	Step 18768	Loss 0.5951	Prec@(1,5) (85.5%, 99.1%)
05/31 12:46:54午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 0.5973	Prec@(1,5) (85.5%, 99.1%)
05/31 12:46:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][350/391]	Step 18768	Loss 0.5969	Prec@(1,5) (85.4%, 99.2%)
05/31 12:47:03午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 0.5955	Prec@(1,5) (85.5%, 99.2%)
05/31 12:47:03午前 searchStage_trainer.py:260 [INFO] Valid: [ 47/49] Final Prec@1 85.4720%
05/31 12:47:03午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:47:03午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.7040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2632, 0.3106, 0.2499, 0.1762],
        [0.2722, 0.3042, 0.2445, 0.1791]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2417, 0.3236, 0.2579, 0.1769],
        [0.2544, 0.3102, 0.2638, 0.1716],
        [0.2661, 0.3425, 0.2092, 0.1822]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2959, 0.2715, 0.1772],
        [0.2727, 0.3277, 0.2280, 0.1716],
        [0.2738, 0.3107, 0.2180, 0.1974]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2703, 0.3119, 0.2271, 0.1907],
        [0.2715, 0.3031, 0.2223, 0.2032],
        [0.2698, 0.3046, 0.2320, 0.1936]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2819, 0.3141, 0.2162, 0.1878],
        [0.2818, 0.3091, 0.2232, 0.1859],
        [0.2755, 0.3080, 0.2211, 0.1954]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2771, 0.3029, 0.2244, 0.1956],
        [0.2746, 0.2980, 0.2193, 0.2081],
        [0.2664, 0.2956, 0.2272, 0.2108]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2512, 0.2526, 0.2491],
        [0.2495, 0.2531, 0.2511, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2520, 0.2526, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2490]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2513, 0.2519, 0.2475],
        [0.2512, 0.2547, 0.2494, 0.2447],
        [0.2518, 0.2527, 0.2476, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2525, 0.2499, 0.2471],
        [0.2512, 0.2523, 0.2496, 0.2469],
        [0.2521, 0.2510, 0.2476, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2537, 0.2520, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2510]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2509],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2501, 0.2504, 0.2512],
        [0.2467, 0.2534, 0.2514, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2546, 0.2515, 0.2466],
        [0.2473, 0.2500, 0.2489, 0.2538],
        [0.2469, 0.2525, 0.2511, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2497, 0.2513],
        [0.2466, 0.2518, 0.2486, 0.2530],
        [0.2457, 0.2544, 0.2536, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2489],
        [0.2495, 0.2504, 0.2477, 0.2524],
        [0.2497, 0.2513, 0.2508, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2424, 0.2456, 0.2658],
        [0.2417, 0.2555, 0.2612, 0.2415],
        [0.2428, 0.2525, 0.2585, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2195, 0.2624, 0.2769, 0.2412],
        [0.2224, 0.2610, 0.2681, 0.2484],
        [0.2331, 0.2521, 0.2569, 0.2579]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:47:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][50/390]	Step 18818	lr 0.00109	Loss 0.0382 (0.0480)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:48:00午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.0667 (0.0503)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:48:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][150/390]	Step 18918	lr 0.00109	Loss 0.0360 (0.0475)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:48:55午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.0255 (0.0459)	Prec@(1,5) (98.5%, 100.0%)	
05/31 12:49:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][250/390]	Step 19018	lr 0.00109	Loss 0.0158 (0.0449)	Prec@(1,5) (98.5%, 100.0%)	
05/31 12:49:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.0075 (0.0442)	Prec@(1,5) (98.6%, 100.0%)	
05/31 12:50:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][350/390]	Step 19118	lr 0.00109	Loss 0.0228 (0.0436)	Prec@(1,5) (98.6%, 100.0%)	
05/31 12:50:42午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.1364 (0.0436)	Prec@(1,5) (98.6%, 100.0%)	
05/31 12:50:42午前 searchStage_trainer.py:221 [INFO] Train: [ 48/49] Final Prec@1 98.6360%
05/31 12:50:47午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][50/391]	Step 19159	Loss 0.5967	Prec@(1,5) (86.2%, 99.3%)
05/31 12:50:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 0.5909	Prec@(1,5) (85.9%, 99.3%)
05/31 12:50:56午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][150/391]	Step 19159	Loss 0.5821	Prec@(1,5) (86.1%, 99.3%)
05/31 12:51:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 0.5895	Prec@(1,5) (85.8%, 99.2%)
05/31 12:51:05午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][250/391]	Step 19159	Loss 0.5908	Prec@(1,5) (85.8%, 99.2%)
05/31 12:51:10午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 0.5901	Prec@(1,5) (85.8%, 99.2%)
05/31 12:51:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][350/391]	Step 19159	Loss 0.5917	Prec@(1,5) (85.8%, 99.1%)
05/31 12:51:18午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 0.5916	Prec@(1,5) (85.8%, 99.1%)
05/31 12:51:19午前 searchStage_trainer.py:260 [INFO] Valid: [ 48/49] Final Prec@1 85.8440%
05/31 12:51:19午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:51:19午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.8440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2632, 0.3105, 0.2500, 0.1763],
        [0.2722, 0.3041, 0.2446, 0.1792]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2417, 0.3235, 0.2579, 0.1769],
        [0.2544, 0.3102, 0.2639, 0.1716],
        [0.2661, 0.3424, 0.2092, 0.1823]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2958, 0.2715, 0.1772],
        [0.2727, 0.3276, 0.2280, 0.1717],
        [0.2738, 0.3106, 0.2180, 0.1975]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2703, 0.3118, 0.2271, 0.1908],
        [0.2715, 0.3030, 0.2223, 0.2033],
        [0.2698, 0.3046, 0.2320, 0.1936]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2818, 0.3141, 0.2162, 0.1879],
        [0.2818, 0.3090, 0.2233, 0.1860],
        [0.2755, 0.3080, 0.2211, 0.1954]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2771, 0.3028, 0.2245, 0.1956],
        [0.2746, 0.2979, 0.2193, 0.2082],
        [0.2663, 0.2955, 0.2272, 0.2109]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2512, 0.2526, 0.2491],
        [0.2495, 0.2531, 0.2511, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2495, 0.2521, 0.2468],
        [0.2553, 0.2520, 0.2526, 0.2401],
        [0.2518, 0.2533, 0.2459, 0.2491]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2513, 0.2519, 0.2476],
        [0.2512, 0.2547, 0.2494, 0.2447],
        [0.2518, 0.2527, 0.2476, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2525, 0.2499, 0.2471],
        [0.2512, 0.2523, 0.2496, 0.2469],
        [0.2521, 0.2510, 0.2476, 0.2493]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2517, 0.2490, 0.2454],
        [0.2537, 0.2520, 0.2459, 0.2483],
        [0.2530, 0.2501, 0.2459, 0.2510]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2518, 0.2475, 0.2509],
        [0.2516, 0.2513, 0.2484, 0.2487],
        [0.2513, 0.2523, 0.2491, 0.2474]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2501, 0.2504, 0.2512],
        [0.2467, 0.2534, 0.2514, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2546, 0.2515, 0.2466],
        [0.2473, 0.2499, 0.2489, 0.2538],
        [0.2469, 0.2525, 0.2511, 0.2495]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2531, 0.2497, 0.2513],
        [0.2466, 0.2518, 0.2486, 0.2530],
        [0.2457, 0.2543, 0.2536, 0.2464]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2516, 0.2489],
        [0.2495, 0.2504, 0.2477, 0.2524],
        [0.2497, 0.2513, 0.2508, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2424, 0.2456, 0.2658],
        [0.2417, 0.2555, 0.2612, 0.2415],
        [0.2428, 0.2525, 0.2585, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2195, 0.2624, 0.2769, 0.2412],
        [0.2224, 0.2610, 0.2681, 0.2484],
        [0.2331, 0.2521, 0.2569, 0.2579]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:51:48午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][50/390]	Step 19209	lr 0.00102	Loss 0.0629 (0.0405)	Prec@(1,5) (98.8%, 100.0%)	
05/31 12:52:16午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.0482 (0.0419)	Prec@(1,5) (98.6%, 100.0%)	
05/31 12:52:44午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][150/390]	Step 19309	lr 0.00102	Loss 0.0565 (0.0411)	Prec@(1,5) (98.7%, 100.0%)	
05/31 12:53:11午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.0148 (0.0424)	Prec@(1,5) (98.6%, 100.0%)	
05/31 12:53:39午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][250/390]	Step 19409	lr 0.00102	Loss 0.0860 (0.0426)	Prec@(1,5) (98.6%, 100.0%)	
05/31 12:54:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.0391 (0.0415)	Prec@(1,5) (98.7%, 100.0%)	
05/31 12:54:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][350/390]	Step 19509	lr 0.00102	Loss 0.0114 (0.0407)	Prec@(1,5) (98.7%, 100.0%)	
05/31 12:54:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.0267 (0.0396)	Prec@(1,5) (98.8%, 100.0%)	
05/31 12:54:58午前 searchStage_trainer.py:221 [INFO] Train: [ 49/49] Final Prec@1 98.7720%
05/31 12:55:03午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][50/391]	Step 19550	Loss 0.5774	Prec@(1,5) (86.2%, 99.0%)
05/31 12:55:08午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 0.5769	Prec@(1,5) (86.3%, 99.0%)
05/31 12:55:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][150/391]	Step 19550	Loss 0.5778	Prec@(1,5) (86.3%, 99.0%)
05/31 12:55:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 0.5706	Prec@(1,5) (86.4%, 99.1%)
05/31 12:55:22午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][250/391]	Step 19550	Loss 0.5767	Prec@(1,5) (86.2%, 99.1%)
05/31 12:55:26午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 0.5726	Prec@(1,5) (86.3%, 99.2%)
05/31 12:55:31午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][350/391]	Step 19550	Loss 0.5871	Prec@(1,5) (86.0%, 99.1%)
05/31 12:55:35午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 0.5895	Prec@(1,5) (86.0%, 99.1%)
05/31 12:55:35午前 searchStage_trainer.py:260 [INFO] Valid: [ 49/49] Final Prec@1 86.0320%
05/31 12:55:35午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:55:35午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.0320%
05/31 12:55:35午前 searchStage_main.py:84 [INFO] Final best Prec@1 = 86.0320%
05/31 12:55:35午前 searchStage_main.py:85 [INFO] Final Best Genotype = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
