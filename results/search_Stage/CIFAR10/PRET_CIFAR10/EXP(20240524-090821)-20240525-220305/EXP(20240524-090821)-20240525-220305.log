05/25 10:03:05PM parser.py:28 [INFO] 
05/25 10:03:05PM parser.py:29 [INFO] Parameters:
05/25 10:03:05PM parser.py:31 [INFO] DAG_PATH=results/search_Stage/CIFAR10/PRET_CIFAR10/EXP(20240524-090821)-20240525-220305/DAG
05/25 10:03:05PM parser.py:31 [INFO] ALPHA_LR=0.0003
05/25 10:03:05PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
05/25 10:03:05PM parser.py:31 [INFO] BATCH_SIZE=64
05/25 10:03:05PM parser.py:31 [INFO] CHECKPOINT_RESET=True
05/25 10:03:05PM parser.py:31 [INFO] DATA_PATH=../data/
05/25 10:03:05PM parser.py:31 [INFO] DATASET=CIFAR10
05/25 10:03:05PM parser.py:31 [INFO] EPOCHS=50
05/25 10:03:05PM parser.py:31 [INFO] EXP_NAME=EXP(20240524-090821)-20240525-220305
05/25 10:03:05PM parser.py:31 [INFO] GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('skip_connect', 2)]], normal_concat=[2, 3, 4, 5], reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce_concat=[2, 3, 4, 5])
05/25 10:03:05PM parser.py:31 [INFO] GPUS=[0]
05/25 10:03:05PM parser.py:31 [INFO] INIT_CHANNELS=16
05/25 10:03:05PM parser.py:31 [INFO] LAYERS=20
05/25 10:03:05PM parser.py:31 [INFO] LOCAL_RANK=0
05/25 10:03:05PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/25 10:03:05PM parser.py:31 [INFO] NAME=PRET_CIFAR10
05/25 10:03:05PM parser.py:31 [INFO] PATH=results/search_Stage/CIFAR10/PRET_CIFAR10/EXP(20240524-090821)-20240525-220305
05/25 10:03:05PM parser.py:31 [INFO] PRINT_FREQ=50
05/25 10:03:05PM parser.py:31 [INFO] RESUME_PATH=/home/miura/lab/research-hdas/results/search_Stage/CIFAR10/SCRATCH/EXP-20240524-090821/best.pth.tar
05/25 10:03:05PM parser.py:31 [INFO] SAVE=EXP(20240524-090821)
05/25 10:03:05PM parser.py:31 [INFO] SEED=0
05/25 10:03:05PM parser.py:31 [INFO] SHARE_STAGE=True
05/25 10:03:05PM parser.py:31 [INFO] TRAIN_PORTION=0.5
05/25 10:03:05PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
05/25 10:03:05PM parser.py:31 [INFO] W_LR=0.025
05/25 10:03:05PM parser.py:31 [INFO] W_LR_MIN=0.001
05/25 10:03:05PM parser.py:31 [INFO] W_MOMENTUM=0.9
05/25 10:03:05PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
05/25 10:03:05PM parser.py:31 [INFO] WORKERS=4
05/25 10:03:05PM parser.py:32 [INFO] 
05/25 10:03:07PM searchStage_trainer.py:110 [INFO] --> Loaded checkpoint '/home/miura/lab/research-hdas/results/search_Stage/CIFAR10/SCRATCH/EXP-20240524-090821/best.pth.tar'(Reseted epoch 0)
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2763, 0.2581, 0.2160],
        [0.2519, 0.2809, 0.2662, 0.2011]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2736, 0.2662, 0.1965],
        [0.2576, 0.2701, 0.2661, 0.2062],
        [0.2660, 0.2876, 0.2210, 0.2255]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2674, 0.2621, 0.2021],
        [0.2643, 0.2722, 0.2271, 0.2363],
        [0.2668, 0.2754, 0.2375, 0.2203]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2653, 0.2409, 0.2327],
        [0.2646, 0.2637, 0.2345, 0.2372],
        [0.2595, 0.2695, 0.2435, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2578, 0.2634, 0.2417, 0.2371],
        [0.2668, 0.2622, 0.2346, 0.2363],
        [0.2575, 0.2704, 0.2452, 0.2269]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2596, 0.2484, 0.2404],
        [0.2526, 0.2619, 0.2551, 0.2304],
        [0.2551, 0.2620, 0.2386, 0.2443]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:03:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 0.6355 (0.4943)	Prec@(1,5) (86.1%, 99.2%)	
05/25 10:03:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 0.8556 (0.5924)	Prec@(1,5) (81.2%, 98.8%)	
05/25 10:04:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 0.7466 (0.6144)	Prec@(1,5) (79.9%, 98.8%)	
05/25 10:04:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 0.5792 (0.6051)	Prec@(1,5) (80.0%, 98.9%)	
05/25 10:04:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 0.5414 (0.5891)	Prec@(1,5) (80.3%, 98.9%)	
05/25 10:04:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 0.6320 (0.5771)	Prec@(1,5) (80.6%, 99.0%)	
05/25 10:05:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 0.4251 (0.5703)	Prec@(1,5) (80.7%, 99.0%)	
05/25 10:05:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 0.5244 (0.5672)	Prec@(1,5) (80.8%, 99.1%)	
05/25 10:05:25PM searchShareStage_trainer.py:134 [INFO] Train: [  0/49] Final Prec@1 80.8000%
05/25 10:05:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 0.7174	Prec@(1,5) (76.8%, 98.1%)
05/25 10:05:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 0.7154	Prec@(1,5) (76.6%, 98.0%)
05/25 10:05:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 0.7010	Prec@(1,5) (77.0%, 98.2%)
05/25 10:05:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 0.6979	Prec@(1,5) (77.0%, 98.2%)
05/25 10:05:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 0.6952	Prec@(1,5) (76.9%, 98.2%)
05/25 10:05:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 0.6986	Prec@(1,5) (76.8%, 98.1%)
05/25 10:05:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 0.7017	Prec@(1,5) (76.7%, 98.1%)
05/25 10:05:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 0.7045	Prec@(1,5) (76.6%, 98.1%)
05/25 10:05:48PM searchStage_trainer.py:260 [INFO] Valid: [  0/49] Final Prec@1 76.5760%
05/25 10:05:48PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:05:48PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 76.5760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2762, 0.2582, 0.2159],
        [0.2520, 0.2808, 0.2661, 0.2011]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2736, 0.2662, 0.1965],
        [0.2577, 0.2700, 0.2662, 0.2061],
        [0.2660, 0.2876, 0.2209, 0.2255]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2673, 0.2621, 0.2021],
        [0.2642, 0.2723, 0.2272, 0.2363],
        [0.2667, 0.2755, 0.2374, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2612, 0.2652, 0.2408, 0.2328],
        [0.2647, 0.2637, 0.2344, 0.2372],
        [0.2595, 0.2696, 0.2434, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2578, 0.2635, 0.2418, 0.2370],
        [0.2668, 0.2622, 0.2347, 0.2363],
        [0.2574, 0.2704, 0.2451, 0.2270]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2595, 0.2483, 0.2405],
        [0.2526, 0.2620, 0.2552, 0.2303],
        [0.2550, 0.2619, 0.2386, 0.2444]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:06:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 0.3652 (0.4716)	Prec@(1,5) (83.1%, 99.5%)	
05/25 10:06:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 0.3435 (0.4794)	Prec@(1,5) (83.0%, 99.4%)	
05/25 10:06:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 0.6160 (0.4868)	Prec@(1,5) (82.9%, 99.4%)	
05/25 10:07:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 0.4185 (0.4878)	Prec@(1,5) (82.7%, 99.4%)	
05/25 10:07:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 0.3953 (0.4922)	Prec@(1,5) (82.5%, 99.4%)	
05/25 10:07:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 0.2987 (0.4924)	Prec@(1,5) (82.6%, 99.4%)	
05/25 10:07:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 0.3249 (0.4928)	Prec@(1,5) (82.6%, 99.4%)	
05/25 10:08:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 0.4142 (0.4914)	Prec@(1,5) (82.6%, 99.4%)	
05/25 10:08:07PM searchShareStage_trainer.py:134 [INFO] Train: [  1/49] Final Prec@1 82.6360%
05/25 10:08:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 0.6698	Prec@(1,5) (77.3%, 98.6%)
05/25 10:08:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 0.6547	Prec@(1,5) (77.6%, 98.5%)
05/25 10:08:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 0.6557	Prec@(1,5) (77.6%, 98.4%)
05/25 10:08:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 0.6481	Prec@(1,5) (77.8%, 98.4%)
05/25 10:08:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 0.6475	Prec@(1,5) (77.8%, 98.4%)
05/25 10:08:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 0.6493	Prec@(1,5) (77.9%, 98.4%)
05/25 10:08:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 0.6474	Prec@(1,5) (77.9%, 98.3%)
05/25 10:08:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 0.6463	Prec@(1,5) (77.9%, 98.4%)
05/25 10:08:30PM searchStage_trainer.py:260 [INFO] Valid: [  1/49] Final Prec@1 77.9240%
05/25 10:08:30PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:08:31PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 77.9240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2762, 0.2582, 0.2159],
        [0.2520, 0.2808, 0.2661, 0.2011]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2736, 0.2662, 0.1965],
        [0.2577, 0.2700, 0.2662, 0.2061],
        [0.2660, 0.2876, 0.2209, 0.2256]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2673, 0.2621, 0.2021],
        [0.2642, 0.2723, 0.2272, 0.2363],
        [0.2667, 0.2755, 0.2374, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2612, 0.2652, 0.2408, 0.2328],
        [0.2647, 0.2637, 0.2344, 0.2372],
        [0.2595, 0.2696, 0.2434, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2578, 0.2635, 0.2418, 0.2370],
        [0.2668, 0.2621, 0.2347, 0.2363],
        [0.2574, 0.2704, 0.2451, 0.2271]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2595, 0.2483, 0.2405],
        [0.2526, 0.2620, 0.2552, 0.2303],
        [0.2550, 0.2619, 0.2386, 0.2445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:08:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 0.5035 (0.4347)	Prec@(1,5) (84.7%, 99.3%)	
05/25 10:09:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 0.4241 (0.4338)	Prec@(1,5) (84.9%, 99.5%)	
05/25 10:09:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 0.6407 (0.4490)	Prec@(1,5) (84.6%, 99.5%)	
05/25 10:09:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 0.7413 (0.4555)	Prec@(1,5) (84.2%, 99.5%)	
05/25 10:09:59PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 0.4739 (0.4636)	Prec@(1,5) (83.9%, 99.5%)	
05/25 10:10:17PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 0.5624 (0.4667)	Prec@(1,5) (83.8%, 99.4%)	
05/25 10:10:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 0.4090 (0.4654)	Prec@(1,5) (83.8%, 99.4%)	
05/25 10:10:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 0.5684 (0.4628)	Prec@(1,5) (83.9%, 99.5%)	
05/25 10:10:48PM searchShareStage_trainer.py:134 [INFO] Train: [  2/49] Final Prec@1 83.9240%
05/25 10:10:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 0.7703	Prec@(1,5) (73.7%, 98.3%)
05/25 10:10:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 0.7456	Prec@(1,5) (74.3%, 98.6%)
05/25 10:10:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 0.7262	Prec@(1,5) (75.4%, 98.7%)
05/25 10:10:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 0.7164	Prec@(1,5) (75.7%, 98.7%)
05/25 10:11:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 0.7165	Prec@(1,5) (75.7%, 98.7%)
05/25 10:11:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 0.7182	Prec@(1,5) (75.7%, 98.7%)
05/25 10:11:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 0.7177	Prec@(1,5) (75.7%, 98.6%)
05/25 10:11:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 0.7200	Prec@(1,5) (75.6%, 98.6%)
05/25 10:11:10PM searchStage_trainer.py:260 [INFO] Valid: [  2/49] Final Prec@1 75.6440%
05/25 10:11:10PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:11:11PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 77.9240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2762, 0.2582, 0.2159],
        [0.2519, 0.2808, 0.2662, 0.2011]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2735, 0.2662, 0.1965],
        [0.2577, 0.2700, 0.2662, 0.2061],
        [0.2660, 0.2876, 0.2209, 0.2256]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2673, 0.2621, 0.2022],
        [0.2642, 0.2723, 0.2272, 0.2363],
        [0.2667, 0.2755, 0.2374, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2612, 0.2652, 0.2408, 0.2328],
        [0.2647, 0.2636, 0.2344, 0.2372],
        [0.2595, 0.2696, 0.2435, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2578, 0.2634, 0.2418, 0.2370],
        [0.2668, 0.2621, 0.2348, 0.2363],
        [0.2574, 0.2703, 0.2452, 0.2271]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2595, 0.2483, 0.2405],
        [0.2526, 0.2619, 0.2552, 0.2303],
        [0.2550, 0.2619, 0.2386, 0.2445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:11:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 0.2667 (0.4294)	Prec@(1,5) (85.2%, 99.4%)	
05/25 10:11:46PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 0.2716 (0.4265)	Prec@(1,5) (85.2%, 99.5%)	
05/25 10:12:04PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 0.4380 (0.4227)	Prec@(1,5) (85.0%, 99.5%)	
05/25 10:12:22PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 0.5619 (0.4277)	Prec@(1,5) (84.9%, 99.5%)	
05/25 10:12:40PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 0.3478 (0.4381)	Prec@(1,5) (84.5%, 99.4%)	
05/25 10:12:57PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 0.4195 (0.4424)	Prec@(1,5) (84.4%, 99.4%)	
05/25 10:13:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 0.3837 (0.4434)	Prec@(1,5) (84.4%, 99.4%)	
05/25 10:13:30PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 0.4355 (0.4431)	Prec@(1,5) (84.4%, 99.4%)	
05/25 10:13:30PM searchShareStage_trainer.py:134 [INFO] Train: [  3/49] Final Prec@1 84.3440%
05/25 10:13:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 0.5832	Prec@(1,5) (80.3%, 98.8%)
05/25 10:13:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 0.5962	Prec@(1,5) (79.6%, 98.8%)
05/25 10:13:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 0.5920	Prec@(1,5) (80.0%, 98.7%)
05/25 10:13:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 0.5847	Prec@(1,5) (80.3%, 98.7%)
05/25 10:13:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 0.5824	Prec@(1,5) (80.5%, 98.7%)
05/25 10:13:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 0.5787	Prec@(1,5) (80.5%, 98.8%)
05/25 10:13:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 0.5797	Prec@(1,5) (80.5%, 98.8%)
05/25 10:13:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 0.5835	Prec@(1,5) (80.4%, 98.8%)
05/25 10:13:51PM searchStage_trainer.py:260 [INFO] Valid: [  3/49] Final Prec@1 80.4120%
05/25 10:13:51PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:13:51PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.4120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2762, 0.2582, 0.2160],
        [0.2519, 0.2807, 0.2662, 0.2012]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2735, 0.2662, 0.1965],
        [0.2577, 0.2700, 0.2662, 0.2061],
        [0.2660, 0.2875, 0.2209, 0.2256]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2673, 0.2621, 0.2022],
        [0.2642, 0.2722, 0.2272, 0.2363],
        [0.2667, 0.2754, 0.2374, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2612, 0.2652, 0.2409, 0.2328],
        [0.2647, 0.2636, 0.2344, 0.2373],
        [0.2594, 0.2696, 0.2435, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2578, 0.2634, 0.2418, 0.2370],
        [0.2668, 0.2621, 0.2348, 0.2364],
        [0.2574, 0.2703, 0.2452, 0.2271]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2595, 0.2483, 0.2405],
        [0.2526, 0.2619, 0.2552, 0.2303],
        [0.2550, 0.2619, 0.2386, 0.2445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:14:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 0.4470 (0.4117)	Prec@(1,5) (85.9%, 99.7%)	
05/25 10:14:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 0.3286 (0.3996)	Prec@(1,5) (86.2%, 99.6%)	
05/25 10:14:45PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 0.4481 (0.4017)	Prec@(1,5) (86.2%, 99.5%)	
05/25 10:15:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 0.3830 (0.4048)	Prec@(1,5) (86.0%, 99.5%)	
05/25 10:15:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 0.3840 (0.4121)	Prec@(1,5) (85.8%, 99.5%)	
05/25 10:15:38PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 0.4895 (0.4189)	Prec@(1,5) (85.5%, 99.5%)	
05/25 10:15:55PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 0.4390 (0.4232)	Prec@(1,5) (85.3%, 99.5%)	
05/25 10:16:09PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 0.6011 (0.4264)	Prec@(1,5) (85.2%, 99.5%)	
05/25 10:16:09PM searchShareStage_trainer.py:134 [INFO] Train: [  4/49] Final Prec@1 85.1960%
05/25 10:16:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 0.6219	Prec@(1,5) (79.0%, 98.6%)
05/25 10:16:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 0.6160	Prec@(1,5) (79.3%, 98.7%)
05/25 10:16:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 0.6177	Prec@(1,5) (79.4%, 98.7%)
05/25 10:16:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 0.6174	Prec@(1,5) (79.3%, 98.7%)
05/25 10:16:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 0.6246	Prec@(1,5) (79.0%, 98.7%)
05/25 10:16:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 0.6231	Prec@(1,5) (79.0%, 98.7%)
05/25 10:16:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 0.6220	Prec@(1,5) (79.1%, 98.7%)
05/25 10:16:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 0.6188	Prec@(1,5) (79.3%, 98.7%)
05/25 10:16:31PM searchStage_trainer.py:260 [INFO] Valid: [  4/49] Final Prec@1 79.3200%
05/25 10:16:31PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:16:32PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.4120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2762, 0.2583, 0.2160],
        [0.2519, 0.2807, 0.2662, 0.2012]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2735, 0.2662, 0.1966],
        [0.2577, 0.2700, 0.2662, 0.2062],
        [0.2660, 0.2875, 0.2209, 0.2256]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2673, 0.2621, 0.2021],
        [0.2642, 0.2722, 0.2272, 0.2364],
        [0.2667, 0.2754, 0.2374, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2652, 0.2409, 0.2328],
        [0.2647, 0.2636, 0.2344, 0.2373],
        [0.2594, 0.2695, 0.2435, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2578, 0.2634, 0.2419, 0.2370],
        [0.2668, 0.2621, 0.2348, 0.2364],
        [0.2574, 0.2703, 0.2452, 0.2271]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2595, 0.2484, 0.2406],
        [0.2526, 0.2619, 0.2552, 0.2303],
        [0.2550, 0.2618, 0.2386, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:16:50PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 0.4443 (0.3735)	Prec@(1,5) (87.4%, 99.6%)	
05/25 10:17:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 0.4773 (0.3806)	Prec@(1,5) (86.9%, 99.6%)	
05/25 10:17:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 0.3370 (0.3942)	Prec@(1,5) (86.5%, 99.6%)	
05/25 10:17:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 0.4536 (0.4023)	Prec@(1,5) (86.3%, 99.5%)	
05/25 10:18:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 0.3326 (0.4060)	Prec@(1,5) (86.2%, 99.5%)	
05/25 10:18:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 0.3937 (0.4088)	Prec@(1,5) (86.1%, 99.5%)	
05/25 10:18:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 0.3797 (0.4060)	Prec@(1,5) (86.2%, 99.5%)	
05/25 10:18:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 0.2470 (0.4056)	Prec@(1,5) (86.2%, 99.5%)	
05/25 10:18:51PM searchShareStage_trainer.py:134 [INFO] Train: [  5/49] Final Prec@1 86.1760%
05/25 10:18:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 0.5963	Prec@(1,5) (80.7%, 98.4%)
05/25 10:18:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 0.5970	Prec@(1,5) (80.5%, 98.6%)
05/25 10:18:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 0.6041	Prec@(1,5) (80.2%, 98.6%)
05/25 10:19:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 0.6014	Prec@(1,5) (80.0%, 98.7%)
05/25 10:19:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 0.6078	Prec@(1,5) (79.9%, 98.7%)
05/25 10:19:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 0.6038	Prec@(1,5) (80.1%, 98.7%)
05/25 10:19:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 0.5987	Prec@(1,5) (80.3%, 98.7%)
05/25 10:19:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 0.6025	Prec@(1,5) (80.1%, 98.7%)
05/25 10:19:13PM searchStage_trainer.py:260 [INFO] Valid: [  5/49] Final Prec@1 80.0760%
05/25 10:19:13PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:19:13PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.4120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2761, 0.2583, 0.2160],
        [0.2519, 0.2807, 0.2662, 0.2012]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2735, 0.2662, 0.1966],
        [0.2577, 0.2699, 0.2662, 0.2062],
        [0.2660, 0.2875, 0.2209, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2673, 0.2622, 0.2022],
        [0.2642, 0.2722, 0.2272, 0.2364],
        [0.2667, 0.2754, 0.2375, 0.2205]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2652, 0.2409, 0.2328],
        [0.2646, 0.2636, 0.2344, 0.2373],
        [0.2594, 0.2695, 0.2435, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2578, 0.2634, 0.2419, 0.2370],
        [0.2668, 0.2620, 0.2348, 0.2364],
        [0.2574, 0.2703, 0.2452, 0.2272]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2594, 0.2484, 0.2406],
        [0.2526, 0.2619, 0.2552, 0.2304],
        [0.2550, 0.2618, 0.2386, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:19:32PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 0.4807 (0.3721)	Prec@(1,5) (87.2%, 99.7%)	
05/25 10:19:50PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 0.1227 (0.3594)	Prec@(1,5) (87.6%, 99.7%)	
05/25 10:20:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 0.2405 (0.3697)	Prec@(1,5) (87.1%, 99.7%)	
05/25 10:20:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 0.3140 (0.3804)	Prec@(1,5) (86.6%, 99.6%)	
05/25 10:20:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 0.4635 (0.3876)	Prec@(1,5) (86.4%, 99.7%)	
05/25 10:21:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 0.7244 (0.3940)	Prec@(1,5) (86.2%, 99.6%)	
05/25 10:21:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 0.5129 (0.3947)	Prec@(1,5) (86.1%, 99.6%)	
05/25 10:21:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 0.2838 (0.3961)	Prec@(1,5) (86.1%, 99.6%)	
05/25 10:21:31PM searchShareStage_trainer.py:134 [INFO] Train: [  6/49] Final Prec@1 86.1080%
05/25 10:21:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 0.6909	Prec@(1,5) (79.0%, 98.5%)
05/25 10:21:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 0.6886	Prec@(1,5) (78.4%, 98.4%)
05/25 10:21:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 0.6908	Prec@(1,5) (78.4%, 98.2%)
05/25 10:21:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 0.6792	Prec@(1,5) (78.9%, 98.2%)
05/25 10:21:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 0.6845	Prec@(1,5) (78.5%, 98.2%)
05/25 10:21:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 0.6832	Prec@(1,5) (78.5%, 98.3%)
05/25 10:21:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 0.6878	Prec@(1,5) (78.3%, 98.2%)
05/25 10:21:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 0.6902	Prec@(1,5) (78.3%, 98.2%)
05/25 10:21:55PM searchStage_trainer.py:260 [INFO] Valid: [  6/49] Final Prec@1 78.2760%
05/25 10:21:55PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:21:55PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.4120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2761, 0.2583, 0.2160],
        [0.2519, 0.2807, 0.2662, 0.2012]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2735, 0.2662, 0.1966],
        [0.2577, 0.2699, 0.2662, 0.2062],
        [0.2660, 0.2874, 0.2209, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2672, 0.2622, 0.2022],
        [0.2642, 0.2722, 0.2272, 0.2364],
        [0.2667, 0.2754, 0.2375, 0.2205]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2652, 0.2409, 0.2328],
        [0.2646, 0.2635, 0.2345, 0.2374],
        [0.2594, 0.2695, 0.2435, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2578, 0.2634, 0.2419, 0.2370],
        [0.2667, 0.2620, 0.2348, 0.2365],
        [0.2574, 0.2703, 0.2452, 0.2272]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2594, 0.2484, 0.2406],
        [0.2526, 0.2618, 0.2552, 0.2304],
        [0.2549, 0.2618, 0.2387, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:22:12PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 0.3752 (0.3787)	Prec@(1,5) (86.7%, 99.8%)	
05/25 10:22:30PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 0.4215 (0.3697)	Prec@(1,5) (86.9%, 99.8%)	
05/25 10:22:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 0.3357 (0.3724)	Prec@(1,5) (86.6%, 99.8%)	
05/25 10:23:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 0.4425 (0.3709)	Prec@(1,5) (86.6%, 99.8%)	
05/25 10:23:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 0.4349 (0.3717)	Prec@(1,5) (86.7%, 99.8%)	
05/25 10:23:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 0.5010 (0.3750)	Prec@(1,5) (86.7%, 99.7%)	
05/25 10:24:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 0.4363 (0.3806)	Prec@(1,5) (86.5%, 99.7%)	
05/25 10:24:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 0.3001 (0.3791)	Prec@(1,5) (86.5%, 99.7%)	
05/25 10:24:13PM searchShareStage_trainer.py:134 [INFO] Train: [  7/49] Final Prec@1 86.5000%
05/25 10:24:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 0.6231	Prec@(1,5) (79.5%, 98.6%)
05/25 10:24:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 0.6235	Prec@(1,5) (79.5%, 98.7%)
05/25 10:24:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 0.6126	Prec@(1,5) (80.0%, 98.7%)
05/25 10:24:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 0.6125	Prec@(1,5) (80.0%, 98.8%)
05/25 10:24:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 0.6159	Prec@(1,5) (79.9%, 98.8%)
05/25 10:24:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 0.6172	Prec@(1,5) (79.9%, 98.8%)
05/25 10:24:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 0.6183	Prec@(1,5) (80.0%, 98.7%)
05/25 10:24:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 0.6149	Prec@(1,5) (80.0%, 98.8%)
05/25 10:24:37PM searchStage_trainer.py:260 [INFO] Valid: [  7/49] Final Prec@1 79.9600%
05/25 10:24:37PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:24:37PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.4120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2761, 0.2583, 0.2160],
        [0.2519, 0.2807, 0.2662, 0.2012]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2734, 0.2662, 0.1966],
        [0.2577, 0.2699, 0.2663, 0.2062],
        [0.2660, 0.2874, 0.2209, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2673, 0.2622, 0.2022],
        [0.2642, 0.2722, 0.2272, 0.2364],
        [0.2667, 0.2754, 0.2375, 0.2205]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2651, 0.2409, 0.2329],
        [0.2646, 0.2634, 0.2345, 0.2374],
        [0.2594, 0.2695, 0.2434, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2634, 0.2419, 0.2370],
        [0.2667, 0.2620, 0.2348, 0.2365],
        [0.2573, 0.2702, 0.2451, 0.2273]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2594, 0.2484, 0.2406],
        [0.2525, 0.2618, 0.2552, 0.2305],
        [0.2549, 0.2617, 0.2387, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:24:55PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 0.2769 (0.3511)	Prec@(1,5) (88.3%, 99.7%)	
05/25 10:25:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 0.3275 (0.3511)	Prec@(1,5) (87.9%, 99.6%)	
05/25 10:25:30PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 0.4933 (0.3593)	Prec@(1,5) (87.6%, 99.6%)	
05/25 10:25:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 0.1424 (0.3626)	Prec@(1,5) (87.4%, 99.6%)	
05/25 10:26:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 0.4387 (0.3708)	Prec@(1,5) (86.9%, 99.6%)	
05/25 10:26:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 0.4399 (0.3739)	Prec@(1,5) (86.8%, 99.6%)	
05/25 10:26:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 0.3582 (0.3721)	Prec@(1,5) (87.0%, 99.6%)	
05/25 10:26:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 0.5299 (0.3759)	Prec@(1,5) (86.9%, 99.6%)	
05/25 10:26:56PM searchShareStage_trainer.py:134 [INFO] Train: [  8/49] Final Prec@1 86.8720%
05/25 10:26:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 0.6203	Prec@(1,5) (79.2%, 98.9%)
05/25 10:27:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 0.6136	Prec@(1,5) (79.6%, 98.8%)
05/25 10:27:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 0.6138	Prec@(1,5) (79.9%, 98.8%)
05/25 10:27:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 0.6074	Prec@(1,5) (80.0%, 98.8%)
05/25 10:27:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 0.6030	Prec@(1,5) (80.0%, 98.8%)
05/25 10:27:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 0.5990	Prec@(1,5) (80.1%, 98.8%)
05/25 10:27:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 0.5974	Prec@(1,5) (80.1%, 98.8%)
05/25 10:27:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 0.5982	Prec@(1,5) (80.1%, 98.9%)
05/25 10:27:18PM searchStage_trainer.py:260 [INFO] Valid: [  8/49] Final Prec@1 80.0800%
05/25 10:27:18PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:27:18PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.4120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2761, 0.2583, 0.2160],
        [0.2519, 0.2807, 0.2662, 0.2012]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2734, 0.2662, 0.1966],
        [0.2576, 0.2699, 0.2663, 0.2062],
        [0.2659, 0.2874, 0.2209, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2672, 0.2622, 0.2022],
        [0.2642, 0.2722, 0.2272, 0.2364],
        [0.2667, 0.2753, 0.2375, 0.2205]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2651, 0.2409, 0.2328],
        [0.2646, 0.2634, 0.2345, 0.2375],
        [0.2594, 0.2695, 0.2434, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2578, 0.2634, 0.2419, 0.2370],
        [0.2667, 0.2619, 0.2348, 0.2365],
        [0.2573, 0.2702, 0.2451, 0.2273]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2594, 0.2484, 0.2406],
        [0.2525, 0.2618, 0.2552, 0.2305],
        [0.2548, 0.2617, 0.2387, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:27:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 0.4011 (0.3265)	Prec@(1,5) (88.6%, 99.7%)	
05/25 10:27:55PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 0.4013 (0.3370)	Prec@(1,5) (88.3%, 99.7%)	
05/25 10:28:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 0.3186 (0.3411)	Prec@(1,5) (88.2%, 99.7%)	
05/25 10:28:30PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 0.2961 (0.3446)	Prec@(1,5) (88.0%, 99.7%)	
05/25 10:28:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 0.3734 (0.3529)	Prec@(1,5) (87.8%, 99.7%)	
05/25 10:29:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 0.2343 (0.3583)	Prec@(1,5) (87.5%, 99.7%)	
05/25 10:29:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 0.4668 (0.3620)	Prec@(1,5) (87.4%, 99.7%)	
05/25 10:29:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 0.4875 (0.3640)	Prec@(1,5) (87.3%, 99.7%)	
05/25 10:29:38PM searchShareStage_trainer.py:134 [INFO] Train: [  9/49] Final Prec@1 87.2920%
05/25 10:29:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 0.5274	Prec@(1,5) (81.6%, 99.1%)
05/25 10:29:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 0.5209	Prec@(1,5) (82.3%, 98.9%)
05/25 10:29:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 0.5194	Prec@(1,5) (82.2%, 98.9%)
05/25 10:29:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 0.5210	Prec@(1,5) (82.3%, 98.9%)
05/25 10:29:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 0.5159	Prec@(1,5) (82.6%, 98.9%)
05/25 10:29:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 0.5137	Prec@(1,5) (82.7%, 98.9%)
05/25 10:29:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 0.5143	Prec@(1,5) (82.7%, 98.9%)
05/25 10:30:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 0.5144	Prec@(1,5) (82.7%, 98.9%)
05/25 10:30:00PM searchStage_trainer.py:260 [INFO] Valid: [  9/49] Final Prec@1 82.7520%
05/25 10:30:00PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:30:01PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.7520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2761, 0.2583, 0.2160],
        [0.2519, 0.2807, 0.2662, 0.2012]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2734, 0.2662, 0.1967],
        [0.2576, 0.2699, 0.2663, 0.2062],
        [0.2659, 0.2873, 0.2209, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2672, 0.2622, 0.2022],
        [0.2642, 0.2722, 0.2272, 0.2364],
        [0.2667, 0.2753, 0.2375, 0.2205]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2651, 0.2409, 0.2329],
        [0.2646, 0.2634, 0.2345, 0.2375],
        [0.2594, 0.2694, 0.2435, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2634, 0.2419, 0.2370],
        [0.2667, 0.2619, 0.2348, 0.2366],
        [0.2573, 0.2702, 0.2452, 0.2273]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2594, 0.2484, 0.2406],
        [0.2525, 0.2617, 0.2552, 0.2305],
        [0.2549, 0.2617, 0.2387, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:30:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 0.4529 (0.3195)	Prec@(1,5) (89.6%, 99.7%)	
05/25 10:30:36PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 0.2936 (0.3176)	Prec@(1,5) (89.2%, 99.8%)	
05/25 10:30:54PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 0.2978 (0.3211)	Prec@(1,5) (89.0%, 99.7%)	
05/25 10:31:12PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 0.1980 (0.3274)	Prec@(1,5) (88.7%, 99.7%)	
05/25 10:31:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 0.3662 (0.3328)	Prec@(1,5) (88.5%, 99.8%)	
05/25 10:31:46PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 0.4899 (0.3396)	Prec@(1,5) (88.2%, 99.7%)	
05/25 10:32:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 0.2208 (0.3413)	Prec@(1,5) (88.1%, 99.7%)	
05/25 10:32:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 0.3665 (0.3430)	Prec@(1,5) (88.0%, 99.7%)	
05/25 10:32:19PM searchShareStage_trainer.py:134 [INFO] Train: [ 10/49] Final Prec@1 88.0200%
05/25 10:32:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 0.5315	Prec@(1,5) (82.6%, 99.2%)
05/25 10:32:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 0.5270	Prec@(1,5) (82.7%, 99.2%)
05/25 10:32:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 0.5242	Prec@(1,5) (82.6%, 99.1%)
05/25 10:32:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 0.5275	Prec@(1,5) (82.6%, 99.0%)
05/25 10:32:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 0.5245	Prec@(1,5) (82.8%, 99.1%)
05/25 10:32:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 0.5315	Prec@(1,5) (82.4%, 99.0%)
05/25 10:32:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 0.5387	Prec@(1,5) (82.2%, 99.0%)
05/25 10:32:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 0.5364	Prec@(1,5) (82.2%, 99.0%)
05/25 10:32:40PM searchStage_trainer.py:260 [INFO] Valid: [ 10/49] Final Prec@1 82.2360%
05/25 10:32:40PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:32:40PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.7520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2761, 0.2583, 0.2160],
        [0.2519, 0.2806, 0.2662, 0.2013]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2734, 0.2662, 0.1967],
        [0.2576, 0.2699, 0.2663, 0.2062],
        [0.2659, 0.2873, 0.2209, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2672, 0.2622, 0.2022],
        [0.2642, 0.2722, 0.2272, 0.2364],
        [0.2667, 0.2753, 0.2375, 0.2206]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2651, 0.2409, 0.2329],
        [0.2646, 0.2634, 0.2345, 0.2375],
        [0.2594, 0.2694, 0.2435, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2633, 0.2419, 0.2370],
        [0.2667, 0.2619, 0.2348, 0.2366],
        [0.2573, 0.2701, 0.2452, 0.2274]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2594, 0.2484, 0.2406],
        [0.2525, 0.2617, 0.2552, 0.2306],
        [0.2548, 0.2616, 0.2387, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:32:59PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 0.3361 (0.3018)	Prec@(1,5) (89.5%, 99.8%)	
05/25 10:33:17PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 0.2188 (0.3167)	Prec@(1,5) (88.9%, 99.8%)	
05/25 10:33:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 0.3640 (0.3215)	Prec@(1,5) (88.7%, 99.8%)	
05/25 10:33:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 0.2425 (0.3173)	Prec@(1,5) (88.9%, 99.7%)	
05/25 10:34:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 0.2155 (0.3246)	Prec@(1,5) (88.8%, 99.7%)	
05/25 10:34:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 0.2455 (0.3336)	Prec@(1,5) (88.4%, 99.7%)	
05/25 10:34:46PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 0.3068 (0.3380)	Prec@(1,5) (88.2%, 99.7%)	
05/25 10:35:01PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 0.3761 (0.3399)	Prec@(1,5) (88.2%, 99.7%)	
05/25 10:35:01PM searchShareStage_trainer.py:134 [INFO] Train: [ 11/49] Final Prec@1 88.1360%
05/25 10:35:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 0.5361	Prec@(1,5) (81.5%, 99.3%)
05/25 10:35:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 0.5301	Prec@(1,5) (82.6%, 99.1%)
05/25 10:35:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 0.5360	Prec@(1,5) (82.6%, 99.0%)
05/25 10:35:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 0.5407	Prec@(1,5) (82.4%, 99.0%)
05/25 10:35:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 0.5464	Prec@(1,5) (82.3%, 99.0%)
05/25 10:35:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 0.5421	Prec@(1,5) (82.4%, 99.0%)
05/25 10:35:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 0.5393	Prec@(1,5) (82.5%, 99.0%)
05/25 10:35:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 0.5396	Prec@(1,5) (82.4%, 99.0%)
05/25 10:35:23PM searchStage_trainer.py:260 [INFO] Valid: [ 11/49] Final Prec@1 82.4200%
05/25 10:35:23PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:35:23PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.7520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2761, 0.2583, 0.2161],
        [0.2519, 0.2806, 0.2662, 0.2013]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2734, 0.2663, 0.1967],
        [0.2576, 0.2699, 0.2663, 0.2062],
        [0.2659, 0.2873, 0.2209, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2672, 0.2622, 0.2022],
        [0.2642, 0.2721, 0.2272, 0.2365],
        [0.2667, 0.2752, 0.2375, 0.2206]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2651, 0.2409, 0.2329],
        [0.2646, 0.2634, 0.2345, 0.2375],
        [0.2593, 0.2693, 0.2435, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2633, 0.2419, 0.2370],
        [0.2667, 0.2619, 0.2348, 0.2366],
        [0.2573, 0.2701, 0.2452, 0.2274]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2594, 0.2484, 0.2406],
        [0.2525, 0.2617, 0.2552, 0.2306],
        [0.2548, 0.2616, 0.2388, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:35:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 0.3069 (0.2986)	Prec@(1,5) (89.7%, 99.7%)	
05/25 10:35:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 0.2598 (0.3104)	Prec@(1,5) (89.2%, 99.7%)	
05/25 10:36:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 0.6219 (0.3178)	Prec@(1,5) (88.8%, 99.7%)	
05/25 10:36:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 0.3089 (0.3243)	Prec@(1,5) (88.6%, 99.7%)	
05/25 10:36:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 0.2017 (0.3209)	Prec@(1,5) (88.8%, 99.7%)	
05/25 10:37:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 0.1791 (0.3244)	Prec@(1,5) (88.6%, 99.7%)	
05/25 10:37:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 0.4052 (0.3243)	Prec@(1,5) (88.6%, 99.7%)	
05/25 10:37:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 0.5383 (0.3272)	Prec@(1,5) (88.5%, 99.7%)	
05/25 10:37:42PM searchShareStage_trainer.py:134 [INFO] Train: [ 12/49] Final Prec@1 88.5120%
05/25 10:37:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 0.6866	Prec@(1,5) (78.6%, 98.5%)
05/25 10:37:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 0.7012	Prec@(1,5) (78.6%, 98.7%)
05/25 10:37:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 0.6881	Prec@(1,5) (79.0%, 98.7%)
05/25 10:37:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 0.6831	Prec@(1,5) (79.2%, 98.6%)
05/25 10:37:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 0.6877	Prec@(1,5) (79.0%, 98.6%)
05/25 10:37:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 0.6916	Prec@(1,5) (79.0%, 98.6%)
05/25 10:38:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 0.6884	Prec@(1,5) (79.0%, 98.6%)
05/25 10:38:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 0.6885	Prec@(1,5) (79.0%, 98.6%)
05/25 10:38:04PM searchStage_trainer.py:260 [INFO] Valid: [ 12/49] Final Prec@1 78.9560%
05/25 10:38:04PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:38:04PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.7520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2761, 0.2583, 0.2161],
        [0.2519, 0.2806, 0.2662, 0.2013]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2733, 0.2663, 0.1967],
        [0.2576, 0.2698, 0.2663, 0.2062],
        [0.2659, 0.2873, 0.2209, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2672, 0.2622, 0.2022],
        [0.2642, 0.2721, 0.2272, 0.2365],
        [0.2667, 0.2752, 0.2375, 0.2206]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2651, 0.2409, 0.2329],
        [0.2646, 0.2633, 0.2345, 0.2376],
        [0.2593, 0.2693, 0.2435, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2633, 0.2420, 0.2370],
        [0.2667, 0.2619, 0.2348, 0.2366],
        [0.2573, 0.2701, 0.2452, 0.2274]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2594, 0.2484, 0.2407],
        [0.2525, 0.2617, 0.2552, 0.2306],
        [0.2548, 0.2616, 0.2388, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:38:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 0.2301 (0.2757)	Prec@(1,5) (91.1%, 99.9%)	
05/25 10:38:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 0.2731 (0.2820)	Prec@(1,5) (90.5%, 99.9%)	
05/25 10:38:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 0.3985 (0.2864)	Prec@(1,5) (90.1%, 99.8%)	
05/25 10:39:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 0.1607 (0.2920)	Prec@(1,5) (89.8%, 99.8%)	
05/25 10:39:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 0.1382 (0.3000)	Prec@(1,5) (89.6%, 99.8%)	
05/25 10:39:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 0.4325 (0.3050)	Prec@(1,5) (89.3%, 99.8%)	
05/25 10:40:09PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 0.4644 (0.3100)	Prec@(1,5) (89.2%, 99.8%)	
05/25 10:40:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 0.1636 (0.3095)	Prec@(1,5) (89.3%, 99.8%)	
05/25 10:40:25PM searchShareStage_trainer.py:134 [INFO] Train: [ 13/49] Final Prec@1 89.2920%
05/25 10:40:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 0.5800	Prec@(1,5) (82.1%, 99.1%)
05/25 10:40:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 0.5735	Prec@(1,5) (82.1%, 99.0%)
05/25 10:40:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 0.5639	Prec@(1,5) (82.2%, 99.1%)
05/25 10:40:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 0.5590	Prec@(1,5) (82.4%, 99.1%)
05/25 10:40:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 0.5622	Prec@(1,5) (82.2%, 99.1%)
05/25 10:40:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 0.5594	Prec@(1,5) (82.3%, 99.1%)
05/25 10:40:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 0.5611	Prec@(1,5) (82.3%, 99.1%)
05/25 10:40:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 0.5606	Prec@(1,5) (82.3%, 99.1%)
05/25 10:40:46PM searchStage_trainer.py:260 [INFO] Valid: [ 13/49] Final Prec@1 82.2680%
05/25 10:40:46PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:40:46PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.7520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2760, 0.2583, 0.2161],
        [0.2519, 0.2806, 0.2663, 0.2013]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2733, 0.2663, 0.1967],
        [0.2576, 0.2698, 0.2663, 0.2063],
        [0.2659, 0.2872, 0.2209, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2672, 0.2622, 0.2022],
        [0.2642, 0.2721, 0.2272, 0.2365],
        [0.2666, 0.2752, 0.2375, 0.2206]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2650, 0.2409, 0.2329],
        [0.2646, 0.2633, 0.2345, 0.2376],
        [0.2594, 0.2693, 0.2435, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2633, 0.2420, 0.2370],
        [0.2667, 0.2618, 0.2349, 0.2366],
        [0.2573, 0.2701, 0.2452, 0.2274]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2594, 0.2484, 0.2407],
        [0.2525, 0.2617, 0.2552, 0.2306],
        [0.2548, 0.2616, 0.2388, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:41:04PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 0.2480 (0.3098)	Prec@(1,5) (89.2%, 99.7%)	
05/25 10:41:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 0.3084 (0.3006)	Prec@(1,5) (89.5%, 99.7%)	
05/25 10:41:40PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 0.2793 (0.3025)	Prec@(1,5) (89.5%, 99.7%)	
05/25 10:41:57PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 0.3572 (0.3054)	Prec@(1,5) (89.4%, 99.7%)	
05/25 10:42:14PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 0.3484 (0.3090)	Prec@(1,5) (89.2%, 99.7%)	
05/25 10:42:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 0.1491 (0.3095)	Prec@(1,5) (89.4%, 99.7%)	
05/25 10:42:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 0.2855 (0.3100)	Prec@(1,5) (89.3%, 99.7%)	
05/25 10:43:03PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 0.3203 (0.3136)	Prec@(1,5) (89.1%, 99.7%)	
05/25 10:43:04PM searchShareStage_trainer.py:134 [INFO] Train: [ 14/49] Final Prec@1 89.1360%
05/25 10:43:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 0.5195	Prec@(1,5) (82.7%, 99.0%)
05/25 10:43:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 0.5410	Prec@(1,5) (82.1%, 99.0%)
05/25 10:43:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 0.5684	Prec@(1,5) (81.4%, 98.8%)
05/25 10:43:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 0.5574	Prec@(1,5) (81.8%, 98.9%)
05/25 10:43:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 0.5545	Prec@(1,5) (81.9%, 98.9%)
05/25 10:43:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 0.5558	Prec@(1,5) (82.0%, 98.9%)
05/25 10:43:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 0.5554	Prec@(1,5) (82.0%, 98.9%)
05/25 10:43:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 0.5522	Prec@(1,5) (82.1%, 98.9%)
05/25 10:43:25PM searchStage_trainer.py:260 [INFO] Valid: [ 14/49] Final Prec@1 82.1280%
05/25 10:43:25PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:43:25PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.7520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2760, 0.2584, 0.2161],
        [0.2518, 0.2805, 0.2663, 0.2013]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2733, 0.2663, 0.1967],
        [0.2576, 0.2698, 0.2663, 0.2063],
        [0.2659, 0.2872, 0.2209, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2672, 0.2622, 0.2022],
        [0.2642, 0.2721, 0.2272, 0.2365],
        [0.2666, 0.2752, 0.2375, 0.2207]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2650, 0.2409, 0.2330],
        [0.2646, 0.2633, 0.2345, 0.2376],
        [0.2593, 0.2693, 0.2435, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2633, 0.2420, 0.2370],
        [0.2667, 0.2618, 0.2348, 0.2366],
        [0.2573, 0.2701, 0.2452, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2593, 0.2484, 0.2407],
        [0.2525, 0.2617, 0.2552, 0.2306],
        [0.2548, 0.2615, 0.2388, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:43:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 0.2444 (0.2411)	Prec@(1,5) (91.6%, 99.9%)	
05/25 10:44:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 0.3315 (0.2612)	Prec@(1,5) (90.9%, 99.9%)	
05/25 10:44:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 0.2289 (0.2709)	Prec@(1,5) (90.5%, 99.8%)	
05/25 10:44:38PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 0.3507 (0.2697)	Prec@(1,5) (90.6%, 99.8%)	
05/25 10:44:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 0.3314 (0.2743)	Prec@(1,5) (90.5%, 99.8%)	
05/25 10:45:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 0.4833 (0.2834)	Prec@(1,5) (90.2%, 99.8%)	
05/25 10:45:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 0.2613 (0.2863)	Prec@(1,5) (90.1%, 99.8%)	
05/25 10:45:45PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 0.2985 (0.2862)	Prec@(1,5) (90.1%, 99.8%)	
05/25 10:45:46PM searchShareStage_trainer.py:134 [INFO] Train: [ 15/49] Final Prec@1 90.0600%
05/25 10:45:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 0.5152	Prec@(1,5) (84.1%, 98.9%)
05/25 10:45:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 0.5194	Prec@(1,5) (83.6%, 99.0%)
05/25 10:45:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 0.5178	Prec@(1,5) (83.5%, 98.9%)
05/25 10:45:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 0.5168	Prec@(1,5) (83.2%, 99.0%)
05/25 10:46:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 0.5148	Prec@(1,5) (83.3%, 99.0%)
05/25 10:46:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 0.5123	Prec@(1,5) (83.3%, 99.0%)
05/25 10:46:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 0.5116	Prec@(1,5) (83.3%, 99.0%)
05/25 10:46:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 0.5105	Prec@(1,5) (83.4%, 99.0%)
05/25 10:46:07PM searchStage_trainer.py:260 [INFO] Valid: [ 15/49] Final Prec@1 83.3560%
05/25 10:46:07PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:46:07PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.3560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2760, 0.2584, 0.2161],
        [0.2518, 0.2805, 0.2663, 0.2013]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2733, 0.2663, 0.1967],
        [0.2576, 0.2698, 0.2663, 0.2063],
        [0.2659, 0.2872, 0.2210, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2672, 0.2622, 0.2022],
        [0.2642, 0.2721, 0.2272, 0.2366],
        [0.2666, 0.2752, 0.2375, 0.2207]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2650, 0.2409, 0.2330],
        [0.2646, 0.2633, 0.2345, 0.2376],
        [0.2593, 0.2693, 0.2435, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2632, 0.2420, 0.2371],
        [0.2667, 0.2618, 0.2349, 0.2367],
        [0.2573, 0.2701, 0.2452, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2593, 0.2484, 0.2407],
        [0.2525, 0.2616, 0.2553, 0.2307],
        [0.2548, 0.2615, 0.2388, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:46:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 0.3420 (0.2702)	Prec@(1,5) (89.8%, 99.8%)	
05/25 10:46:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 0.1358 (0.2712)	Prec@(1,5) (90.2%, 99.8%)	
05/25 10:47:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 0.2860 (0.2784)	Prec@(1,5) (90.0%, 99.8%)	
05/25 10:47:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 0.2891 (0.2827)	Prec@(1,5) (89.9%, 99.7%)	
05/25 10:47:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 0.1814 (0.2791)	Prec@(1,5) (90.1%, 99.7%)	
05/25 10:47:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 0.1820 (0.2798)	Prec@(1,5) (90.1%, 99.7%)	
05/25 10:48:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 0.3428 (0.2845)	Prec@(1,5) (89.9%, 99.8%)	
05/25 10:48:27PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 0.2069 (0.2850)	Prec@(1,5) (89.9%, 99.8%)	
05/25 10:48:28PM searchShareStage_trainer.py:134 [INFO] Train: [ 16/49] Final Prec@1 89.9240%
05/25 10:48:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 0.4987	Prec@(1,5) (83.2%, 98.9%)
05/25 10:48:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 0.5235	Prec@(1,5) (83.0%, 98.8%)
05/25 10:48:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 0.5157	Prec@(1,5) (83.2%, 99.0%)
05/25 10:48:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 0.5165	Prec@(1,5) (83.1%, 99.0%)
05/25 10:48:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 0.5249	Prec@(1,5) (83.0%, 99.0%)
05/25 10:48:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 0.5203	Prec@(1,5) (83.1%, 99.0%)
05/25 10:48:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 0.5188	Prec@(1,5) (83.1%, 99.0%)
05/25 10:48:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 0.5183	Prec@(1,5) (83.1%, 99.0%)
05/25 10:48:50PM searchStage_trainer.py:260 [INFO] Valid: [ 16/49] Final Prec@1 83.0880%
05/25 10:48:50PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:48:51PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.3560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2760, 0.2584, 0.2161],
        [0.2518, 0.2805, 0.2663, 0.2013]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2733, 0.2663, 0.1968],
        [0.2576, 0.2698, 0.2663, 0.2063],
        [0.2659, 0.2872, 0.2210, 0.2260]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2672, 0.2622, 0.2022],
        [0.2642, 0.2720, 0.2272, 0.2366],
        [0.2666, 0.2751, 0.2375, 0.2207]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2650, 0.2409, 0.2330],
        [0.2646, 0.2632, 0.2345, 0.2377],
        [0.2593, 0.2692, 0.2435, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2632, 0.2420, 0.2371],
        [0.2667, 0.2618, 0.2349, 0.2367],
        [0.2573, 0.2700, 0.2452, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2593, 0.2484, 0.2407],
        [0.2525, 0.2616, 0.2553, 0.2307],
        [0.2548, 0.2615, 0.2388, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:49:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 0.1350 (0.2288)	Prec@(1,5) (92.4%, 99.9%)	
05/25 10:49:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 0.2037 (0.2411)	Prec@(1,5) (91.7%, 99.9%)	
05/25 10:49:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 0.2543 (0.2453)	Prec@(1,5) (91.4%, 99.9%)	
05/25 10:50:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 0.2181 (0.2555)	Prec@(1,5) (91.2%, 99.8%)	
05/25 10:50:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 0.3890 (0.2652)	Prec@(1,5) (90.7%, 99.8%)	
05/25 10:50:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 0.3089 (0.2716)	Prec@(1,5) (90.5%, 99.8%)	
05/25 10:50:55PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 0.1908 (0.2719)	Prec@(1,5) (90.5%, 99.8%)	
05/25 10:51:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 0.3182 (0.2736)	Prec@(1,5) (90.5%, 99.8%)	
05/25 10:51:10PM searchShareStage_trainer.py:134 [INFO] Train: [ 17/49] Final Prec@1 90.4600%
05/25 10:51:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 0.5631	Prec@(1,5) (81.6%, 98.9%)
05/25 10:51:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 0.5495	Prec@(1,5) (81.9%, 99.0%)
05/25 10:51:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 0.5542	Prec@(1,5) (81.9%, 99.0%)
05/25 10:51:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 0.5597	Prec@(1,5) (82.0%, 99.0%)
05/25 10:51:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 0.5519	Prec@(1,5) (82.1%, 99.0%)
05/25 10:51:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 0.5541	Prec@(1,5) (82.0%, 99.0%)
05/25 10:51:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 0.5587	Prec@(1,5) (81.9%, 99.1%)
05/25 10:51:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 0.5560	Prec@(1,5) (82.0%, 99.0%)
05/25 10:51:33PM searchStage_trainer.py:260 [INFO] Valid: [ 17/49] Final Prec@1 82.0040%
05/25 10:51:33PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:51:33PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.3560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2760, 0.2584, 0.2161],
        [0.2518, 0.2805, 0.2663, 0.2014]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2733, 0.2663, 0.1968],
        [0.2576, 0.2698, 0.2663, 0.2063],
        [0.2659, 0.2872, 0.2210, 0.2260]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2622, 0.2022],
        [0.2642, 0.2720, 0.2272, 0.2366],
        [0.2666, 0.2751, 0.2375, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2650, 0.2409, 0.2330],
        [0.2645, 0.2632, 0.2345, 0.2377],
        [0.2593, 0.2692, 0.2435, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2632, 0.2420, 0.2371],
        [0.2666, 0.2617, 0.2349, 0.2367],
        [0.2572, 0.2700, 0.2452, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2593, 0.2484, 0.2408],
        [0.2524, 0.2616, 0.2553, 0.2307],
        [0.2548, 0.2615, 0.2388, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:51:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 0.2724 (0.2613)	Prec@(1,5) (90.7%, 99.9%)	
05/25 10:52:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 0.2198 (0.2488)	Prec@(1,5) (91.2%, 99.9%)	
05/25 10:52:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 0.2746 (0.2468)	Prec@(1,5) (91.1%, 99.9%)	
05/25 10:52:46PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 0.2580 (0.2536)	Prec@(1,5) (90.9%, 99.9%)	
05/25 10:53:04PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 0.2936 (0.2601)	Prec@(1,5) (90.7%, 99.9%)	
05/25 10:53:21PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 0.2724 (0.2633)	Prec@(1,5) (90.7%, 99.9%)	
05/25 10:53:39PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 0.4347 (0.2662)	Prec@(1,5) (90.6%, 99.9%)	
05/25 10:53:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 0.2246 (0.2687)	Prec@(1,5) (90.5%, 99.8%)	
05/25 10:53:54PM searchShareStage_trainer.py:134 [INFO] Train: [ 18/49] Final Prec@1 90.5240%
05/25 10:53:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 0.5192	Prec@(1,5) (83.5%, 98.9%)
05/25 10:54:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 0.5472	Prec@(1,5) (82.6%, 99.0%)
05/25 10:54:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 0.5471	Prec@(1,5) (82.7%, 99.1%)
05/25 10:54:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 0.5509	Prec@(1,5) (82.7%, 99.1%)
05/25 10:54:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 0.5485	Prec@(1,5) (82.7%, 99.1%)
05/25 10:54:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 0.5442	Prec@(1,5) (82.8%, 99.1%)
05/25 10:54:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 0.5453	Prec@(1,5) (82.6%, 99.1%)
05/25 10:54:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 0.5475	Prec@(1,5) (82.6%, 99.1%)
05/25 10:54:15PM searchStage_trainer.py:260 [INFO] Valid: [ 18/49] Final Prec@1 82.5880%
05/25 10:54:15PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:54:16PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.3560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2759, 0.2584, 0.2162],
        [0.2518, 0.2805, 0.2663, 0.2014]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2733, 0.2663, 0.1968],
        [0.2576, 0.2698, 0.2663, 0.2063],
        [0.2659, 0.2872, 0.2210, 0.2260]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2642, 0.2720, 0.2272, 0.2366],
        [0.2666, 0.2751, 0.2375, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2650, 0.2409, 0.2330],
        [0.2645, 0.2632, 0.2345, 0.2377],
        [0.2593, 0.2692, 0.2435, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2632, 0.2420, 0.2371],
        [0.2666, 0.2617, 0.2349, 0.2368],
        [0.2572, 0.2700, 0.2452, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2593, 0.2484, 0.2408],
        [0.2524, 0.2615, 0.2553, 0.2308],
        [0.2547, 0.2614, 0.2388, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:54:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 0.1313 (0.2253)	Prec@(1,5) (92.4%, 99.9%)	
05/25 10:54:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 0.2937 (0.2255)	Prec@(1,5) (92.4%, 99.9%)	
05/25 10:55:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 0.1180 (0.2310)	Prec@(1,5) (92.0%, 99.9%)	
05/25 10:55:27PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 0.3592 (0.2339)	Prec@(1,5) (91.9%, 99.9%)	
05/25 10:55:45PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 0.3074 (0.2329)	Prec@(1,5) (91.8%, 99.9%)	
05/25 10:56:04PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 0.2150 (0.2382)	Prec@(1,5) (91.6%, 99.9%)	
05/25 10:56:21PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 0.2004 (0.2422)	Prec@(1,5) (91.6%, 99.9%)	
05/25 10:56:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 0.2731 (0.2463)	Prec@(1,5) (91.3%, 99.9%)	
05/25 10:56:36PM searchShareStage_trainer.py:134 [INFO] Train: [ 19/49] Final Prec@1 91.3320%
05/25 10:56:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 0.5420	Prec@(1,5) (82.9%, 98.9%)
05/25 10:56:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 0.5377	Prec@(1,5) (82.7%, 99.0%)
05/25 10:56:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 0.5325	Prec@(1,5) (82.7%, 99.0%)
05/25 10:56:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 0.5352	Prec@(1,5) (82.7%, 98.9%)
05/25 10:56:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 0.5286	Prec@(1,5) (82.9%, 99.0%)
05/25 10:56:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 0.5292	Prec@(1,5) (82.8%, 99.0%)
05/25 10:56:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 0.5314	Prec@(1,5) (82.7%, 99.0%)
05/25 10:57:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 0.5368	Prec@(1,5) (82.6%, 99.0%)
05/25 10:57:00PM searchStage_trainer.py:260 [INFO] Valid: [ 19/49] Final Prec@1 82.5920%
05/25 10:57:00PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:57:00PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.3560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2759, 0.2584, 0.2162],
        [0.2518, 0.2805, 0.2663, 0.2014]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2733, 0.2663, 0.1968],
        [0.2576, 0.2698, 0.2663, 0.2063],
        [0.2659, 0.2872, 0.2210, 0.2260]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2642, 0.2720, 0.2272, 0.2366],
        [0.2666, 0.2751, 0.2375, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2650, 0.2409, 0.2330],
        [0.2645, 0.2632, 0.2345, 0.2377],
        [0.2593, 0.2692, 0.2435, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2632, 0.2420, 0.2371],
        [0.2666, 0.2617, 0.2349, 0.2368],
        [0.2572, 0.2700, 0.2452, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2593, 0.2484, 0.2408],
        [0.2524, 0.2615, 0.2553, 0.2308],
        [0.2547, 0.2614, 0.2388, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 10:57:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 0.0402 (0.2252)	Prec@(1,5) (92.3%, 99.9%)	
05/25 10:57:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 0.2318 (0.2194)	Prec@(1,5) (92.2%, 100.0%)	
05/25 10:57:54PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 0.3273 (0.2268)	Prec@(1,5) (92.1%, 99.9%)	
05/25 10:58:12PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 0.1534 (0.2311)	Prec@(1,5) (92.0%, 99.9%)	
05/25 10:58:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 0.1541 (0.2340)	Prec@(1,5) (91.9%, 99.9%)	
05/25 10:58:47PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 0.2236 (0.2379)	Prec@(1,5) (91.7%, 99.9%)	
05/25 10:59:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 0.3191 (0.2394)	Prec@(1,5) (91.6%, 99.9%)	
05/25 10:59:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 0.3058 (0.2411)	Prec@(1,5) (91.6%, 99.8%)	
05/25 10:59:20PM searchShareStage_trainer.py:134 [INFO] Train: [ 20/49] Final Prec@1 91.5920%
05/25 10:59:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 0.5354	Prec@(1,5) (83.2%, 99.0%)
05/25 10:59:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 0.5461	Prec@(1,5) (82.9%, 99.0%)
05/25 10:59:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 0.5397	Prec@(1,5) (82.8%, 99.1%)
05/25 10:59:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 0.5359	Prec@(1,5) (82.9%, 99.0%)
05/25 10:59:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 0.5376	Prec@(1,5) (83.0%, 99.0%)
05/25 10:59:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 0.5304	Prec@(1,5) (83.1%, 99.0%)
05/25 10:59:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 0.5294	Prec@(1,5) (83.2%, 99.0%)
05/25 10:59:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 0.5279	Prec@(1,5) (83.2%, 99.0%)
05/25 10:59:41PM searchStage_trainer.py:260 [INFO] Valid: [ 20/49] Final Prec@1 83.1880%
05/25 10:59:41PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 10:59:41PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.3560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2759, 0.2584, 0.2162],
        [0.2518, 0.2805, 0.2663, 0.2014]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2732, 0.2663, 0.1968],
        [0.2576, 0.2698, 0.2663, 0.2063],
        [0.2659, 0.2871, 0.2210, 0.2260]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2641, 0.2720, 0.2272, 0.2366],
        [0.2666, 0.2751, 0.2375, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2650, 0.2409, 0.2331],
        [0.2645, 0.2632, 0.2345, 0.2377],
        [0.2593, 0.2692, 0.2435, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2632, 0.2420, 0.2371],
        [0.2666, 0.2617, 0.2349, 0.2368],
        [0.2572, 0.2700, 0.2452, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2593, 0.2484, 0.2408],
        [0.2524, 0.2615, 0.2553, 0.2308],
        [0.2547, 0.2614, 0.2388, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:00:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 0.2796 (0.2375)	Prec@(1,5) (91.6%, 99.8%)	
05/25 11:00:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 0.2892 (0.2298)	Prec@(1,5) (91.8%, 99.9%)	
05/25 11:00:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 0.3558 (0.2224)	Prec@(1,5) (92.1%, 99.9%)	
05/25 11:00:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 0.1932 (0.2175)	Prec@(1,5) (92.3%, 99.9%)	
05/25 11:01:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 0.1821 (0.2254)	Prec@(1,5) (92.0%, 99.9%)	
05/25 11:01:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 0.1914 (0.2281)	Prec@(1,5) (91.9%, 99.9%)	
05/25 11:01:46PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 0.2007 (0.2291)	Prec@(1,5) (91.9%, 99.9%)	
05/25 11:02:01PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 0.2510 (0.2297)	Prec@(1,5) (91.9%, 99.9%)	
05/25 11:02:01PM searchShareStage_trainer.py:134 [INFO] Train: [ 21/49] Final Prec@1 91.8960%
05/25 11:02:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 0.5250	Prec@(1,5) (83.5%, 99.2%)
05/25 11:02:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 0.5159	Prec@(1,5) (83.5%, 99.2%)
05/25 11:02:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 0.5197	Prec@(1,5) (83.5%, 99.2%)
05/25 11:02:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 0.5167	Prec@(1,5) (83.6%, 99.2%)
05/25 11:02:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 0.5161	Prec@(1,5) (83.5%, 99.2%)
05/25 11:02:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 0.5129	Prec@(1,5) (83.6%, 99.2%)
05/25 11:02:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 0.5136	Prec@(1,5) (83.7%, 99.2%)
05/25 11:02:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 0.5141	Prec@(1,5) (83.7%, 99.2%)
05/25 11:02:25PM searchStage_trainer.py:260 [INFO] Valid: [ 21/49] Final Prec@1 83.7000%
05/25 11:02:25PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:02:26PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.7000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2759, 0.2584, 0.2162],
        [0.2518, 0.2804, 0.2664, 0.2014]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2732, 0.2663, 0.1968],
        [0.2576, 0.2698, 0.2663, 0.2063],
        [0.2659, 0.2871, 0.2210, 0.2260]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2641, 0.2720, 0.2272, 0.2366],
        [0.2666, 0.2751, 0.2375, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2650, 0.2409, 0.2331],
        [0.2645, 0.2632, 0.2346, 0.2377],
        [0.2593, 0.2691, 0.2435, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2632, 0.2420, 0.2371],
        [0.2666, 0.2617, 0.2349, 0.2368],
        [0.2572, 0.2699, 0.2453, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2593, 0.2484, 0.2408],
        [0.2524, 0.2615, 0.2553, 0.2308],
        [0.2547, 0.2614, 0.2388, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:02:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 0.3194 (0.2084)	Prec@(1,5) (92.4%, 99.9%)	
05/25 11:03:01PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 0.2063 (0.2051)	Prec@(1,5) (92.8%, 99.9%)	
05/25 11:03:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 0.1692 (0.2048)	Prec@(1,5) (92.8%, 99.9%)	
05/25 11:03:36PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 0.3286 (0.2103)	Prec@(1,5) (92.7%, 99.9%)	
05/25 11:03:54PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 0.1454 (0.2101)	Prec@(1,5) (92.7%, 99.9%)	
05/25 11:04:12PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 0.1504 (0.2096)	Prec@(1,5) (92.7%, 99.9%)	
05/25 11:04:30PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 0.1743 (0.2098)	Prec@(1,5) (92.7%, 99.9%)	
05/25 11:04:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 0.1462 (0.2117)	Prec@(1,5) (92.7%, 99.9%)	
05/25 11:04:44PM searchShareStage_trainer.py:134 [INFO] Train: [ 22/49] Final Prec@1 92.6520%
05/25 11:04:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 0.5288	Prec@(1,5) (84.0%, 99.1%)
05/25 11:04:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 0.5203	Prec@(1,5) (84.2%, 99.1%)
05/25 11:04:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 0.5149	Prec@(1,5) (84.5%, 99.1%)
05/25 11:04:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 0.5246	Prec@(1,5) (84.3%, 99.1%)
05/25 11:04:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 0.5135	Prec@(1,5) (84.5%, 99.2%)
05/25 11:05:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 0.5120	Prec@(1,5) (84.4%, 99.2%)
05/25 11:05:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 0.5142	Prec@(1,5) (84.4%, 99.2%)
05/25 11:05:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 0.5159	Prec@(1,5) (84.3%, 99.2%)
05/25 11:05:06PM searchStage_trainer.py:260 [INFO] Valid: [ 22/49] Final Prec@1 84.3360%
05/25 11:05:06PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:05:06PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 84.3360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2759, 0.2584, 0.2162],
        [0.2518, 0.2804, 0.2664, 0.2015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2732, 0.2663, 0.1968],
        [0.2576, 0.2697, 0.2663, 0.2063],
        [0.2659, 0.2871, 0.2210, 0.2261]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2641, 0.2720, 0.2272, 0.2367],
        [0.2666, 0.2751, 0.2375, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2650, 0.2409, 0.2331],
        [0.2645, 0.2632, 0.2345, 0.2377],
        [0.2593, 0.2691, 0.2435, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2631, 0.2421, 0.2371],
        [0.2666, 0.2617, 0.2349, 0.2368],
        [0.2572, 0.2699, 0.2453, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2592, 0.2484, 0.2408],
        [0.2524, 0.2615, 0.2553, 0.2308],
        [0.2547, 0.2614, 0.2388, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:05:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 0.2347 (0.1948)	Prec@(1,5) (93.3%, 99.9%)	
05/25 11:05:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 0.1184 (0.1898)	Prec@(1,5) (93.3%, 99.9%)	
05/25 11:06:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 0.1524 (0.1945)	Prec@(1,5) (93.3%, 99.9%)	
05/25 11:06:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 0.3058 (0.1917)	Prec@(1,5) (93.4%, 99.9%)	
05/25 11:06:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 0.2839 (0.1950)	Prec@(1,5) (93.4%, 99.9%)	
05/25 11:06:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 0.2838 (0.1959)	Prec@(1,5) (93.3%, 99.9%)	
05/25 11:07:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 0.2588 (0.2004)	Prec@(1,5) (93.2%, 99.9%)	
05/25 11:07:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 0.2520 (0.2041)	Prec@(1,5) (93.0%, 99.9%)	
05/25 11:07:26PM searchShareStage_trainer.py:134 [INFO] Train: [ 23/49] Final Prec@1 93.0200%
05/25 11:07:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 0.5091	Prec@(1,5) (85.1%, 99.1%)
05/25 11:07:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 0.5247	Prec@(1,5) (84.2%, 99.2%)
05/25 11:07:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 0.5179	Prec@(1,5) (84.2%, 99.3%)
05/25 11:07:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 0.5138	Prec@(1,5) (84.2%, 99.2%)
05/25 11:07:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 0.5126	Prec@(1,5) (84.2%, 99.2%)
05/25 11:07:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 0.5158	Prec@(1,5) (84.2%, 99.2%)
05/25 11:07:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 0.5164	Prec@(1,5) (84.1%, 99.2%)
05/25 11:07:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 0.5234	Prec@(1,5) (83.9%, 99.2%)
05/25 11:07:48PM searchStage_trainer.py:260 [INFO] Valid: [ 23/49] Final Prec@1 83.9200%
05/25 11:07:48PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:07:48PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 84.3360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2759, 0.2584, 0.2162],
        [0.2518, 0.2804, 0.2664, 0.2015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2732, 0.2663, 0.1969],
        [0.2576, 0.2697, 0.2663, 0.2064],
        [0.2658, 0.2871, 0.2210, 0.2261]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2641, 0.2720, 0.2272, 0.2367],
        [0.2666, 0.2750, 0.2376, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2611, 0.2650, 0.2409, 0.2331],
        [0.2645, 0.2632, 0.2345, 0.2377],
        [0.2593, 0.2691, 0.2435, 0.2281]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2631, 0.2421, 0.2371],
        [0.2666, 0.2617, 0.2349, 0.2368],
        [0.2572, 0.2699, 0.2453, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2592, 0.2484, 0.2408],
        [0.2524, 0.2615, 0.2553, 0.2308],
        [0.2547, 0.2614, 0.2388, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:08:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 0.2356 (0.1810)	Prec@(1,5) (93.7%, 99.9%)	
05/25 11:08:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 0.1694 (0.1707)	Prec@(1,5) (93.9%, 99.9%)	
05/25 11:08:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 0.0992 (0.1703)	Prec@(1,5) (94.0%, 100.0%)	
05/25 11:09:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 0.1887 (0.1783)	Prec@(1,5) (93.6%, 100.0%)	
05/25 11:09:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 0.3292 (0.1812)	Prec@(1,5) (93.5%, 100.0%)	
05/25 11:09:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 0.2272 (0.1870)	Prec@(1,5) (93.3%, 100.0%)	
05/25 11:09:54PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 0.2338 (0.1898)	Prec@(1,5) (93.2%, 99.9%)	
05/25 11:10:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 0.2786 (0.1927)	Prec@(1,5) (93.2%, 99.9%)	
05/25 11:10:09PM searchShareStage_trainer.py:134 [INFO] Train: [ 24/49] Final Prec@1 93.1640%
05/25 11:10:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 0.5005	Prec@(1,5) (84.8%, 99.4%)
05/25 11:10:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 0.5081	Prec@(1,5) (84.5%, 99.3%)
05/25 11:10:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 0.4980	Prec@(1,5) (84.8%, 99.3%)
05/25 11:10:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 0.5041	Prec@(1,5) (84.6%, 99.3%)
05/25 11:10:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 0.5066	Prec@(1,5) (84.5%, 99.3%)
05/25 11:10:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 0.5065	Prec@(1,5) (84.4%, 99.3%)
05/25 11:10:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 0.5061	Prec@(1,5) (84.4%, 99.3%)
05/25 11:10:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 0.5040	Prec@(1,5) (84.4%, 99.3%)
05/25 11:10:31PM searchStage_trainer.py:260 [INFO] Valid: [ 24/49] Final Prec@1 84.3600%
05/25 11:10:31PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:10:31PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 84.3600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2759, 0.2584, 0.2162],
        [0.2518, 0.2804, 0.2664, 0.2015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2732, 0.2663, 0.1969],
        [0.2576, 0.2697, 0.2663, 0.2064],
        [0.2658, 0.2870, 0.2210, 0.2261]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2641, 0.2720, 0.2272, 0.2367],
        [0.2666, 0.2750, 0.2376, 0.2209]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2650, 0.2409, 0.2331],
        [0.2645, 0.2631, 0.2346, 0.2378],
        [0.2593, 0.2691, 0.2436, 0.2281]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2631, 0.2421, 0.2371],
        [0.2666, 0.2617, 0.2349, 0.2368],
        [0.2572, 0.2699, 0.2453, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2592, 0.2484, 0.2408],
        [0.2524, 0.2615, 0.2553, 0.2308],
        [0.2547, 0.2613, 0.2388, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:10:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 0.1405 (0.1847)	Prec@(1,5) (93.3%, 100.0%)	
05/25 11:11:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 0.3274 (0.1661)	Prec@(1,5) (94.0%, 100.0%)	
05/25 11:11:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 0.2166 (0.1649)	Prec@(1,5) (94.0%, 100.0%)	
05/25 11:11:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 0.1709 (0.1689)	Prec@(1,5) (93.8%, 100.0%)	
05/25 11:11:59PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 0.1191 (0.1738)	Prec@(1,5) (93.7%, 99.9%)	
05/25 11:12:17PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 0.2296 (0.1756)	Prec@(1,5) (93.7%, 99.9%)	
05/25 11:12:36PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 0.1652 (0.1782)	Prec@(1,5) (93.6%, 99.9%)	
05/25 11:12:50PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 0.1810 (0.1800)	Prec@(1,5) (93.6%, 99.9%)	
05/25 11:12:50PM searchShareStage_trainer.py:134 [INFO] Train: [ 25/49] Final Prec@1 93.5920%
05/25 11:12:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 0.4867	Prec@(1,5) (84.6%, 99.2%)
05/25 11:12:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 0.4778	Prec@(1,5) (84.9%, 99.2%)
05/25 11:12:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 0.4780	Prec@(1,5) (85.0%, 99.1%)
05/25 11:13:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 0.4766	Prec@(1,5) (85.1%, 99.2%)
05/25 11:13:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 0.4809	Prec@(1,5) (84.9%, 99.3%)
05/25 11:13:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 0.4768	Prec@(1,5) (84.9%, 99.3%)
05/25 11:13:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 0.4784	Prec@(1,5) (84.9%, 99.3%)
05/25 11:13:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 0.4737	Prec@(1,5) (85.0%, 99.3%)
05/25 11:13:12PM searchStage_trainer.py:260 [INFO] Valid: [ 25/49] Final Prec@1 85.0200%
05/25 11:13:12PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:13:12PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.0200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2759, 0.2584, 0.2162],
        [0.2518, 0.2804, 0.2664, 0.2015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2732, 0.2663, 0.1969],
        [0.2576, 0.2697, 0.2663, 0.2064],
        [0.2658, 0.2870, 0.2210, 0.2262]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2641, 0.2719, 0.2272, 0.2367],
        [0.2666, 0.2750, 0.2376, 0.2209]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2331],
        [0.2645, 0.2631, 0.2346, 0.2378],
        [0.2592, 0.2691, 0.2436, 0.2281]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2631, 0.2421, 0.2371],
        [0.2666, 0.2616, 0.2349, 0.2369],
        [0.2572, 0.2698, 0.2453, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2592, 0.2484, 0.2408],
        [0.2524, 0.2615, 0.2553, 0.2309],
        [0.2547, 0.2613, 0.2388, 0.2452]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:13:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 0.1120 (0.1659)	Prec@(1,5) (94.2%, 99.9%)	
05/25 11:13:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 0.1418 (0.1548)	Prec@(1,5) (94.4%, 100.0%)	
05/25 11:14:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 0.2070 (0.1584)	Prec@(1,5) (94.4%, 99.9%)	
05/25 11:14:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.1984 (0.1661)	Prec@(1,5) (94.1%, 99.9%)	
05/25 11:14:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 0.2369 (0.1661)	Prec@(1,5) (94.2%, 99.9%)	
05/25 11:15:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 0.1380 (0.1646)	Prec@(1,5) (94.3%, 100.0%)	
05/25 11:15:17PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 0.1275 (0.1633)	Prec@(1,5) (94.4%, 100.0%)	
05/25 11:15:32PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 0.1147 (0.1655)	Prec@(1,5) (94.2%, 100.0%)	
05/25 11:15:33PM searchShareStage_trainer.py:134 [INFO] Train: [ 26/49] Final Prec@1 94.2440%
05/25 11:15:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 0.5688	Prec@(1,5) (83.7%, 99.1%)
05/25 11:15:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 0.5481	Prec@(1,5) (84.3%, 99.1%)
05/25 11:15:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 0.5370	Prec@(1,5) (84.4%, 99.1%)
05/25 11:15:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 0.5284	Prec@(1,5) (84.7%, 99.2%)
05/25 11:15:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 0.5284	Prec@(1,5) (84.7%, 99.2%)
05/25 11:15:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 0.5288	Prec@(1,5) (84.6%, 99.2%)
05/25 11:15:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 0.5268	Prec@(1,5) (84.5%, 99.2%)
05/25 11:15:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 0.5260	Prec@(1,5) (84.6%, 99.2%)
05/25 11:15:55PM searchStage_trainer.py:260 [INFO] Valid: [ 26/49] Final Prec@1 84.5640%
05/25 11:15:55PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:15:55PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.0200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2758, 0.2585, 0.2163],
        [0.2518, 0.2804, 0.2664, 0.2015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2732, 0.2663, 0.1969],
        [0.2576, 0.2697, 0.2663, 0.2064],
        [0.2658, 0.2870, 0.2210, 0.2262]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2641, 0.2719, 0.2272, 0.2368],
        [0.2666, 0.2750, 0.2376, 0.2209]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2331],
        [0.2645, 0.2631, 0.2346, 0.2378],
        [0.2592, 0.2690, 0.2436, 0.2281]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2631, 0.2421, 0.2371],
        [0.2666, 0.2616, 0.2349, 0.2369],
        [0.2572, 0.2698, 0.2453, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2592, 0.2484, 0.2408],
        [0.2524, 0.2614, 0.2553, 0.2309],
        [0.2547, 0.2613, 0.2388, 0.2452]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:16:14PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 0.0852 (0.1495)	Prec@(1,5) (94.5%, 100.0%)	
05/25 11:16:32PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 0.1116 (0.1361)	Prec@(1,5) (95.3%, 100.0%)	
05/25 11:16:50PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 0.2204 (0.1378)	Prec@(1,5) (95.1%, 100.0%)	
05/25 11:17:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 0.1781 (0.1391)	Prec@(1,5) (95.1%, 100.0%)	
05/25 11:17:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 0.0943 (0.1446)	Prec@(1,5) (94.8%, 100.0%)	
05/25 11:17:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 0.1990 (0.1482)	Prec@(1,5) (94.7%, 100.0%)	
05/25 11:18:01PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 0.2142 (0.1502)	Prec@(1,5) (94.6%, 100.0%)	
05/25 11:18:14PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 0.1423 (0.1510)	Prec@(1,5) (94.6%, 100.0%)	
05/25 11:18:15PM searchShareStage_trainer.py:134 [INFO] Train: [ 27/49] Final Prec@1 94.5840%
05/25 11:18:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 0.4514	Prec@(1,5) (86.3%, 99.2%)
05/25 11:18:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 0.4723	Prec@(1,5) (85.5%, 99.4%)
05/25 11:18:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 0.4899	Prec@(1,5) (85.4%, 99.3%)
05/25 11:18:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 0.4902	Prec@(1,5) (85.5%, 99.2%)
05/25 11:18:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 0.4876	Prec@(1,5) (85.5%, 99.3%)
05/25 11:18:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 0.4844	Prec@(1,5) (85.5%, 99.2%)
05/25 11:18:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 0.4824	Prec@(1,5) (85.7%, 99.3%)
05/25 11:18:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 0.4853	Prec@(1,5) (85.6%, 99.2%)
05/25 11:18:37PM searchStage_trainer.py:260 [INFO] Valid: [ 27/49] Final Prec@1 85.5800%
05/25 11:18:37PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:18:38PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.5800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2758, 0.2585, 0.2163],
        [0.2518, 0.2803, 0.2664, 0.2015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2732, 0.2663, 0.1969],
        [0.2576, 0.2697, 0.2664, 0.2064],
        [0.2658, 0.2870, 0.2210, 0.2262]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2641, 0.2719, 0.2272, 0.2368],
        [0.2666, 0.2750, 0.2376, 0.2209]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2631, 0.2346, 0.2379],
        [0.2592, 0.2690, 0.2436, 0.2281]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2631, 0.2421, 0.2371],
        [0.2666, 0.2616, 0.2349, 0.2369],
        [0.2572, 0.2698, 0.2453, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2592, 0.2485, 0.2408],
        [0.2524, 0.2614, 0.2553, 0.2309],
        [0.2547, 0.2613, 0.2388, 0.2452]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:18:57PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 0.1745 (0.1248)	Prec@(1,5) (95.6%, 100.0%)	
05/25 11:19:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 0.1420 (0.1261)	Prec@(1,5) (95.3%, 100.0%)	
05/25 11:19:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 0.1636 (0.1280)	Prec@(1,5) (95.3%, 100.0%)	
05/25 11:19:50PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.0772 (0.1331)	Prec@(1,5) (95.2%, 100.0%)	
05/25 11:20:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 0.0932 (0.1361)	Prec@(1,5) (95.1%, 100.0%)	
05/25 11:20:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 0.3065 (0.1408)	Prec@(1,5) (95.0%, 100.0%)	
05/25 11:20:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][350/390]	Step 11298	lr 0.01075	Loss 0.1679 (0.1438)	Prec@(1,5) (94.9%, 100.0%)	
05/25 11:20:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 0.2069 (0.1442)	Prec@(1,5) (94.9%, 100.0%)	
05/25 11:20:59PM searchShareStage_trainer.py:134 [INFO] Train: [ 28/49] Final Prec@1 94.8840%
05/25 11:21:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][50/391]	Step 11339	Loss 0.4489	Prec@(1,5) (86.8%, 99.3%)
05/25 11:21:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 0.4609	Prec@(1,5) (86.3%, 99.3%)
05/25 11:21:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][150/391]	Step 11339	Loss 0.4654	Prec@(1,5) (86.2%, 99.3%)
05/25 11:21:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 0.4743	Prec@(1,5) (86.2%, 99.3%)
05/25 11:21:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][250/391]	Step 11339	Loss 0.4849	Prec@(1,5) (86.0%, 99.3%)
05/25 11:21:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 0.4816	Prec@(1,5) (86.0%, 99.3%)
05/25 11:21:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][350/391]	Step 11339	Loss 0.4826	Prec@(1,5) (85.9%, 99.3%)
05/25 11:21:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 0.4868	Prec@(1,5) (85.9%, 99.3%)
05/25 11:21:21PM searchStage_trainer.py:260 [INFO] Valid: [ 28/49] Final Prec@1 85.8840%
05/25 11:21:21PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:21:22PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.8840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2758, 0.2585, 0.2163],
        [0.2518, 0.2803, 0.2664, 0.2015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2731, 0.2664, 0.1969],
        [0.2575, 0.2697, 0.2664, 0.2064],
        [0.2658, 0.2870, 0.2210, 0.2262]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2641, 0.2719, 0.2272, 0.2368],
        [0.2666, 0.2750, 0.2376, 0.2209]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2631, 0.2346, 0.2379],
        [0.2592, 0.2690, 0.2436, 0.2281]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2630, 0.2421, 0.2372],
        [0.2666, 0.2616, 0.2349, 0.2369],
        [0.2572, 0.2698, 0.2453, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2592, 0.2485, 0.2409],
        [0.2524, 0.2614, 0.2553, 0.2309],
        [0.2547, 0.2613, 0.2388, 0.2452]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:21:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][50/390]	Step 11389	lr 0.01002	Loss 0.1100 (0.1240)	Prec@(1,5) (95.6%, 100.0%)	
05/25 11:21:59PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 0.0947 (0.1222)	Prec@(1,5) (95.8%, 100.0%)	
05/25 11:22:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][150/390]	Step 11489	lr 0.01002	Loss 0.0796 (0.1237)	Prec@(1,5) (95.8%, 100.0%)	
05/25 11:22:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.2923 (0.1251)	Prec@(1,5) (95.8%, 100.0%)	
05/25 11:22:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][250/390]	Step 11589	lr 0.01002	Loss 0.2285 (0.1250)	Prec@(1,5) (95.8%, 100.0%)	
05/25 11:23:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 0.2042 (0.1261)	Prec@(1,5) (95.7%, 100.0%)	
05/25 11:23:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][350/390]	Step 11689	lr 0.01002	Loss 0.1768 (0.1265)	Prec@(1,5) (95.7%, 100.0%)	
05/25 11:23:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 0.1921 (0.1255)	Prec@(1,5) (95.8%, 100.0%)	
05/25 11:23:43PM searchShareStage_trainer.py:134 [INFO] Train: [ 29/49] Final Prec@1 95.7640%
05/25 11:23:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][50/391]	Step 11730	Loss 0.4915	Prec@(1,5) (85.6%, 99.2%)
05/25 11:23:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 0.4712	Prec@(1,5) (86.4%, 99.1%)
05/25 11:23:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][150/391]	Step 11730	Loss 0.4765	Prec@(1,5) (86.4%, 99.2%)
05/25 11:23:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 0.4733	Prec@(1,5) (86.3%, 99.2%)
05/25 11:23:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][250/391]	Step 11730	Loss 0.4665	Prec@(1,5) (86.4%, 99.3%)
05/25 11:24:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 0.4691	Prec@(1,5) (86.4%, 99.3%)
05/25 11:24:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][350/391]	Step 11730	Loss 0.4746	Prec@(1,5) (86.3%, 99.3%)
05/25 11:24:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 0.4755	Prec@(1,5) (86.3%, 99.2%)
05/25 11:24:05PM searchStage_trainer.py:260 [INFO] Valid: [ 29/49] Final Prec@1 86.3440%
05/25 11:24:05PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:24:06PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.3440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2758, 0.2585, 0.2163],
        [0.2518, 0.2803, 0.2664, 0.2015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2731, 0.2664, 0.1969],
        [0.2575, 0.2697, 0.2664, 0.2064],
        [0.2658, 0.2870, 0.2210, 0.2262]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2641, 0.2719, 0.2272, 0.2368],
        [0.2666, 0.2749, 0.2376, 0.2209]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2631, 0.2346, 0.2379],
        [0.2592, 0.2690, 0.2436, 0.2282]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2630, 0.2422, 0.2372],
        [0.2666, 0.2616, 0.2349, 0.2369],
        [0.2572, 0.2698, 0.2453, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2524, 0.2614, 0.2553, 0.2309],
        [0.2547, 0.2613, 0.2388, 0.2452]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:24:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][50/390]	Step 11780	lr 0.00929	Loss 0.1109 (0.0939)	Prec@(1,5) (96.8%, 100.0%)	
05/25 11:24:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 0.0525 (0.0974)	Prec@(1,5) (96.7%, 100.0%)	
05/25 11:25:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][150/390]	Step 11880	lr 0.00929	Loss 0.0416 (0.0992)	Prec@(1,5) (96.6%, 100.0%)	
05/25 11:25:17PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 0.0748 (0.1001)	Prec@(1,5) (96.6%, 100.0%)	
05/25 11:25:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][250/390]	Step 11980	lr 0.00929	Loss 0.1049 (0.1003)	Prec@(1,5) (96.6%, 100.0%)	
05/25 11:25:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 0.0651 (0.1011)	Prec@(1,5) (96.6%, 100.0%)	
05/25 11:26:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][350/390]	Step 12080	lr 0.00929	Loss 0.2253 (0.1054)	Prec@(1,5) (96.4%, 100.0%)	
05/25 11:26:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 0.1317 (0.1056)	Prec@(1,5) (96.4%, 100.0%)	
05/25 11:26:25PM searchShareStage_trainer.py:134 [INFO] Train: [ 30/49] Final Prec@1 96.4040%
05/25 11:26:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][50/391]	Step 12121	Loss 0.4557	Prec@(1,5) (87.2%, 99.1%)
05/25 11:26:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 0.4645	Prec@(1,5) (86.9%, 99.1%)
05/25 11:26:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][150/391]	Step 12121	Loss 0.4693	Prec@(1,5) (86.6%, 99.2%)
05/25 11:26:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 0.4778	Prec@(1,5) (86.3%, 99.3%)
05/25 11:26:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][250/391]	Step 12121	Loss 0.4849	Prec@(1,5) (86.1%, 99.3%)
05/25 11:26:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 0.4874	Prec@(1,5) (86.0%, 99.3%)
05/25 11:26:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][350/391]	Step 12121	Loss 0.4790	Prec@(1,5) (86.2%, 99.3%)
05/25 11:26:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 0.4818	Prec@(1,5) (86.3%, 99.3%)
05/25 11:26:48PM searchStage_trainer.py:260 [INFO] Valid: [ 30/49] Final Prec@1 86.2640%
05/25 11:26:48PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:26:48PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.3440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2758, 0.2585, 0.2163],
        [0.2517, 0.2803, 0.2664, 0.2015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2731, 0.2664, 0.1969],
        [0.2575, 0.2697, 0.2664, 0.2064],
        [0.2658, 0.2870, 0.2210, 0.2262]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2022],
        [0.2641, 0.2719, 0.2272, 0.2368],
        [0.2665, 0.2749, 0.2376, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2631, 0.2346, 0.2379],
        [0.2592, 0.2690, 0.2436, 0.2282]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2630, 0.2422, 0.2372],
        [0.2666, 0.2616, 0.2349, 0.2369],
        [0.2572, 0.2698, 0.2453, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2524, 0.2614, 0.2553, 0.2309],
        [0.2547, 0.2612, 0.2389, 0.2452]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:27:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][50/390]	Step 12171	lr 0.00858	Loss 0.0740 (0.0986)	Prec@(1,5) (96.7%, 99.9%)	
05/25 11:27:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.0717 (0.1002)	Prec@(1,5) (96.4%, 100.0%)	
05/25 11:27:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][150/390]	Step 12271	lr 0.00858	Loss 0.1226 (0.0964)	Prec@(1,5) (96.6%, 99.9%)	
05/25 11:28:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.0771 (0.0926)	Prec@(1,5) (96.8%, 100.0%)	
05/25 11:28:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][250/390]	Step 12371	lr 0.00858	Loss 0.1651 (0.0943)	Prec@(1,5) (96.7%, 100.0%)	
05/25 11:28:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 0.0395 (0.0983)	Prec@(1,5) (96.5%, 100.0%)	
05/25 11:28:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][350/390]	Step 12471	lr 0.00858	Loss 0.1271 (0.1011)	Prec@(1,5) (96.4%, 100.0%)	
05/25 11:29:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 0.0296 (0.1038)	Prec@(1,5) (96.4%, 100.0%)	
05/25 11:29:08PM searchShareStage_trainer.py:134 [INFO] Train: [ 31/49] Final Prec@1 96.3520%
05/25 11:29:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][50/391]	Step 12512	Loss 0.4614	Prec@(1,5) (87.4%, 99.2%)
05/25 11:29:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 0.4554	Prec@(1,5) (87.3%, 99.4%)
05/25 11:29:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][150/391]	Step 12512	Loss 0.4716	Prec@(1,5) (86.7%, 99.3%)
05/25 11:29:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 0.4808	Prec@(1,5) (86.4%, 99.3%)
05/25 11:29:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][250/391]	Step 12512	Loss 0.4872	Prec@(1,5) (86.2%, 99.3%)
05/25 11:29:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 0.4803	Prec@(1,5) (86.3%, 99.3%)
05/25 11:29:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][350/391]	Step 12512	Loss 0.4855	Prec@(1,5) (86.1%, 99.3%)
05/25 11:29:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 0.4867	Prec@(1,5) (86.2%, 99.3%)
05/25 11:29:30PM searchStage_trainer.py:260 [INFO] Valid: [ 31/49] Final Prec@1 86.1960%
05/25 11:29:30PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:29:30PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.3440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2757, 0.2585, 0.2163],
        [0.2517, 0.2803, 0.2665, 0.2015]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2731, 0.2664, 0.1969],
        [0.2575, 0.2697, 0.2664, 0.2064],
        [0.2658, 0.2870, 0.2210, 0.2262]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2671, 0.2623, 0.2023],
        [0.2641, 0.2719, 0.2272, 0.2368],
        [0.2665, 0.2749, 0.2376, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2631, 0.2346, 0.2379],
        [0.2592, 0.2690, 0.2436, 0.2282]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2630, 0.2422, 0.2372],
        [0.2666, 0.2616, 0.2349, 0.2369],
        [0.2572, 0.2698, 0.2453, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2524, 0.2614, 0.2553, 0.2309],
        [0.2546, 0.2612, 0.2389, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:29:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][50/390]	Step 12562	lr 0.00789	Loss 0.0155 (0.0797)	Prec@(1,5) (97.1%, 100.0%)	
05/25 11:30:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.0402 (0.0782)	Prec@(1,5) (97.3%, 100.0%)	
05/25 11:30:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][150/390]	Step 12662	lr 0.00789	Loss 0.0439 (0.0784)	Prec@(1,5) (97.4%, 100.0%)	
05/25 11:30:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.0469 (0.0778)	Prec@(1,5) (97.4%, 100.0%)	
05/25 11:31:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][250/390]	Step 12762	lr 0.00789	Loss 0.0897 (0.0822)	Prec@(1,5) (97.3%, 100.0%)	
05/25 11:31:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.0775 (0.0838)	Prec@(1,5) (97.2%, 100.0%)	
05/25 11:31:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][350/390]	Step 12862	lr 0.00789	Loss 0.0902 (0.0835)	Prec@(1,5) (97.2%, 100.0%)	
05/25 11:31:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 0.0445 (0.0845)	Prec@(1,5) (97.2%, 100.0%)	
05/25 11:31:50PM searchShareStage_trainer.py:134 [INFO] Train: [ 32/49] Final Prec@1 97.1720%
05/25 11:31:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][50/391]	Step 12903	Loss 0.4573	Prec@(1,5) (87.2%, 99.4%)
05/25 11:31:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 0.4576	Prec@(1,5) (87.4%, 99.3%)
05/25 11:31:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][150/391]	Step 12903	Loss 0.4677	Prec@(1,5) (87.0%, 99.3%)
05/25 11:32:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 0.4638	Prec@(1,5) (87.1%, 99.4%)
05/25 11:32:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][250/391]	Step 12903	Loss 0.4743	Prec@(1,5) (86.8%, 99.4%)
05/25 11:32:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 0.4781	Prec@(1,5) (86.8%, 99.3%)
05/25 11:32:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][350/391]	Step 12903	Loss 0.4772	Prec@(1,5) (86.8%, 99.4%)
05/25 11:32:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 0.4709	Prec@(1,5) (86.9%, 99.4%)
05/25 11:32:13PM searchStage_trainer.py:260 [INFO] Valid: [ 32/49] Final Prec@1 86.8560%
05/25 11:32:13PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:32:13PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.8560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2757, 0.2585, 0.2164],
        [0.2517, 0.2803, 0.2665, 0.2016]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2731, 0.2664, 0.1969],
        [0.2575, 0.2697, 0.2664, 0.2064],
        [0.2658, 0.2869, 0.2210, 0.2262]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2719, 0.2272, 0.2368],
        [0.2665, 0.2749, 0.2376, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2630, 0.2346, 0.2379],
        [0.2592, 0.2690, 0.2436, 0.2282]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2630, 0.2422, 0.2372],
        [0.2666, 0.2615, 0.2349, 0.2369],
        [0.2572, 0.2698, 0.2453, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2524, 0.2614, 0.2553, 0.2309],
        [0.2546, 0.2612, 0.2389, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:32:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][50/390]	Step 12953	lr 0.00722	Loss 0.1020 (0.0741)	Prec@(1,5) (97.6%, 100.0%)	
05/25 11:32:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.0329 (0.0738)	Prec@(1,5) (97.5%, 100.0%)	
05/25 11:33:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][150/390]	Step 13053	lr 0.00722	Loss 0.0431 (0.0761)	Prec@(1,5) (97.5%, 100.0%)	
05/25 11:33:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.0644 (0.0762)	Prec@(1,5) (97.5%, 100.0%)	
05/25 11:33:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][250/390]	Step 13153	lr 0.00722	Loss 0.0746 (0.0772)	Prec@(1,5) (97.4%, 100.0%)	
05/25 11:34:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.1797 (0.0771)	Prec@(1,5) (97.3%, 100.0%)	
05/25 11:34:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][350/390]	Step 13253	lr 0.00722	Loss 0.0755 (0.0765)	Prec@(1,5) (97.3%, 100.0%)	
05/25 11:34:32PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 0.1447 (0.0779)	Prec@(1,5) (97.2%, 100.0%)	
05/25 11:34:32PM searchShareStage_trainer.py:134 [INFO] Train: [ 33/49] Final Prec@1 97.2480%
05/25 11:34:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][50/391]	Step 13294	Loss 0.5365	Prec@(1,5) (86.0%, 99.1%)
05/25 11:34:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 0.5087	Prec@(1,5) (86.7%, 99.1%)
05/25 11:34:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][150/391]	Step 13294	Loss 0.4888	Prec@(1,5) (86.9%, 99.2%)
05/25 11:34:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 0.4993	Prec@(1,5) (86.7%, 99.2%)
05/25 11:34:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][250/391]	Step 13294	Loss 0.5043	Prec@(1,5) (86.6%, 99.2%)
05/25 11:34:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 0.5011	Prec@(1,5) (86.7%, 99.3%)
05/25 11:34:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][350/391]	Step 13294	Loss 0.5018	Prec@(1,5) (86.6%, 99.3%)
05/25 11:34:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 0.4964	Prec@(1,5) (86.7%, 99.3%)
05/25 11:34:56PM searchStage_trainer.py:260 [INFO] Valid: [ 33/49] Final Prec@1 86.7080%
05/25 11:34:56PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:34:56PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.8560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2757, 0.2585, 0.2164],
        [0.2517, 0.2803, 0.2665, 0.2016]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2731, 0.2664, 0.1969],
        [0.2575, 0.2697, 0.2664, 0.2064],
        [0.2658, 0.2869, 0.2210, 0.2263]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2719, 0.2272, 0.2368],
        [0.2665, 0.2749, 0.2376, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2630, 0.2346, 0.2379],
        [0.2592, 0.2690, 0.2436, 0.2282]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2630, 0.2422, 0.2372],
        [0.2666, 0.2615, 0.2349, 0.2370],
        [0.2572, 0.2697, 0.2453, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2523, 0.2613, 0.2553, 0.2310],
        [0.2546, 0.2612, 0.2389, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:35:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][50/390]	Step 13344	lr 0.00657	Loss 0.1488 (0.0580)	Prec@(1,5) (98.1%, 100.0%)	
05/25 11:35:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.0735 (0.0538)	Prec@(1,5) (98.2%, 100.0%)	
05/25 11:35:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][150/390]	Step 13444	lr 0.00657	Loss 0.0608 (0.0572)	Prec@(1,5) (98.1%, 100.0%)	
05/25 11:36:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.0310 (0.0571)	Prec@(1,5) (98.2%, 100.0%)	
05/25 11:36:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][250/390]	Step 13544	lr 0.00657	Loss 0.0806 (0.0566)	Prec@(1,5) (98.2%, 100.0%)	
05/25 11:36:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.0621 (0.0562)	Prec@(1,5) (98.2%, 100.0%)	
05/25 11:37:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][350/390]	Step 13644	lr 0.00657	Loss 0.1649 (0.0572)	Prec@(1,5) (98.1%, 100.0%)	
05/25 11:37:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 0.0432 (0.0585)	Prec@(1,5) (98.1%, 100.0%)	
05/25 11:37:15PM searchShareStage_trainer.py:134 [INFO] Train: [ 34/49] Final Prec@1 98.0840%
05/25 11:37:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][50/391]	Step 13685	Loss 0.4990	Prec@(1,5) (86.8%, 99.4%)
05/25 11:37:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 0.4791	Prec@(1,5) (87.6%, 99.4%)
05/25 11:37:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][150/391]	Step 13685	Loss 0.4763	Prec@(1,5) (87.5%, 99.5%)
05/25 11:37:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 0.4848	Prec@(1,5) (87.3%, 99.4%)
05/25 11:37:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][250/391]	Step 13685	Loss 0.4908	Prec@(1,5) (87.1%, 99.4%)
05/25 11:37:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 0.4857	Prec@(1,5) (87.2%, 99.3%)
05/25 11:37:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][350/391]	Step 13685	Loss 0.4864	Prec@(1,5) (87.2%, 99.4%)
05/25 11:37:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 0.4855	Prec@(1,5) (87.2%, 99.4%)
05/25 11:37:38PM searchStage_trainer.py:260 [INFO] Valid: [ 34/49] Final Prec@1 87.2320%
05/25 11:37:38PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:37:39PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.2320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2757, 0.2585, 0.2164],
        [0.2517, 0.2803, 0.2665, 0.2016]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2731, 0.2664, 0.1969],
        [0.2575, 0.2697, 0.2664, 0.2064],
        [0.2658, 0.2869, 0.2210, 0.2263]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2719, 0.2272, 0.2368],
        [0.2665, 0.2748, 0.2376, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2630, 0.2346, 0.2379],
        [0.2592, 0.2689, 0.2436, 0.2283]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2630, 0.2422, 0.2372],
        [0.2666, 0.2615, 0.2349, 0.2370],
        [0.2571, 0.2697, 0.2453, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2523, 0.2613, 0.2553, 0.2310],
        [0.2546, 0.2612, 0.2389, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:37:57PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][50/390]	Step 13735	lr 0.00595	Loss 0.0580 (0.0458)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:38:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.0128 (0.0490)	Prec@(1,5) (98.5%, 100.0%)	
05/25 11:38:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][150/390]	Step 13835	lr 0.00595	Loss 0.0538 (0.0486)	Prec@(1,5) (98.4%, 100.0%)	
05/25 11:38:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 0.0358 (0.0500)	Prec@(1,5) (98.3%, 100.0%)	
05/25 11:39:09PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][250/390]	Step 13935	lr 0.00595	Loss 0.0099 (0.0496)	Prec@(1,5) (98.4%, 100.0%)	
05/25 11:39:27PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 0.0390 (0.0517)	Prec@(1,5) (98.3%, 100.0%)	
05/25 11:39:45PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][350/390]	Step 14035	lr 0.00595	Loss 0.1007 (0.0534)	Prec@(1,5) (98.2%, 100.0%)	
05/25 11:39:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 0.0809 (0.0526)	Prec@(1,5) (98.2%, 100.0%)	
05/25 11:39:59PM searchShareStage_trainer.py:134 [INFO] Train: [ 35/49] Final Prec@1 98.2360%
05/25 11:40:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][50/391]	Step 14076	Loss 0.4577	Prec@(1,5) (87.7%, 99.3%)
05/25 11:40:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 0.4810	Prec@(1,5) (87.4%, 99.3%)
05/25 11:40:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][150/391]	Step 14076	Loss 0.4663	Prec@(1,5) (87.8%, 99.3%)
05/25 11:40:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 0.4731	Prec@(1,5) (87.6%, 99.4%)
05/25 11:40:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][250/391]	Step 14076	Loss 0.4808	Prec@(1,5) (87.4%, 99.4%)
05/25 11:40:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 0.4820	Prec@(1,5) (87.4%, 99.4%)
05/25 11:40:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][350/391]	Step 14076	Loss 0.4816	Prec@(1,5) (87.4%, 99.4%)
05/25 11:40:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 0.4846	Prec@(1,5) (87.4%, 99.4%)
05/25 11:40:21PM searchStage_trainer.py:260 [INFO] Valid: [ 35/49] Final Prec@1 87.3640%
05/25 11:40:21PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:40:22PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.3640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2757, 0.2585, 0.2164],
        [0.2517, 0.2803, 0.2665, 0.2016]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2731, 0.2664, 0.1970],
        [0.2575, 0.2696, 0.2664, 0.2064],
        [0.2658, 0.2869, 0.2210, 0.2263]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2718, 0.2272, 0.2368],
        [0.2665, 0.2748, 0.2376, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2630, 0.2346, 0.2379],
        [0.2592, 0.2689, 0.2436, 0.2283]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2630, 0.2422, 0.2372],
        [0.2666, 0.2615, 0.2349, 0.2370],
        [0.2571, 0.2697, 0.2453, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2523, 0.2613, 0.2553, 0.2310],
        [0.2546, 0.2612, 0.2389, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:40:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][50/390]	Step 14126	lr 0.00535	Loss 0.0536 (0.0449)	Prec@(1,5) (98.8%, 100.0%)	
05/25 11:40:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.0767 (0.0464)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:41:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][150/390]	Step 14226	lr 0.00535	Loss 0.0776 (0.0452)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:41:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.1163 (0.0440)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:41:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][250/390]	Step 14326	lr 0.00535	Loss 0.0112 (0.0436)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:42:09PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.0226 (0.0435)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:42:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][350/390]	Step 14426	lr 0.00535	Loss 0.0148 (0.0439)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:42:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.0547 (0.0452)	Prec@(1,5) (98.5%, 100.0%)	
05/25 11:42:43PM searchShareStage_trainer.py:134 [INFO] Train: [ 36/49] Final Prec@1 98.5480%
05/25 11:42:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][50/391]	Step 14467	Loss 0.4967	Prec@(1,5) (87.2%, 99.4%)
05/25 11:42:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 0.4981	Prec@(1,5) (87.0%, 99.5%)
05/25 11:42:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][150/391]	Step 14467	Loss 0.4775	Prec@(1,5) (87.4%, 99.5%)
05/25 11:42:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 0.4789	Prec@(1,5) (87.5%, 99.4%)
05/25 11:42:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][250/391]	Step 14467	Loss 0.4786	Prec@(1,5) (87.5%, 99.4%)
05/25 11:42:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 0.4880	Prec@(1,5) (87.4%, 99.4%)
05/25 11:43:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][350/391]	Step 14467	Loss 0.4906	Prec@(1,5) (87.4%, 99.4%)
05/25 11:43:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 0.4922	Prec@(1,5) (87.4%, 99.4%)
05/25 11:43:04PM searchStage_trainer.py:260 [INFO] Valid: [ 36/49] Final Prec@1 87.3800%
05/25 11:43:04PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:43:04PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.3800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2757, 0.2585, 0.2164],
        [0.2517, 0.2803, 0.2665, 0.2016]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2731, 0.2664, 0.1970],
        [0.2575, 0.2696, 0.2664, 0.2065],
        [0.2658, 0.2870, 0.2210, 0.2263]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2718, 0.2272, 0.2368],
        [0.2665, 0.2749, 0.2376, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2630, 0.2346, 0.2379],
        [0.2592, 0.2689, 0.2436, 0.2283]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2630, 0.2422, 0.2372],
        [0.2666, 0.2615, 0.2349, 0.2370],
        [0.2571, 0.2697, 0.2453, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2523, 0.2613, 0.2553, 0.2310],
        [0.2546, 0.2612, 0.2389, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:43:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][50/390]	Step 14517	lr 0.00479	Loss 0.0137 (0.0431)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:43:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.0777 (0.0428)	Prec@(1,5) (98.7%, 100.0%)	
05/25 11:43:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][150/390]	Step 14617	lr 0.00479	Loss 0.0487 (0.0434)	Prec@(1,5) (98.7%, 100.0%)	
05/25 11:44:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.0789 (0.0429)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:44:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][250/390]	Step 14717	lr 0.00479	Loss 0.0265 (0.0424)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:44:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.0200 (0.0423)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:45:09PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][350/390]	Step 14817	lr 0.00479	Loss 0.0204 (0.0424)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:45:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.0083 (0.0426)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:45:24PM searchShareStage_trainer.py:134 [INFO] Train: [ 37/49] Final Prec@1 98.6160%
05/25 11:45:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][50/391]	Step 14858	Loss 0.5171	Prec@(1,5) (87.1%, 99.5%)
05/25 11:45:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 0.4925	Prec@(1,5) (87.6%, 99.4%)
05/25 11:45:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][150/391]	Step 14858	Loss 0.4866	Prec@(1,5) (87.6%, 99.5%)
05/25 11:45:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 0.4839	Prec@(1,5) (87.8%, 99.4%)
05/25 11:45:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][250/391]	Step 14858	Loss 0.4869	Prec@(1,5) (87.6%, 99.4%)
05/25 11:45:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 0.4898	Prec@(1,5) (87.6%, 99.5%)
05/25 11:45:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][350/391]	Step 14858	Loss 0.4840	Prec@(1,5) (87.6%, 99.5%)
05/25 11:45:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 0.4807	Prec@(1,5) (87.6%, 99.5%)
05/25 11:45:47PM searchStage_trainer.py:260 [INFO] Valid: [ 37/49] Final Prec@1 87.5600%
05/25 11:45:47PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:45:47PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.5600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2757, 0.2585, 0.2164],
        [0.2517, 0.2802, 0.2665, 0.2016]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2731, 0.2664, 0.1970],
        [0.2575, 0.2696, 0.2664, 0.2065],
        [0.2658, 0.2869, 0.2210, 0.2263]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2718, 0.2272, 0.2369],
        [0.2665, 0.2749, 0.2376, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2630, 0.2346, 0.2380],
        [0.2592, 0.2689, 0.2436, 0.2283]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2630, 0.2422, 0.2372],
        [0.2665, 0.2615, 0.2350, 0.2370],
        [0.2571, 0.2697, 0.2453, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2523, 0.2613, 0.2553, 0.2310],
        [0.2546, 0.2612, 0.2389, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:46:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][50/390]	Step 14908	lr 0.00425	Loss 0.0170 (0.0413)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:46:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.0247 (0.0401)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:46:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][150/390]	Step 15008	lr 0.00425	Loss 0.0687 (0.0399)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:46:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.0352 (0.0400)	Prec@(1,5) (98.6%, 100.0%)	
05/25 11:47:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][250/390]	Step 15108	lr 0.00425	Loss 0.0208 (0.0380)	Prec@(1,5) (98.7%, 100.0%)	
05/25 11:47:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.0247 (0.0376)	Prec@(1,5) (98.8%, 100.0%)	
05/25 11:47:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][350/390]	Step 15208	lr 0.00425	Loss 0.0183 (0.0371)	Prec@(1,5) (98.8%, 100.0%)	
05/25 11:48:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.0118 (0.0369)	Prec@(1,5) (98.8%, 100.0%)	
05/25 11:48:06PM searchShareStage_trainer.py:134 [INFO] Train: [ 38/49] Final Prec@1 98.7800%
05/25 11:48:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][50/391]	Step 15249	Loss 0.4683	Prec@(1,5) (87.7%, 99.5%)
05/25 11:48:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 0.4613	Prec@(1,5) (88.1%, 99.5%)
05/25 11:48:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][150/391]	Step 15249	Loss 0.4716	Prec@(1,5) (87.8%, 99.6%)
05/25 11:48:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 0.4737	Prec@(1,5) (87.8%, 99.5%)
05/25 11:48:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][250/391]	Step 15249	Loss 0.4731	Prec@(1,5) (87.8%, 99.5%)
05/25 11:48:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 0.4696	Prec@(1,5) (87.9%, 99.5%)
05/25 11:48:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][350/391]	Step 15249	Loss 0.4737	Prec@(1,5) (87.8%, 99.5%)
05/25 11:48:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 0.4730	Prec@(1,5) (87.8%, 99.4%)
05/25 11:48:30PM searchStage_trainer.py:260 [INFO] Valid: [ 38/49] Final Prec@1 87.8400%
05/25 11:48:30PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:48:30PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.8400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2757, 0.2585, 0.2164],
        [0.2517, 0.2802, 0.2665, 0.2016]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2730, 0.2664, 0.1970],
        [0.2575, 0.2696, 0.2664, 0.2065],
        [0.2658, 0.2869, 0.2210, 0.2264]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2718, 0.2272, 0.2369],
        [0.2665, 0.2748, 0.2376, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2332],
        [0.2645, 0.2630, 0.2346, 0.2380],
        [0.2592, 0.2689, 0.2436, 0.2283]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2630, 0.2422, 0.2372],
        [0.2665, 0.2615, 0.2350, 0.2370],
        [0.2571, 0.2697, 0.2453, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2523, 0.2613, 0.2553, 0.2310],
        [0.2546, 0.2612, 0.2389, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:48:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][50/390]	Step 15299	lr 0.00375	Loss 0.0025 (0.0282)	Prec@(1,5) (99.3%, 100.0%)	
05/25 11:49:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.0391 (0.0308)	Prec@(1,5) (99.1%, 100.0%)	
05/25 11:49:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][150/390]	Step 15399	lr 0.00375	Loss 0.0205 (0.0303)	Prec@(1,5) (99.1%, 100.0%)	
05/25 11:49:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.0307 (0.0287)	Prec@(1,5) (99.1%, 100.0%)	
05/25 11:50:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][250/390]	Step 15499	lr 0.00375	Loss 0.0139 (0.0289)	Prec@(1,5) (99.1%, 100.0%)	
05/25 11:50:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.0351 (0.0287)	Prec@(1,5) (99.1%, 100.0%)	
05/25 11:50:36PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][350/390]	Step 15599	lr 0.00375	Loss 0.0323 (0.0290)	Prec@(1,5) (99.1%, 100.0%)	
05/25 11:50:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.0297 (0.0286)	Prec@(1,5) (99.1%, 100.0%)	
05/25 11:50:51PM searchShareStage_trainer.py:134 [INFO] Train: [ 39/49] Final Prec@1 99.1480%
05/25 11:50:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][50/391]	Step 15640	Loss 0.4606	Prec@(1,5) (87.7%, 99.4%)
05/25 11:50:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 0.4676	Prec@(1,5) (87.6%, 99.4%)
05/25 11:51:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][150/391]	Step 15640	Loss 0.4647	Prec@(1,5) (88.0%, 99.4%)
05/25 11:51:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 0.4734	Prec@(1,5) (87.9%, 99.5%)
05/25 11:51:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][250/391]	Step 15640	Loss 0.4703	Prec@(1,5) (87.9%, 99.5%)
05/25 11:51:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 0.4768	Prec@(1,5) (87.7%, 99.5%)
05/25 11:51:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][350/391]	Step 15640	Loss 0.4780	Prec@(1,5) (87.7%, 99.5%)
05/25 11:51:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 0.4831	Prec@(1,5) (87.8%, 99.5%)
05/25 11:51:13PM searchStage_trainer.py:260 [INFO] Valid: [ 39/49] Final Prec@1 87.7520%
05/25 11:51:13PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:51:13PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.8400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2757, 0.2585, 0.2164],
        [0.2517, 0.2802, 0.2665, 0.2016]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2730, 0.2664, 0.1970],
        [0.2575, 0.2696, 0.2664, 0.2065],
        [0.2658, 0.2869, 0.2210, 0.2264]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2718, 0.2272, 0.2369],
        [0.2665, 0.2748, 0.2376, 0.2211]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2648, 0.2409, 0.2333],
        [0.2645, 0.2630, 0.2346, 0.2380],
        [0.2592, 0.2688, 0.2436, 0.2284]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2630, 0.2421, 0.2372],
        [0.2665, 0.2615, 0.2350, 0.2370],
        [0.2571, 0.2696, 0.2454, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2523, 0.2613, 0.2553, 0.2310],
        [0.2546, 0.2611, 0.2389, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:51:32PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][50/390]	Step 15690	lr 0.00329	Loss 0.0048 (0.0179)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:51:50PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.0169 (0.0224)	Prec@(1,5) (99.3%, 100.0%)	
05/25 11:52:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][150/390]	Step 15790	lr 0.00329	Loss 0.0491 (0.0224)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:52:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.0079 (0.0216)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:52:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][250/390]	Step 15890	lr 0.00329	Loss 0.0874 (0.0224)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:53:01PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.0158 (0.0227)	Prec@(1,5) (99.3%, 100.0%)	
05/25 11:53:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][350/390]	Step 15990	lr 0.00329	Loss 0.0324 (0.0235)	Prec@(1,5) (99.3%, 100.0%)	
05/25 11:53:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.0115 (0.0247)	Prec@(1,5) (99.3%, 100.0%)	
05/25 11:53:33PM searchShareStage_trainer.py:134 [INFO] Train: [ 40/49] Final Prec@1 99.2640%
05/25 11:53:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][50/391]	Step 16031	Loss 0.4875	Prec@(1,5) (87.3%, 99.4%)
05/25 11:53:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 0.4788	Prec@(1,5) (87.7%, 99.5%)
05/25 11:53:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][150/391]	Step 16031	Loss 0.4849	Prec@(1,5) (87.8%, 99.4%)
05/25 11:53:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 0.4930	Prec@(1,5) (87.7%, 99.4%)
05/25 11:53:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][250/391]	Step 16031	Loss 0.4950	Prec@(1,5) (87.6%, 99.4%)
05/25 11:53:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 0.4948	Prec@(1,5) (87.6%, 99.4%)
05/25 11:53:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][350/391]	Step 16031	Loss 0.4862	Prec@(1,5) (87.8%, 99.5%)
05/25 11:53:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 0.4836	Prec@(1,5) (87.9%, 99.4%)
05/25 11:53:55PM searchStage_trainer.py:260 [INFO] Valid: [ 40/49] Final Prec@1 87.8640%
05/25 11:53:55PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:53:55PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.8640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2756, 0.2586, 0.2164],
        [0.2517, 0.2801, 0.2665, 0.2017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2730, 0.2664, 0.1970],
        [0.2575, 0.2696, 0.2664, 0.2065],
        [0.2658, 0.2869, 0.2210, 0.2264]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2718, 0.2272, 0.2369],
        [0.2665, 0.2748, 0.2376, 0.2211]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2648, 0.2409, 0.2333],
        [0.2645, 0.2630, 0.2346, 0.2380],
        [0.2592, 0.2688, 0.2436, 0.2284]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2630, 0.2422, 0.2372],
        [0.2665, 0.2615, 0.2350, 0.2370],
        [0.2571, 0.2696, 0.2454, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2409],
        [0.2523, 0.2613, 0.2553, 0.2310],
        [0.2546, 0.2611, 0.2389, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:54:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][50/390]	Step 16081	lr 0.00287	Loss 0.0135 (0.0215)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:54:30PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.0746 (0.0239)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:54:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][150/390]	Step 16181	lr 0.00287	Loss 0.0227 (0.0223)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:55:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.0104 (0.0215)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:55:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][250/390]	Step 16281	lr 0.00287	Loss 0.0690 (0.0215)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:55:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.0030 (0.0213)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:55:59PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][350/390]	Step 16381	lr 0.00287	Loss 0.0126 (0.0209)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:56:14PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.0460 (0.0212)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:56:14PM searchShareStage_trainer.py:134 [INFO] Train: [ 41/49] Final Prec@1 99.4120%
05/25 11:56:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][50/391]	Step 16422	Loss 0.4636	Prec@(1,5) (88.8%, 99.6%)
05/25 11:56:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 0.4841	Prec@(1,5) (88.1%, 99.5%)
05/25 11:56:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][150/391]	Step 16422	Loss 0.4861	Prec@(1,5) (88.3%, 99.4%)
05/25 11:56:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 0.4754	Prec@(1,5) (88.4%, 99.5%)
05/25 11:56:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][250/391]	Step 16422	Loss 0.4796	Prec@(1,5) (88.3%, 99.5%)
05/25 11:56:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 0.4792	Prec@(1,5) (88.3%, 99.5%)
05/25 11:56:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][350/391]	Step 16422	Loss 0.4808	Prec@(1,5) (88.3%, 99.5%)
05/25 11:56:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 0.4794	Prec@(1,5) (88.2%, 99.5%)
05/25 11:56:37PM searchStage_trainer.py:260 [INFO] Valid: [ 41/49] Final Prec@1 88.2400%
05/25 11:56:37PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:56:37PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 88.2400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2756, 0.2586, 0.2165],
        [0.2517, 0.2801, 0.2665, 0.2017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2730, 0.2664, 0.1970],
        [0.2575, 0.2696, 0.2664, 0.2065],
        [0.2658, 0.2869, 0.2210, 0.2264]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2718, 0.2272, 0.2369],
        [0.2665, 0.2748, 0.2376, 0.2211]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2648, 0.2409, 0.2333],
        [0.2645, 0.2630, 0.2346, 0.2380],
        [0.2592, 0.2688, 0.2436, 0.2284]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2629, 0.2422, 0.2372],
        [0.2665, 0.2615, 0.2350, 0.2370],
        [0.2571, 0.2696, 0.2454, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2590, 0.2485, 0.2410],
        [0.2523, 0.2613, 0.2553, 0.2310],
        [0.2546, 0.2611, 0.2389, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:56:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][50/390]	Step 16472	lr 0.00248	Loss 0.0221 (0.0189)	Prec@(1,5) (99.4%, 100.0%)	
05/25 11:57:14PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.0320 (0.0157)	Prec@(1,5) (99.6%, 100.0%)	
05/25 11:57:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][150/390]	Step 16572	lr 0.00248	Loss 0.0194 (0.0178)	Prec@(1,5) (99.5%, 100.0%)	
05/25 11:57:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.0323 (0.0170)	Prec@(1,5) (99.5%, 100.0%)	
05/25 11:58:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][250/390]	Step 16672	lr 0.00248	Loss 0.0053 (0.0175)	Prec@(1,5) (99.5%, 100.0%)	
05/25 11:58:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.0157 (0.0175)	Prec@(1,5) (99.5%, 100.0%)	
05/25 11:58:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][350/390]	Step 16772	lr 0.00248	Loss 0.0215 (0.0180)	Prec@(1,5) (99.5%, 100.0%)	
05/25 11:58:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.0106 (0.0178)	Prec@(1,5) (99.5%, 100.0%)	
05/25 11:58:57PM searchShareStage_trainer.py:134 [INFO] Train: [ 42/49] Final Prec@1 99.4760%
05/25 11:59:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][50/391]	Step 16813	Loss 0.5301	Prec@(1,5) (88.0%, 99.4%)
05/25 11:59:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 0.5089	Prec@(1,5) (88.0%, 99.4%)
05/25 11:59:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][150/391]	Step 16813	Loss 0.4863	Prec@(1,5) (88.4%, 99.4%)
05/25 11:59:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 0.4865	Prec@(1,5) (88.2%, 99.5%)
05/25 11:59:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][250/391]	Step 16813	Loss 0.4798	Prec@(1,5) (88.3%, 99.5%)
05/25 11:59:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 0.4789	Prec@(1,5) (88.3%, 99.5%)
05/25 11:59:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][350/391]	Step 16813	Loss 0.4794	Prec@(1,5) (88.3%, 99.4%)
05/25 11:59:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 0.4793	Prec@(1,5) (88.4%, 99.4%)
05/25 11:59:20PM searchStage_trainer.py:260 [INFO] Valid: [ 42/49] Final Prec@1 88.3800%
05/25 11:59:20PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/25 11:59:20PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 88.3800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2756, 0.2586, 0.2165],
        [0.2517, 0.2801, 0.2665, 0.2017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2730, 0.2664, 0.1971],
        [0.2575, 0.2696, 0.2664, 0.2065],
        [0.2658, 0.2869, 0.2210, 0.2264]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2718, 0.2272, 0.2369],
        [0.2665, 0.2748, 0.2376, 0.2211]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2648, 0.2409, 0.2333],
        [0.2645, 0.2630, 0.2346, 0.2380],
        [0.2592, 0.2688, 0.2436, 0.2284]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2629, 0.2422, 0.2372],
        [0.2665, 0.2615, 0.2350, 0.2370],
        [0.2571, 0.2696, 0.2454, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2590, 0.2485, 0.2410],
        [0.2523, 0.2613, 0.2553, 0.2310],
        [0.2546, 0.2611, 0.2389, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/25 11:59:38PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][50/390]	Step 16863	lr 0.00214	Loss 0.0174 (0.0206)	Prec@(1,5) (99.5%, 100.0%)	
05/25 11:59:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.0028 (0.0191)	Prec@(1,5) (99.5%, 100.0%)	
05/26 12:00:14AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][150/390]	Step 16963	lr 0.00214	Loss 0.0344 (0.0191)	Prec@(1,5) (99.5%, 100.0%)	
05/26 12:00:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.0100 (0.0176)	Prec@(1,5) (99.5%, 100.0%)	
05/26 12:00:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][250/390]	Step 17063	lr 0.00214	Loss 0.0054 (0.0176)	Prec@(1,5) (99.5%, 100.0%)	
05/26 12:01:07AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.0166 (0.0177)	Prec@(1,5) (99.5%, 100.0%)	
05/26 12:01:25AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][350/390]	Step 17163	lr 0.00214	Loss 0.0228 (0.0178)	Prec@(1,5) (99.5%, 100.0%)	
05/26 12:01:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.0065 (0.0176)	Prec@(1,5) (99.5%, 100.0%)	
05/26 12:01:39AM searchShareStage_trainer.py:134 [INFO] Train: [ 43/49] Final Prec@1 99.5000%
05/26 12:01:42AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][50/391]	Step 17204	Loss 0.4773	Prec@(1,5) (88.7%, 99.5%)
05/26 12:01:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 0.4876	Prec@(1,5) (88.1%, 99.4%)
05/26 12:01:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][150/391]	Step 17204	Loss 0.4669	Prec@(1,5) (88.5%, 99.4%)
05/26 12:01:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 0.4772	Prec@(1,5) (88.6%, 99.4%)
05/26 12:01:54AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][250/391]	Step 17204	Loss 0.4740	Prec@(1,5) (88.5%, 99.4%)
05/26 12:01:56AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 0.4820	Prec@(1,5) (88.4%, 99.3%)
05/26 12:01:59AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][350/391]	Step 17204	Loss 0.4781	Prec@(1,5) (88.5%, 99.3%)
05/26 12:02:01AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 0.4783	Prec@(1,5) (88.6%, 99.3%)
05/26 12:02:01AM searchStage_trainer.py:260 [INFO] Valid: [ 43/49] Final Prec@1 88.5600%
05/26 12:02:01AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/26 12:02:02AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 88.5600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2756, 0.2586, 0.2165],
        [0.2517, 0.2801, 0.2665, 0.2017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2730, 0.2664, 0.1971],
        [0.2575, 0.2696, 0.2664, 0.2065],
        [0.2658, 0.2868, 0.2210, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2718, 0.2272, 0.2369],
        [0.2665, 0.2747, 0.2376, 0.2211]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2648, 0.2409, 0.2333],
        [0.2645, 0.2629, 0.2346, 0.2380],
        [0.2592, 0.2688, 0.2436, 0.2284]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2629, 0.2422, 0.2372],
        [0.2665, 0.2615, 0.2350, 0.2370],
        [0.2571, 0.2696, 0.2454, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2590, 0.2485, 0.2410],
        [0.2523, 0.2613, 0.2553, 0.2310],
        [0.2546, 0.2611, 0.2389, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/26 12:02:21AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][50/390]	Step 17254	lr 0.00184	Loss 0.0032 (0.0138)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:02:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.0090 (0.0142)	Prec@(1,5) (99.6%, 100.0%)	
05/26 12:02:56AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][150/390]	Step 17354	lr 0.00184	Loss 0.0162 (0.0150)	Prec@(1,5) (99.5%, 100.0%)	
05/26 12:03:14AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.0066 (0.0150)	Prec@(1,5) (99.5%, 100.0%)	
05/26 12:03:32AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][250/390]	Step 17454	lr 0.00184	Loss 0.0056 (0.0143)	Prec@(1,5) (99.6%, 100.0%)	
05/26 12:03:48AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.0193 (0.0145)	Prec@(1,5) (99.6%, 100.0%)	
05/26 12:04:07AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][350/390]	Step 17554	lr 0.00184	Loss 0.0052 (0.0145)	Prec@(1,5) (99.6%, 100.0%)	
05/26 12:04:21AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.0022 (0.0143)	Prec@(1,5) (99.6%, 100.0%)	
05/26 12:04:21AM searchShareStage_trainer.py:134 [INFO] Train: [ 44/49] Final Prec@1 99.5920%
05/26 12:04:25AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][50/391]	Step 17595	Loss 0.4767	Prec@(1,5) (88.5%, 99.2%)
05/26 12:04:28AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 0.4653	Prec@(1,5) (88.9%, 99.4%)
05/26 12:04:31AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][150/391]	Step 17595	Loss 0.4703	Prec@(1,5) (88.7%, 99.4%)
05/26 12:04:34AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 0.4828	Prec@(1,5) (88.4%, 99.4%)
05/26 12:04:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][250/391]	Step 17595	Loss 0.4859	Prec@(1,5) (88.3%, 99.4%)
05/26 12:04:39AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 0.4883	Prec@(1,5) (88.2%, 99.4%)
05/26 12:04:42AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][350/391]	Step 17595	Loss 0.4877	Prec@(1,5) (88.3%, 99.4%)
05/26 12:04:44AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 0.4872	Prec@(1,5) (88.3%, 99.4%)
05/26 12:04:45AM searchStage_trainer.py:260 [INFO] Valid: [ 44/49] Final Prec@1 88.3240%
05/26 12:04:45AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/26 12:04:45AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 88.5600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2756, 0.2586, 0.2165],
        [0.2517, 0.2801, 0.2665, 0.2017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2730, 0.2664, 0.1971],
        [0.2575, 0.2696, 0.2664, 0.2065],
        [0.2658, 0.2868, 0.2210, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2670, 0.2623, 0.2023],
        [0.2641, 0.2718, 0.2272, 0.2370],
        [0.2665, 0.2747, 0.2377, 0.2212]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2648, 0.2409, 0.2333],
        [0.2644, 0.2629, 0.2346, 0.2381],
        [0.2591, 0.2687, 0.2437, 0.2285]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2576, 0.2629, 0.2422, 0.2372],
        [0.2665, 0.2614, 0.2350, 0.2371],
        [0.2571, 0.2695, 0.2454, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2590, 0.2485, 0.2410],
        [0.2523, 0.2612, 0.2554, 0.2311],
        [0.2546, 0.2610, 0.2390, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/26 12:05:03AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][50/390]	Step 17645	lr 0.00159	Loss 0.0189 (0.0136)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:05:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.0018 (0.0123)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:05:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][150/390]	Step 17745	lr 0.00159	Loss 0.0029 (0.0123)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:05:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.0073 (0.0116)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:06:15AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][250/390]	Step 17845	lr 0.00159	Loss 0.0112 (0.0118)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:06:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.0136 (0.0120)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:06:51AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][350/390]	Step 17945	lr 0.00159	Loss 0.0078 (0.0123)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:07:05AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.0062 (0.0124)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:07:05AM searchShareStage_trainer.py:134 [INFO] Train: [ 45/49] Final Prec@1 99.7160%
05/26 12:07:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][50/391]	Step 17986	Loss 0.5131	Prec@(1,5) (87.5%, 99.3%)
05/26 12:07:11AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 0.4996	Prec@(1,5) (88.3%, 99.4%)
05/26 12:07:14AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][150/391]	Step 17986	Loss 0.4990	Prec@(1,5) (88.3%, 99.4%)
05/26 12:07:17AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 0.5035	Prec@(1,5) (88.1%, 99.4%)
05/26 12:07:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][250/391]	Step 17986	Loss 0.4977	Prec@(1,5) (88.3%, 99.4%)
05/26 12:07:22AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 0.4958	Prec@(1,5) (88.3%, 99.4%)
05/26 12:07:25AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][350/391]	Step 17986	Loss 0.4896	Prec@(1,5) (88.4%, 99.4%)
05/26 12:07:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 0.4862	Prec@(1,5) (88.4%, 99.4%)
05/26 12:07:27AM searchStage_trainer.py:260 [INFO] Valid: [ 45/49] Final Prec@1 88.4160%
05/26 12:07:27AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/26 12:07:27AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 88.5600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2756, 0.2586, 0.2165],
        [0.2517, 0.2801, 0.2665, 0.2017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2730, 0.2664, 0.1971],
        [0.2575, 0.2696, 0.2664, 0.2065],
        [0.2658, 0.2868, 0.2210, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2669, 0.2623, 0.2024],
        [0.2641, 0.2718, 0.2272, 0.2370],
        [0.2665, 0.2747, 0.2377, 0.2212]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2409, 0.2333],
        [0.2644, 0.2629, 0.2346, 0.2381],
        [0.2591, 0.2687, 0.2437, 0.2285]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2629, 0.2422, 0.2372],
        [0.2665, 0.2614, 0.2350, 0.2371],
        [0.2571, 0.2695, 0.2454, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2590, 0.2485, 0.2410],
        [0.2523, 0.2612, 0.2554, 0.2311],
        [0.2545, 0.2610, 0.2389, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/26 12:07:45AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][50/390]	Step 18036	lr 0.00138	Loss 0.0063 (0.0129)	Prec@(1,5) (99.6%, 100.0%)	
05/26 12:08:03AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.0106 (0.0114)	Prec@(1,5) (99.6%, 100.0%)	
05/26 12:08:21AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][150/390]	Step 18136	lr 0.00138	Loss 0.0148 (0.0105)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:08:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.0022 (0.0099)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:08:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][250/390]	Step 18236	lr 0.00138	Loss 0.0014 (0.0105)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:09:15AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.0267 (0.0112)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:09:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][350/390]	Step 18336	lr 0.00138	Loss 0.0033 (0.0112)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:09:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.0212 (0.0114)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:09:47AM searchShareStage_trainer.py:134 [INFO] Train: [ 46/49] Final Prec@1 99.7240%
05/26 12:09:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][50/391]	Step 18377	Loss 0.4738	Prec@(1,5) (88.5%, 99.4%)
05/26 12:09:53AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 0.4920	Prec@(1,5) (88.4%, 99.3%)
05/26 12:09:57AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][150/391]	Step 18377	Loss 0.4788	Prec@(1,5) (88.4%, 99.3%)
05/26 12:09:59AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 0.4762	Prec@(1,5) (88.6%, 99.3%)
05/26 12:10:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][250/391]	Step 18377	Loss 0.4775	Prec@(1,5) (88.6%, 99.3%)
05/26 12:10:05AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 0.4799	Prec@(1,5) (88.6%, 99.3%)
05/26 12:10:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][350/391]	Step 18377	Loss 0.4837	Prec@(1,5) (88.5%, 99.3%)
05/26 12:10:10AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 0.4835	Prec@(1,5) (88.5%, 99.3%)
05/26 12:10:10AM searchStage_trainer.py:260 [INFO] Valid: [ 46/49] Final Prec@1 88.5240%
05/26 12:10:10AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/26 12:10:10AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 88.5600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2756, 0.2586, 0.2165],
        [0.2517, 0.2801, 0.2665, 0.2017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2635, 0.2729, 0.2664, 0.1971],
        [0.2575, 0.2695, 0.2665, 0.2065],
        [0.2658, 0.2868, 0.2210, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2669, 0.2623, 0.2024],
        [0.2641, 0.2718, 0.2272, 0.2370],
        [0.2665, 0.2747, 0.2377, 0.2212]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2408, 0.2332],
        [0.2644, 0.2629, 0.2346, 0.2381],
        [0.2591, 0.2687, 0.2437, 0.2285]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2629, 0.2422, 0.2372],
        [0.2665, 0.2614, 0.2350, 0.2371],
        [0.2571, 0.2695, 0.2454, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2590, 0.2485, 0.2410],
        [0.2523, 0.2612, 0.2554, 0.2311],
        [0.2545, 0.2610, 0.2389, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/26 12:10:29AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][50/390]	Step 18427	lr 0.00121	Loss 0.0065 (0.0120)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:10:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.0359 (0.0120)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:11:04AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][150/390]	Step 18527	lr 0.00121	Loss 0.0248 (0.0120)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:11:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.0077 (0.0115)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:11:41AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][250/390]	Step 18627	lr 0.00121	Loss 0.0027 (0.0108)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:11:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.0128 (0.0112)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:12:16AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][350/390]	Step 18727	lr 0.00121	Loss 0.0098 (0.0109)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:12:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.0011 (0.0109)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:12:31AM searchShareStage_trainer.py:134 [INFO] Train: [ 47/49] Final Prec@1 99.7680%
05/26 12:12:34AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][50/391]	Step 18768	Loss 0.4478	Prec@(1,5) (89.6%, 99.5%)
05/26 12:12:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 0.4692	Prec@(1,5) (88.9%, 99.5%)
05/26 12:12:40AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][150/391]	Step 18768	Loss 0.4793	Prec@(1,5) (88.6%, 99.5%)
05/26 12:12:43AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 0.4947	Prec@(1,5) (88.5%, 99.5%)
05/26 12:12:46AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][250/391]	Step 18768	Loss 0.4924	Prec@(1,5) (88.5%, 99.5%)
05/26 12:12:49AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 0.4920	Prec@(1,5) (88.4%, 99.4%)
05/26 12:12:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][350/391]	Step 18768	Loss 0.4864	Prec@(1,5) (88.4%, 99.4%)
05/26 12:12:54AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 0.4911	Prec@(1,5) (88.4%, 99.4%)
05/26 12:12:54AM searchStage_trainer.py:260 [INFO] Valid: [ 47/49] Final Prec@1 88.3880%
05/26 12:12:54AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/26 12:12:54AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 88.5600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2756, 0.2586, 0.2165],
        [0.2517, 0.2801, 0.2665, 0.2017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2635, 0.2729, 0.2664, 0.1971],
        [0.2574, 0.2695, 0.2665, 0.2066],
        [0.2658, 0.2868, 0.2210, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2669, 0.2623, 0.2024],
        [0.2641, 0.2718, 0.2272, 0.2370],
        [0.2665, 0.2747, 0.2377, 0.2212]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2408, 0.2332],
        [0.2644, 0.2629, 0.2346, 0.2381],
        [0.2591, 0.2687, 0.2437, 0.2285]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2630, 0.2422, 0.2372],
        [0.2665, 0.2614, 0.2350, 0.2371],
        [0.2571, 0.2695, 0.2454, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2590, 0.2485, 0.2410],
        [0.2523, 0.2612, 0.2554, 0.2311],
        [0.2545, 0.2610, 0.2389, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/26 12:13:12AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][50/390]	Step 18818	lr 0.00109	Loss 0.0157 (0.0101)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:13:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.0039 (0.0095)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:13:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][150/390]	Step 18918	lr 0.00109	Loss 0.0059 (0.0100)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:14:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.0280 (0.0097)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:14:24AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][250/390]	Step 19018	lr 0.00109	Loss 0.0014 (0.0106)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:14:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.0324 (0.0105)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:15:00AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][350/390]	Step 19118	lr 0.00109	Loss 0.0063 (0.0107)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:15:14AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.0048 (0.0105)	Prec@(1,5) (99.8%, 100.0%)	
05/26 12:15:14AM searchShareStage_trainer.py:134 [INFO] Train: [ 48/49] Final Prec@1 99.7560%
05/26 12:15:18AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][50/391]	Step 19159	Loss 0.5107	Prec@(1,5) (87.6%, 99.3%)
05/26 12:15:20AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 0.4935	Prec@(1,5) (88.1%, 99.5%)
05/26 12:15:23AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][150/391]	Step 19159	Loss 0.4915	Prec@(1,5) (88.0%, 99.4%)
05/26 12:15:26AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 0.4958	Prec@(1,5) (88.0%, 99.4%)
05/26 12:15:29AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][250/391]	Step 19159	Loss 0.5006	Prec@(1,5) (88.0%, 99.3%)
05/26 12:15:32AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 0.4931	Prec@(1,5) (88.2%, 99.3%)
05/26 12:15:34AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][350/391]	Step 19159	Loss 0.4872	Prec@(1,5) (88.2%, 99.4%)
05/26 12:15:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 0.4824	Prec@(1,5) (88.3%, 99.4%)
05/26 12:15:37AM searchStage_trainer.py:260 [INFO] Valid: [ 48/49] Final Prec@1 88.3360%
05/26 12:15:37AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/26 12:15:37AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 88.5600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2756, 0.2586, 0.2165],
        [0.2517, 0.2801, 0.2665, 0.2017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2635, 0.2729, 0.2664, 0.1972],
        [0.2574, 0.2695, 0.2665, 0.2066],
        [0.2658, 0.2868, 0.2210, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.2669, 0.2623, 0.2024],
        [0.2641, 0.2718, 0.2272, 0.2370],
        [0.2665, 0.2747, 0.2377, 0.2212]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2610, 0.2649, 0.2408, 0.2332],
        [0.2644, 0.2629, 0.2346, 0.2381],
        [0.2591, 0.2687, 0.2437, 0.2285]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2630, 0.2422, 0.2372],
        [0.2665, 0.2614, 0.2350, 0.2371],
        [0.2571, 0.2695, 0.2454, 0.2280]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2591, 0.2485, 0.2410],
        [0.2523, 0.2612, 0.2554, 0.2311],
        [0.2545, 0.2610, 0.2389, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/26 12:15:55AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][50/390]	Step 19209	lr 0.00102	Loss 0.0105 (0.0122)	Prec@(1,5) (99.6%, 100.0%)	
05/26 12:16:13AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.0018 (0.0113)	Prec@(1,5) (99.6%, 100.0%)	
05/26 12:16:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][150/390]	Step 19309	lr 0.00102	Loss 0.0065 (0.0100)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:16:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.0149 (0.0097)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:17:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][250/390]	Step 19409	lr 0.00102	Loss 0.0112 (0.0103)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:17:24AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.0166 (0.0102)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:17:42AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][350/390]	Step 19509	lr 0.00102	Loss 0.0199 (0.0103)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:17:56AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.0054 (0.0101)	Prec@(1,5) (99.7%, 100.0%)	
05/26 12:17:57AM searchShareStage_trainer.py:134 [INFO] Train: [ 49/49] Final Prec@1 99.7120%
05/26 12:18:00AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][50/391]	Step 19550	Loss 0.4430	Prec@(1,5) (89.2%, 99.2%)
05/26 12:18:03AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 0.4846	Prec@(1,5) (88.8%, 99.2%)
05/26 12:18:06AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][150/391]	Step 19550	Loss 0.4802	Prec@(1,5) (88.6%, 99.3%)
05/26 12:18:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 0.4827	Prec@(1,5) (88.7%, 99.3%)
05/26 12:18:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][250/391]	Step 19550	Loss 0.4854	Prec@(1,5) (88.7%, 99.3%)
05/26 12:18:14AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 0.4857	Prec@(1,5) (88.6%, 99.3%)
05/26 12:18:17AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][350/391]	Step 19550	Loss 0.4900	Prec@(1,5) (88.6%, 99.4%)
05/26 12:18:20AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 0.4936	Prec@(1,5) (88.5%, 99.4%)
05/26 12:18:20AM searchStage_trainer.py:260 [INFO] Valid: [ 49/49] Final Prec@1 88.5320%
05/26 12:18:20AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/26 12:18:20AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 88.5600%
05/26 12:18:20AM searchStage_main.py:73 [INFO] Final best Prec@1 = 88.5600%
05/26 12:18:20AM searchStage_main.py:74 [INFO] Final Best Genotype = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
