05/30 09:36:08PM parser.py:28 [INFO] 
05/30 09:36:08PM parser.py:29 [INFO] Parameters:
05/30 09:36:08PM parser.py:31 [INFO] DAG_PATH=results/search_Stage/CIFAR10/SCRATCH_SPEC_SPEC/EXP-20240530-213608/DAG
05/30 09:36:08PM parser.py:31 [INFO] ALPHA_LR=0.0003
05/30 09:36:08PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
05/30 09:36:08PM parser.py:31 [INFO] BATCH_SIZE=64
05/30 09:36:08PM parser.py:31 [INFO] CHECKPOINT_RESET=False
05/30 09:36:08PM parser.py:31 [INFO] DATA_PATH=../data/
05/30 09:36:08PM parser.py:31 [INFO] DATASET=CIFAR10
05/30 09:36:08PM parser.py:31 [INFO] EPOCHS=50
05/30 09:36:08PM parser.py:31 [INFO] EXP_NAME=EXP-20240530-213608
05/30 09:36:08PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 3)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('max_pool_3x3', 1), ('dil_conv_5x5', 3)], [('max_pool_3x3', 1), ('skip_connect', 4)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 2)], [('dil_conv_3x3', 0), ('sep_conv_5x5', 2)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], normal3_concat=range(2, 6))
05/30 09:36:08PM parser.py:31 [INFO] GPUS=[0]
05/30 09:36:08PM parser.py:31 [INFO] INIT_CHANNELS=16
05/30 09:36:08PM parser.py:31 [INFO] LAYERS=20
05/30 09:36:08PM parser.py:31 [INFO] LOCAL_RANK=0
05/30 09:36:08PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/30 09:36:08PM parser.py:31 [INFO] NAME=SCRATCH_SPEC_SPEC
05/30 09:36:08PM parser.py:31 [INFO] PATH=results/search_Stage/CIFAR10/SCRATCH_SPEC_SPEC/EXP-20240530-213608
05/30 09:36:08PM parser.py:31 [INFO] PRINT_FREQ=50
05/30 09:36:08PM parser.py:31 [INFO] RESUME_PATH=None
05/30 09:36:08PM parser.py:31 [INFO] SAVE=EXP
05/30 09:36:08PM parser.py:31 [INFO] SEED=2
05/30 09:36:08PM parser.py:31 [INFO] SHARE_STAGE=False
05/30 09:36:08PM parser.py:31 [INFO] SPEC_CELL=True
05/30 09:36:08PM parser.py:31 [INFO] TRAIN_PORTION=0.5
05/30 09:36:08PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
05/30 09:36:08PM parser.py:31 [INFO] W_LR=0.025
05/30 09:36:08PM parser.py:31 [INFO] W_LR_MIN=0.001
05/30 09:36:08PM parser.py:31 [INFO] W_MOMENTUM=0.9
05/30 09:36:08PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
05/30 09:36:08PM parser.py:31 [INFO] WORKERS=4
05/30 09:36:08PM parser.py:32 [INFO] 
05/30 09:36:09PM searchStage_trainer.py:103 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2500, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2500, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2497, 0.2503, 0.2501],
        [0.2499, 0.2499, 0.2502, 0.2500],
        [0.2502, 0.2499, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2498, 0.2503, 0.2498],
        [0.2499, 0.2501, 0.2501, 0.2500],
        [0.2500, 0.2501, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2501, 0.2499],
        [0.2497, 0.2504, 0.2499, 0.2501],
        [0.2501, 0.2501, 0.2501, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2501, 0.2499, 0.2502],
        [0.2502, 0.2499, 0.2499, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2496, 0.2502],
        [0.2504, 0.2499, 0.2499, 0.2498],
        [0.2501, 0.2501, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2500, 0.2501, 0.2503],
        [0.2501, 0.2498, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2497, 0.2503, 0.2502],
        [0.2497, 0.2501, 0.2500, 0.2502],
        [0.2504, 0.2500, 0.2498, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2500, 0.2502],
        [0.2500, 0.2503, 0.2497, 0.2499],
        [0.2497, 0.2498, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2503, 0.2498, 0.2500],
        [0.2497, 0.2501, 0.2501, 0.2501],
        [0.2498, 0.2499, 0.2500, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2502, 0.2501],
        [0.2503, 0.2500, 0.2499, 0.2498],
        [0.2502, 0.2501, 0.2497, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2505, 0.2499],
        [0.2497, 0.2500, 0.2501, 0.2501],
        [0.2501, 0.2503, 0.2496, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2497, 0.2502, 0.2501],
        [0.2497, 0.2500, 0.2502, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2498, 0.2502, 0.2502],
        [0.2500, 0.2500, 0.2498, 0.2502],
        [0.2496, 0.2503, 0.2503, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2502, 0.2498],
        [0.2497, 0.2498, 0.2504, 0.2501],
        [0.2501, 0.2500, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2503, 0.2497, 0.2498],
        [0.2501, 0.2495, 0.2503, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2501, 0.2499, 0.2498],
        [0.2502, 0.2500, 0.2496, 0.2502],
        [0.2498, 0.2502, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2499, 0.2502],
        [0.2499, 0.2504, 0.2497, 0.2500],
        [0.2497, 0.2500, 0.2501, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:36:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 2.3118 (2.2877)	Prec@(1,5) (14.9%, 62.1%)	
05/30 09:36:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 2.0200 (2.2160)	Prec@(1,5) (17.0%, 68.1%)	
05/30 09:37:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 1.7614 (2.1366)	Prec@(1,5) (20.0%, 72.0%)	
05/30 09:37:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 1.8453 (2.0696)	Prec@(1,5) (22.1%, 74.9%)	
05/30 09:37:34PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 1.7833 (2.0156)	Prec@(1,5) (24.0%, 77.1%)	
05/30 09:37:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 1.8734 (1.9756)	Prec@(1,5) (25.7%, 78.6%)	
05/30 09:38:08PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 1.7098 (1.9391)	Prec@(1,5) (27.2%, 80.0%)	
05/30 09:38:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 1.6670 (1.9129)	Prec@(1,5) (28.2%, 80.9%)	
05/30 09:38:22PM searchStage_trainer.py:221 [INFO] Train: [  0/49] Final Prec@1 28.1640%
05/30 09:38:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 1.7439	Prec@(1,5) (34.5%, 85.7%)
05/30 09:38:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 1.7391	Prec@(1,5) (35.0%, 86.1%)
05/30 09:38:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 1.7537	Prec@(1,5) (34.4%, 86.2%)
05/30 09:38:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 1.7484	Prec@(1,5) (34.5%, 86.5%)
05/30 09:38:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 1.7453	Prec@(1,5) (34.5%, 86.5%)
05/30 09:38:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 1.7423	Prec@(1,5) (34.8%, 86.5%)
05/30 09:38:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 1.7396	Prec@(1,5) (34.9%, 86.7%)
05/30 09:38:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 1.7379	Prec@(1,5) (34.9%, 86.8%)
05/30 09:38:44PM searchStage_trainer.py:260 [INFO] Valid: [  0/49] Final Prec@1 34.8640%
05/30 09:38:44PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 3)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('avg_pool_3x3', 6), ('skip_connect', 4)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:38:44PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 34.8640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2491, 0.2544, 0.2468, 0.2498],
        [0.2483, 0.2567, 0.2504, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2536, 0.2503, 0.2473],
        [0.2483, 0.2554, 0.2501, 0.2462],
        [0.2476, 0.2570, 0.2492, 0.2462]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2540, 0.2521, 0.2449],
        [0.2529, 0.2565, 0.2459, 0.2447],
        [0.2509, 0.2548, 0.2450, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2548, 0.2485, 0.2464],
        [0.2510, 0.2546, 0.2471, 0.2474],
        [0.2513, 0.2540, 0.2442, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2531, 0.2478, 0.2484],
        [0.2516, 0.2538, 0.2472, 0.2474],
        [0.2512, 0.2528, 0.2454, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2520, 0.2466, 0.2517],
        [0.2500, 0.2525, 0.2487, 0.2487],
        [0.2496, 0.2532, 0.2497, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2503, 0.2505, 0.2502],
        [0.2497, 0.2502, 0.2505, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2505, 0.2497],
        [0.2493, 0.2502, 0.2503, 0.2502],
        [0.2509, 0.2503, 0.2493, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2504, 0.2501, 0.2502],
        [0.2508, 0.2507, 0.2497, 0.2489],
        [0.2495, 0.2500, 0.2501, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2507, 0.2502, 0.2492],
        [0.2498, 0.2503, 0.2496, 0.2502],
        [0.2499, 0.2501, 0.2497, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2501, 0.2506, 0.2493],
        [0.2502, 0.2502, 0.2500, 0.2496],
        [0.2504, 0.2504, 0.2491, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2507, 0.2495],
        [0.2498, 0.2500, 0.2499, 0.2503],
        [0.2504, 0.2506, 0.2493, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2497, 0.2504, 0.2502],
        [0.2496, 0.2502, 0.2503, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2500, 0.2502, 0.2502],
        [0.2498, 0.2501, 0.2496, 0.2505],
        [0.2489, 0.2507, 0.2508, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2500, 0.2502, 0.2492],
        [0.2496, 0.2500, 0.2507, 0.2497],
        [0.2501, 0.2496, 0.2495, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2503, 0.2499, 0.2498],
        [0.2496, 0.2498, 0.2511, 0.2495],
        [0.2498, 0.2501, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2503, 0.2502, 0.2504],
        [0.2488, 0.2504, 0.2506, 0.2502],
        [0.2482, 0.2508, 0.2517, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2496, 0.2508, 0.2503],
        [0.2485, 0.2506, 0.2508, 0.2501],
        [0.2488, 0.2500, 0.2512, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:39:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 1.5677 (1.6161)	Prec@(1,5) (39.5%, 90.2%)	
05/30 09:39:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 1.4325 (1.6054)	Prec@(1,5) (40.1%, 90.0%)	
05/30 09:39:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 1.5257 (1.5866)	Prec@(1,5) (40.4%, 90.5%)	
05/30 09:39:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 1.5528 (1.5798)	Prec@(1,5) (40.8%, 90.6%)	
05/30 09:40:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 1.2307 (1.5688)	Prec@(1,5) (41.4%, 90.5%)	
05/30 09:40:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 1.4442 (1.5523)	Prec@(1,5) (42.1%, 90.7%)	
05/30 09:40:43PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 1.5023 (1.5330)	Prec@(1,5) (42.9%, 90.9%)	
05/30 09:40:57PM searchStage_trainer.py:211 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 1.3802 (1.5187)	Prec@(1,5) (43.3%, 91.1%)	
05/30 09:40:57PM searchStage_trainer.py:221 [INFO] Train: [  1/49] Final Prec@1 43.3000%
05/30 09:41:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 1.5780	Prec@(1,5) (42.6%, 89.6%)
05/30 09:41:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 1.5382	Prec@(1,5) (44.0%, 90.5%)
05/30 09:41:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 1.5272	Prec@(1,5) (44.5%, 90.9%)
05/30 09:41:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 1.5128	Prec@(1,5) (44.7%, 91.2%)
05/30 09:41:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 1.5132	Prec@(1,5) (44.7%, 91.2%)
05/30 09:41:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 1.5164	Prec@(1,5) (44.6%, 91.1%)
05/30 09:41:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 1.5131	Prec@(1,5) (44.8%, 91.1%)
05/30 09:41:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 1.5128	Prec@(1,5) (44.6%, 91.2%)
05/30 09:41:18PM searchStage_trainer.py:260 [INFO] Valid: [  1/49] Final Prec@1 44.6080%
05/30 09:41:18PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 3)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:41:19PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 44.6080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2515, 0.2575, 0.2447, 0.2464],
        [0.2472, 0.2603, 0.2509, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2578, 0.2490, 0.2431],
        [0.2493, 0.2599, 0.2492, 0.2415],
        [0.2481, 0.2653, 0.2423, 0.2443]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2563, 0.2523, 0.2422],
        [0.2553, 0.2610, 0.2424, 0.2413],
        [0.2532, 0.2597, 0.2414, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2587, 0.2466, 0.2426],
        [0.2524, 0.2585, 0.2433, 0.2459],
        [0.2545, 0.2571, 0.2388, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2569, 0.2488, 0.2437],
        [0.2524, 0.2577, 0.2436, 0.2463],
        [0.2525, 0.2562, 0.2404, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2543, 0.2449, 0.2514],
        [0.2507, 0.2547, 0.2465, 0.2481],
        [0.2506, 0.2564, 0.2476, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2506, 0.2503, 0.2500],
        [0.2494, 0.2506, 0.2505, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2508, 0.2494],
        [0.2495, 0.2500, 0.2504, 0.2501],
        [0.2513, 0.2508, 0.2489, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2502, 0.2503, 0.2501],
        [0.2511, 0.2513, 0.2493, 0.2483],
        [0.2497, 0.2502, 0.2498, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2509, 0.2502, 0.2488],
        [0.2500, 0.2504, 0.2491, 0.2505],
        [0.2503, 0.2503, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2503, 0.2507, 0.2487],
        [0.2506, 0.2502, 0.2492, 0.2499],
        [0.2506, 0.2508, 0.2490, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2501, 0.2504, 0.2495],
        [0.2499, 0.2501, 0.2500, 0.2500],
        [0.2505, 0.2508, 0.2492, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2497, 0.2505, 0.2503],
        [0.2493, 0.2503, 0.2506, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2504, 0.2506, 0.2497],
        [0.2495, 0.2503, 0.2495, 0.2507],
        [0.2485, 0.2509, 0.2509, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2499, 0.2502, 0.2489],
        [0.2496, 0.2502, 0.2509, 0.2493],
        [0.2499, 0.2495, 0.2492, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2504, 0.2498, 0.2500],
        [0.2492, 0.2501, 0.2518, 0.2490],
        [0.2497, 0.2501, 0.2496, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2505, 0.2504, 0.2509],
        [0.2476, 0.2507, 0.2515, 0.2502],
        [0.2470, 0.2513, 0.2531, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2494, 0.2515, 0.2503],
        [0.2473, 0.2507, 0.2517, 0.2503],
        [0.2478, 0.2502, 0.2523, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:41:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 1.2814 (1.4233)	Prec@(1,5) (47.7%, 92.7%)	
05/30 09:41:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 1.4457 (1.4019)	Prec@(1,5) (48.4%, 92.8%)	
05/30 09:42:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 1.4041 (1.3917)	Prec@(1,5) (49.2%, 92.9%)	
05/30 09:42:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 1.5001 (1.3800)	Prec@(1,5) (49.5%, 93.1%)	
05/30 09:42:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 1.0228 (1.3709)	Prec@(1,5) (50.0%, 93.2%)	
05/30 09:43:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 1.2301 (1.3644)	Prec@(1,5) (50.2%, 93.2%)	
05/30 09:43:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 1.1483 (1.3571)	Prec@(1,5) (50.5%, 93.3%)	
05/30 09:43:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 1.0892 (1.3513)	Prec@(1,5) (50.7%, 93.4%)	
05/30 09:43:32PM searchStage_trainer.py:221 [INFO] Train: [  2/49] Final Prec@1 50.6880%
05/30 09:43:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 1.3138	Prec@(1,5) (52.8%, 93.5%)
05/30 09:43:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 1.3153	Prec@(1,5) (52.7%, 93.7%)
05/30 09:43:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 1.3125	Prec@(1,5) (53.1%, 93.8%)
05/30 09:43:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 1.3166	Prec@(1,5) (53.3%, 93.8%)
05/30 09:43:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 1.3164	Prec@(1,5) (53.1%, 93.9%)
05/30 09:43:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 1.3141	Prec@(1,5) (53.0%, 94.0%)
05/30 09:43:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 1.3210	Prec@(1,5) (52.7%, 93.9%)
05/30 09:43:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 1.3202	Prec@(1,5) (52.6%, 93.9%)
05/30 09:43:53PM searchStage_trainer.py:260 [INFO] Valid: [  2/49] Final Prec@1 52.6560%
05/30 09:43:53PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:43:54PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 52.6560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2505, 0.2634, 0.2441, 0.2420],
        [0.2459, 0.2650, 0.2501, 0.2390]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2630, 0.2486, 0.2378],
        [0.2482, 0.2637, 0.2500, 0.2382],
        [0.2476, 0.2735, 0.2372, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2590, 0.2508, 0.2397],
        [0.2583, 0.2670, 0.2354, 0.2393],
        [0.2521, 0.2657, 0.2400, 0.2422]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2627, 0.2452, 0.2393],
        [0.2537, 0.2628, 0.2385, 0.2450],
        [0.2547, 0.2604, 0.2367, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2605, 0.2464, 0.2408],
        [0.2544, 0.2610, 0.2406, 0.2440],
        [0.2540, 0.2591, 0.2363, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2576, 0.2428, 0.2495],
        [0.2502, 0.2567, 0.2456, 0.2476],
        [0.2516, 0.2594, 0.2442, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2503, 0.2496],
        [0.2494, 0.2504, 0.2506, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2506, 0.2513, 0.2489],
        [0.2493, 0.2496, 0.2503, 0.2508],
        [0.2514, 0.2513, 0.2489, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2505, 0.2497],
        [0.2509, 0.2517, 0.2494, 0.2480],
        [0.2498, 0.2503, 0.2494, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2510, 0.2500, 0.2489],
        [0.2501, 0.2505, 0.2493, 0.2500],
        [0.2503, 0.2504, 0.2496, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2505, 0.2506, 0.2482],
        [0.2506, 0.2504, 0.2493, 0.2497],
        [0.2507, 0.2509, 0.2488, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2502, 0.2501, 0.2495],
        [0.2498, 0.2502, 0.2500, 0.2499],
        [0.2506, 0.2511, 0.2491, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2498, 0.2507, 0.2504],
        [0.2492, 0.2505, 0.2507, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2507, 0.2507, 0.2497],
        [0.2494, 0.2504, 0.2495, 0.2507],
        [0.2481, 0.2510, 0.2511, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2500, 0.2502, 0.2485],
        [0.2496, 0.2503, 0.2510, 0.2491],
        [0.2499, 0.2494, 0.2489, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2505, 0.2499, 0.2499],
        [0.2489, 0.2502, 0.2522, 0.2487],
        [0.2496, 0.2500, 0.2494, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2506, 0.2506, 0.2513],
        [0.2467, 0.2510, 0.2521, 0.2502],
        [0.2460, 0.2517, 0.2542, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2493, 0.2522, 0.2503],
        [0.2463, 0.2508, 0.2525, 0.2505],
        [0.2469, 0.2503, 0.2531, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:44:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 1.2270 (1.2436)	Prec@(1,5) (55.2%, 94.8%)	
05/30 09:44:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 1.1327 (1.2420)	Prec@(1,5) (55.4%, 94.9%)	
05/30 09:44:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 1.0802 (1.2398)	Prec@(1,5) (55.4%, 94.6%)	
05/30 09:45:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 1.2661 (1.2222)	Prec@(1,5) (55.9%, 94.8%)	
05/30 09:45:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 1.3753 (1.2206)	Prec@(1,5) (55.9%, 94.8%)	
05/30 09:45:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 1.2434 (1.2175)	Prec@(1,5) (56.2%, 94.8%)	
05/30 09:45:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 1.0553 (1.2085)	Prec@(1,5) (56.5%, 94.9%)	
05/30 09:46:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 1.0623 (1.2012)	Prec@(1,5) (56.8%, 95.0%)	
05/30 09:46:07PM searchStage_trainer.py:221 [INFO] Train: [  3/49] Final Prec@1 56.7840%
05/30 09:46:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 1.3083	Prec@(1,5) (53.1%, 94.2%)
05/30 09:46:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 1.3077	Prec@(1,5) (53.3%, 94.0%)
05/30 09:46:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 1.2934	Prec@(1,5) (53.9%, 93.8%)
05/30 09:46:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 1.2939	Prec@(1,5) (54.1%, 93.6%)
05/30 09:46:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 1.2904	Prec@(1,5) (54.1%, 93.7%)
05/30 09:46:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 1.2890	Prec@(1,5) (54.2%, 93.6%)
05/30 09:46:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 1.2813	Prec@(1,5) (54.5%, 93.7%)
05/30 09:46:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 1.2807	Prec@(1,5) (54.5%, 93.7%)
05/30 09:46:28PM searchStage_trainer.py:260 [INFO] Valid: [  3/49] Final Prec@1 54.5280%
05/30 09:46:28PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:46:28PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 54.5280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2687, 0.2427, 0.2393],
        [0.2446, 0.2712, 0.2493, 0.2348]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2686, 0.2480, 0.2328],
        [0.2479, 0.2670, 0.2488, 0.2364],
        [0.2468, 0.2797, 0.2344, 0.2391]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2618, 0.2488, 0.2366],
        [0.2577, 0.2719, 0.2343, 0.2362],
        [0.2528, 0.2698, 0.2361, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2655, 0.2441, 0.2379],
        [0.2538, 0.2662, 0.2358, 0.2442],
        [0.2551, 0.2640, 0.2352, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2638, 0.2446, 0.2378],
        [0.2550, 0.2637, 0.2383, 0.2429],
        [0.2536, 0.2612, 0.2356, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2597, 0.2409, 0.2488],
        [0.2504, 0.2586, 0.2431, 0.2479],
        [0.2521, 0.2626, 0.2432, 0.2422]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2505, 0.2495],
        [0.2494, 0.2503, 0.2507, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2508, 0.2515, 0.2486],
        [0.2493, 0.2495, 0.2503, 0.2510],
        [0.2515, 0.2513, 0.2489, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2503, 0.2505, 0.2496],
        [0.2509, 0.2518, 0.2494, 0.2478],
        [0.2498, 0.2503, 0.2494, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2510, 0.2499, 0.2488],
        [0.2503, 0.2506, 0.2493, 0.2498],
        [0.2503, 0.2504, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2506, 0.2506, 0.2480],
        [0.2507, 0.2505, 0.2492, 0.2496],
        [0.2509, 0.2510, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2504, 0.2500, 0.2491],
        [0.2499, 0.2503, 0.2500, 0.2499],
        [0.2507, 0.2511, 0.2489, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2498, 0.2507, 0.2506],
        [0.2489, 0.2506, 0.2510, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2508, 0.2508, 0.2497],
        [0.2493, 0.2504, 0.2495, 0.2508],
        [0.2480, 0.2511, 0.2512, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2501, 0.2503, 0.2479],
        [0.2496, 0.2502, 0.2510, 0.2491],
        [0.2497, 0.2493, 0.2488, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2506, 0.2499, 0.2498],
        [0.2487, 0.2503, 0.2526, 0.2484],
        [0.2496, 0.2499, 0.2493, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2507, 0.2507, 0.2516],
        [0.2460, 0.2511, 0.2526, 0.2503],
        [0.2453, 0.2519, 0.2550, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2491, 0.2528, 0.2503],
        [0.2455, 0.2508, 0.2531, 0.2506],
        [0.2463, 0.2504, 0.2538, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:46:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 1.1030 (1.1066)	Prec@(1,5) (61.1%, 95.9%)	
05/30 09:47:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 1.1724 (1.1165)	Prec@(1,5) (60.3%, 96.0%)	
05/30 09:47:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 1.2091 (1.1049)	Prec@(1,5) (60.7%, 95.8%)	
05/30 09:47:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 1.1703 (1.0947)	Prec@(1,5) (61.1%, 95.8%)	
05/30 09:47:54PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 1.2161 (1.0956)	Prec@(1,5) (60.9%, 95.9%)	
05/30 09:48:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 1.0437 (1.0832)	Prec@(1,5) (61.3%, 95.9%)	
05/30 09:48:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 0.9469 (1.0775)	Prec@(1,5) (61.6%, 96.0%)	
05/30 09:48:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 1.1615 (1.0721)	Prec@(1,5) (61.7%, 96.0%)	
05/30 09:48:41PM searchStage_trainer.py:221 [INFO] Train: [  4/49] Final Prec@1 61.7200%
05/30 09:48:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 1.1847	Prec@(1,5) (58.3%, 95.6%)
05/30 09:48:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 1.1735	Prec@(1,5) (59.3%, 95.6%)
05/30 09:48:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 1.1601	Prec@(1,5) (59.6%, 95.5%)
05/30 09:48:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 1.1662	Prec@(1,5) (59.4%, 95.5%)
05/30 09:48:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 1.1654	Prec@(1,5) (59.5%, 95.4%)
05/30 09:48:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 1.1698	Prec@(1,5) (59.2%, 95.4%)
05/30 09:49:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 1.1685	Prec@(1,5) (59.1%, 95.5%)
05/30 09:49:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 1.1637	Prec@(1,5) (59.3%, 95.5%)
05/30 09:49:03PM searchStage_trainer.py:260 [INFO] Valid: [  4/49] Final Prec@1 59.3440%
05/30 09:49:03PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:49:03PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 59.3440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2500, 0.2732, 0.2400, 0.2368],
        [0.2445, 0.2756, 0.2486, 0.2314]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2726, 0.2475, 0.2302],
        [0.2462, 0.2721, 0.2485, 0.2332],
        [0.2452, 0.2846, 0.2332, 0.2369]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2642, 0.2487, 0.2336],
        [0.2582, 0.2747, 0.2322, 0.2349],
        [0.2533, 0.2725, 0.2345, 0.2397]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2681, 0.2438, 0.2352],
        [0.2546, 0.2678, 0.2332, 0.2445],
        [0.2554, 0.2659, 0.2342, 0.2445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2662, 0.2441, 0.2358],
        [0.2563, 0.2658, 0.2357, 0.2423],
        [0.2539, 0.2630, 0.2344, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2613, 0.2397, 0.2477],
        [0.2510, 0.2602, 0.2419, 0.2469],
        [0.2531, 0.2642, 0.2409, 0.2418]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2506, 0.2495],
        [0.2493, 0.2504, 0.2508, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2507, 0.2514, 0.2485],
        [0.2492, 0.2495, 0.2504, 0.2509],
        [0.2515, 0.2513, 0.2488, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2507, 0.2505, 0.2493],
        [0.2509, 0.2520, 0.2494, 0.2477],
        [0.2498, 0.2503, 0.2493, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2511, 0.2501, 0.2485],
        [0.2501, 0.2506, 0.2493, 0.2500],
        [0.2504, 0.2505, 0.2496, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2507, 0.2504, 0.2482],
        [0.2509, 0.2505, 0.2491, 0.2495],
        [0.2509, 0.2511, 0.2487, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2498, 0.2491],
        [0.2500, 0.2504, 0.2501, 0.2496],
        [0.2508, 0.2511, 0.2488, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2498, 0.2508, 0.2505],
        [0.2487, 0.2507, 0.2511, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2510, 0.2510, 0.2495],
        [0.2492, 0.2503, 0.2495, 0.2510],
        [0.2478, 0.2512, 0.2514, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2502, 0.2505, 0.2474],
        [0.2496, 0.2503, 0.2510, 0.2491],
        [0.2496, 0.2492, 0.2487, 0.2525]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2506, 0.2500, 0.2498],
        [0.2484, 0.2505, 0.2529, 0.2482],
        [0.2495, 0.2499, 0.2492, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2508, 0.2509, 0.2518],
        [0.2454, 0.2512, 0.2530, 0.2505],
        [0.2447, 0.2522, 0.2556, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2490, 0.2532, 0.2503],
        [0.2449, 0.2508, 0.2536, 0.2507],
        [0.2460, 0.2505, 0.2543, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:49:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 0.9448 (0.9731)	Prec@(1,5) (65.5%, 96.9%)	
05/30 09:49:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 1.2308 (0.9733)	Prec@(1,5) (65.2%, 96.8%)	
05/30 09:49:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 0.8573 (0.9763)	Prec@(1,5) (65.1%, 96.9%)	
05/30 09:50:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 0.9953 (0.9817)	Prec@(1,5) (65.0%, 96.9%)	
05/30 09:50:29PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 1.0110 (0.9801)	Prec@(1,5) (65.1%, 96.9%)	
05/30 09:50:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 0.7739 (0.9722)	Prec@(1,5) (65.6%, 96.9%)	
05/30 09:51:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 1.1703 (0.9703)	Prec@(1,5) (65.7%, 96.9%)	
05/30 09:51:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 0.8227 (0.9693)	Prec@(1,5) (65.8%, 97.0%)	
05/30 09:51:17PM searchStage_trainer.py:221 [INFO] Train: [  5/49] Final Prec@1 65.7800%
05/30 09:51:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 1.0927	Prec@(1,5) (62.0%, 96.5%)
05/30 09:51:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 1.0944	Prec@(1,5) (61.8%, 96.4%)
05/30 09:51:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 1.0902	Prec@(1,5) (61.5%, 96.4%)
05/30 09:51:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 1.0980	Prec@(1,5) (61.2%, 96.2%)
05/30 09:51:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 1.0989	Prec@(1,5) (61.2%, 96.2%)
05/30 09:51:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 1.1008	Prec@(1,5) (61.1%, 96.2%)
05/30 09:51:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 1.1052	Prec@(1,5) (60.9%, 96.1%)
05/30 09:51:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 1.1052	Prec@(1,5) (60.8%, 96.1%)
05/30 09:51:38PM searchStage_trainer.py:260 [INFO] Valid: [  5/49] Final Prec@1 60.8200%
05/30 09:51:38PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:51:38PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 60.8200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2497, 0.2772, 0.2394, 0.2337],
        [0.2436, 0.2797, 0.2469, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2774, 0.2470, 0.2259],
        [0.2474, 0.2743, 0.2476, 0.2306],
        [0.2455, 0.2873, 0.2299, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2656, 0.2484, 0.2320],
        [0.2592, 0.2763, 0.2310, 0.2335],
        [0.2539, 0.2740, 0.2332, 0.2389]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2697, 0.2424, 0.2348],
        [0.2548, 0.2694, 0.2319, 0.2439],
        [0.2553, 0.2674, 0.2341, 0.2432]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2685, 0.2425, 0.2344],
        [0.2565, 0.2672, 0.2351, 0.2412],
        [0.2536, 0.2644, 0.2332, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2626, 0.2392, 0.2465],
        [0.2514, 0.2614, 0.2410, 0.2461],
        [0.2534, 0.2653, 0.2397, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2505, 0.2495],
        [0.2492, 0.2504, 0.2509, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2516, 0.2484],
        [0.2492, 0.2496, 0.2506, 0.2506],
        [0.2516, 0.2513, 0.2487, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2510, 0.2505, 0.2490],
        [0.2508, 0.2521, 0.2493, 0.2478],
        [0.2498, 0.2504, 0.2493, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2511, 0.2500, 0.2485],
        [0.2502, 0.2506, 0.2491, 0.2500],
        [0.2504, 0.2506, 0.2496, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2484],
        [0.2509, 0.2506, 0.2493, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2500, 0.2489],
        [0.2499, 0.2505, 0.2501, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2498, 0.2509, 0.2506],
        [0.2485, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2511, 0.2512, 0.2494],
        [0.2491, 0.2503, 0.2494, 0.2511],
        [0.2477, 0.2512, 0.2514, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2503, 0.2506, 0.2471],
        [0.2495, 0.2503, 0.2511, 0.2490],
        [0.2496, 0.2492, 0.2486, 0.2526]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2507, 0.2500, 0.2497],
        [0.2483, 0.2505, 0.2531, 0.2480],
        [0.2494, 0.2498, 0.2491, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2509, 0.2512, 0.2519],
        [0.2449, 0.2512, 0.2533, 0.2506],
        [0.2443, 0.2523, 0.2560, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2489, 0.2535, 0.2503],
        [0.2444, 0.2508, 0.2540, 0.2508],
        [0.2456, 0.2505, 0.2546, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:51:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 0.8476 (0.8973)	Prec@(1,5) (68.7%, 96.9%)	
05/30 09:52:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 0.7994 (0.8949)	Prec@(1,5) (68.7%, 97.2%)	
05/30 09:52:29PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 0.7350 (0.8967)	Prec@(1,5) (68.5%, 97.3%)	
05/30 09:52:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 0.8815 (0.8953)	Prec@(1,5) (68.7%, 97.4%)	
05/30 09:53:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 1.0695 (0.8884)	Prec@(1,5) (69.0%, 97.4%)	
05/30 09:53:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 0.7534 (0.8833)	Prec@(1,5) (69.1%, 97.4%)	
05/30 09:53:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 0.8080 (0.8808)	Prec@(1,5) (69.1%, 97.4%)	
05/30 09:53:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 0.8618 (0.8759)	Prec@(1,5) (69.3%, 97.4%)	
05/30 09:53:51PM searchStage_trainer.py:221 [INFO] Train: [  6/49] Final Prec@1 69.3080%
05/30 09:53:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 0.9758	Prec@(1,5) (66.4%, 97.3%)
05/30 09:53:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 0.9793	Prec@(1,5) (66.1%, 97.1%)
05/30 09:53:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 0.9780	Prec@(1,5) (65.9%, 97.0%)
05/30 09:54:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 0.9713	Prec@(1,5) (66.0%, 96.9%)
05/30 09:54:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 0.9708	Prec@(1,5) (66.0%, 96.9%)
05/30 09:54:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 0.9700	Prec@(1,5) (66.0%, 96.9%)
05/30 09:54:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 0.9742	Prec@(1,5) (65.9%, 96.9%)
05/30 09:54:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 0.9696	Prec@(1,5) (66.1%, 96.9%)
05/30 09:54:12PM searchStage_trainer.py:260 [INFO] Valid: [  6/49] Final Prec@1 66.0760%
05/30 09:54:12PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:54:13PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 66.0760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2509, 0.2790, 0.2377, 0.2324],
        [0.2432, 0.2825, 0.2464, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2802, 0.2468, 0.2234],
        [0.2465, 0.2761, 0.2483, 0.2290],
        [0.2464, 0.2883, 0.2282, 0.2371]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2673, 0.2490, 0.2299],
        [0.2598, 0.2770, 0.2304, 0.2328],
        [0.2548, 0.2746, 0.2318, 0.2388]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2703, 0.2419, 0.2342],
        [0.2554, 0.2705, 0.2304, 0.2436],
        [0.2559, 0.2682, 0.2332, 0.2427]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2698, 0.2418, 0.2340],
        [0.2566, 0.2687, 0.2344, 0.2403],
        [0.2535, 0.2650, 0.2329, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2633, 0.2385, 0.2467],
        [0.2518, 0.2624, 0.2404, 0.2454],
        [0.2538, 0.2661, 0.2391, 0.2410]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2509, 0.2505, 0.2496],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2507, 0.2516, 0.2482],
        [0.2492, 0.2495, 0.2506, 0.2507],
        [0.2516, 0.2514, 0.2486, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2509, 0.2505, 0.2492],
        [0.2508, 0.2521, 0.2494, 0.2477],
        [0.2497, 0.2504, 0.2494, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2512, 0.2499, 0.2484],
        [0.2502, 0.2506, 0.2491, 0.2501],
        [0.2504, 0.2506, 0.2496, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2506, 0.2500, 0.2486],
        [0.2509, 0.2507, 0.2494, 0.2490],
        [0.2510, 0.2512, 0.2485, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2505, 0.2499, 0.2490],
        [0.2498, 0.2505, 0.2502, 0.2495],
        [0.2508, 0.2512, 0.2487, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2498, 0.2509, 0.2505],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2512, 0.2513, 0.2493],
        [0.2490, 0.2505, 0.2495, 0.2510],
        [0.2476, 0.2512, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2503, 0.2507, 0.2468],
        [0.2495, 0.2504, 0.2511, 0.2490],
        [0.2495, 0.2492, 0.2485, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2507, 0.2501, 0.2496],
        [0.2483, 0.2506, 0.2532, 0.2479],
        [0.2494, 0.2498, 0.2491, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2509, 0.2513, 0.2520],
        [0.2446, 0.2513, 0.2535, 0.2507],
        [0.2440, 0.2524, 0.2563, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2488, 0.2538, 0.2504],
        [0.2440, 0.2508, 0.2543, 0.2508],
        [0.2454, 0.2505, 0.2548, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:54:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 0.7219 (0.8384)	Prec@(1,5) (70.5%, 97.7%)	
05/30 09:54:47PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 0.8393 (0.8271)	Prec@(1,5) (71.3%, 97.6%)	
05/30 09:55:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 0.9816 (0.8225)	Prec@(1,5) (71.2%, 97.7%)	
05/30 09:55:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 0.8695 (0.8236)	Prec@(1,5) (70.9%, 97.9%)	
05/30 09:55:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 0.9594 (0.8167)	Prec@(1,5) (71.1%, 97.9%)	
05/30 09:55:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 0.7623 (0.8166)	Prec@(1,5) (71.1%, 97.9%)	
05/30 09:56:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 0.7629 (0.8170)	Prec@(1,5) (71.2%, 97.9%)	
05/30 09:56:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 0.5999 (0.8153)	Prec@(1,5) (71.3%, 97.9%)	
05/30 09:56:27PM searchStage_trainer.py:221 [INFO] Train: [  7/49] Final Prec@1 71.2600%
05/30 09:56:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 0.8765	Prec@(1,5) (70.0%, 96.6%)
05/30 09:56:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 0.8701	Prec@(1,5) (70.3%, 96.7%)
05/30 09:56:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 0.8591	Prec@(1,5) (70.6%, 97.0%)
05/30 09:56:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 0.8585	Prec@(1,5) (70.6%, 97.0%)
05/30 09:56:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 0.8546	Prec@(1,5) (70.7%, 97.1%)
05/30 09:56:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 0.8609	Prec@(1,5) (70.5%, 97.0%)
05/30 09:56:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 0.8584	Prec@(1,5) (70.6%, 97.0%)
05/30 09:56:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 0.8569	Prec@(1,5) (70.6%, 97.0%)
05/30 09:56:48PM searchStage_trainer.py:260 [INFO] Valid: [  7/49] Final Prec@1 70.5760%
05/30 09:56:48PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:56:48PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 70.5760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2510, 0.2810, 0.2368, 0.2312],
        [0.2423, 0.2843, 0.2465, 0.2269]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2821, 0.2473, 0.2211],
        [0.2469, 0.2770, 0.2477, 0.2284],
        [0.2465, 0.2891, 0.2276, 0.2368]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2682, 0.2483, 0.2302],
        [0.2598, 0.2776, 0.2301, 0.2325],
        [0.2546, 0.2753, 0.2320, 0.2380]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2707, 0.2419, 0.2335],
        [0.2557, 0.2708, 0.2301, 0.2434],
        [0.2562, 0.2685, 0.2326, 0.2427]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2706, 0.2408, 0.2339],
        [0.2566, 0.2693, 0.2344, 0.2397],
        [0.2536, 0.2653, 0.2325, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2639, 0.2379, 0.2462],
        [0.2520, 0.2628, 0.2397, 0.2455],
        [0.2536, 0.2664, 0.2393, 0.2408]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2509, 0.2505, 0.2496],
        [0.2494, 0.2505, 0.2508, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2509, 0.2518, 0.2480],
        [0.2492, 0.2496, 0.2506, 0.2507],
        [0.2516, 0.2514, 0.2485, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2510, 0.2505, 0.2491],
        [0.2508, 0.2521, 0.2493, 0.2478],
        [0.2497, 0.2504, 0.2494, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2512, 0.2500, 0.2482],
        [0.2502, 0.2506, 0.2490, 0.2501],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2506, 0.2500, 0.2486],
        [0.2509, 0.2507, 0.2495, 0.2488],
        [0.2510, 0.2512, 0.2484, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2505, 0.2502, 0.2494],
        [0.2507, 0.2512, 0.2487, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2510, 0.2506],
        [0.2484, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2512, 0.2514, 0.2492],
        [0.2490, 0.2505, 0.2495, 0.2510],
        [0.2475, 0.2512, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2503, 0.2507, 0.2466],
        [0.2495, 0.2504, 0.2512, 0.2489],
        [0.2495, 0.2491, 0.2485, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2507, 0.2501, 0.2496],
        [0.2482, 0.2506, 0.2533, 0.2478],
        [0.2494, 0.2498, 0.2490, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2509, 0.2514, 0.2522],
        [0.2443, 0.2513, 0.2536, 0.2507],
        [0.2438, 0.2525, 0.2566, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2488, 0.2541, 0.2504],
        [0.2437, 0.2509, 0.2545, 0.2509],
        [0.2453, 0.2506, 0.2550, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:57:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 0.6074 (0.7550)	Prec@(1,5) (74.3%, 98.2%)	
05/30 09:57:22PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 0.7162 (0.7359)	Prec@(1,5) (74.7%, 98.3%)	
05/30 09:57:39PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 0.8330 (0.7594)	Prec@(1,5) (73.9%, 98.1%)	
05/30 09:57:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 0.8365 (0.7542)	Prec@(1,5) (74.0%, 98.2%)	
05/30 09:58:14PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 0.8896 (0.7531)	Prec@(1,5) (73.9%, 98.2%)	
05/30 09:58:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 0.9766 (0.7493)	Prec@(1,5) (74.0%, 98.2%)	
05/30 09:58:48PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 0.5268 (0.7493)	Prec@(1,5) (74.0%, 98.2%)	
05/30 09:59:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 0.8049 (0.7485)	Prec@(1,5) (73.9%, 98.2%)	
05/30 09:59:02PM searchStage_trainer.py:221 [INFO] Train: [  8/49] Final Prec@1 73.9440%
05/30 09:59:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 0.7885	Prec@(1,5) (73.0%, 97.9%)
05/30 09:59:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 0.7818	Prec@(1,5) (73.0%, 97.8%)
05/30 09:59:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 0.7686	Prec@(1,5) (73.5%, 97.9%)
05/30 09:59:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 0.7645	Prec@(1,5) (73.6%, 98.0%)
05/30 09:59:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 0.7660	Prec@(1,5) (73.6%, 97.9%)
05/30 09:59:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 0.7698	Prec@(1,5) (73.4%, 97.9%)
05/30 09:59:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 0.7774	Prec@(1,5) (73.0%, 97.9%)
05/30 09:59:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 0.7775	Prec@(1,5) (73.0%, 97.9%)
05/30 09:59:23PM searchStage_trainer.py:260 [INFO] Valid: [  8/49] Final Prec@1 73.0200%
05/30 09:59:23PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 09:59:23PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 73.0200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2507, 0.2819, 0.2372, 0.2302],
        [0.2425, 0.2849, 0.2461, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2830, 0.2477, 0.2201],
        [0.2476, 0.2778, 0.2472, 0.2274],
        [0.2465, 0.2890, 0.2278, 0.2367]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2686, 0.2477, 0.2298],
        [0.2602, 0.2775, 0.2295, 0.2328],
        [0.2550, 0.2755, 0.2322, 0.2374]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2710, 0.2411, 0.2337],
        [0.2555, 0.2709, 0.2306, 0.2429],
        [0.2559, 0.2687, 0.2326, 0.2427]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2707, 0.2410, 0.2335],
        [0.2566, 0.2696, 0.2343, 0.2396],
        [0.2535, 0.2654, 0.2325, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2641, 0.2378, 0.2463],
        [0.2521, 0.2630, 0.2396, 0.2453],
        [0.2535, 0.2666, 0.2393, 0.2406]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2509, 0.2504, 0.2497],
        [0.2494, 0.2506, 0.2508, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2509, 0.2519, 0.2480],
        [0.2493, 0.2494, 0.2506, 0.2507],
        [0.2517, 0.2514, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2510, 0.2505, 0.2492],
        [0.2508, 0.2521, 0.2493, 0.2478],
        [0.2497, 0.2505, 0.2495, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2513, 0.2501, 0.2482],
        [0.2503, 0.2506, 0.2490, 0.2502],
        [0.2504, 0.2506, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2506, 0.2500, 0.2486],
        [0.2510, 0.2507, 0.2495, 0.2488],
        [0.2510, 0.2512, 0.2485, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2499, 0.2491],
        [0.2498, 0.2505, 0.2503, 0.2494],
        [0.2508, 0.2512, 0.2487, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2498, 0.2510, 0.2506],
        [0.2483, 0.2509, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2513, 0.2515, 0.2492],
        [0.2489, 0.2505, 0.2495, 0.2511],
        [0.2475, 0.2512, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2504, 0.2508, 0.2464],
        [0.2495, 0.2504, 0.2511, 0.2490],
        [0.2495, 0.2491, 0.2485, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2507, 0.2501, 0.2496],
        [0.2482, 0.2506, 0.2534, 0.2477],
        [0.2494, 0.2498, 0.2490, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2509, 0.2514, 0.2523],
        [0.2441, 0.2514, 0.2538, 0.2507],
        [0.2437, 0.2526, 0.2567, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2487, 0.2542, 0.2504],
        [0.2435, 0.2509, 0.2547, 0.2509],
        [0.2452, 0.2505, 0.2551, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 09:59:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 0.7711 (0.6971)	Prec@(1,5) (75.5%, 98.9%)	
05/30 09:59:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 0.9264 (0.7165)	Prec@(1,5) (75.3%, 98.6%)	
05/30 10:00:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 0.6552 (0.7099)	Prec@(1,5) (75.3%, 98.6%)	
05/30 10:00:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 0.5905 (0.7145)	Prec@(1,5) (75.2%, 98.5%)	
05/30 10:00:49PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 0.7273 (0.7101)	Prec@(1,5) (75.4%, 98.6%)	
05/30 10:01:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 0.7491 (0.7089)	Prec@(1,5) (75.4%, 98.6%)	
05/30 10:01:23PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 0.5590 (0.7089)	Prec@(1,5) (75.4%, 98.5%)	
05/30 10:01:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 0.5139 (0.7053)	Prec@(1,5) (75.4%, 98.4%)	
05/30 10:01:37PM searchStage_trainer.py:221 [INFO] Train: [  9/49] Final Prec@1 75.4040%
05/30 10:01:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 0.9402	Prec@(1,5) (68.2%, 96.3%)
05/30 10:01:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 0.9022	Prec@(1,5) (69.7%, 96.6%)
05/30 10:01:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 0.8967	Prec@(1,5) (69.9%, 96.7%)
05/30 10:01:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 0.8960	Prec@(1,5) (69.9%, 96.6%)
05/30 10:01:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 0.9049	Prec@(1,5) (69.9%, 96.5%)
05/30 10:01:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 0.9015	Prec@(1,5) (70.0%, 96.6%)
05/30 10:01:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 0.9038	Prec@(1,5) (69.8%, 96.6%)
05/30 10:01:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 0.9053	Prec@(1,5) (69.8%, 96.5%)
05/30 10:01:58PM searchStage_trainer.py:260 [INFO] Valid: [  9/49] Final Prec@1 69.8240%
05/30 10:01:58PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:01:58PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 73.0200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2820, 0.2376, 0.2302],
        [0.2420, 0.2857, 0.2465, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2833, 0.2483, 0.2188],
        [0.2474, 0.2784, 0.2471, 0.2271],
        [0.2464, 0.2889, 0.2278, 0.2368]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2690, 0.2483, 0.2293],
        [0.2601, 0.2776, 0.2294, 0.2329],
        [0.2549, 0.2756, 0.2322, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2710, 0.2412, 0.2335],
        [0.2558, 0.2708, 0.2305, 0.2429],
        [0.2558, 0.2686, 0.2329, 0.2428]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2707, 0.2407, 0.2333],
        [0.2566, 0.2696, 0.2340, 0.2398],
        [0.2536, 0.2652, 0.2327, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2643, 0.2377, 0.2465],
        [0.2519, 0.2632, 0.2397, 0.2452],
        [0.2532, 0.2666, 0.2396, 0.2406]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2509, 0.2504, 0.2497],
        [0.2494, 0.2505, 0.2508, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2520, 0.2480],
        [0.2492, 0.2496, 0.2507, 0.2505],
        [0.2516, 0.2514, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2509, 0.2504, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2478],
        [0.2497, 0.2505, 0.2495, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2513, 0.2501, 0.2481],
        [0.2503, 0.2506, 0.2490, 0.2502],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2500, 0.2486],
        [0.2510, 0.2507, 0.2494, 0.2489],
        [0.2510, 0.2512, 0.2485, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2499, 0.2490],
        [0.2498, 0.2505, 0.2502, 0.2495],
        [0.2508, 0.2512, 0.2487, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2498, 0.2510, 0.2506],
        [0.2483, 0.2509, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2514, 0.2515, 0.2491],
        [0.2489, 0.2505, 0.2495, 0.2511],
        [0.2474, 0.2512, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2504, 0.2508, 0.2462],
        [0.2496, 0.2504, 0.2512, 0.2489],
        [0.2495, 0.2491, 0.2485, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2507, 0.2500, 0.2496],
        [0.2482, 0.2507, 0.2535, 0.2477],
        [0.2494, 0.2497, 0.2489, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2509, 0.2515, 0.2524],
        [0.2440, 0.2514, 0.2539, 0.2508],
        [0.2436, 0.2526, 0.2569, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2487, 0.2544, 0.2504],
        [0.2433, 0.2509, 0.2549, 0.2509],
        [0.2451, 0.2505, 0.2552, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:02:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 0.6208 (0.6183)	Prec@(1,5) (78.7%, 98.8%)	
05/30 10:02:33PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 0.6605 (0.6466)	Prec@(1,5) (77.5%, 98.7%)	
05/30 10:02:50PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 0.5091 (0.6475)	Prec@(1,5) (77.5%, 98.8%)	
05/30 10:03:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 0.6473 (0.6547)	Prec@(1,5) (77.3%, 98.7%)	
05/30 10:03:24PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 0.6943 (0.6574)	Prec@(1,5) (77.2%, 98.6%)	
05/30 10:03:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 0.6311 (0.6566)	Prec@(1,5) (77.1%, 98.7%)	
05/30 10:03:57PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 0.6222 (0.6605)	Prec@(1,5) (77.0%, 98.6%)	
05/30 10:04:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 0.9174 (0.6624)	Prec@(1,5) (77.0%, 98.6%)	
05/30 10:04:11PM searchStage_trainer.py:221 [INFO] Train: [ 10/49] Final Prec@1 76.9720%
05/30 10:04:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 0.7622	Prec@(1,5) (74.0%, 98.1%)
05/30 10:04:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 0.7705	Prec@(1,5) (73.8%, 97.7%)
05/30 10:04:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 0.7639	Prec@(1,5) (73.9%, 97.8%)
05/30 10:04:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 0.7517	Prec@(1,5) (74.4%, 97.9%)
05/30 10:04:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 0.7520	Prec@(1,5) (74.4%, 98.0%)
05/30 10:04:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 0.7521	Prec@(1,5) (74.5%, 98.0%)
05/30 10:04:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 0.7485	Prec@(1,5) (74.5%, 98.0%)
05/30 10:04:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 0.7496	Prec@(1,5) (74.4%, 98.1%)
05/30 10:04:33PM searchStage_trainer.py:260 [INFO] Valid: [ 10/49] Final Prec@1 74.4320%
05/30 10:04:33PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:04:33PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 74.4320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2503, 0.2818, 0.2375, 0.2304],
        [0.2425, 0.2858, 0.2464, 0.2253]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2835, 0.2483, 0.2182],
        [0.2479, 0.2786, 0.2474, 0.2262],
        [0.2465, 0.2888, 0.2276, 0.2372]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2691, 0.2484, 0.2288],
        [0.2603, 0.2775, 0.2294, 0.2328],
        [0.2551, 0.2755, 0.2320, 0.2374]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2709, 0.2415, 0.2333],
        [0.2557, 0.2708, 0.2309, 0.2426],
        [0.2557, 0.2684, 0.2327, 0.2432]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2705, 0.2408, 0.2333],
        [0.2567, 0.2695, 0.2340, 0.2399],
        [0.2535, 0.2651, 0.2329, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2643, 0.2379, 0.2465],
        [0.2518, 0.2631, 0.2397, 0.2454],
        [0.2531, 0.2665, 0.2398, 0.2405]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2509, 0.2504, 0.2497],
        [0.2494, 0.2505, 0.2508, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2520, 0.2480],
        [0.2491, 0.2496, 0.2507, 0.2505],
        [0.2517, 0.2514, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2509, 0.2504, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2478],
        [0.2497, 0.2505, 0.2495, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2506, 0.2489, 0.2503],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2501, 0.2485],
        [0.2510, 0.2507, 0.2494, 0.2489],
        [0.2509, 0.2512, 0.2485, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2499, 0.2490],
        [0.2498, 0.2505, 0.2502, 0.2494],
        [0.2508, 0.2512, 0.2487, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2497, 0.2511, 0.2506],
        [0.2483, 0.2509, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2514, 0.2515, 0.2491],
        [0.2488, 0.2506, 0.2495, 0.2511],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2504, 0.2508, 0.2460],
        [0.2496, 0.2504, 0.2512, 0.2489],
        [0.2495, 0.2491, 0.2484, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2508, 0.2500, 0.2496],
        [0.2482, 0.2507, 0.2535, 0.2476],
        [0.2494, 0.2497, 0.2489, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2510, 0.2515, 0.2525],
        [0.2438, 0.2514, 0.2540, 0.2508],
        [0.2435, 0.2527, 0.2569, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2486, 0.2545, 0.2504],
        [0.2432, 0.2509, 0.2550, 0.2509],
        [0.2451, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:04:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 0.5815 (0.6003)	Prec@(1,5) (78.9%, 98.7%)	
05/30 10:05:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 0.5542 (0.6186)	Prec@(1,5) (78.2%, 98.5%)	
05/30 10:05:24PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 0.6932 (0.6225)	Prec@(1,5) (78.0%, 98.7%)	
05/30 10:05:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 0.6530 (0.6291)	Prec@(1,5) (77.9%, 98.6%)	
05/30 10:05:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 0.7222 (0.6278)	Prec@(1,5) (78.0%, 98.6%)	
05/30 10:06:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 0.5551 (0.6307)	Prec@(1,5) (78.1%, 98.6%)	
05/30 10:06:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 0.6023 (0.6326)	Prec@(1,5) (78.0%, 98.7%)	
05/30 10:06:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 0.6454 (0.6285)	Prec@(1,5) (78.2%, 98.7%)	
05/30 10:06:46PM searchStage_trainer.py:221 [INFO] Train: [ 11/49] Final Prec@1 78.1760%
05/30 10:06:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 0.7243	Prec@(1,5) (75.2%, 98.2%)
05/30 10:06:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 0.7118	Prec@(1,5) (75.7%, 98.1%)
05/30 10:06:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 0.7134	Prec@(1,5) (75.6%, 98.1%)
05/30 10:06:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 0.7066	Prec@(1,5) (75.7%, 98.2%)
05/30 10:07:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 0.7095	Prec@(1,5) (75.6%, 98.2%)
05/30 10:07:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 0.7059	Prec@(1,5) (75.7%, 98.2%)
05/30 10:07:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 0.7066	Prec@(1,5) (75.8%, 98.2%)
05/30 10:07:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 0.7071	Prec@(1,5) (75.8%, 98.2%)
05/30 10:07:08PM searchStage_trainer.py:260 [INFO] Valid: [ 11/49] Final Prec@1 75.8200%
05/30 10:07:08PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:07:08PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 75.8200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2500, 0.2821, 0.2383, 0.2296],
        [0.2420, 0.2858, 0.2463, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2834, 0.2485, 0.2185],
        [0.2474, 0.2791, 0.2476, 0.2259],
        [0.2464, 0.2886, 0.2278, 0.2371]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2696, 0.2489, 0.2287],
        [0.2604, 0.2774, 0.2295, 0.2328],
        [0.2549, 0.2753, 0.2322, 0.2375]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2710, 0.2415, 0.2330],
        [0.2557, 0.2707, 0.2307, 0.2430],
        [0.2557, 0.2682, 0.2329, 0.2432]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2706, 0.2410, 0.2329],
        [0.2568, 0.2694, 0.2337, 0.2401],
        [0.2534, 0.2649, 0.2330, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2642, 0.2378, 0.2467],
        [0.2518, 0.2630, 0.2398, 0.2454],
        [0.2532, 0.2664, 0.2398, 0.2407]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2508, 0.2503, 0.2498],
        [0.2495, 0.2505, 0.2508, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2511, 0.2520, 0.2479],
        [0.2490, 0.2496, 0.2508, 0.2506],
        [0.2516, 0.2514, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2509, 0.2504, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2505, 0.2496, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2505, 0.2489, 0.2503],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2507, 0.2494, 0.2490],
        [0.2510, 0.2512, 0.2485, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2507, 0.2500, 0.2489],
        [0.2498, 0.2505, 0.2502, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2497, 0.2511, 0.2506],
        [0.2483, 0.2509, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2514, 0.2516, 0.2491],
        [0.2488, 0.2505, 0.2495, 0.2512],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2504, 0.2508, 0.2459],
        [0.2496, 0.2504, 0.2512, 0.2489],
        [0.2495, 0.2491, 0.2484, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2508, 0.2500, 0.2496],
        [0.2482, 0.2507, 0.2535, 0.2476],
        [0.2494, 0.2497, 0.2489, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2510, 0.2515, 0.2525],
        [0.2437, 0.2515, 0.2540, 0.2508],
        [0.2434, 0.2527, 0.2570, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2486, 0.2547, 0.2504],
        [0.2431, 0.2509, 0.2551, 0.2510],
        [0.2451, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:07:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 0.5672 (0.5818)	Prec@(1,5) (79.7%, 99.2%)	
05/30 10:07:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 0.5971 (0.5958)	Prec@(1,5) (79.7%, 98.9%)	
05/30 10:07:59PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 0.4554 (0.5864)	Prec@(1,5) (80.1%, 98.9%)	
05/30 10:08:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 0.6453 (0.5935)	Prec@(1,5) (79.7%, 98.9%)	
05/30 10:08:33PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 0.6681 (0.5927)	Prec@(1,5) (79.6%, 98.9%)	
05/30 10:08:50PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 0.6346 (0.5925)	Prec@(1,5) (79.5%, 98.9%)	
05/30 10:09:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 0.5575 (0.5950)	Prec@(1,5) (79.4%, 98.9%)	
05/30 10:09:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 0.8574 (0.5978)	Prec@(1,5) (79.4%, 98.9%)	
05/30 10:09:21PM searchStage_trainer.py:221 [INFO] Train: [ 12/49] Final Prec@1 79.3480%
05/30 10:09:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 0.7373	Prec@(1,5) (75.0%, 98.1%)
05/30 10:09:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 0.7417	Prec@(1,5) (74.7%, 98.1%)
05/30 10:09:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 0.7320	Prec@(1,5) (75.0%, 98.2%)
05/30 10:09:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 0.7249	Prec@(1,5) (75.2%, 98.3%)
05/30 10:09:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 0.7164	Prec@(1,5) (75.5%, 98.3%)
05/30 10:09:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 0.7069	Prec@(1,5) (75.9%, 98.3%)
05/30 10:09:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 0.7050	Prec@(1,5) (75.9%, 98.4%)
05/30 10:09:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 0.7001	Prec@(1,5) (76.1%, 98.4%)
05/30 10:09:42PM searchStage_trainer.py:260 [INFO] Valid: [ 12/49] Final Prec@1 76.0720%
05/30 10:09:42PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:09:42PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 76.0720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2498, 0.2825, 0.2382, 0.2295],
        [0.2418, 0.2858, 0.2464, 0.2260]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2833, 0.2489, 0.2183],
        [0.2477, 0.2789, 0.2477, 0.2256],
        [0.2464, 0.2884, 0.2279, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2698, 0.2492, 0.2282],
        [0.2604, 0.2771, 0.2295, 0.2330],
        [0.2549, 0.2751, 0.2324, 0.2376]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2710, 0.2413, 0.2332],
        [0.2557, 0.2706, 0.2308, 0.2430],
        [0.2556, 0.2681, 0.2331, 0.2433]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2706, 0.2410, 0.2330],
        [0.2567, 0.2694, 0.2339, 0.2400],
        [0.2533, 0.2646, 0.2331, 0.2489]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2642, 0.2378, 0.2467],
        [0.2516, 0.2629, 0.2400, 0.2455],
        [0.2531, 0.2662, 0.2400, 0.2408]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2509, 0.2504, 0.2498],
        [0.2494, 0.2505, 0.2508, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2520, 0.2480],
        [0.2490, 0.2496, 0.2508, 0.2507],
        [0.2516, 0.2514, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2509, 0.2505, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2505, 0.2496, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2513, 0.2502, 0.2480],
        [0.2503, 0.2505, 0.2489, 0.2503],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2493, 0.2490],
        [0.2509, 0.2512, 0.2485, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2505, 0.2502, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2497, 0.2511, 0.2506],
        [0.2483, 0.2509, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2515, 0.2516, 0.2491],
        [0.2487, 0.2506, 0.2495, 0.2512],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2505, 0.2508, 0.2457],
        [0.2496, 0.2504, 0.2512, 0.2489],
        [0.2495, 0.2490, 0.2484, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2508, 0.2500, 0.2496],
        [0.2482, 0.2507, 0.2536, 0.2476],
        [0.2494, 0.2497, 0.2488, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2510, 0.2516, 0.2526],
        [0.2436, 0.2515, 0.2541, 0.2508],
        [0.2434, 0.2527, 0.2571, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2486, 0.2548, 0.2504],
        [0.2430, 0.2509, 0.2552, 0.2510],
        [0.2451, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:10:00PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 0.4866 (0.5570)	Prec@(1,5) (81.4%, 99.2%)	
05/30 10:10:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 0.5893 (0.5609)	Prec@(1,5) (81.2%, 99.2%)	
05/30 10:10:34PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 0.5818 (0.5509)	Prec@(1,5) (81.4%, 99.1%)	
05/30 10:10:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 0.7412 (0.5570)	Prec@(1,5) (80.8%, 99.0%)	
05/30 10:11:08PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 0.7945 (0.5600)	Prec@(1,5) (80.7%, 98.9%)	
05/30 10:11:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 0.7296 (0.5673)	Prec@(1,5) (80.5%, 98.9%)	
05/30 10:11:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 0.5832 (0.5659)	Prec@(1,5) (80.6%, 98.9%)	
05/30 10:11:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 0.3187 (0.5638)	Prec@(1,5) (80.7%, 98.9%)	
05/30 10:11:56PM searchStage_trainer.py:221 [INFO] Train: [ 13/49] Final Prec@1 80.6680%
05/30 10:11:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 0.6318	Prec@(1,5) (79.1%, 98.3%)
05/30 10:12:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 0.6399	Prec@(1,5) (78.0%, 98.5%)
05/30 10:12:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 0.6306	Prec@(1,5) (78.2%, 98.5%)
05/30 10:12:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 0.6377	Prec@(1,5) (78.0%, 98.5%)
05/30 10:12:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 0.6428	Prec@(1,5) (77.8%, 98.4%)
05/30 10:12:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 0.6455	Prec@(1,5) (77.8%, 98.4%)
05/30 10:12:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 0.6451	Prec@(1,5) (77.8%, 98.4%)
05/30 10:12:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 0.6413	Prec@(1,5) (78.0%, 98.4%)
05/30 10:12:17PM searchStage_trainer.py:260 [INFO] Valid: [ 13/49] Final Prec@1 77.9760%
05/30 10:12:17PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:12:17PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.9760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2498, 0.2824, 0.2386, 0.2292],
        [0.2418, 0.2855, 0.2464, 0.2264]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2834, 0.2493, 0.2177],
        [0.2475, 0.2791, 0.2475, 0.2259],
        [0.2464, 0.2882, 0.2280, 0.2374]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2703, 0.2492, 0.2274],
        [0.2603, 0.2769, 0.2296, 0.2332],
        [0.2550, 0.2748, 0.2324, 0.2378]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2710, 0.2411, 0.2334],
        [0.2557, 0.2704, 0.2309, 0.2429],
        [0.2556, 0.2679, 0.2332, 0.2434]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2706, 0.2408, 0.2331],
        [0.2568, 0.2692, 0.2341, 0.2399],
        [0.2533, 0.2644, 0.2332, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2640, 0.2379, 0.2468],
        [0.2518, 0.2628, 0.2400, 0.2455],
        [0.2531, 0.2660, 0.2400, 0.2410]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2509, 0.2504, 0.2498],
        [0.2494, 0.2505, 0.2508, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2520, 0.2480],
        [0.2490, 0.2496, 0.2507, 0.2507],
        [0.2517, 0.2514, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2509, 0.2505, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2505, 0.2496, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2505, 0.2489, 0.2503],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2501, 0.2485],
        [0.2510, 0.2506, 0.2493, 0.2490],
        [0.2509, 0.2512, 0.2485, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2505, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2497, 0.2511, 0.2506],
        [0.2483, 0.2509, 0.2514, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2515, 0.2516, 0.2490],
        [0.2487, 0.2506, 0.2495, 0.2512],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2505, 0.2508, 0.2456],
        [0.2496, 0.2504, 0.2512, 0.2488],
        [0.2495, 0.2490, 0.2484, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2508, 0.2500, 0.2496],
        [0.2482, 0.2507, 0.2536, 0.2475],
        [0.2494, 0.2497, 0.2488, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2510, 0.2516, 0.2526],
        [0.2436, 0.2515, 0.2541, 0.2508],
        [0.2434, 0.2527, 0.2571, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2486, 0.2549, 0.2504],
        [0.2429, 0.2509, 0.2552, 0.2510],
        [0.2450, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:12:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 0.5204 (0.5027)	Prec@(1,5) (83.2%, 99.2%)	
05/30 10:12:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 0.6636 (0.5162)	Prec@(1,5) (82.5%, 99.2%)	
05/30 10:13:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 0.4160 (0.5267)	Prec@(1,5) (82.0%, 99.2%)	
05/30 10:13:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 0.3769 (0.5328)	Prec@(1,5) (81.7%, 99.1%)	
05/30 10:13:43PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 0.2979 (0.5299)	Prec@(1,5) (81.8%, 99.1%)	
05/30 10:14:00PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 0.5680 (0.5274)	Prec@(1,5) (81.9%, 99.2%)	
05/30 10:14:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 0.5218 (0.5295)	Prec@(1,5) (81.8%, 99.1%)	
05/30 10:14:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 0.5502 (0.5270)	Prec@(1,5) (81.8%, 99.1%)	
05/30 10:14:31PM searchStage_trainer.py:221 [INFO] Train: [ 14/49] Final Prec@1 81.8240%
05/30 10:14:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 0.6945	Prec@(1,5) (75.8%, 98.6%)
05/30 10:14:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 0.6930	Prec@(1,5) (76.1%, 98.4%)
05/30 10:14:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 0.6843	Prec@(1,5) (76.3%, 98.6%)
05/30 10:14:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 0.6792	Prec@(1,5) (76.5%, 98.6%)
05/30 10:14:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 0.6880	Prec@(1,5) (76.3%, 98.5%)
05/30 10:14:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 0.6815	Prec@(1,5) (76.6%, 98.5%)
05/30 10:14:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 0.6856	Prec@(1,5) (76.5%, 98.5%)
05/30 10:14:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 0.6828	Prec@(1,5) (76.6%, 98.5%)
05/30 10:14:52PM searchStage_trainer.py:260 [INFO] Valid: [ 14/49] Final Prec@1 76.6320%
05/30 10:14:52PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:14:52PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.9760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2497, 0.2823, 0.2390, 0.2291],
        [0.2419, 0.2853, 0.2462, 0.2266]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2833, 0.2492, 0.2176],
        [0.2475, 0.2789, 0.2474, 0.2261],
        [0.2464, 0.2880, 0.2282, 0.2374]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2704, 0.2495, 0.2270],
        [0.2602, 0.2768, 0.2296, 0.2334],
        [0.2549, 0.2747, 0.2325, 0.2379]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2708, 0.2413, 0.2334],
        [0.2558, 0.2702, 0.2308, 0.2432],
        [0.2556, 0.2677, 0.2333, 0.2434]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2705, 0.2410, 0.2331],
        [0.2567, 0.2691, 0.2342, 0.2400],
        [0.2533, 0.2642, 0.2333, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2639, 0.2380, 0.2468],
        [0.2518, 0.2627, 0.2400, 0.2455],
        [0.2531, 0.2657, 0.2401, 0.2411]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2509, 0.2504, 0.2498],
        [0.2494, 0.2505, 0.2508, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2520, 0.2480],
        [0.2490, 0.2496, 0.2507, 0.2507],
        [0.2517, 0.2514, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2509, 0.2505, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2505, 0.2496, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2513, 0.2502, 0.2479],
        [0.2503, 0.2505, 0.2489, 0.2503],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2501, 0.2485],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2485, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2499, 0.2490],
        [0.2498, 0.2505, 0.2503, 0.2495],
        [0.2508, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2497, 0.2511, 0.2505],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2515, 0.2516, 0.2490],
        [0.2487, 0.2506, 0.2495, 0.2512],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2505, 0.2508, 0.2455],
        [0.2496, 0.2504, 0.2512, 0.2488],
        [0.2495, 0.2490, 0.2483, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2508, 0.2500, 0.2496],
        [0.2482, 0.2507, 0.2536, 0.2475],
        [0.2495, 0.2497, 0.2488, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2510, 0.2516, 0.2527],
        [0.2435, 0.2515, 0.2542, 0.2508],
        [0.2433, 0.2527, 0.2571, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2485, 0.2549, 0.2504],
        [0.2429, 0.2509, 0.2553, 0.2510],
        [0.2450, 0.2505, 0.2554, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:15:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 0.4774 (0.4769)	Prec@(1,5) (83.3%, 99.2%)	
05/30 10:15:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 0.3034 (0.5021)	Prec@(1,5) (82.6%, 99.1%)	
05/30 10:15:43PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 0.4717 (0.5071)	Prec@(1,5) (82.5%, 99.1%)	
05/30 10:16:00PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 0.5420 (0.4992)	Prec@(1,5) (82.7%, 99.2%)	
05/30 10:16:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 0.4406 (0.5046)	Prec@(1,5) (82.5%, 99.2%)	
05/30 10:16:34PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 0.4564 (0.5039)	Prec@(1,5) (82.5%, 99.2%)	
05/30 10:16:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 0.5245 (0.5060)	Prec@(1,5) (82.5%, 99.1%)	
05/30 10:17:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 0.4627 (0.5051)	Prec@(1,5) (82.6%, 99.2%)	
05/30 10:17:05PM searchStage_trainer.py:221 [INFO] Train: [ 15/49] Final Prec@1 82.5760%
05/30 10:17:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 0.6261	Prec@(1,5) (77.9%, 98.7%)
05/30 10:17:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 0.6103	Prec@(1,5) (78.9%, 98.6%)
05/30 10:17:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 0.6145	Prec@(1,5) (79.0%, 98.6%)
05/30 10:17:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 0.6109	Prec@(1,5) (79.1%, 98.6%)
05/30 10:17:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 0.6144	Prec@(1,5) (78.9%, 98.6%)
05/30 10:17:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 0.6135	Prec@(1,5) (79.0%, 98.6%)
05/30 10:17:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 0.6154	Prec@(1,5) (79.0%, 98.6%)
05/30 10:17:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 0.6165	Prec@(1,5) (79.0%, 98.6%)
05/30 10:17:26PM searchStage_trainer.py:260 [INFO] Valid: [ 15/49] Final Prec@1 78.9640%
05/30 10:17:26PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:17:27PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 78.9640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2497, 0.2821, 0.2391, 0.2292],
        [0.2418, 0.2852, 0.2464, 0.2266]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2834, 0.2496, 0.2174],
        [0.2475, 0.2790, 0.2474, 0.2261],
        [0.2464, 0.2878, 0.2282, 0.2376]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2706, 0.2495, 0.2266],
        [0.2602, 0.2766, 0.2295, 0.2337],
        [0.2550, 0.2745, 0.2326, 0.2379]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2707, 0.2413, 0.2336],
        [0.2558, 0.2700, 0.2309, 0.2432],
        [0.2556, 0.2675, 0.2334, 0.2435]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2703, 0.2408, 0.2334],
        [0.2568, 0.2690, 0.2342, 0.2400],
        [0.2532, 0.2640, 0.2334, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2638, 0.2381, 0.2469],
        [0.2517, 0.2625, 0.2400, 0.2457],
        [0.2530, 0.2656, 0.2402, 0.2412]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2509, 0.2504, 0.2498],
        [0.2494, 0.2505, 0.2509, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2520, 0.2480],
        [0.2490, 0.2496, 0.2508, 0.2507],
        [0.2517, 0.2514, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2509, 0.2506, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2505, 0.2496, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2505, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2501, 0.2484],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2485, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2499, 0.2490],
        [0.2498, 0.2505, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2511, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2515, 0.2517, 0.2490],
        [0.2487, 0.2506, 0.2495, 0.2512],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2506, 0.2508, 0.2454],
        [0.2496, 0.2504, 0.2512, 0.2488],
        [0.2495, 0.2490, 0.2483, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2508, 0.2500, 0.2496],
        [0.2482, 0.2507, 0.2536, 0.2475],
        [0.2495, 0.2497, 0.2488, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2510, 0.2516, 0.2527],
        [0.2435, 0.2515, 0.2542, 0.2508],
        [0.2433, 0.2527, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2485, 0.2550, 0.2504],
        [0.2428, 0.2509, 0.2553, 0.2510],
        [0.2451, 0.2505, 0.2554, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:17:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 0.4166 (0.4402)	Prec@(1,5) (85.1%, 99.4%)	
05/30 10:18:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 0.5230 (0.4404)	Prec@(1,5) (84.8%, 99.5%)	
05/30 10:18:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 0.4045 (0.4527)	Prec@(1,5) (84.1%, 99.4%)	
05/30 10:18:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 0.5588 (0.4611)	Prec@(1,5) (84.0%, 99.4%)	
05/30 10:18:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 0.5287 (0.4652)	Prec@(1,5) (83.8%, 99.4%)	
05/30 10:19:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 0.6588 (0.4676)	Prec@(1,5) (83.7%, 99.4%)	
05/30 10:19:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 0.5245 (0.4737)	Prec@(1,5) (83.5%, 99.4%)	
05/30 10:19:39PM searchStage_trainer.py:211 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 0.7277 (0.4786)	Prec@(1,5) (83.3%, 99.4%)	
05/30 10:19:40PM searchStage_trainer.py:221 [INFO] Train: [ 16/49] Final Prec@1 83.3480%
05/30 10:19:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 0.6042	Prec@(1,5) (79.2%, 99.0%)
05/30 10:19:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 0.6476	Prec@(1,5) (77.6%, 99.0%)
05/30 10:19:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 0.6465	Prec@(1,5) (77.6%, 98.9%)
05/30 10:19:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 0.6518	Prec@(1,5) (77.4%, 98.9%)
05/30 10:19:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 0.6473	Prec@(1,5) (77.6%, 98.9%)
05/30 10:19:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 0.6490	Prec@(1,5) (77.6%, 98.9%)
05/30 10:19:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 0.6442	Prec@(1,5) (77.8%, 98.9%)
05/30 10:20:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 0.6471	Prec@(1,5) (77.7%, 98.8%)
05/30 10:20:01PM searchStage_trainer.py:260 [INFO] Valid: [ 16/49] Final Prec@1 77.7480%
05/30 10:20:01PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:20:01PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 78.9640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2498, 0.2819, 0.2391, 0.2293],
        [0.2418, 0.2852, 0.2464, 0.2266]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2834, 0.2496, 0.2173],
        [0.2474, 0.2790, 0.2474, 0.2262],
        [0.2464, 0.2876, 0.2283, 0.2377]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2707, 0.2497, 0.2265],
        [0.2602, 0.2766, 0.2295, 0.2338],
        [0.2550, 0.2744, 0.2327, 0.2380]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2706, 0.2415, 0.2335],
        [0.2558, 0.2699, 0.2309, 0.2434],
        [0.2556, 0.2674, 0.2334, 0.2436]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2703, 0.2410, 0.2332],
        [0.2567, 0.2689, 0.2343, 0.2401],
        [0.2532, 0.2639, 0.2334, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2638, 0.2380, 0.2469],
        [0.2517, 0.2624, 0.2401, 0.2458],
        [0.2530, 0.2654, 0.2403, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2509, 0.2504, 0.2498],
        [0.2494, 0.2505, 0.2509, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2520, 0.2480],
        [0.2489, 0.2496, 0.2508, 0.2507],
        [0.2517, 0.2514, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2509, 0.2506, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2505, 0.2496, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2505, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2501, 0.2484],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2485, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2505, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2511, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2515, 0.2517, 0.2490],
        [0.2487, 0.2506, 0.2495, 0.2512],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2506, 0.2509, 0.2453],
        [0.2496, 0.2504, 0.2512, 0.2488],
        [0.2495, 0.2490, 0.2483, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2508, 0.2500, 0.2496],
        [0.2482, 0.2507, 0.2536, 0.2475],
        [0.2495, 0.2497, 0.2487, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2510, 0.2516, 0.2528],
        [0.2434, 0.2515, 0.2542, 0.2508],
        [0.2433, 0.2527, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2485, 0.2551, 0.2504],
        [0.2428, 0.2509, 0.2554, 0.2510],
        [0.2451, 0.2505, 0.2554, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:20:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 0.5316 (0.4376)	Prec@(1,5) (86.2%, 99.4%)	
05/30 10:20:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 0.6315 (0.4580)	Prec@(1,5) (84.7%, 99.4%)	
05/30 10:20:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 0.3245 (0.4590)	Prec@(1,5) (84.4%, 99.3%)	
05/30 10:21:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 0.4629 (0.4565)	Prec@(1,5) (84.5%, 99.3%)	
05/30 10:21:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 0.5915 (0.4586)	Prec@(1,5) (84.2%, 99.4%)	
05/30 10:21:43PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 0.8022 (0.4621)	Prec@(1,5) (84.2%, 99.3%)	
05/30 10:22:00PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 0.2995 (0.4632)	Prec@(1,5) (84.2%, 99.3%)	
05/30 10:22:14PM searchStage_trainer.py:211 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 0.3727 (0.4641)	Prec@(1,5) (84.1%, 99.3%)	
05/30 10:22:14PM searchStage_trainer.py:221 [INFO] Train: [ 17/49] Final Prec@1 84.0760%
05/30 10:22:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 0.6486	Prec@(1,5) (77.7%, 98.3%)
05/30 10:22:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 0.6309	Prec@(1,5) (78.3%, 98.4%)
05/30 10:22:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 0.6251	Prec@(1,5) (78.7%, 98.5%)
05/30 10:22:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 0.6201	Prec@(1,5) (78.9%, 98.6%)
05/30 10:22:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 0.6160	Prec@(1,5) (79.0%, 98.6%)
05/30 10:22:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 0.6159	Prec@(1,5) (79.0%, 98.6%)
05/30 10:22:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 0.6175	Prec@(1,5) (78.9%, 98.6%)
05/30 10:22:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 0.6222	Prec@(1,5) (78.8%, 98.6%)
05/30 10:22:35PM searchStage_trainer.py:260 [INFO] Valid: [ 17/49] Final Prec@1 78.7720%
05/30 10:22:35PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:22:36PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 78.9640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2497, 0.2818, 0.2393, 0.2292],
        [0.2417, 0.2850, 0.2465, 0.2268]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2834, 0.2496, 0.2173],
        [0.2472, 0.2790, 0.2474, 0.2263],
        [0.2464, 0.2875, 0.2284, 0.2377]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2707, 0.2500, 0.2264],
        [0.2601, 0.2765, 0.2296, 0.2338],
        [0.2550, 0.2742, 0.2327, 0.2381]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2705, 0.2414, 0.2336],
        [0.2559, 0.2698, 0.2309, 0.2435],
        [0.2556, 0.2673, 0.2335, 0.2437]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2702, 0.2411, 0.2331],
        [0.2567, 0.2688, 0.2344, 0.2402],
        [0.2532, 0.2637, 0.2334, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2637, 0.2381, 0.2471],
        [0.2517, 0.2623, 0.2402, 0.2458],
        [0.2530, 0.2652, 0.2404, 0.2414]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2509, 0.2504, 0.2499],
        [0.2494, 0.2505, 0.2509, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2520, 0.2480],
        [0.2489, 0.2496, 0.2508, 0.2507],
        [0.2517, 0.2514, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2509, 0.2505, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2505, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2484],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2485, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2511, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2515, 0.2517, 0.2490],
        [0.2487, 0.2506, 0.2495, 0.2512],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2506, 0.2509, 0.2452],
        [0.2496, 0.2504, 0.2511, 0.2488],
        [0.2495, 0.2490, 0.2483, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2499, 0.2496],
        [0.2482, 0.2507, 0.2536, 0.2475],
        [0.2495, 0.2497, 0.2487, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2510, 0.2516, 0.2528],
        [0.2434, 0.2515, 0.2542, 0.2508],
        [0.2433, 0.2527, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2485, 0.2551, 0.2504],
        [0.2427, 0.2509, 0.2554, 0.2510],
        [0.2451, 0.2505, 0.2554, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:22:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 0.3873 (0.4433)	Prec@(1,5) (84.4%, 99.6%)	
05/30 10:23:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 0.5231 (0.4339)	Prec@(1,5) (85.1%, 99.5%)	
05/30 10:23:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 0.4958 (0.4370)	Prec@(1,5) (84.8%, 99.5%)	
05/30 10:23:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 0.3891 (0.4321)	Prec@(1,5) (85.0%, 99.5%)	
05/30 10:24:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 0.4017 (0.4351)	Prec@(1,5) (85.0%, 99.4%)	
05/30 10:24:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 0.5660 (0.4383)	Prec@(1,5) (84.8%, 99.4%)	
05/30 10:24:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 0.4688 (0.4417)	Prec@(1,5) (84.7%, 99.4%)	
05/30 10:24:49PM searchStage_trainer.py:211 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 0.4749 (0.4471)	Prec@(1,5) (84.5%, 99.4%)	
05/30 10:24:49PM searchStage_trainer.py:221 [INFO] Train: [ 18/49] Final Prec@1 84.4960%
05/30 10:24:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 0.6301	Prec@(1,5) (79.3%, 98.4%)
05/30 10:24:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 0.6306	Prec@(1,5) (79.5%, 98.5%)
05/30 10:24:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 0.6242	Prec@(1,5) (79.5%, 98.4%)
05/30 10:25:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 0.6301	Prec@(1,5) (79.4%, 98.4%)
05/30 10:25:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 0.6349	Prec@(1,5) (79.3%, 98.4%)
05/30 10:25:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 0.6317	Prec@(1,5) (79.2%, 98.4%)
05/30 10:25:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 0.6287	Prec@(1,5) (79.2%, 98.4%)
05/30 10:25:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 0.6284	Prec@(1,5) (79.3%, 98.4%)
05/30 10:25:10PM searchStage_trainer.py:260 [INFO] Valid: [ 18/49] Final Prec@1 79.2560%
05/30 10:25:10PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:25:10PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 79.2560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2817, 0.2394, 0.2293],
        [0.2417, 0.2849, 0.2466, 0.2268]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2834, 0.2497, 0.2172],
        [0.2473, 0.2791, 0.2475, 0.2261],
        [0.2464, 0.2873, 0.2284, 0.2379]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2706, 0.2506, 0.2259],
        [0.2601, 0.2763, 0.2297, 0.2339],
        [0.2550, 0.2740, 0.2328, 0.2382]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2703, 0.2414, 0.2336],
        [0.2559, 0.2696, 0.2309, 0.2435],
        [0.2555, 0.2671, 0.2336, 0.2437]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2701, 0.2411, 0.2332],
        [0.2566, 0.2687, 0.2345, 0.2402],
        [0.2532, 0.2636, 0.2335, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2636, 0.2381, 0.2472],
        [0.2517, 0.2622, 0.2403, 0.2459],
        [0.2529, 0.2651, 0.2405, 0.2415]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2494, 0.2505, 0.2509, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2520, 0.2480],
        [0.2490, 0.2496, 0.2508, 0.2507],
        [0.2517, 0.2514, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2509, 0.2505, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2505, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2484],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2485, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2489],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2510, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2516, 0.2517, 0.2490],
        [0.2487, 0.2506, 0.2495, 0.2513],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2506, 0.2509, 0.2452],
        [0.2497, 0.2504, 0.2511, 0.2488],
        [0.2495, 0.2490, 0.2483, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2499, 0.2496],
        [0.2482, 0.2507, 0.2536, 0.2475],
        [0.2495, 0.2497, 0.2487, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2510, 0.2517, 0.2528],
        [0.2434, 0.2515, 0.2543, 0.2509],
        [0.2433, 0.2527, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2485, 0.2552, 0.2504],
        [0.2427, 0.2509, 0.2554, 0.2510],
        [0.2451, 0.2505, 0.2554, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:25:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 0.4628 (0.3872)	Prec@(1,5) (86.4%, 99.6%)	
05/30 10:25:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 0.4608 (0.3958)	Prec@(1,5) (86.3%, 99.5%)	
05/30 10:26:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 0.3602 (0.3974)	Prec@(1,5) (86.2%, 99.6%)	
05/30 10:26:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 0.4694 (0.3981)	Prec@(1,5) (86.2%, 99.5%)	
05/30 10:26:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 0.3323 (0.4068)	Prec@(1,5) (85.9%, 99.5%)	
05/30 10:26:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 0.4681 (0.4132)	Prec@(1,5) (85.7%, 99.5%)	
05/30 10:27:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 0.3799 (0.4182)	Prec@(1,5) (85.5%, 99.4%)	
05/30 10:27:23PM searchStage_trainer.py:211 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 0.5075 (0.4238)	Prec@(1,5) (85.3%, 99.4%)	
05/30 10:27:23PM searchStage_trainer.py:221 [INFO] Train: [ 19/49] Final Prec@1 85.3360%
05/30 10:27:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 0.5463	Prec@(1,5) (80.6%, 99.3%)
05/30 10:27:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 0.5556	Prec@(1,5) (80.6%, 99.2%)
05/30 10:27:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 0.5621	Prec@(1,5) (80.4%, 99.2%)
05/30 10:27:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 0.5657	Prec@(1,5) (80.4%, 99.1%)
05/30 10:27:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 0.5637	Prec@(1,5) (80.5%, 99.1%)
05/30 10:27:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 0.5632	Prec@(1,5) (80.5%, 99.0%)
05/30 10:27:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 0.5687	Prec@(1,5) (80.4%, 99.0%)
05/30 10:27:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 0.5679	Prec@(1,5) (80.4%, 99.0%)
05/30 10:27:45PM searchStage_trainer.py:260 [INFO] Valid: [ 19/49] Final Prec@1 80.3840%
05/30 10:27:45PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:27:45PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.3840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2816, 0.2395, 0.2293],
        [0.2416, 0.2848, 0.2468, 0.2269]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2834, 0.2497, 0.2172],
        [0.2473, 0.2792, 0.2475, 0.2260],
        [0.2463, 0.2871, 0.2285, 0.2380]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2707, 0.2505, 0.2259],
        [0.2601, 0.2762, 0.2298, 0.2339],
        [0.2549, 0.2739, 0.2329, 0.2383]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2702, 0.2413, 0.2337],
        [0.2559, 0.2695, 0.2310, 0.2437],
        [0.2555, 0.2670, 0.2337, 0.2438]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2701, 0.2411, 0.2331],
        [0.2567, 0.2685, 0.2345, 0.2404],
        [0.2532, 0.2634, 0.2336, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2635, 0.2381, 0.2473],
        [0.2517, 0.2620, 0.2403, 0.2460],
        [0.2529, 0.2649, 0.2406, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2494, 0.2505, 0.2509, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2520, 0.2480],
        [0.2490, 0.2495, 0.2508, 0.2507],
        [0.2517, 0.2514, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2510, 0.2505, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2505, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2484],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2485, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2510, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2516, 0.2517, 0.2489],
        [0.2487, 0.2506, 0.2494, 0.2513],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2506, 0.2509, 0.2451],
        [0.2497, 0.2504, 0.2511, 0.2488],
        [0.2496, 0.2490, 0.2483, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2499, 0.2496],
        [0.2482, 0.2507, 0.2536, 0.2474],
        [0.2495, 0.2497, 0.2487, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2510, 0.2517, 0.2529],
        [0.2433, 0.2515, 0.2543, 0.2508],
        [0.2433, 0.2527, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2485, 0.2552, 0.2504],
        [0.2427, 0.2509, 0.2554, 0.2510],
        [0.2451, 0.2505, 0.2554, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:28:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 0.5511 (0.3763)	Prec@(1,5) (86.8%, 99.8%)	
05/30 10:28:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 0.2186 (0.3822)	Prec@(1,5) (86.5%, 99.6%)	
05/30 10:28:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 0.4096 (0.3928)	Prec@(1,5) (86.2%, 99.6%)	
05/30 10:28:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 0.4011 (0.3997)	Prec@(1,5) (86.1%, 99.5%)	
05/30 10:29:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 0.4209 (0.4037)	Prec@(1,5) (86.0%, 99.5%)	
05/30 10:29:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 0.5089 (0.4015)	Prec@(1,5) (86.1%, 99.5%)	
05/30 10:29:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 0.3067 (0.4013)	Prec@(1,5) (86.1%, 99.5%)	
05/30 10:29:57PM searchStage_trainer.py:211 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 0.4848 (0.4028)	Prec@(1,5) (86.0%, 99.5%)	
05/30 10:29:58PM searchStage_trainer.py:221 [INFO] Train: [ 20/49] Final Prec@1 86.0440%
05/30 10:30:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 0.5834	Prec@(1,5) (81.3%, 98.8%)
05/30 10:30:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 0.5920	Prec@(1,5) (80.8%, 98.7%)
05/30 10:30:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 0.5986	Prec@(1,5) (80.8%, 98.6%)
05/30 10:30:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 0.6081	Prec@(1,5) (80.7%, 98.6%)
05/30 10:30:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 0.6111	Prec@(1,5) (80.4%, 98.5%)
05/30 10:30:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 0.6146	Prec@(1,5) (80.2%, 98.5%)
05/30 10:30:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 0.6149	Prec@(1,5) (80.2%, 98.5%)
05/30 10:30:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 0.6187	Prec@(1,5) (80.0%, 98.5%)
05/30 10:30:19PM searchStage_trainer.py:260 [INFO] Valid: [ 20/49] Final Prec@1 79.9680%
05/30 10:30:19PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:30:19PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.3840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2814, 0.2395, 0.2294],
        [0.2417, 0.2846, 0.2469, 0.2268]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2834, 0.2498, 0.2172],
        [0.2472, 0.2791, 0.2476, 0.2260],
        [0.2463, 0.2870, 0.2286, 0.2381]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2707, 0.2504, 0.2259],
        [0.2601, 0.2761, 0.2298, 0.2340],
        [0.2549, 0.2738, 0.2329, 0.2384]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2702, 0.2413, 0.2338],
        [0.2560, 0.2694, 0.2309, 0.2438],
        [0.2555, 0.2669, 0.2338, 0.2438]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2700, 0.2410, 0.2333],
        [0.2567, 0.2684, 0.2345, 0.2404],
        [0.2531, 0.2633, 0.2337, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2634, 0.2381, 0.2474],
        [0.2516, 0.2619, 0.2404, 0.2460],
        [0.2529, 0.2648, 0.2407, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2494, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2521, 0.2480],
        [0.2490, 0.2495, 0.2508, 0.2507],
        [0.2517, 0.2514, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2510, 0.2505, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2505, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2484],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2485, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2510, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2516, 0.2517, 0.2489],
        [0.2487, 0.2506, 0.2494, 0.2513],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2506, 0.2509, 0.2450],
        [0.2497, 0.2504, 0.2511, 0.2488],
        [0.2496, 0.2490, 0.2483, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2499, 0.2496],
        [0.2482, 0.2507, 0.2536, 0.2474],
        [0.2495, 0.2497, 0.2487, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2444, 0.2510, 0.2517, 0.2529],
        [0.2433, 0.2515, 0.2543, 0.2509],
        [0.2433, 0.2527, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2484, 0.2552, 0.2504],
        [0.2426, 0.2509, 0.2554, 0.2510],
        [0.2451, 0.2505, 0.2554, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:30:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 0.5141 (0.3639)	Prec@(1,5) (87.4%, 99.5%)	
05/30 10:30:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 0.3208 (0.3710)	Prec@(1,5) (87.0%, 99.5%)	
05/30 10:31:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 0.2555 (0.3733)	Prec@(1,5) (87.1%, 99.6%)	
05/30 10:31:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 0.3292 (0.3819)	Prec@(1,5) (86.7%, 99.6%)	
05/30 10:31:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 0.2714 (0.3814)	Prec@(1,5) (86.6%, 99.5%)	
05/30 10:32:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 0.2937 (0.3870)	Prec@(1,5) (86.4%, 99.5%)	
05/30 10:32:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 0.3173 (0.3840)	Prec@(1,5) (86.5%, 99.5%)	
05/30 10:32:31PM searchStage_trainer.py:211 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 0.4283 (0.3810)	Prec@(1,5) (86.8%, 99.5%)	
05/30 10:32:32PM searchStage_trainer.py:221 [INFO] Train: [ 21/49] Final Prec@1 86.7640%
05/30 10:32:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 0.6111	Prec@(1,5) (80.1%, 98.7%)
05/30 10:32:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 0.6142	Prec@(1,5) (80.4%, 98.6%)
05/30 10:32:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 0.6055	Prec@(1,5) (80.4%, 98.6%)
05/30 10:32:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 0.6012	Prec@(1,5) (80.4%, 98.6%)
05/30 10:32:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 0.5995	Prec@(1,5) (80.5%, 98.7%)
05/30 10:32:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 0.6058	Prec@(1,5) (80.3%, 98.7%)
05/30 10:32:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 0.6012	Prec@(1,5) (80.4%, 98.7%)
05/30 10:32:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 0.5995	Prec@(1,5) (80.5%, 98.7%)
05/30 10:32:53PM searchStage_trainer.py:260 [INFO] Valid: [ 21/49] Final Prec@1 80.4800%
05/30 10:32:53PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:32:53PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.4800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2813, 0.2396, 0.2294],
        [0.2416, 0.2845, 0.2470, 0.2269]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2833, 0.2498, 0.2174],
        [0.2472, 0.2791, 0.2477, 0.2259],
        [0.2463, 0.2869, 0.2287, 0.2382]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2706, 0.2505, 0.2261],
        [0.2601, 0.2761, 0.2298, 0.2340],
        [0.2549, 0.2736, 0.2330, 0.2384]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2701, 0.2414, 0.2337],
        [0.2560, 0.2692, 0.2310, 0.2439],
        [0.2555, 0.2668, 0.2338, 0.2439]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2700, 0.2409, 0.2334],
        [0.2567, 0.2683, 0.2345, 0.2405],
        [0.2531, 0.2632, 0.2337, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2634, 0.2381, 0.2474],
        [0.2516, 0.2618, 0.2405, 0.2461],
        [0.2529, 0.2647, 0.2407, 0.2417]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2494, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2514, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2510, 0.2505, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2484],
        [0.2510, 0.2506, 0.2492, 0.2491],
        [0.2509, 0.2512, 0.2485, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2510, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2516, 0.2517, 0.2489],
        [0.2487, 0.2506, 0.2494, 0.2513],
        [0.2474, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2506, 0.2509, 0.2450],
        [0.2497, 0.2504, 0.2511, 0.2488],
        [0.2496, 0.2490, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2499, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2496, 0.2497, 0.2487, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2444, 0.2510, 0.2517, 0.2529],
        [0.2433, 0.2515, 0.2543, 0.2509],
        [0.2433, 0.2527, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2484, 0.2553, 0.2504],
        [0.2426, 0.2509, 0.2555, 0.2510],
        [0.2451, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:33:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 0.3975 (0.3653)	Prec@(1,5) (87.8%, 99.7%)	
05/30 10:33:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 0.3854 (0.3496)	Prec@(1,5) (87.8%, 99.6%)	
05/30 10:33:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 0.3092 (0.3565)	Prec@(1,5) (87.5%, 99.7%)	
05/30 10:34:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 0.1963 (0.3523)	Prec@(1,5) (87.6%, 99.6%)	
05/30 10:34:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 0.4844 (0.3583)	Prec@(1,5) (87.4%, 99.6%)	
05/30 10:34:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 0.5738 (0.3638)	Prec@(1,5) (87.4%, 99.6%)	
05/30 10:34:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 0.3802 (0.3688)	Prec@(1,5) (87.1%, 99.6%)	
05/30 10:35:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 0.5109 (0.3697)	Prec@(1,5) (87.1%, 99.6%)	
05/30 10:35:06PM searchStage_trainer.py:221 [INFO] Train: [ 22/49] Final Prec@1 87.0960%
05/30 10:35:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 0.5552	Prec@(1,5) (82.0%, 98.8%)
05/30 10:35:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 0.5607	Prec@(1,5) (81.8%, 98.8%)
05/30 10:35:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 0.5708	Prec@(1,5) (81.7%, 98.7%)
05/30 10:35:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 0.5679	Prec@(1,5) (81.5%, 98.8%)
05/30 10:35:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 0.5699	Prec@(1,5) (81.3%, 98.8%)
05/30 10:35:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 0.5634	Prec@(1,5) (81.5%, 98.9%)
05/30 10:35:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 0.5589	Prec@(1,5) (81.6%, 98.9%)
05/30 10:35:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 0.5553	Prec@(1,5) (81.7%, 98.9%)
05/30 10:35:28PM searchStage_trainer.py:260 [INFO] Valid: [ 22/49] Final Prec@1 81.7160%
05/30 10:35:28PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:35:28PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.7160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2812, 0.2397, 0.2295],
        [0.2416, 0.2844, 0.2470, 0.2269]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2832, 0.2498, 0.2174],
        [0.2472, 0.2791, 0.2478, 0.2260],
        [0.2463, 0.2867, 0.2288, 0.2382]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2704, 0.2505, 0.2262],
        [0.2601, 0.2760, 0.2298, 0.2341],
        [0.2549, 0.2735, 0.2331, 0.2385]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2700, 0.2414, 0.2338],
        [0.2559, 0.2691, 0.2310, 0.2439],
        [0.2555, 0.2666, 0.2339, 0.2440]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2699, 0.2410, 0.2334],
        [0.2567, 0.2682, 0.2346, 0.2406],
        [0.2531, 0.2631, 0.2338, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2633, 0.2381, 0.2474],
        [0.2516, 0.2617, 0.2405, 0.2462],
        [0.2529, 0.2646, 0.2408, 0.2418]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2494, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2507],
        [0.2517, 0.2513, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2510, 0.2505, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2484],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2485, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2510, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2516, 0.2517, 0.2489],
        [0.2487, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2506, 0.2509, 0.2449],
        [0.2497, 0.2504, 0.2511, 0.2488],
        [0.2496, 0.2490, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2499, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2496, 0.2497, 0.2487, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2444, 0.2510, 0.2517, 0.2529],
        [0.2433, 0.2515, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2484, 0.2553, 0.2504],
        [0.2426, 0.2509, 0.2555, 0.2511],
        [0.2451, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:35:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 0.2406 (0.3311)	Prec@(1,5) (88.0%, 99.6%)	
05/30 10:36:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 0.3813 (0.3286)	Prec@(1,5) (88.5%, 99.7%)	
05/30 10:36:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 0.5112 (0.3416)	Prec@(1,5) (88.1%, 99.7%)	
05/30 10:36:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 0.4806 (0.3468)	Prec@(1,5) (88.0%, 99.6%)	
05/30 10:36:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 0.3627 (0.3484)	Prec@(1,5) (87.9%, 99.6%)	
05/30 10:37:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 0.3530 (0.3489)	Prec@(1,5) (87.9%, 99.7%)	
05/30 10:37:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 0.2753 (0.3446)	Prec@(1,5) (88.0%, 99.7%)	
05/30 10:37:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 0.1612 (0.3489)	Prec@(1,5) (87.9%, 99.6%)	
05/30 10:37:41PM searchStage_trainer.py:221 [INFO] Train: [ 23/49] Final Prec@1 87.8480%
05/30 10:37:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 0.5670	Prec@(1,5) (80.9%, 99.3%)
05/30 10:37:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 0.5520	Prec@(1,5) (81.7%, 99.2%)
05/30 10:37:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 0.5589	Prec@(1,5) (81.7%, 99.1%)
05/30 10:37:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 0.5585	Prec@(1,5) (81.7%, 99.0%)
05/30 10:37:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 0.5592	Prec@(1,5) (81.8%, 98.9%)
05/30 10:37:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 0.5546	Prec@(1,5) (81.8%, 99.0%)
05/30 10:38:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 0.5519	Prec@(1,5) (81.9%, 99.0%)
05/30 10:38:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 0.5484	Prec@(1,5) (82.0%, 99.0%)
05/30 10:38:03PM searchStage_trainer.py:260 [INFO] Valid: [ 23/49] Final Prec@1 81.9880%
05/30 10:38:03PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:38:03PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.9880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2811, 0.2397, 0.2296],
        [0.2417, 0.2843, 0.2471, 0.2269]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2830, 0.2499, 0.2175],
        [0.2473, 0.2790, 0.2479, 0.2259],
        [0.2463, 0.2866, 0.2288, 0.2383]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2705, 0.2505, 0.2262],
        [0.2601, 0.2759, 0.2299, 0.2342],
        [0.2549, 0.2734, 0.2331, 0.2385]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2699, 0.2414, 0.2339],
        [0.2559, 0.2690, 0.2310, 0.2440],
        [0.2555, 0.2665, 0.2340, 0.2440]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2698, 0.2410, 0.2334],
        [0.2567, 0.2681, 0.2346, 0.2407],
        [0.2531, 0.2630, 0.2338, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2633, 0.2381, 0.2475],
        [0.2516, 0.2616, 0.2406, 0.2462],
        [0.2528, 0.2644, 0.2409, 0.2419]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2510, 0.2505, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2510, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2516, 0.2517, 0.2489],
        [0.2487, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2506, 0.2509, 0.2449],
        [0.2497, 0.2504, 0.2511, 0.2488],
        [0.2496, 0.2490, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2499, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2496, 0.2497, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2444, 0.2510, 0.2517, 0.2529],
        [0.2433, 0.2515, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2484, 0.2553, 0.2504],
        [0.2426, 0.2509, 0.2555, 0.2511],
        [0.2451, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:38:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 0.4651 (0.3146)	Prec@(1,5) (89.7%, 99.5%)	
05/30 10:38:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 0.2890 (0.3034)	Prec@(1,5) (90.0%, 99.7%)	
05/30 10:38:54PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 0.1690 (0.3058)	Prec@(1,5) (89.8%, 99.6%)	
05/30 10:39:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 0.2900 (0.3104)	Prec@(1,5) (89.7%, 99.7%)	
05/30 10:39:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 0.4883 (0.3151)	Prec@(1,5) (89.5%, 99.7%)	
05/30 10:39:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 0.3346 (0.3171)	Prec@(1,5) (89.4%, 99.7%)	
05/30 10:40:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 0.3116 (0.3202)	Prec@(1,5) (89.2%, 99.7%)	
05/30 10:40:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 0.3135 (0.3216)	Prec@(1,5) (89.1%, 99.7%)	
05/30 10:40:16PM searchStage_trainer.py:221 [INFO] Train: [ 24/49] Final Prec@1 89.1080%
05/30 10:40:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 0.5549	Prec@(1,5) (81.7%, 99.2%)
05/30 10:40:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 0.5619	Prec@(1,5) (81.6%, 99.1%)
05/30 10:40:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 0.5502	Prec@(1,5) (81.8%, 99.1%)
05/30 10:40:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 0.5536	Prec@(1,5) (81.9%, 99.1%)
05/30 10:40:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 0.5596	Prec@(1,5) (81.6%, 99.0%)
05/30 10:40:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 0.5555	Prec@(1,5) (81.8%, 99.0%)
05/30 10:40:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 0.5553	Prec@(1,5) (81.9%, 99.0%)
05/30 10:40:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 0.5564	Prec@(1,5) (81.8%, 99.0%)
05/30 10:40:37PM searchStage_trainer.py:260 [INFO] Valid: [ 24/49] Final Prec@1 81.8280%
05/30 10:40:37PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:40:38PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.9880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2810, 0.2398, 0.2296],
        [0.2417, 0.2842, 0.2471, 0.2270]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2830, 0.2499, 0.2174],
        [0.2473, 0.2789, 0.2479, 0.2260],
        [0.2463, 0.2865, 0.2289, 0.2383]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2705, 0.2504, 0.2262],
        [0.2601, 0.2758, 0.2299, 0.2343],
        [0.2549, 0.2733, 0.2332, 0.2385]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2699, 0.2416, 0.2338],
        [0.2559, 0.2690, 0.2311, 0.2441],
        [0.2555, 0.2664, 0.2340, 0.2441]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2698, 0.2411, 0.2334],
        [0.2567, 0.2680, 0.2346, 0.2407],
        [0.2531, 0.2629, 0.2339, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2633, 0.2381, 0.2475],
        [0.2516, 0.2615, 0.2407, 0.2463],
        [0.2528, 0.2643, 0.2409, 0.2419]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2521, 0.2480],
        [0.2490, 0.2495, 0.2508, 0.2507],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2505, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2510, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2516, 0.2517, 0.2489],
        [0.2487, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2506, 0.2509, 0.2449],
        [0.2497, 0.2504, 0.2511, 0.2488],
        [0.2496, 0.2490, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2499, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2496, 0.2496, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2510, 0.2517, 0.2530],
        [0.2433, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2484, 0.2554, 0.2504],
        [0.2426, 0.2509, 0.2555, 0.2511],
        [0.2451, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:40:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 0.2088 (0.2778)	Prec@(1,5) (90.6%, 99.7%)	
05/30 10:41:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 0.3290 (0.2906)	Prec@(1,5) (89.7%, 99.7%)	
05/30 10:41:29PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 0.2386 (0.2868)	Prec@(1,5) (89.7%, 99.7%)	
05/30 10:41:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 0.4142 (0.2942)	Prec@(1,5) (89.6%, 99.7%)	
05/30 10:42:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 0.2927 (0.3027)	Prec@(1,5) (89.4%, 99.7%)	
05/30 10:42:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 0.2265 (0.3055)	Prec@(1,5) (89.3%, 99.7%)	
05/30 10:42:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 0.3739 (0.3072)	Prec@(1,5) (89.2%, 99.7%)	
05/30 10:42:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 0.5105 (0.3125)	Prec@(1,5) (89.1%, 99.7%)	
05/30 10:42:51PM searchStage_trainer.py:221 [INFO] Train: [ 25/49] Final Prec@1 89.0800%
05/30 10:42:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 0.5378	Prec@(1,5) (82.2%, 98.8%)
05/30 10:42:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 0.5319	Prec@(1,5) (82.3%, 99.0%)
05/30 10:42:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 0.5469	Prec@(1,5) (82.3%, 98.8%)
05/30 10:43:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 0.5572	Prec@(1,5) (81.9%, 98.8%)
05/30 10:43:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 0.5561	Prec@(1,5) (81.7%, 98.9%)
05/30 10:43:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 0.5590	Prec@(1,5) (81.6%, 98.9%)
05/30 10:43:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 0.5518	Prec@(1,5) (81.9%, 98.9%)
05/30 10:43:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 0.5497	Prec@(1,5) (82.0%, 98.9%)
05/30 10:43:12PM searchStage_trainer.py:260 [INFO] Valid: [ 25/49] Final Prec@1 82.0200%
05/30 10:43:12PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:43:13PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 82.0200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2809, 0.2398, 0.2297],
        [0.2416, 0.2841, 0.2472, 0.2270]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2830, 0.2499, 0.2174],
        [0.2473, 0.2788, 0.2479, 0.2260],
        [0.2463, 0.2864, 0.2289, 0.2384]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2705, 0.2505, 0.2262],
        [0.2600, 0.2757, 0.2300, 0.2343],
        [0.2549, 0.2732, 0.2332, 0.2386]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2698, 0.2416, 0.2338],
        [0.2559, 0.2689, 0.2311, 0.2441],
        [0.2554, 0.2664, 0.2341, 0.2441]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2697, 0.2411, 0.2334],
        [0.2567, 0.2679, 0.2346, 0.2408],
        [0.2531, 0.2628, 0.2339, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2632, 0.2381, 0.2476],
        [0.2516, 0.2614, 0.2407, 0.2463],
        [0.2528, 0.2642, 0.2410, 0.2420]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2505, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2510, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2516, 0.2517, 0.2489],
        [0.2487, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2506, 0.2509, 0.2448],
        [0.2497, 0.2504, 0.2511, 0.2488],
        [0.2496, 0.2490, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2499, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2496, 0.2496, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2510, 0.2517, 0.2530],
        [0.2433, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2484, 0.2554, 0.2504],
        [0.2426, 0.2509, 0.2555, 0.2511],
        [0.2451, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:43:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 0.4350 (0.2928)	Prec@(1,5) (90.1%, 99.9%)	
05/30 10:43:47PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 0.4041 (0.2821)	Prec@(1,5) (90.3%, 99.8%)	
05/30 10:44:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 0.1377 (0.2760)	Prec@(1,5) (90.4%, 99.9%)	
05/30 10:44:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.2933 (0.2803)	Prec@(1,5) (90.3%, 99.8%)	
05/30 10:44:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 0.1558 (0.2828)	Prec@(1,5) (90.3%, 99.8%)	
05/30 10:44:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 0.3394 (0.2847)	Prec@(1,5) (90.1%, 99.8%)	
05/30 10:45:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 0.4602 (0.2885)	Prec@(1,5) (90.0%, 99.8%)	
05/30 10:45:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 0.2699 (0.2871)	Prec@(1,5) (90.0%, 99.8%)	
05/30 10:45:26PM searchStage_trainer.py:221 [INFO] Train: [ 26/49] Final Prec@1 89.9640%
05/30 10:45:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 0.5304	Prec@(1,5) (82.7%, 98.9%)
05/30 10:45:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 0.5231	Prec@(1,5) (83.0%, 99.0%)
05/30 10:45:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 0.5257	Prec@(1,5) (83.0%, 99.0%)
05/30 10:45:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 0.5232	Prec@(1,5) (83.0%, 99.0%)
05/30 10:45:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 0.5239	Prec@(1,5) (83.0%, 99.0%)
05/30 10:45:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 0.5227	Prec@(1,5) (83.1%, 99.0%)
05/30 10:45:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 0.5255	Prec@(1,5) (83.2%, 99.1%)
05/30 10:45:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 0.5293	Prec@(1,5) (83.0%, 99.0%)
05/30 10:45:47PM searchStage_trainer.py:260 [INFO] Valid: [ 26/49] Final Prec@1 83.0280%
05/30 10:45:47PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:45:48PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.0280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2809, 0.2399, 0.2297],
        [0.2416, 0.2840, 0.2472, 0.2271]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2829, 0.2500, 0.2174],
        [0.2473, 0.2787, 0.2479, 0.2261],
        [0.2463, 0.2863, 0.2290, 0.2384]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2705, 0.2506, 0.2260],
        [0.2600, 0.2756, 0.2300, 0.2343],
        [0.2549, 0.2731, 0.2333, 0.2387]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2698, 0.2416, 0.2338],
        [0.2559, 0.2688, 0.2311, 0.2442],
        [0.2554, 0.2663, 0.2341, 0.2441]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2696, 0.2412, 0.2334],
        [0.2567, 0.2678, 0.2347, 0.2409],
        [0.2531, 0.2627, 0.2340, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2631, 0.2382, 0.2476],
        [0.2515, 0.2613, 0.2407, 0.2464],
        [0.2528, 0.2641, 0.2410, 0.2420]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2521, 0.2480],
        [0.2490, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2505, 0.2494],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2492, 0.2491],
        [0.2509, 0.2512, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2510, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2516, 0.2517, 0.2489],
        [0.2487, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2516, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2506, 0.2509, 0.2448],
        [0.2497, 0.2504, 0.2511, 0.2488],
        [0.2496, 0.2490, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2499, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2496, 0.2496, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2510, 0.2517, 0.2530],
        [0.2433, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2484, 0.2554, 0.2504],
        [0.2426, 0.2509, 0.2555, 0.2511],
        [0.2451, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:46:05PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 0.2978 (0.2617)	Prec@(1,5) (91.1%, 99.7%)	
05/30 10:46:22PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 0.2107 (0.2558)	Prec@(1,5) (91.1%, 99.8%)	
05/30 10:46:39PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 0.3324 (0.2678)	Prec@(1,5) (90.6%, 99.8%)	
05/30 10:46:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 0.3980 (0.2710)	Prec@(1,5) (90.5%, 99.8%)	
05/30 10:47:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 0.1228 (0.2680)	Prec@(1,5) (90.5%, 99.8%)	
05/30 10:47:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 0.2006 (0.2702)	Prec@(1,5) (90.4%, 99.8%)	
05/30 10:47:47PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 0.3563 (0.2717)	Prec@(1,5) (90.3%, 99.8%)	
05/30 10:48:00PM searchStage_trainer.py:211 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 0.2368 (0.2741)	Prec@(1,5) (90.3%, 99.8%)	
05/30 10:48:01PM searchStage_trainer.py:221 [INFO] Train: [ 27/49] Final Prec@1 90.3040%
05/30 10:48:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 0.5677	Prec@(1,5) (82.0%, 98.7%)
05/30 10:48:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 0.5650	Prec@(1,5) (82.1%, 98.9%)
05/30 10:48:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 0.5624	Prec@(1,5) (82.3%, 98.8%)
05/30 10:48:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 0.5552	Prec@(1,5) (82.3%, 98.9%)
05/30 10:48:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 0.5558	Prec@(1,5) (82.3%, 98.9%)
05/30 10:48:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 0.5524	Prec@(1,5) (82.2%, 98.9%)
05/30 10:48:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 0.5473	Prec@(1,5) (82.4%, 99.0%)
05/30 10:48:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 0.5470	Prec@(1,5) (82.4%, 98.9%)
05/30 10:48:22PM searchStage_trainer.py:260 [INFO] Valid: [ 27/49] Final Prec@1 82.4480%
05/30 10:48:22PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:48:22PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.0280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2808, 0.2400, 0.2297],
        [0.2416, 0.2839, 0.2473, 0.2271]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2829, 0.2500, 0.2173],
        [0.2473, 0.2787, 0.2479, 0.2261],
        [0.2463, 0.2862, 0.2290, 0.2385]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2705, 0.2506, 0.2260],
        [0.2600, 0.2756, 0.2300, 0.2343],
        [0.2549, 0.2731, 0.2333, 0.2387]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2698, 0.2417, 0.2338],
        [0.2559, 0.2687, 0.2311, 0.2443],
        [0.2554, 0.2662, 0.2342, 0.2442]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2696, 0.2413, 0.2335],
        [0.2567, 0.2677, 0.2347, 0.2409],
        [0.2530, 0.2626, 0.2340, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2631, 0.2382, 0.2476],
        [0.2515, 0.2613, 0.2408, 0.2464],
        [0.2528, 0.2641, 0.2411, 0.2421]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2505, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2492, 0.2491],
        [0.2509, 0.2512, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2497, 0.2510, 0.2506],
        [0.2483, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2517, 0.2489],
        [0.2487, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2506, 0.2509, 0.2448],
        [0.2498, 0.2504, 0.2511, 0.2488],
        [0.2496, 0.2490, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2508, 0.2498, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2496, 0.2496, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2510, 0.2517, 0.2530],
        [0.2432, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2484, 0.2554, 0.2504],
        [0.2426, 0.2509, 0.2555, 0.2511],
        [0.2452, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:48:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 0.1730 (0.2277)	Prec@(1,5) (91.8%, 99.9%)	
05/30 10:48:57PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 0.3837 (0.2349)	Prec@(1,5) (92.0%, 99.8%)	
05/30 10:49:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 0.3853 (0.2412)	Prec@(1,5) (91.8%, 99.8%)	
05/30 10:49:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.2571 (0.2545)	Prec@(1,5) (91.2%, 99.8%)	
05/30 10:49:47PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 0.2437 (0.2544)	Prec@(1,5) (91.1%, 99.8%)	
05/30 10:50:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 0.2646 (0.2557)	Prec@(1,5) (91.1%, 99.8%)	
05/30 10:50:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][350/390]	Step 11298	lr 0.01075	Loss 0.2538 (0.2531)	Prec@(1,5) (91.1%, 99.8%)	
05/30 10:50:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 0.3870 (0.2544)	Prec@(1,5) (91.1%, 99.8%)	
05/30 10:50:35PM searchStage_trainer.py:221 [INFO] Train: [ 28/49] Final Prec@1 91.0520%
05/30 10:50:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][50/391]	Step 11339	Loss 0.4812	Prec@(1,5) (84.7%, 98.8%)
05/30 10:50:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 0.5113	Prec@(1,5) (84.3%, 98.8%)
05/30 10:50:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][150/391]	Step 11339	Loss 0.5033	Prec@(1,5) (84.2%, 99.0%)
05/30 10:50:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 0.5111	Prec@(1,5) (84.0%, 99.0%)
05/30 10:50:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][250/391]	Step 11339	Loss 0.5131	Prec@(1,5) (83.9%, 99.0%)
05/30 10:50:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 0.5126	Prec@(1,5) (84.0%, 99.0%)
05/30 10:50:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][350/391]	Step 11339	Loss 0.5128	Prec@(1,5) (83.9%, 99.0%)
05/30 10:50:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 0.5118	Prec@(1,5) (83.9%, 99.0%)
05/30 10:50:56PM searchStage_trainer.py:260 [INFO] Valid: [ 28/49] Final Prec@1 83.8920%
05/30 10:50:56PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:50:57PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.8920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2807, 0.2400, 0.2297],
        [0.2416, 0.2839, 0.2474, 0.2272]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2829, 0.2501, 0.2173],
        [0.2473, 0.2787, 0.2479, 0.2261],
        [0.2463, 0.2861, 0.2291, 0.2386]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2704, 0.2507, 0.2261],
        [0.2600, 0.2755, 0.2301, 0.2344],
        [0.2549, 0.2730, 0.2334, 0.2387]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2697, 0.2417, 0.2339],
        [0.2559, 0.2686, 0.2311, 0.2443],
        [0.2554, 0.2661, 0.2342, 0.2442]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2695, 0.2413, 0.2335],
        [0.2567, 0.2677, 0.2347, 0.2409],
        [0.2530, 0.2625, 0.2341, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2630, 0.2382, 0.2477],
        [0.2515, 0.2612, 0.2408, 0.2465],
        [0.2528, 0.2640, 0.2411, 0.2421]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2510, 0.2505, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2492, 0.2491],
        [0.2509, 0.2512, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2510, 0.2506],
        [0.2484, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2517, 0.2489],
        [0.2487, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2506, 0.2509, 0.2448],
        [0.2498, 0.2504, 0.2511, 0.2487],
        [0.2496, 0.2489, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2496, 0.2496, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2510, 0.2517, 0.2530],
        [0.2432, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2484, 0.2555, 0.2504],
        [0.2425, 0.2509, 0.2555, 0.2511],
        [0.2452, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:51:14PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][50/390]	Step 11389	lr 0.01002	Loss 0.2054 (0.2236)	Prec@(1,5) (92.2%, 99.8%)	
05/30 10:51:31PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 0.2607 (0.2168)	Prec@(1,5) (92.5%, 99.9%)	
05/30 10:51:49PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][150/390]	Step 11489	lr 0.01002	Loss 0.2336 (0.2173)	Prec@(1,5) (92.5%, 99.9%)	
05/30 10:52:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.1433 (0.2198)	Prec@(1,5) (92.4%, 99.9%)	
05/30 10:52:23PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][250/390]	Step 11589	lr 0.01002	Loss 0.4082 (0.2274)	Prec@(1,5) (92.1%, 99.8%)	
05/30 10:52:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 0.2138 (0.2294)	Prec@(1,5) (92.0%, 99.9%)	
05/30 10:52:57PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][350/390]	Step 11689	lr 0.01002	Loss 0.2316 (0.2311)	Prec@(1,5) (92.0%, 99.9%)	
05/30 10:53:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 0.4307 (0.2306)	Prec@(1,5) (91.9%, 99.9%)	
05/30 10:53:11PM searchStage_trainer.py:221 [INFO] Train: [ 29/49] Final Prec@1 91.9200%
05/30 10:53:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][50/391]	Step 11730	Loss 0.5330	Prec@(1,5) (83.8%, 99.2%)
05/30 10:53:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 0.5386	Prec@(1,5) (83.6%, 99.0%)
05/30 10:53:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][150/391]	Step 11730	Loss 0.5266	Prec@(1,5) (83.9%, 99.0%)
05/30 10:53:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 0.5302	Prec@(1,5) (83.7%, 99.0%)
05/30 10:53:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][250/391]	Step 11730	Loss 0.5330	Prec@(1,5) (83.5%, 99.0%)
05/30 10:53:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 0.5322	Prec@(1,5) (83.4%, 99.0%)
05/30 10:53:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][350/391]	Step 11730	Loss 0.5298	Prec@(1,5) (83.6%, 99.0%)
05/30 10:53:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 0.5356	Prec@(1,5) (83.4%, 99.0%)
05/30 10:53:32PM searchStage_trainer.py:260 [INFO] Valid: [ 29/49] Final Prec@1 83.3600%
05/30 10:53:32PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:53:32PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.8920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2806, 0.2401, 0.2298],
        [0.2416, 0.2838, 0.2474, 0.2272]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2828, 0.2501, 0.2173],
        [0.2473, 0.2786, 0.2480, 0.2261],
        [0.2463, 0.2860, 0.2291, 0.2386]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2704, 0.2507, 0.2261],
        [0.2600, 0.2755, 0.2301, 0.2344],
        [0.2549, 0.2729, 0.2334, 0.2388]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2697, 0.2417, 0.2339],
        [0.2559, 0.2686, 0.2312, 0.2443],
        [0.2554, 0.2660, 0.2343, 0.2443]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2695, 0.2414, 0.2334],
        [0.2567, 0.2676, 0.2347, 0.2410],
        [0.2530, 0.2625, 0.2341, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2630, 0.2382, 0.2477],
        [0.2515, 0.2611, 0.2409, 0.2465],
        [0.2528, 0.2639, 0.2412, 0.2422]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2510, 0.2505, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2480],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2492, 0.2491],
        [0.2509, 0.2512, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2489],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2511, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2510, 0.2506],
        [0.2484, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2517, 0.2489],
        [0.2487, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2507, 0.2509, 0.2447],
        [0.2498, 0.2504, 0.2511, 0.2487],
        [0.2497, 0.2489, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2496, 0.2496, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2510, 0.2517, 0.2530],
        [0.2432, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2484, 0.2555, 0.2504],
        [0.2425, 0.2509, 0.2555, 0.2511],
        [0.2452, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:53:50PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][50/390]	Step 11780	lr 0.00929	Loss 0.1491 (0.2198)	Prec@(1,5) (92.6%, 99.9%)	
05/30 10:54:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 0.1324 (0.2158)	Prec@(1,5) (92.6%, 99.9%)	
05/30 10:54:24PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][150/390]	Step 11880	lr 0.00929	Loss 0.2532 (0.2100)	Prec@(1,5) (92.7%, 99.9%)	
05/30 10:54:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 0.4126 (0.2076)	Prec@(1,5) (92.9%, 99.9%)	
05/30 10:54:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][250/390]	Step 11980	lr 0.00929	Loss 0.2226 (0.2105)	Prec@(1,5) (92.6%, 99.9%)	
05/30 10:55:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 0.2546 (0.2120)	Prec@(1,5) (92.5%, 99.9%)	
05/30 10:55:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][350/390]	Step 12080	lr 0.00929	Loss 0.2956 (0.2170)	Prec@(1,5) (92.3%, 99.9%)	
05/30 10:55:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 0.2143 (0.2183)	Prec@(1,5) (92.3%, 99.9%)	
05/30 10:55:46PM searchStage_trainer.py:221 [INFO] Train: [ 30/49] Final Prec@1 92.2720%
05/30 10:55:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][50/391]	Step 12121	Loss 0.5784	Prec@(1,5) (82.3%, 99.1%)
05/30 10:55:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 0.5633	Prec@(1,5) (82.8%, 99.1%)
05/30 10:55:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][150/391]	Step 12121	Loss 0.5609	Prec@(1,5) (82.9%, 99.1%)
05/30 10:55:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 0.5565	Prec@(1,5) (83.0%, 99.1%)
05/30 10:56:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][250/391]	Step 12121	Loss 0.5496	Prec@(1,5) (83.1%, 99.1%)
05/30 10:56:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 0.5537	Prec@(1,5) (83.1%, 99.1%)
05/30 10:56:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][350/391]	Step 12121	Loss 0.5565	Prec@(1,5) (83.1%, 99.0%)
05/30 10:56:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 0.5549	Prec@(1,5) (83.2%, 99.1%)
05/30 10:56:07PM searchStage_trainer.py:260 [INFO] Valid: [ 30/49] Final Prec@1 83.1840%
05/30 10:56:07PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:56:08PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.8920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2806, 0.2401, 0.2298],
        [0.2416, 0.2837, 0.2475, 0.2272]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2828, 0.2501, 0.2174],
        [0.2473, 0.2785, 0.2480, 0.2262],
        [0.2463, 0.2860, 0.2292, 0.2386]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2703, 0.2507, 0.2261],
        [0.2600, 0.2754, 0.2301, 0.2345],
        [0.2549, 0.2728, 0.2334, 0.2388]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2696, 0.2417, 0.2339],
        [0.2559, 0.2685, 0.2312, 0.2444],
        [0.2554, 0.2660, 0.2343, 0.2443]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2694, 0.2414, 0.2335],
        [0.2567, 0.2675, 0.2348, 0.2411],
        [0.2530, 0.2624, 0.2341, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2629, 0.2383, 0.2477],
        [0.2515, 0.2611, 0.2409, 0.2465],
        [0.2528, 0.2638, 0.2412, 0.2422]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2510, 0.2505, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2489],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2510, 0.2506],
        [0.2484, 0.2508, 0.2513, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2517, 0.2489],
        [0.2488, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2507, 0.2509, 0.2447],
        [0.2498, 0.2504, 0.2511, 0.2487],
        [0.2497, 0.2489, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2496, 0.2496, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2510, 0.2517, 0.2530],
        [0.2432, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2484, 0.2555, 0.2504],
        [0.2425, 0.2509, 0.2555, 0.2511],
        [0.2452, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:56:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][50/390]	Step 12171	lr 0.00858	Loss 0.2172 (0.1896)	Prec@(1,5) (93.3%, 100.0%)	
05/30 10:56:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.1186 (0.2032)	Prec@(1,5) (93.0%, 100.0%)	
05/30 10:56:59PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][150/390]	Step 12271	lr 0.00858	Loss 0.2143 (0.2000)	Prec@(1,5) (93.2%, 99.9%)	
05/30 10:57:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.1616 (0.1989)	Prec@(1,5) (93.2%, 99.9%)	
05/30 10:57:33PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][250/390]	Step 12371	lr 0.00858	Loss 0.1607 (0.1968)	Prec@(1,5) (93.2%, 99.9%)	
05/30 10:57:50PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 0.3558 (0.2005)	Prec@(1,5) (93.1%, 99.9%)	
05/30 10:58:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][350/390]	Step 12471	lr 0.00858	Loss 0.2114 (0.2038)	Prec@(1,5) (92.9%, 99.9%)	
05/30 10:58:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 0.1519 (0.2054)	Prec@(1,5) (92.8%, 99.9%)	
05/30 10:58:21PM searchStage_trainer.py:221 [INFO] Train: [ 31/49] Final Prec@1 92.8160%
05/30 10:58:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][50/391]	Step 12512	Loss 0.5582	Prec@(1,5) (83.0%, 98.9%)
05/30 10:58:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 0.5445	Prec@(1,5) (83.3%, 99.1%)
05/30 10:58:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][150/391]	Step 12512	Loss 0.5537	Prec@(1,5) (83.3%, 99.1%)
05/30 10:58:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 0.5544	Prec@(1,5) (83.3%, 99.1%)
05/30 10:58:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][250/391]	Step 12512	Loss 0.5514	Prec@(1,5) (83.4%, 99.1%)
05/30 10:58:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 0.5557	Prec@(1,5) (83.2%, 99.1%)
05/30 10:58:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][350/391]	Step 12512	Loss 0.5558	Prec@(1,5) (83.2%, 99.1%)
05/30 10:58:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 0.5574	Prec@(1,5) (83.2%, 99.1%)
05/30 10:58:43PM searchStage_trainer.py:260 [INFO] Valid: [ 31/49] Final Prec@1 83.2040%
05/30 10:58:43PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:58:43PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.8920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2805, 0.2402, 0.2298],
        [0.2416, 0.2837, 0.2475, 0.2272]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2828, 0.2502, 0.2173],
        [0.2474, 0.2784, 0.2480, 0.2262],
        [0.2462, 0.2859, 0.2292, 0.2387]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2703, 0.2508, 0.2262],
        [0.2600, 0.2754, 0.2302, 0.2345],
        [0.2549, 0.2728, 0.2335, 0.2389]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2696, 0.2417, 0.2339],
        [0.2559, 0.2685, 0.2312, 0.2444],
        [0.2554, 0.2659, 0.2344, 0.2443]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2694, 0.2414, 0.2335],
        [0.2567, 0.2674, 0.2348, 0.2411],
        [0.2530, 0.2623, 0.2342, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2629, 0.2383, 0.2478],
        [0.2515, 0.2610, 0.2409, 0.2466],
        [0.2528, 0.2637, 0.2413, 0.2422]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2510, 0.2505, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2505, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2512, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2489],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2510, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2517, 0.2489],
        [0.2488, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2507, 0.2509, 0.2447],
        [0.2498, 0.2504, 0.2511, 0.2487],
        [0.2497, 0.2489, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2497, 0.2496, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2510, 0.2517, 0.2530],
        [0.2432, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2484, 0.2555, 0.2504],
        [0.2425, 0.2509, 0.2556, 0.2511],
        [0.2452, 0.2505, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:59:00PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][50/390]	Step 12562	lr 0.00789	Loss 0.1044 (0.1756)	Prec@(1,5) (94.0%, 100.0%)	
05/30 10:59:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.2141 (0.1844)	Prec@(1,5) (93.7%, 100.0%)	
05/30 10:59:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][150/390]	Step 12662	lr 0.00789	Loss 0.1561 (0.1865)	Prec@(1,5) (93.6%, 99.9%)	
05/30 10:59:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.2146 (0.1841)	Prec@(1,5) (93.6%, 99.9%)	
05/30 11:00:08PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][250/390]	Step 12762	lr 0.00789	Loss 0.2862 (0.1833)	Prec@(1,5) (93.7%, 99.9%)	
05/30 11:00:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.1457 (0.1846)	Prec@(1,5) (93.6%, 99.9%)	
05/30 11:00:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][350/390]	Step 12862	lr 0.00789	Loss 0.2614 (0.1839)	Prec@(1,5) (93.7%, 99.9%)	
05/30 11:00:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 0.0873 (0.1839)	Prec@(1,5) (93.6%, 99.9%)	
05/30 11:00:56PM searchStage_trainer.py:221 [INFO] Train: [ 32/49] Final Prec@1 93.6400%
05/30 11:00:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][50/391]	Step 12903	Loss 0.5354	Prec@(1,5) (84.0%, 99.0%)
05/30 11:01:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 0.5461	Prec@(1,5) (83.9%, 99.0%)
05/30 11:01:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][150/391]	Step 12903	Loss 0.5337	Prec@(1,5) (84.0%, 98.9%)
05/30 11:01:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 0.5388	Prec@(1,5) (83.8%, 99.0%)
05/30 11:01:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][250/391]	Step 12903	Loss 0.5380	Prec@(1,5) (83.8%, 99.0%)
05/30 11:01:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 0.5431	Prec@(1,5) (83.7%, 99.0%)
05/30 11:01:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][350/391]	Step 12903	Loss 0.5414	Prec@(1,5) (83.7%, 99.0%)
05/30 11:01:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 0.5433	Prec@(1,5) (83.7%, 99.0%)
05/30 11:01:18PM searchStage_trainer.py:260 [INFO] Valid: [ 32/49] Final Prec@1 83.6680%
05/30 11:01:18PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:01:18PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.8920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2804, 0.2402, 0.2299],
        [0.2416, 0.2836, 0.2475, 0.2273]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2827, 0.2502, 0.2174],
        [0.2474, 0.2784, 0.2480, 0.2262],
        [0.2462, 0.2858, 0.2292, 0.2387]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2703, 0.2508, 0.2262],
        [0.2600, 0.2753, 0.2302, 0.2345],
        [0.2549, 0.2727, 0.2335, 0.2389]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2695, 0.2417, 0.2340],
        [0.2559, 0.2684, 0.2312, 0.2444],
        [0.2554, 0.2658, 0.2344, 0.2444]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2693, 0.2415, 0.2335],
        [0.2566, 0.2674, 0.2348, 0.2411],
        [0.2530, 0.2623, 0.2342, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2628, 0.2383, 0.2478],
        [0.2515, 0.2609, 0.2410, 0.2466],
        [0.2528, 0.2637, 0.2413, 0.2423]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2505, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2493, 0.2491],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2489],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2510, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2517, 0.2489],
        [0.2488, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2507, 0.2509, 0.2447],
        [0.2498, 0.2504, 0.2511, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2497, 0.2496, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2484, 0.2555, 0.2504],
        [0.2425, 0.2509, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:01:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][50/390]	Step 12953	lr 0.00722	Loss 0.0982 (0.1740)	Prec@(1,5) (94.0%, 100.0%)	
05/30 11:01:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.1851 (0.1689)	Prec@(1,5) (94.2%, 100.0%)	
05/30 11:02:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][150/390]	Step 13053	lr 0.00722	Loss 0.0750 (0.1700)	Prec@(1,5) (94.0%, 100.0%)	
05/30 11:02:26PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.1086 (0.1648)	Prec@(1,5) (94.1%, 100.0%)	
05/30 11:02:43PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][250/390]	Step 13153	lr 0.00722	Loss 0.1087 (0.1632)	Prec@(1,5) (94.3%, 100.0%)	
05/30 11:03:00PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.1395 (0.1650)	Prec@(1,5) (94.3%, 99.9%)	
05/30 11:03:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][350/390]	Step 13253	lr 0.00722	Loss 0.1113 (0.1677)	Prec@(1,5) (94.2%, 99.9%)	
05/30 11:03:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 0.1481 (0.1684)	Prec@(1,5) (94.2%, 99.9%)	
05/30 11:03:31PM searchStage_trainer.py:221 [INFO] Train: [ 33/49] Final Prec@1 94.1560%
05/30 11:03:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][50/391]	Step 13294	Loss 0.5108	Prec@(1,5) (84.3%, 99.2%)
05/30 11:03:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 0.5362	Prec@(1,5) (83.9%, 99.2%)
05/30 11:03:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][150/391]	Step 13294	Loss 0.5262	Prec@(1,5) (84.4%, 99.2%)
05/30 11:03:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 0.5281	Prec@(1,5) (84.4%, 99.1%)
05/30 11:03:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][250/391]	Step 13294	Loss 0.5326	Prec@(1,5) (84.3%, 99.1%)
05/30 11:03:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 0.5450	Prec@(1,5) (84.2%, 99.0%)
05/30 11:03:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][350/391]	Step 13294	Loss 0.5452	Prec@(1,5) (84.2%, 99.0%)
05/30 11:03:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 0.5508	Prec@(1,5) (84.1%, 99.0%)
05/30 11:03:52PM searchStage_trainer.py:260 [INFO] Valid: [ 33/49] Final Prec@1 84.0720%
05/30 11:03:52PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:03:52PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.0720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2804, 0.2403, 0.2299],
        [0.2416, 0.2836, 0.2476, 0.2273]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2827, 0.2502, 0.2174],
        [0.2473, 0.2784, 0.2481, 0.2262],
        [0.2462, 0.2858, 0.2293, 0.2387]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2703, 0.2508, 0.2262],
        [0.2600, 0.2753, 0.2302, 0.2345],
        [0.2549, 0.2726, 0.2335, 0.2389]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2695, 0.2418, 0.2340],
        [0.2559, 0.2683, 0.2313, 0.2445],
        [0.2554, 0.2658, 0.2344, 0.2444]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2693, 0.2415, 0.2335],
        [0.2566, 0.2673, 0.2349, 0.2412],
        [0.2530, 0.2622, 0.2342, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2628, 0.2383, 0.2478],
        [0.2515, 0.2609, 0.2410, 0.2466],
        [0.2527, 0.2636, 0.2413, 0.2423]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2505, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2493, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2489],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2510, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2489],
        [0.2488, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2507, 0.2509, 0.2446],
        [0.2498, 0.2504, 0.2511, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2483, 0.2507, 0.2536, 0.2474],
        [0.2497, 0.2496, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2484, 0.2555, 0.2504],
        [0.2425, 0.2509, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:04:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][50/390]	Step 13344	lr 0.00657	Loss 0.1834 (0.1478)	Prec@(1,5) (95.0%, 100.0%)	
05/30 11:04:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.0638 (0.1447)	Prec@(1,5) (95.0%, 99.9%)	
05/30 11:04:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][150/390]	Step 13444	lr 0.00657	Loss 0.3706 (0.1439)	Prec@(1,5) (95.1%, 99.9%)	
05/30 11:05:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.1392 (0.1444)	Prec@(1,5) (95.2%, 99.9%)	
05/30 11:05:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][250/390]	Step 13544	lr 0.00657	Loss 0.1276 (0.1426)	Prec@(1,5) (95.2%, 99.9%)	
05/30 11:05:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.2289 (0.1429)	Prec@(1,5) (95.1%, 100.0%)	
05/30 11:05:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][350/390]	Step 13644	lr 0.00657	Loss 0.1888 (0.1473)	Prec@(1,5) (95.0%, 100.0%)	
05/30 11:06:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 0.2573 (0.1482)	Prec@(1,5) (94.9%, 99.9%)	
05/30 11:06:06PM searchStage_trainer.py:221 [INFO] Train: [ 34/49] Final Prec@1 94.9040%
05/30 11:06:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][50/391]	Step 13685	Loss 0.5672	Prec@(1,5) (84.4%, 98.8%)
05/30 11:06:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 0.5398	Prec@(1,5) (84.7%, 98.9%)
05/30 11:06:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][150/391]	Step 13685	Loss 0.5485	Prec@(1,5) (84.5%, 98.9%)
05/30 11:06:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 0.5495	Prec@(1,5) (84.2%, 99.0%)
05/30 11:06:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][250/391]	Step 13685	Loss 0.5368	Prec@(1,5) (84.5%, 99.0%)
05/30 11:06:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 0.5388	Prec@(1,5) (84.3%, 99.1%)
05/30 11:06:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][350/391]	Step 13685	Loss 0.5410	Prec@(1,5) (84.4%, 99.0%)
05/30 11:06:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 0.5455	Prec@(1,5) (84.4%, 99.0%)
05/30 11:06:27PM searchStage_trainer.py:260 [INFO] Valid: [ 34/49] Final Prec@1 84.3480%
05/30 11:06:27PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:06:28PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.3480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2803, 0.2403, 0.2299],
        [0.2416, 0.2835, 0.2476, 0.2273]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2827, 0.2502, 0.2174],
        [0.2473, 0.2784, 0.2481, 0.2262],
        [0.2462, 0.2857, 0.2293, 0.2388]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2703, 0.2509, 0.2262],
        [0.2600, 0.2752, 0.2303, 0.2345],
        [0.2549, 0.2726, 0.2336, 0.2390]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2695, 0.2417, 0.2340],
        [0.2559, 0.2683, 0.2313, 0.2445],
        [0.2554, 0.2657, 0.2345, 0.2445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2692, 0.2416, 0.2335],
        [0.2566, 0.2673, 0.2349, 0.2412],
        [0.2530, 0.2621, 0.2343, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2627, 0.2383, 0.2479],
        [0.2515, 0.2608, 0.2410, 0.2467],
        [0.2527, 0.2635, 0.2414, 0.2423]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2506, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2492, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2489],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2489],
        [0.2488, 0.2506, 0.2494, 0.2513],
        [0.2475, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2507, 0.2509, 0.2446],
        [0.2498, 0.2504, 0.2511, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2536, 0.2474],
        [0.2497, 0.2496, 0.2486, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2484, 0.2555, 0.2504],
        [0.2425, 0.2509, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:06:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][50/390]	Step 13735	lr 0.00595	Loss 0.1723 (0.1172)	Prec@(1,5) (95.9%, 100.0%)	
05/30 11:07:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.0703 (0.1187)	Prec@(1,5) (95.9%, 100.0%)	
05/30 11:07:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][150/390]	Step 13835	lr 0.00595	Loss 0.0419 (0.1194)	Prec@(1,5) (95.8%, 100.0%)	
05/30 11:07:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 0.0398 (0.1208)	Prec@(1,5) (95.8%, 100.0%)	
05/30 11:07:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][250/390]	Step 13935	lr 0.00595	Loss 0.1187 (0.1215)	Prec@(1,5) (95.8%, 100.0%)	
05/30 11:08:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 0.1148 (0.1258)	Prec@(1,5) (95.6%, 99.9%)	
05/30 11:08:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][350/390]	Step 14035	lr 0.00595	Loss 0.3320 (0.1278)	Prec@(1,5) (95.5%, 99.9%)	
05/30 11:08:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 0.0821 (0.1295)	Prec@(1,5) (95.5%, 100.0%)	
05/30 11:08:41PM searchStage_trainer.py:221 [INFO] Train: [ 35/49] Final Prec@1 95.5240%
05/30 11:08:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][50/391]	Step 14076	Loss 0.5345	Prec@(1,5) (84.9%, 99.1%)
05/30 11:08:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 0.5288	Prec@(1,5) (85.0%, 99.0%)
05/30 11:08:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][150/391]	Step 14076	Loss 0.5419	Prec@(1,5) (84.8%, 98.9%)
05/30 11:08:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 0.5299	Prec@(1,5) (84.9%, 99.0%)
05/30 11:08:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][250/391]	Step 14076	Loss 0.5256	Prec@(1,5) (84.9%, 99.0%)
05/30 11:08:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 0.5235	Prec@(1,5) (84.8%, 99.1%)
05/30 11:09:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][350/391]	Step 14076	Loss 0.5261	Prec@(1,5) (84.8%, 99.1%)
05/30 11:09:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 0.5251	Prec@(1,5) (84.8%, 99.1%)
05/30 11:09:02PM searchStage_trainer.py:260 [INFO] Valid: [ 35/49] Final Prec@1 84.8160%
05/30 11:09:02PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:09:03PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.8160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2803, 0.2403, 0.2299],
        [0.2415, 0.2834, 0.2477, 0.2273]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2827, 0.2503, 0.2174],
        [0.2473, 0.2783, 0.2481, 0.2262],
        [0.2462, 0.2857, 0.2293, 0.2388]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2702, 0.2509, 0.2262],
        [0.2600, 0.2752, 0.2303, 0.2345],
        [0.2549, 0.2725, 0.2336, 0.2390]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2694, 0.2417, 0.2340],
        [0.2559, 0.2682, 0.2313, 0.2445],
        [0.2554, 0.2657, 0.2345, 0.2445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2692, 0.2416, 0.2336],
        [0.2566, 0.2672, 0.2349, 0.2412],
        [0.2530, 0.2621, 0.2343, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2627, 0.2383, 0.2479],
        [0.2515, 0.2608, 0.2411, 0.2467],
        [0.2527, 0.2635, 0.2414, 0.2424]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2506, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2504],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2492, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2507, 0.2509, 0.2446],
        [0.2498, 0.2504, 0.2511, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2536, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2509, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:09:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][50/390]	Step 14126	lr 0.00535	Loss 0.0933 (0.1136)	Prec@(1,5) (95.6%, 100.0%)	
05/30 11:09:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.0884 (0.1111)	Prec@(1,5) (95.8%, 100.0%)	
05/30 11:09:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][150/390]	Step 14226	lr 0.00535	Loss 0.0380 (0.1137)	Prec@(1,5) (95.9%, 100.0%)	
05/30 11:10:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.1908 (0.1166)	Prec@(1,5) (95.9%, 100.0%)	
05/30 11:10:29PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][250/390]	Step 14326	lr 0.00535	Loss 0.0906 (0.1194)	Prec@(1,5) (95.9%, 100.0%)	
05/30 11:10:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.1780 (0.1214)	Prec@(1,5) (95.8%, 100.0%)	
05/30 11:11:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][350/390]	Step 14426	lr 0.00535	Loss 0.1390 (0.1221)	Prec@(1,5) (95.8%, 100.0%)	
05/30 11:11:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.3598 (0.1235)	Prec@(1,5) (95.7%, 100.0%)	
05/30 11:11:17PM searchStage_trainer.py:221 [INFO] Train: [ 36/49] Final Prec@1 95.7440%
05/30 11:11:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][50/391]	Step 14467	Loss 0.5577	Prec@(1,5) (84.7%, 99.0%)
05/30 11:11:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 0.5581	Prec@(1,5) (84.8%, 99.1%)
05/30 11:11:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][150/391]	Step 14467	Loss 0.5282	Prec@(1,5) (85.5%, 99.2%)
05/30 11:11:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 0.5298	Prec@(1,5) (85.3%, 99.2%)
05/30 11:11:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][250/391]	Step 14467	Loss 0.5300	Prec@(1,5) (85.3%, 99.2%)
05/30 11:11:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 0.5359	Prec@(1,5) (85.2%, 99.2%)
05/30 11:11:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][350/391]	Step 14467	Loss 0.5372	Prec@(1,5) (85.2%, 99.2%)
05/30 11:11:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 0.5345	Prec@(1,5) (85.2%, 99.2%)
05/30 11:11:38PM searchStage_trainer.py:260 [INFO] Valid: [ 36/49] Final Prec@1 85.2240%
05/30 11:11:38PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:11:39PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.2240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2802, 0.2404, 0.2299],
        [0.2415, 0.2834, 0.2477, 0.2274]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2826, 0.2503, 0.2173],
        [0.2473, 0.2783, 0.2481, 0.2263],
        [0.2462, 0.2856, 0.2293, 0.2388]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2702, 0.2509, 0.2263],
        [0.2600, 0.2751, 0.2303, 0.2346],
        [0.2549, 0.2725, 0.2336, 0.2390]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2694, 0.2417, 0.2340],
        [0.2559, 0.2682, 0.2313, 0.2445],
        [0.2554, 0.2656, 0.2345, 0.2445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2692, 0.2416, 0.2336],
        [0.2566, 0.2672, 0.2350, 0.2412],
        [0.2530, 0.2620, 0.2343, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2626, 0.2384, 0.2479],
        [0.2515, 0.2607, 0.2411, 0.2467],
        [0.2527, 0.2634, 0.2414, 0.2424]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2484, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2511, 0.2506, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2492, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2507, 0.2508, 0.2446],
        [0.2498, 0.2504, 0.2511, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2536, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2509, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:11:56PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][50/390]	Step 14517	lr 0.00479	Loss 0.1423 (0.1105)	Prec@(1,5) (96.0%, 100.0%)	
05/30 11:12:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.1310 (0.1061)	Prec@(1,5) (96.3%, 100.0%)	
05/30 11:12:30PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][150/390]	Step 14617	lr 0.00479	Loss 0.2595 (0.1059)	Prec@(1,5) (96.4%, 100.0%)	
05/30 11:12:47PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.1294 (0.1045)	Prec@(1,5) (96.4%, 100.0%)	
05/30 11:13:04PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][250/390]	Step 14717	lr 0.00479	Loss 0.0760 (0.1048)	Prec@(1,5) (96.3%, 100.0%)	
05/30 11:13:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.0829 (0.1051)	Prec@(1,5) (96.3%, 100.0%)	
05/30 11:13:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][350/390]	Step 14817	lr 0.00479	Loss 0.2161 (0.1068)	Prec@(1,5) (96.3%, 100.0%)	
05/30 11:13:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.0667 (0.1071)	Prec@(1,5) (96.2%, 100.0%)	
05/30 11:13:52PM searchStage_trainer.py:221 [INFO] Train: [ 37/49] Final Prec@1 96.2400%
05/30 11:13:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][50/391]	Step 14858	Loss 0.5391	Prec@(1,5) (85.6%, 99.3%)
05/30 11:13:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 0.4977	Prec@(1,5) (86.3%, 99.3%)
05/30 11:14:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][150/391]	Step 14858	Loss 0.5058	Prec@(1,5) (86.1%, 99.3%)
05/30 11:14:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 0.5137	Prec@(1,5) (86.0%, 99.4%)
05/30 11:14:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][250/391]	Step 14858	Loss 0.5144	Prec@(1,5) (86.1%, 99.3%)
05/30 11:14:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 0.5222	Prec@(1,5) (86.0%, 99.3%)
05/30 11:14:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][350/391]	Step 14858	Loss 0.5234	Prec@(1,5) (85.9%, 99.2%)
05/30 11:14:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 0.5215	Prec@(1,5) (85.9%, 99.3%)
05/30 11:14:14PM searchStage_trainer.py:260 [INFO] Valid: [ 37/49] Final Prec@1 85.8760%
05/30 11:14:14PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:14:14PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.8760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2802, 0.2404, 0.2299],
        [0.2415, 0.2834, 0.2477, 0.2274]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2826, 0.2503, 0.2174],
        [0.2473, 0.2783, 0.2481, 0.2263],
        [0.2462, 0.2856, 0.2294, 0.2388]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2702, 0.2510, 0.2262],
        [0.2600, 0.2751, 0.2303, 0.2346],
        [0.2549, 0.2724, 0.2336, 0.2390]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2694, 0.2418, 0.2340],
        [0.2559, 0.2681, 0.2314, 0.2446],
        [0.2554, 0.2656, 0.2345, 0.2445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2691, 0.2416, 0.2337],
        [0.2566, 0.2671, 0.2350, 0.2413],
        [0.2530, 0.2620, 0.2344, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2626, 0.2384, 0.2480],
        [0.2515, 0.2607, 0.2411, 0.2467],
        [0.2527, 0.2634, 0.2415, 0.2424]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2506, 0.2492],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2492, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2507, 0.2508, 0.2446],
        [0.2498, 0.2504, 0.2511, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2536, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2509, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:14:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][50/390]	Step 14908	lr 0.00425	Loss 0.1077 (0.0875)	Prec@(1,5) (97.2%, 100.0%)	
05/30 11:14:49PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.1261 (0.0877)	Prec@(1,5) (97.2%, 100.0%)	
05/30 11:15:06PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][150/390]	Step 15008	lr 0.00425	Loss 0.0994 (0.0837)	Prec@(1,5) (97.3%, 100.0%)	
05/30 11:15:23PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.0416 (0.0850)	Prec@(1,5) (97.1%, 100.0%)	
05/30 11:15:40PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][250/390]	Step 15108	lr 0.00425	Loss 0.1574 (0.0858)	Prec@(1,5) (97.1%, 100.0%)	
05/30 11:15:57PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.0135 (0.0875)	Prec@(1,5) (97.0%, 100.0%)	
05/30 11:16:14PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][350/390]	Step 15208	lr 0.00425	Loss 0.1304 (0.0913)	Prec@(1,5) (96.9%, 100.0%)	
05/30 11:16:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.0899 (0.0931)	Prec@(1,5) (96.8%, 100.0%)	
05/30 11:16:28PM searchStage_trainer.py:221 [INFO] Train: [ 38/49] Final Prec@1 96.8440%
05/30 11:16:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][50/391]	Step 15249	Loss 0.5041	Prec@(1,5) (85.5%, 99.5%)
05/30 11:16:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 0.4996	Prec@(1,5) (86.1%, 99.4%)
05/30 11:16:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][150/391]	Step 15249	Loss 0.5153	Prec@(1,5) (85.9%, 99.3%)
05/30 11:16:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 0.5181	Prec@(1,5) (86.0%, 99.3%)
05/30 11:16:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][250/391]	Step 15249	Loss 0.5222	Prec@(1,5) (85.8%, 99.2%)
05/30 11:16:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 0.5233	Prec@(1,5) (85.9%, 99.2%)
05/30 11:16:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][350/391]	Step 15249	Loss 0.5225	Prec@(1,5) (85.9%, 99.2%)
05/30 11:16:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 0.5244	Prec@(1,5) (85.8%, 99.2%)
05/30 11:16:49PM searchStage_trainer.py:260 [INFO] Valid: [ 38/49] Final Prec@1 85.8440%
05/30 11:16:49PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:16:49PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.8760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2801, 0.2404, 0.2299],
        [0.2415, 0.2833, 0.2478, 0.2274]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2826, 0.2504, 0.2173],
        [0.2473, 0.2782, 0.2481, 0.2263],
        [0.2462, 0.2855, 0.2294, 0.2389]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2702, 0.2511, 0.2262],
        [0.2600, 0.2751, 0.2303, 0.2346],
        [0.2549, 0.2724, 0.2337, 0.2391]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2694, 0.2418, 0.2341],
        [0.2559, 0.2681, 0.2314, 0.2446],
        [0.2554, 0.2655, 0.2346, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2691, 0.2416, 0.2337],
        [0.2566, 0.2671, 0.2350, 0.2413],
        [0.2530, 0.2619, 0.2344, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2625, 0.2384, 0.2480],
        [0.2515, 0.2606, 0.2411, 0.2468],
        [0.2527, 0.2633, 0.2415, 0.2425]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2499],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2506, 0.2492],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2492, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2507, 0.2509, 0.2446],
        [0.2498, 0.2504, 0.2510, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2536, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2509, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:17:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][50/390]	Step 15299	lr 0.00375	Loss 0.0836 (0.0765)	Prec@(1,5) (97.4%, 100.0%)	
05/30 11:17:24PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.0653 (0.0784)	Prec@(1,5) (97.2%, 100.0%)	
05/30 11:17:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][150/390]	Step 15399	lr 0.00375	Loss 0.1383 (0.0804)	Prec@(1,5) (97.2%, 100.0%)	
05/30 11:17:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.0881 (0.0789)	Prec@(1,5) (97.2%, 100.0%)	
05/30 11:18:15PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][250/390]	Step 15499	lr 0.00375	Loss 0.0332 (0.0793)	Prec@(1,5) (97.3%, 100.0%)	
05/30 11:18:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.0852 (0.0785)	Prec@(1,5) (97.4%, 100.0%)	
05/30 11:18:49PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][350/390]	Step 15599	lr 0.00375	Loss 0.0725 (0.0797)	Prec@(1,5) (97.3%, 100.0%)	
05/30 11:19:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.0593 (0.0803)	Prec@(1,5) (97.3%, 100.0%)	
05/30 11:19:03PM searchStage_trainer.py:221 [INFO] Train: [ 39/49] Final Prec@1 97.2800%
05/30 11:19:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][50/391]	Step 15640	Loss 0.5491	Prec@(1,5) (85.0%, 99.3%)
05/30 11:19:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 0.5290	Prec@(1,5) (85.7%, 99.3%)
05/30 11:19:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][150/391]	Step 15640	Loss 0.5184	Prec@(1,5) (85.9%, 99.3%)
05/30 11:19:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 0.5326	Prec@(1,5) (85.8%, 99.2%)
05/30 11:19:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][250/391]	Step 15640	Loss 0.5282	Prec@(1,5) (85.8%, 99.2%)
05/30 11:19:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 0.5303	Prec@(1,5) (85.8%, 99.2%)
05/30 11:19:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][350/391]	Step 15640	Loss 0.5381	Prec@(1,5) (85.7%, 99.2%)
05/30 11:19:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 0.5408	Prec@(1,5) (85.7%, 99.2%)
05/30 11:19:25PM searchStage_trainer.py:260 [INFO] Valid: [ 39/49] Final Prec@1 85.7120%
05/30 11:19:25PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:19:25PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.8760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2801, 0.2404, 0.2300],
        [0.2415, 0.2833, 0.2478, 0.2274]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2826, 0.2504, 0.2173],
        [0.2473, 0.2782, 0.2482, 0.2264],
        [0.2462, 0.2855, 0.2294, 0.2389]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2702, 0.2511, 0.2262],
        [0.2600, 0.2750, 0.2303, 0.2346],
        [0.2549, 0.2723, 0.2337, 0.2391]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2694, 0.2418, 0.2341],
        [0.2559, 0.2681, 0.2314, 0.2446],
        [0.2554, 0.2655, 0.2346, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2690, 0.2417, 0.2337],
        [0.2566, 0.2671, 0.2350, 0.2413],
        [0.2530, 0.2619, 0.2344, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2625, 0.2384, 0.2480],
        [0.2514, 0.2606, 0.2412, 0.2468],
        [0.2527, 0.2633, 0.2415, 0.2425]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2500],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2506, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2506, 0.2492, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2507, 0.2509, 0.2445],
        [0.2498, 0.2504, 0.2510, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2536, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2509, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:19:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][50/390]	Step 15690	lr 0.00329	Loss 0.0572 (0.0576)	Prec@(1,5) (98.0%, 100.0%)	
05/30 11:19:59PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.0512 (0.0585)	Prec@(1,5) (98.0%, 100.0%)	
05/30 11:20:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][150/390]	Step 15790	lr 0.00329	Loss 0.0118 (0.0598)	Prec@(1,5) (97.9%, 100.0%)	
05/30 11:20:33PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.0727 (0.0614)	Prec@(1,5) (97.8%, 100.0%)	
05/30 11:20:50PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][250/390]	Step 15890	lr 0.00329	Loss 0.0590 (0.0629)	Prec@(1,5) (97.8%, 100.0%)	
05/30 11:21:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.0136 (0.0641)	Prec@(1,5) (97.7%, 100.0%)	
05/30 11:21:24PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][350/390]	Step 15990	lr 0.00329	Loss 0.0269 (0.0658)	Prec@(1,5) (97.6%, 100.0%)	
05/30 11:21:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.0182 (0.0670)	Prec@(1,5) (97.6%, 100.0%)	
05/30 11:21:38PM searchStage_trainer.py:221 [INFO] Train: [ 40/49] Final Prec@1 97.5960%
05/30 11:21:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][50/391]	Step 16031	Loss 0.5254	Prec@(1,5) (86.2%, 99.2%)
05/30 11:21:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 0.5478	Prec@(1,5) (85.7%, 99.1%)
05/30 11:21:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][150/391]	Step 16031	Loss 0.5470	Prec@(1,5) (85.6%, 99.2%)
05/30 11:21:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 0.5494	Prec@(1,5) (85.7%, 99.1%)
05/30 11:21:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][250/391]	Step 16031	Loss 0.5442	Prec@(1,5) (85.9%, 99.1%)
05/30 11:21:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 0.5528	Prec@(1,5) (85.8%, 99.1%)
05/30 11:21:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][350/391]	Step 16031	Loss 0.5457	Prec@(1,5) (85.9%, 99.1%)
05/30 11:21:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 0.5474	Prec@(1,5) (85.9%, 99.1%)
05/30 11:21:59PM searchStage_trainer.py:260 [INFO] Valid: [ 40/49] Final Prec@1 85.9440%
05/30 11:21:59PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:21:59PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.9440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2801, 0.2405, 0.2300],
        [0.2415, 0.2832, 0.2478, 0.2274]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2826, 0.2505, 0.2173],
        [0.2473, 0.2782, 0.2482, 0.2264],
        [0.2462, 0.2854, 0.2294, 0.2389]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2702, 0.2512, 0.2262],
        [0.2600, 0.2750, 0.2304, 0.2346],
        [0.2549, 0.2723, 0.2337, 0.2391]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2693, 0.2418, 0.2341],
        [0.2559, 0.2680, 0.2314, 0.2446],
        [0.2554, 0.2654, 0.2346, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2690, 0.2417, 0.2337],
        [0.2566, 0.2670, 0.2350, 0.2413],
        [0.2529, 0.2618, 0.2344, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2625, 0.2384, 0.2480],
        [0.2514, 0.2606, 0.2412, 0.2468],
        [0.2527, 0.2632, 0.2416, 0.2425]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2500],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2506, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2505, 0.2502, 0.2485],
        [0.2510, 0.2505, 0.2492, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2507, 0.2509, 0.2445],
        [0.2498, 0.2504, 0.2510, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2536, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2509, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:22:17PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][50/390]	Step 16081	lr 0.00287	Loss 0.0286 (0.0599)	Prec@(1,5) (98.1%, 100.0%)	
05/30 11:22:34PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.0208 (0.0562)	Prec@(1,5) (98.1%, 100.0%)	
05/30 11:22:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][150/390]	Step 16181	lr 0.00287	Loss 0.0469 (0.0539)	Prec@(1,5) (98.2%, 100.0%)	
05/30 11:23:08PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.0971 (0.0534)	Prec@(1,5) (98.3%, 100.0%)	
05/30 11:23:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][250/390]	Step 16281	lr 0.00287	Loss 0.0882 (0.0540)	Prec@(1,5) (98.3%, 100.0%)	
05/30 11:23:42PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.0335 (0.0535)	Prec@(1,5) (98.3%, 100.0%)	
05/30 11:24:00PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][350/390]	Step 16381	lr 0.00287	Loss 0.0239 (0.0537)	Prec@(1,5) (98.2%, 100.0%)	
05/30 11:24:13PM searchStage_trainer.py:211 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.0492 (0.0551)	Prec@(1,5) (98.2%, 100.0%)	
05/30 11:24:14PM searchStage_trainer.py:221 [INFO] Train: [ 41/49] Final Prec@1 98.1680%
05/30 11:24:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][50/391]	Step 16422	Loss 0.5787	Prec@(1,5) (85.9%, 99.0%)
05/30 11:24:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 0.5599	Prec@(1,5) (86.5%, 99.2%)
05/30 11:24:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][150/391]	Step 16422	Loss 0.5674	Prec@(1,5) (86.5%, 99.1%)
05/30 11:24:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 0.5671	Prec@(1,5) (86.1%, 99.2%)
05/30 11:24:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][250/391]	Step 16422	Loss 0.5615	Prec@(1,5) (86.1%, 99.2%)
05/30 11:24:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 0.5594	Prec@(1,5) (86.1%, 99.2%)
05/30 11:24:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][350/391]	Step 16422	Loss 0.5591	Prec@(1,5) (86.1%, 99.2%)
05/30 11:24:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 0.5582	Prec@(1,5) (86.1%, 99.2%)
05/30 11:24:35PM searchStage_trainer.py:260 [INFO] Valid: [ 41/49] Final Prec@1 86.0640%
05/30 11:24:35PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:24:35PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.0640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2800, 0.2405, 0.2300],
        [0.2415, 0.2832, 0.2478, 0.2274]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2825, 0.2505, 0.2173],
        [0.2473, 0.2781, 0.2482, 0.2264],
        [0.2462, 0.2854, 0.2295, 0.2389]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2701, 0.2512, 0.2262],
        [0.2600, 0.2750, 0.2304, 0.2347],
        [0.2549, 0.2722, 0.2337, 0.2391]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2693, 0.2418, 0.2341],
        [0.2559, 0.2680, 0.2314, 0.2446],
        [0.2554, 0.2654, 0.2346, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2690, 0.2417, 0.2337],
        [0.2566, 0.2670, 0.2351, 0.2414],
        [0.2529, 0.2618, 0.2345, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2624, 0.2385, 0.2480],
        [0.2514, 0.2605, 0.2412, 0.2468],
        [0.2527, 0.2632, 0.2416, 0.2425]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2500],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2506, 0.2493],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2505, 0.2502, 0.2485],
        [0.2510, 0.2505, 0.2492, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2507, 0.2509, 0.2445],
        [0.2498, 0.2504, 0.2510, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2535, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2508, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:24:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][50/390]	Step 16472	lr 0.00248	Loss 0.0749 (0.0564)	Prec@(1,5) (98.2%, 100.0%)	
05/30 11:25:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.0719 (0.0519)	Prec@(1,5) (98.2%, 100.0%)	
05/30 11:25:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][150/390]	Step 16572	lr 0.00248	Loss 0.0134 (0.0501)	Prec@(1,5) (98.3%, 100.0%)	
05/30 11:25:44PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.0677 (0.0517)	Prec@(1,5) (98.2%, 100.0%)	
05/30 11:26:01PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][250/390]	Step 16672	lr 0.00248	Loss 0.0126 (0.0500)	Prec@(1,5) (98.3%, 100.0%)	
05/30 11:26:18PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.0677 (0.0506)	Prec@(1,5) (98.3%, 100.0%)	
05/30 11:26:35PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][350/390]	Step 16772	lr 0.00248	Loss 0.0474 (0.0504)	Prec@(1,5) (98.3%, 100.0%)	
05/30 11:26:48PM searchStage_trainer.py:211 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.0181 (0.0492)	Prec@(1,5) (98.3%, 100.0%)	
05/30 11:26:49PM searchStage_trainer.py:221 [INFO] Train: [ 42/49] Final Prec@1 98.3480%
05/30 11:26:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][50/391]	Step 16813	Loss 0.5811	Prec@(1,5) (85.8%, 99.4%)
05/30 11:26:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 0.5649	Prec@(1,5) (86.2%, 99.2%)
05/30 11:26:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][150/391]	Step 16813	Loss 0.5747	Prec@(1,5) (86.0%, 99.2%)
05/30 11:27:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 0.5607	Prec@(1,5) (86.4%, 99.2%)
05/30 11:27:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][250/391]	Step 16813	Loss 0.5542	Prec@(1,5) (86.5%, 99.2%)
05/30 11:27:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 0.5575	Prec@(1,5) (86.5%, 99.2%)
05/30 11:27:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][350/391]	Step 16813	Loss 0.5581	Prec@(1,5) (86.4%, 99.2%)
05/30 11:27:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 0.5613	Prec@(1,5) (86.4%, 99.2%)
05/30 11:27:10PM searchStage_trainer.py:260 [INFO] Valid: [ 42/49] Final Prec@1 86.3520%
05/30 11:27:10PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:27:10PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.3520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2800, 0.2405, 0.2300],
        [0.2415, 0.2832, 0.2479, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2825, 0.2505, 0.2173],
        [0.2472, 0.2781, 0.2483, 0.2264],
        [0.2462, 0.2853, 0.2295, 0.2390]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2701, 0.2512, 0.2262],
        [0.2600, 0.2749, 0.2304, 0.2347],
        [0.2549, 0.2722, 0.2338, 0.2392]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2693, 0.2418, 0.2341],
        [0.2559, 0.2679, 0.2314, 0.2447],
        [0.2553, 0.2653, 0.2347, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2689, 0.2417, 0.2337],
        [0.2566, 0.2669, 0.2351, 0.2414],
        [0.2529, 0.2617, 0.2345, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2624, 0.2385, 0.2481],
        [0.2514, 0.2605, 0.2412, 0.2468],
        [0.2527, 0.2632, 0.2416, 0.2425]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2500],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2511, 0.2506, 0.2492],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2505, 0.2502, 0.2485],
        [0.2510, 0.2505, 0.2493, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2507, 0.2509, 0.2445],
        [0.2498, 0.2504, 0.2510, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2535, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2508, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:27:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][50/390]	Step 16863	lr 0.00214	Loss 0.0381 (0.0405)	Prec@(1,5) (98.7%, 100.0%)	
05/30 11:27:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.0169 (0.0411)	Prec@(1,5) (98.6%, 100.0%)	
05/30 11:28:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][150/390]	Step 16963	lr 0.00214	Loss 0.0426 (0.0404)	Prec@(1,5) (98.6%, 100.0%)	
05/30 11:28:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.0450 (0.0412)	Prec@(1,5) (98.6%, 100.0%)	
05/30 11:28:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][250/390]	Step 17063	lr 0.00214	Loss 0.0563 (0.0408)	Prec@(1,5) (98.6%, 100.0%)	
05/30 11:28:52PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.0747 (0.0423)	Prec@(1,5) (98.6%, 100.0%)	
05/30 11:29:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][350/390]	Step 17163	lr 0.00214	Loss 0.0379 (0.0424)	Prec@(1,5) (98.6%, 100.0%)	
05/30 11:29:23PM searchStage_trainer.py:211 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.0865 (0.0421)	Prec@(1,5) (98.6%, 100.0%)	
05/30 11:29:23PM searchStage_trainer.py:221 [INFO] Train: [ 43/49] Final Prec@1 98.6160%
05/30 11:29:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][50/391]	Step 17204	Loss 0.5426	Prec@(1,5) (86.8%, 99.2%)
05/30 11:29:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 0.5606	Prec@(1,5) (86.6%, 99.3%)
05/30 11:29:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][150/391]	Step 17204	Loss 0.5703	Prec@(1,5) (86.5%, 99.2%)
05/30 11:29:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 0.5685	Prec@(1,5) (86.6%, 99.2%)
05/30 11:29:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][250/391]	Step 17204	Loss 0.5660	Prec@(1,5) (86.7%, 99.2%)
05/30 11:29:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 0.5631	Prec@(1,5) (86.7%, 99.2%)
05/30 11:29:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][350/391]	Step 17204	Loss 0.5652	Prec@(1,5) (86.6%, 99.3%)
05/30 11:29:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 0.5620	Prec@(1,5) (86.6%, 99.3%)
05/30 11:29:44PM searchStage_trainer.py:260 [INFO] Valid: [ 43/49] Final Prec@1 86.5720%
05/30 11:29:44PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:29:45PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.5720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2799, 0.2405, 0.2300],
        [0.2415, 0.2831, 0.2479, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2825, 0.2505, 0.2174],
        [0.2473, 0.2781, 0.2483, 0.2264],
        [0.2462, 0.2853, 0.2295, 0.2390]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2702, 0.2512, 0.2262],
        [0.2600, 0.2749, 0.2304, 0.2347],
        [0.2549, 0.2721, 0.2338, 0.2392]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2693, 0.2418, 0.2341],
        [0.2559, 0.2679, 0.2315, 0.2447],
        [0.2553, 0.2653, 0.2347, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2689, 0.2417, 0.2338],
        [0.2566, 0.2669, 0.2351, 0.2414],
        [0.2529, 0.2617, 0.2345, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2624, 0.2385, 0.2481],
        [0.2514, 0.2604, 0.2413, 0.2469],
        [0.2527, 0.2631, 0.2416, 0.2426]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2500],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2510, 0.2506, 0.2492],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2505, 0.2493, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2507, 0.2509, 0.2445],
        [0.2498, 0.2504, 0.2510, 0.2487],
        [0.2497, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2535, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2508, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:30:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][50/390]	Step 17254	lr 0.00184	Loss 0.0505 (0.0403)	Prec@(1,5) (98.8%, 100.0%)	
05/30 11:30:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.0305 (0.0384)	Prec@(1,5) (98.8%, 100.0%)	
05/30 11:30:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][150/390]	Step 17354	lr 0.00184	Loss 0.0164 (0.0347)	Prec@(1,5) (98.9%, 100.0%)	
05/30 11:30:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.0547 (0.0356)	Prec@(1,5) (98.9%, 100.0%)	
05/30 11:31:10PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][250/390]	Step 17454	lr 0.00184	Loss 0.0125 (0.0368)	Prec@(1,5) (98.9%, 100.0%)	
05/30 11:31:27PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.0116 (0.0370)	Prec@(1,5) (98.8%, 100.0%)	
05/30 11:31:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][350/390]	Step 17554	lr 0.00184	Loss 0.0380 (0.0370)	Prec@(1,5) (98.8%, 100.0%)	
05/30 11:31:58PM searchStage_trainer.py:211 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.0263 (0.0368)	Prec@(1,5) (98.8%, 100.0%)	
05/30 11:31:58PM searchStage_trainer.py:221 [INFO] Train: [ 44/49] Final Prec@1 98.8440%
05/30 11:32:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][50/391]	Step 17595	Loss 0.5587	Prec@(1,5) (86.6%, 99.1%)
05/30 11:32:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 0.5513	Prec@(1,5) (86.8%, 99.1%)
05/30 11:32:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][150/391]	Step 17595	Loss 0.5540	Prec@(1,5) (86.7%, 99.2%)
05/30 11:32:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 0.5537	Prec@(1,5) (86.7%, 99.2%)
05/30 11:32:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][250/391]	Step 17595	Loss 0.5609	Prec@(1,5) (86.7%, 99.2%)
05/30 11:32:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 0.5669	Prec@(1,5) (86.6%, 99.2%)
05/30 11:32:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][350/391]	Step 17595	Loss 0.5692	Prec@(1,5) (86.5%, 99.2%)
05/30 11:32:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 0.5686	Prec@(1,5) (86.5%, 99.2%)
05/30 11:32:20PM searchStage_trainer.py:260 [INFO] Valid: [ 44/49] Final Prec@1 86.5000%
05/30 11:32:20PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:32:20PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.5720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2799, 0.2406, 0.2301],
        [0.2415, 0.2831, 0.2479, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2824, 0.2505, 0.2174],
        [0.2472, 0.2781, 0.2483, 0.2264],
        [0.2462, 0.2853, 0.2295, 0.2390]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2701, 0.2512, 0.2262],
        [0.2600, 0.2749, 0.2304, 0.2347],
        [0.2549, 0.2721, 0.2338, 0.2392]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2693, 0.2418, 0.2341],
        [0.2559, 0.2679, 0.2315, 0.2447],
        [0.2553, 0.2653, 0.2347, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2688, 0.2417, 0.2338],
        [0.2566, 0.2669, 0.2351, 0.2414],
        [0.2529, 0.2617, 0.2345, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2623, 0.2385, 0.2481],
        [0.2514, 0.2604, 0.2413, 0.2469],
        [0.2527, 0.2631, 0.2416, 0.2426]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2500],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2510, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2511, 0.2506, 0.2492],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2484],
        [0.2510, 0.2505, 0.2493, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2507, 0.2509, 0.2445],
        [0.2499, 0.2504, 0.2510, 0.2487],
        [0.2498, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2535, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2543, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2508, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2553, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:32:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][50/390]	Step 17645	lr 0.00159	Loss 0.0235 (0.0388)	Prec@(1,5) (98.7%, 100.0%)	
05/30 11:32:54PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.0458 (0.0359)	Prec@(1,5) (98.9%, 100.0%)	
05/30 11:33:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][150/390]	Step 17745	lr 0.00159	Loss 0.0149 (0.0361)	Prec@(1,5) (98.8%, 100.0%)	
05/30 11:33:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.0339 (0.0362)	Prec@(1,5) (98.8%, 100.0%)	
05/30 11:33:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][250/390]	Step 17845	lr 0.00159	Loss 0.0277 (0.0352)	Prec@(1,5) (98.9%, 100.0%)	
05/30 11:34:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.0637 (0.0354)	Prec@(1,5) (98.9%, 100.0%)	
05/30 11:34:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][350/390]	Step 17945	lr 0.00159	Loss 0.0062 (0.0343)	Prec@(1,5) (98.9%, 100.0%)	
05/30 11:34:32PM searchStage_trainer.py:211 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.0553 (0.0343)	Prec@(1,5) (98.9%, 100.0%)	
05/30 11:34:33PM searchStage_trainer.py:221 [INFO] Train: [ 45/49] Final Prec@1 98.9120%
05/30 11:34:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][50/391]	Step 17986	Loss 0.5538	Prec@(1,5) (86.6%, 99.5%)
05/30 11:34:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 0.5777	Prec@(1,5) (86.6%, 99.2%)
05/30 11:34:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][150/391]	Step 17986	Loss 0.5772	Prec@(1,5) (86.3%, 99.2%)
05/30 11:34:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 0.5827	Prec@(1,5) (86.2%, 99.2%)
05/30 11:34:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][250/391]	Step 17986	Loss 0.5736	Prec@(1,5) (86.4%, 99.2%)
05/30 11:34:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 0.5696	Prec@(1,5) (86.5%, 99.2%)
05/30 11:34:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][350/391]	Step 17986	Loss 0.5714	Prec@(1,5) (86.5%, 99.2%)
05/30 11:34:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 0.5717	Prec@(1,5) (86.5%, 99.2%)
05/30 11:34:54PM searchStage_trainer.py:260 [INFO] Valid: [ 45/49] Final Prec@1 86.4720%
05/30 11:34:54PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:34:54PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.5720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2799, 0.2406, 0.2301],
        [0.2415, 0.2830, 0.2480, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2824, 0.2505, 0.2174],
        [0.2472, 0.2781, 0.2483, 0.2264],
        [0.2462, 0.2852, 0.2296, 0.2390]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2701, 0.2513, 0.2262],
        [0.2600, 0.2749, 0.2304, 0.2347],
        [0.2549, 0.2720, 0.2338, 0.2392]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2548, 0.2692, 0.2418, 0.2341],
        [0.2559, 0.2679, 0.2315, 0.2447],
        [0.2553, 0.2652, 0.2347, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2688, 0.2418, 0.2338],
        [0.2566, 0.2668, 0.2351, 0.2414],
        [0.2529, 0.2616, 0.2345, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2623, 0.2385, 0.2481],
        [0.2514, 0.2604, 0.2413, 0.2469],
        [0.2527, 0.2630, 0.2417, 0.2426]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2500],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2509, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2511, 0.2506, 0.2492],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2505, 0.2493, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2509, 0.2506],
        [0.2484, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2507, 0.2508, 0.2445],
        [0.2499, 0.2504, 0.2510, 0.2487],
        [0.2498, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2535, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2508, 0.2556, 0.2511],
        [0.2452, 0.2504, 0.2552, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:35:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][50/390]	Step 18036	lr 0.00138	Loss 0.0152 (0.0222)	Prec@(1,5) (99.4%, 100.0%)	
05/30 11:35:29PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.0488 (0.0268)	Prec@(1,5) (99.1%, 100.0%)	
05/30 11:35:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][150/390]	Step 18136	lr 0.00138	Loss 0.0136 (0.0270)	Prec@(1,5) (99.1%, 100.0%)	
05/30 11:36:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.0046 (0.0278)	Prec@(1,5) (99.1%, 100.0%)	
05/30 11:36:19PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][250/390]	Step 18236	lr 0.00138	Loss 0.0281 (0.0265)	Prec@(1,5) (99.1%, 100.0%)	
05/30 11:36:36PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.0103 (0.0269)	Prec@(1,5) (99.1%, 100.0%)	
05/30 11:36:53PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][350/390]	Step 18336	lr 0.00138	Loss 0.0470 (0.0278)	Prec@(1,5) (99.1%, 100.0%)	
05/30 11:37:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.0516 (0.0283)	Prec@(1,5) (99.1%, 100.0%)	
05/30 11:37:07PM searchStage_trainer.py:221 [INFO] Train: [ 46/49] Final Prec@1 99.0600%
05/30 11:37:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][50/391]	Step 18377	Loss 0.5425	Prec@(1,5) (87.2%, 99.1%)
05/30 11:37:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 0.5696	Prec@(1,5) (87.0%, 99.1%)
05/30 11:37:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][150/391]	Step 18377	Loss 0.5711	Prec@(1,5) (86.7%, 99.2%)
05/30 11:37:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 0.5760	Prec@(1,5) (86.7%, 99.2%)
05/30 11:37:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][250/391]	Step 18377	Loss 0.5782	Prec@(1,5) (86.7%, 99.2%)
05/30 11:37:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 0.5807	Prec@(1,5) (86.6%, 99.2%)
05/30 11:37:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][350/391]	Step 18377	Loss 0.5817	Prec@(1,5) (86.6%, 99.2%)
05/30 11:37:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 0.5796	Prec@(1,5) (86.7%, 99.2%)
05/30 11:37:28PM searchStage_trainer.py:260 [INFO] Valid: [ 46/49] Final Prec@1 86.7000%
05/30 11:37:28PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:37:29PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.7000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2798, 0.2406, 0.2301],
        [0.2415, 0.2830, 0.2480, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2824, 0.2505, 0.2174],
        [0.2472, 0.2781, 0.2484, 0.2264],
        [0.2462, 0.2852, 0.2296, 0.2390]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2701, 0.2513, 0.2262],
        [0.2600, 0.2748, 0.2305, 0.2347],
        [0.2549, 0.2720, 0.2338, 0.2392]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2692, 0.2418, 0.2341],
        [0.2559, 0.2678, 0.2315, 0.2447],
        [0.2553, 0.2652, 0.2348, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2688, 0.2418, 0.2338],
        [0.2566, 0.2668, 0.2352, 0.2415],
        [0.2529, 0.2616, 0.2346, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2623, 0.2385, 0.2482],
        [0.2514, 0.2603, 0.2413, 0.2469],
        [0.2527, 0.2630, 0.2417, 0.2426]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2500],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2509, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2511, 0.2506, 0.2492],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2502, 0.2485],
        [0.2510, 0.2505, 0.2493, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2497, 0.2509, 0.2506],
        [0.2485, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2507, 0.2508, 0.2445],
        [0.2499, 0.2504, 0.2510, 0.2487],
        [0.2498, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2535, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2508, 0.2556, 0.2511],
        [0.2453, 0.2504, 0.2552, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:37:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][50/390]	Step 18427	lr 0.00121	Loss 0.0298 (0.0241)	Prec@(1,5) (99.3%, 100.0%)	
05/30 11:38:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.0242 (0.0244)	Prec@(1,5) (99.2%, 100.0%)	
05/30 11:38:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][150/390]	Step 18527	lr 0.00121	Loss 0.0039 (0.0251)	Prec@(1,5) (99.1%, 100.0%)	
05/30 11:38:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.0300 (0.0255)	Prec@(1,5) (99.2%, 100.0%)	
05/30 11:38:54PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][250/390]	Step 18627	lr 0.00121	Loss 0.0292 (0.0250)	Prec@(1,5) (99.2%, 100.0%)	
05/30 11:39:11PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.0112 (0.0252)	Prec@(1,5) (99.2%, 100.0%)	
05/30 11:39:28PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][350/390]	Step 18727	lr 0.00121	Loss 0.0126 (0.0246)	Prec@(1,5) (99.3%, 100.0%)	
05/30 11:39:41PM searchStage_trainer.py:211 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.0528 (0.0249)	Prec@(1,5) (99.3%, 100.0%)	
05/30 11:39:42PM searchStage_trainer.py:221 [INFO] Train: [ 47/49] Final Prec@1 99.2600%
05/30 11:39:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][50/391]	Step 18768	Loss 0.6091	Prec@(1,5) (85.9%, 98.9%)
05/30 11:39:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 0.5896	Prec@(1,5) (86.5%, 99.2%)
05/30 11:39:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][150/391]	Step 18768	Loss 0.5946	Prec@(1,5) (86.4%, 99.3%)
05/30 11:39:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 0.6044	Prec@(1,5) (86.2%, 99.2%)
05/30 11:39:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][250/391]	Step 18768	Loss 0.6037	Prec@(1,5) (86.3%, 99.2%)
05/30 11:39:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 0.6002	Prec@(1,5) (86.4%, 99.2%)
05/30 11:40:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][350/391]	Step 18768	Loss 0.5951	Prec@(1,5) (86.5%, 99.2%)
05/30 11:40:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 0.5919	Prec@(1,5) (86.6%, 99.2%)
05/30 11:40:03PM searchStage_trainer.py:260 [INFO] Valid: [ 47/49] Final Prec@1 86.5960%
05/30 11:40:03PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:40:03PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.7000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2798, 0.2407, 0.2301],
        [0.2415, 0.2830, 0.2480, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2824, 0.2506, 0.2174],
        [0.2472, 0.2780, 0.2484, 0.2264],
        [0.2462, 0.2852, 0.2296, 0.2391]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2701, 0.2513, 0.2263],
        [0.2600, 0.2748, 0.2305, 0.2347],
        [0.2549, 0.2720, 0.2339, 0.2392]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2692, 0.2418, 0.2341],
        [0.2559, 0.2678, 0.2315, 0.2448],
        [0.2553, 0.2652, 0.2348, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2688, 0.2418, 0.2338],
        [0.2566, 0.2667, 0.2352, 0.2415],
        [0.2529, 0.2616, 0.2346, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2622, 0.2385, 0.2482],
        [0.2514, 0.2603, 0.2413, 0.2469],
        [0.2527, 0.2630, 0.2417, 0.2427]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2500],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2509, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2511, 0.2506, 0.2492],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2505, 0.2502, 0.2485],
        [0.2510, 0.2505, 0.2493, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2497, 0.2509, 0.2506],
        [0.2485, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2507, 0.2508, 0.2445],
        [0.2499, 0.2504, 0.2510, 0.2487],
        [0.2498, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2535, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2508, 0.2556, 0.2511],
        [0.2453, 0.2504, 0.2552, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:40:21PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][50/390]	Step 18818	lr 0.00109	Loss 0.0328 (0.0327)	Prec@(1,5) (98.7%, 100.0%)	
05/30 11:40:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.0630 (0.0296)	Prec@(1,5) (98.9%, 100.0%)	
05/30 11:40:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][150/390]	Step 18918	lr 0.00109	Loss 0.0091 (0.0285)	Prec@(1,5) (99.0%, 100.0%)	
05/30 11:41:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.0080 (0.0280)	Prec@(1,5) (99.1%, 100.0%)	
05/30 11:41:29PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][250/390]	Step 19018	lr 0.00109	Loss 0.0084 (0.0259)	Prec@(1,5) (99.2%, 100.0%)	
05/30 11:41:45PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.0240 (0.0258)	Prec@(1,5) (99.2%, 100.0%)	
05/30 11:42:02PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][350/390]	Step 19118	lr 0.00109	Loss 0.0397 (0.0255)	Prec@(1,5) (99.2%, 100.0%)	
05/30 11:42:16PM searchStage_trainer.py:211 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.0254 (0.0254)	Prec@(1,5) (99.2%, 100.0%)	
05/30 11:42:16PM searchStage_trainer.py:221 [INFO] Train: [ 48/49] Final Prec@1 99.2120%
05/30 11:42:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][50/391]	Step 19159	Loss 0.6034	Prec@(1,5) (87.2%, 99.2%)
05/30 11:42:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 0.5994	Prec@(1,5) (87.0%, 99.1%)
05/30 11:42:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][150/391]	Step 19159	Loss 0.5982	Prec@(1,5) (87.0%, 99.1%)
05/30 11:42:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 0.5843	Prec@(1,5) (87.3%, 99.2%)
05/30 11:42:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][250/391]	Step 19159	Loss 0.5766	Prec@(1,5) (87.3%, 99.1%)
05/30 11:42:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 0.5757	Prec@(1,5) (87.2%, 99.2%)
05/30 11:42:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][350/391]	Step 19159	Loss 0.5821	Prec@(1,5) (87.0%, 99.2%)
05/30 11:42:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 0.5865	Prec@(1,5) (86.9%, 99.2%)
05/30 11:42:37PM searchStage_trainer.py:260 [INFO] Valid: [ 48/49] Final Prec@1 86.9360%
05/30 11:42:37PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:42:38PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.9360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2798, 0.2407, 0.2301],
        [0.2415, 0.2830, 0.2480, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2823, 0.2506, 0.2174],
        [0.2472, 0.2780, 0.2484, 0.2264],
        [0.2462, 0.2851, 0.2296, 0.2391]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2701, 0.2513, 0.2263],
        [0.2600, 0.2748, 0.2305, 0.2347],
        [0.2549, 0.2720, 0.2339, 0.2393]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2692, 0.2418, 0.2341],
        [0.2559, 0.2678, 0.2315, 0.2448],
        [0.2553, 0.2651, 0.2348, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2687, 0.2418, 0.2338],
        [0.2566, 0.2667, 0.2352, 0.2415],
        [0.2529, 0.2615, 0.2346, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2622, 0.2385, 0.2482],
        [0.2514, 0.2603, 0.2414, 0.2470],
        [0.2527, 0.2629, 0.2417, 0.2427]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2508, 0.2504, 0.2500],
        [0.2493, 0.2505, 0.2509, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2509, 0.2521, 0.2480],
        [0.2489, 0.2495, 0.2508, 0.2508],
        [0.2517, 0.2513, 0.2485, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2511, 0.2506, 0.2492],
        [0.2508, 0.2521, 0.2492, 0.2479],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2513, 0.2501, 0.2479],
        [0.2503, 0.2504, 0.2489, 0.2505],
        [0.2505, 0.2504, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2505, 0.2502, 0.2485],
        [0.2510, 0.2505, 0.2493, 0.2492],
        [0.2509, 0.2511, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2506, 0.2500, 0.2490],
        [0.2498, 0.2504, 0.2503, 0.2495],
        [0.2507, 0.2510, 0.2487, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2497, 0.2509, 0.2506],
        [0.2485, 0.2508, 0.2512, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2516, 0.2516, 0.2488],
        [0.2488, 0.2506, 0.2493, 0.2513],
        [0.2476, 0.2513, 0.2515, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2507, 0.2508, 0.2444],
        [0.2499, 0.2504, 0.2510, 0.2487],
        [0.2498, 0.2489, 0.2481, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2508, 0.2498, 0.2496],
        [0.2484, 0.2507, 0.2535, 0.2474],
        [0.2497, 0.2496, 0.2485, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2510, 0.2517, 0.2531],
        [0.2432, 0.2516, 0.2544, 0.2509],
        [0.2433, 0.2528, 0.2572, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2483, 0.2556, 0.2504],
        [0.2425, 0.2508, 0.2556, 0.2511],
        [0.2453, 0.2504, 0.2552, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:42:55PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][50/390]	Step 19209	lr 0.00102	Loss 0.0037 (0.0234)	Prec@(1,5) (99.3%, 100.0%)	
05/30 11:43:12PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.0567 (0.0236)	Prec@(1,5) (99.3%, 100.0%)	
05/30 11:43:29PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][150/390]	Step 19309	lr 0.00102	Loss 0.0090 (0.0237)	Prec@(1,5) (99.3%, 100.0%)	
05/30 11:43:46PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.0105 (0.0240)	Prec@(1,5) (99.3%, 100.0%)	
05/30 11:44:03PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][250/390]	Step 19409	lr 0.00102	Loss 0.0072 (0.0242)	Prec@(1,5) (99.3%, 100.0%)	
05/30 11:44:20PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.0135 (0.0240)	Prec@(1,5) (99.3%, 100.0%)	
05/30 11:44:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][350/390]	Step 19509	lr 0.00102	Loss 0.0070 (0.0240)	Prec@(1,5) (99.3%, 100.0%)	
05/30 11:44:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.0264 (0.0238)	Prec@(1,5) (99.3%, 100.0%)	
05/30 11:44:51PM searchStage_trainer.py:221 [INFO] Train: [ 49/49] Final Prec@1 99.3320%
05/30 11:44:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][50/391]	Step 19550	Loss 0.6313	Prec@(1,5) (85.3%, 99.1%)
05/30 11:44:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 0.6000	Prec@(1,5) (86.0%, 99.1%)
05/30 11:45:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][150/391]	Step 19550	Loss 0.6052	Prec@(1,5) (86.1%, 99.1%)
05/30 11:45:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 0.6016	Prec@(1,5) (86.2%, 99.1%)
05/30 11:45:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][250/391]	Step 19550	Loss 0.6054	Prec@(1,5) (86.5%, 99.1%)
05/30 11:45:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 0.5880	Prec@(1,5) (86.8%, 99.1%)
05/30 11:45:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][350/391]	Step 19550	Loss 0.5899	Prec@(1,5) (86.7%, 99.2%)
05/30 11:45:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 0.5881	Prec@(1,5) (86.8%, 99.1%)
05/30 11:45:13PM searchStage_trainer.py:260 [INFO] Valid: [ 49/49] Final Prec@1 86.7920%
05/30 11:45:13PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:45:13PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.9360%
05/30 11:45:13PM searchStage_main.py:84 [INFO] Final best Prec@1 = 86.9360%
05/30 11:45:13PM searchStage_main.py:85 [INFO] Final Best Genotype = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
