05/31 12:22:12AM parser.py:28 [INFO] 
05/31 12:22:12AM parser.py:29 [INFO] Parameters:
05/31 12:22:12AM parser.py:31 [INFO] DAG_PATH=results/search_Stage/CIFAR10/SCRATCH_PCDARTS_STAGESPEC/EXP-20240531-002212/DAG
05/31 12:22:12AM parser.py:31 [INFO] ALPHA_LR=0.0003
05/31 12:22:12AM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
05/31 12:22:12AM parser.py:31 [INFO] BATCH_SIZE=64
05/31 12:22:12AM parser.py:31 [INFO] CHECKPOINT_RESET=False
05/31 12:22:12AM parser.py:31 [INFO] DATA_PATH=../data/
05/31 12:22:12AM parser.py:31 [INFO] DATASET=CIFAR10
05/31 12:22:12AM parser.py:31 [INFO] EPOCHS=50
05/31 12:22:12AM parser.py:31 [INFO] EXP_NAME=EXP-20240531-002212
05/31 12:22:12AM parser.py:31 [INFO] GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 3)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('none', 1), ('avg_pool_3x3', 2)], [('skip_connect', 2), ('sep_conv_5x5', 3)], [('none', 3), ('avg_pool_3x3', 4)]], reduce_concat=range(2, 6))
05/31 12:22:12AM parser.py:31 [INFO] GPUS=[0]
05/31 12:22:12AM parser.py:31 [INFO] INIT_CHANNELS=16
05/31 12:22:12AM parser.py:31 [INFO] LAYERS=20
05/31 12:22:12AM parser.py:31 [INFO] LOCAL_RANK=0
05/31 12:22:12AM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/31 12:22:12AM parser.py:31 [INFO] NAME=SCRATCH_PCDARTS_STAGESPEC
05/31 12:22:12AM parser.py:31 [INFO] PATH=results/search_Stage/CIFAR10/SCRATCH_PCDARTS_STAGESPEC/EXP-20240531-002212
05/31 12:22:12AM parser.py:31 [INFO] PRINT_FREQ=50
05/31 12:22:12AM parser.py:31 [INFO] RESUME_PATH=None
05/31 12:22:12AM parser.py:31 [INFO] SAVE=EXP
05/31 12:22:12AM parser.py:31 [INFO] SEED=3
05/31 12:22:12AM parser.py:31 [INFO] SHARE_STAGE=False
05/31 12:22:12AM parser.py:31 [INFO] SPEC_CELL=False
05/31 12:22:12AM parser.py:31 [INFO] TRAIN_PORTION=0.5
05/31 12:22:12AM parser.py:31 [INFO] W_GRAD_CLIP=5.0
05/31 12:22:12AM parser.py:31 [INFO] W_LR=0.025
05/31 12:22:12AM parser.py:31 [INFO] W_LR_MIN=0.001
05/31 12:22:12AM parser.py:31 [INFO] W_MOMENTUM=0.9
05/31 12:22:12AM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
05/31 12:22:12AM parser.py:31 [INFO] WORKERS=4
05/31 12:22:12AM parser.py:32 [INFO] 
05/31 12:22:14AM searchStage_trainer.py:103 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2500, 0.2500, 0.2498],
        [0.2500, 0.2497, 0.2501, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2500, 0.2502],
        [0.2498, 0.2501, 0.2501, 0.2500],
        [0.2501, 0.2501, 0.2500, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2498, 0.2502, 0.2501],
        [0.2500, 0.2501, 0.2498, 0.2502],
        [0.2498, 0.2500, 0.2502, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2498, 0.2501, 0.2502],
        [0.2501, 0.2503, 0.2495, 0.2501],
        [0.2499, 0.2500, 0.2498, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2503, 0.2499, 0.2498],
        [0.2500, 0.2502, 0.2500, 0.2498],
        [0.2501, 0.2497, 0.2498, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2502, 0.2498, 0.2502],
        [0.2500, 0.2501, 0.2500, 0.2499],
        [0.2496, 0.2500, 0.2506, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2498, 0.2501, 0.2501],
        [0.2497, 0.2497, 0.2504, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2501, 0.2501],
        [0.2502, 0.2501, 0.2499, 0.2498],
        [0.2500, 0.2499, 0.2499, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2499, 0.2497, 0.2500],
        [0.2499, 0.2499, 0.2498, 0.2504],
        [0.2500, 0.2498, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2503, 0.2497, 0.2500],
        [0.2501, 0.2498, 0.2502, 0.2499],
        [0.2500, 0.2501, 0.2498, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2504, 0.2502, 0.2498],
        [0.2501, 0.2499, 0.2500, 0.2500],
        [0.2503, 0.2499, 0.2496, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2498, 0.2504, 0.2496],
        [0.2500, 0.2500, 0.2501, 0.2499],
        [0.2499, 0.2500, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2501, 0.2500, 0.2498],
        [0.2501, 0.2498, 0.2502, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2501, 0.2500, 0.2501],
        [0.2500, 0.2503, 0.2499, 0.2498],
        [0.2497, 0.2501, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2501, 0.2500, 0.2497],
        [0.2496, 0.2501, 0.2501, 0.2502],
        [0.2501, 0.2497, 0.2500, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2502, 0.2498, 0.2501],
        [0.2502, 0.2500, 0.2501, 0.2496],
        [0.2500, 0.2498, 0.2501, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2499, 0.2498],
        [0.2497, 0.2497, 0.2502, 0.2505],
        [0.2503, 0.2503, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2498, 0.2502, 0.2497],
        [0.2502, 0.2497, 0.2501, 0.2500],
        [0.2498, 0.2503, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:22:31AM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 2.2707 (2.2749)	Prec@(1,5) (15.4%, 64.9%)	
05/31 12:22:46AM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 2.2055 (2.3009)	Prec@(1,5) (16.9%, 68.2%)	
05/31 12:23:02AM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 2.0928 (2.2754)	Prec@(1,5) (18.2%, 69.8%)	
05/31 12:23:18AM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 1.9253 (2.2071)	Prec@(1,5) (19.7%, 72.8%)	
05/31 12:23:33AM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 2.0327 (2.1517)	Prec@(1,5) (21.1%, 74.8%)	
05/31 12:23:49AM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 1.8091 (2.1076)	Prec@(1,5) (22.3%, 76.5%)	
05/31 12:24:04AM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 1.7867 (2.0693)	Prec@(1,5) (23.6%, 77.8%)	
05/31 12:24:16AM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 1.9581 (2.0437)	Prec@(1,5) (24.2%, 78.6%)	
05/31 12:24:17AM searchStage_trainer.py:221 [INFO] Train: [  0/49] Final Prec@1 24.2360%
05/31 12:24:20AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 1.9254	Prec@(1,5) (28.7%, 83.5%)
05/31 12:24:22AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 1.8983	Prec@(1,5) (29.3%, 84.0%)
05/31 12:24:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 1.8988	Prec@(1,5) (29.7%, 83.7%)
05/31 12:24:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 1.8988	Prec@(1,5) (29.7%, 83.9%)
05/31 12:24:29AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 1.9020	Prec@(1,5) (29.7%, 83.8%)
05/31 12:24:31AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 1.8959	Prec@(1,5) (30.0%, 83.9%)
05/31 12:24:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 1.8953	Prec@(1,5) (30.2%, 83.9%)
05/31 12:24:35AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 1.8971	Prec@(1,5) (30.2%, 83.8%)
05/31 12:24:35AM searchStage_trainer.py:260 [INFO] Valid: [  0/49] Final Prec@1 30.2120%
05/31 12:24:35AM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 4)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('max_pool_3x3', 2), ('avg_pool_3x3', 3)], [('max_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 4), ('avg_pool_3x3', 6)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 2)], [('skip_connect', 3), ('avg_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:24:36AM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 30.2120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2532, 0.2511, 0.2463],
        [0.2471, 0.2559, 0.2500, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2540, 0.2508, 0.2450],
        [0.2495, 0.2539, 0.2523, 0.2444],
        [0.2487, 0.2588, 0.2472, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2524, 0.2497, 0.2442],
        [0.2524, 0.2593, 0.2437, 0.2445],
        [0.2517, 0.2588, 0.2456, 0.2440]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2570, 0.2465, 0.2449],
        [0.2516, 0.2584, 0.2430, 0.2470],
        [0.2503, 0.2591, 0.2471, 0.2435]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2554, 0.2469, 0.2450],
        [0.2532, 0.2570, 0.2462, 0.2436],
        [0.2558, 0.2543, 0.2439, 0.2460]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2558, 0.2451, 0.2462],
        [0.2521, 0.2546, 0.2461, 0.2472],
        [0.2530, 0.2556, 0.2453, 0.2461]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2504, 0.2512, 0.2499],
        [0.2480, 0.2509, 0.2519, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2510, 0.2507, 0.2473],
        [0.2500, 0.2493, 0.2506, 0.2502],
        [0.2498, 0.2512, 0.2499, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2509, 0.2502, 0.2484],
        [0.2514, 0.2506, 0.2483, 0.2497],
        [0.2508, 0.2510, 0.2486, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2509, 0.2496, 0.2491],
        [0.2505, 0.2501, 0.2504, 0.2490],
        [0.2509, 0.2507, 0.2484, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2516, 0.2496, 0.2493],
        [0.2500, 0.2511, 0.2490, 0.2498],
        [0.2502, 0.2507, 0.2499, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2505, 0.2499, 0.2486],
        [0.2506, 0.2502, 0.2490, 0.2503],
        [0.2503, 0.2506, 0.2495, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2498, 0.2504, 0.2500],
        [0.2501, 0.2502, 0.2503, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2506, 0.2505, 0.2495],
        [0.2496, 0.2505, 0.2501, 0.2498],
        [0.2493, 0.2501, 0.2500, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2499, 0.2502, 0.2496],
        [0.2496, 0.2498, 0.2504, 0.2503],
        [0.2498, 0.2497, 0.2504, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2499, 0.2494, 0.2502],
        [0.2507, 0.2500, 0.2502, 0.2491],
        [0.2500, 0.2494, 0.2499, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2511, 0.2520, 0.2492],
        [0.2482, 0.2500, 0.2510, 0.2508],
        [0.2483, 0.2512, 0.2507, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2493, 0.2502, 0.2501],
        [0.2496, 0.2498, 0.2506, 0.2501],
        [0.2486, 0.2506, 0.2510, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:24:53午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 2.1170 (1.7739)	Prec@(1,5) (32.4%, 86.2%)	
05/31 12:25:09午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 1.4618 (1.7563)	Prec@(1,5) (34.0%, 86.8%)	
05/31 12:25:26午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 1.6571 (1.7344)	Prec@(1,5) (34.9%, 87.1%)	
05/31 12:25:42午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 1.5681 (1.7366)	Prec@(1,5) (34.8%, 87.0%)	
05/31 12:25:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 1.4197 (1.7298)	Prec@(1,5) (35.1%, 87.3%)	
05/31 12:26:14午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 1.4494 (1.7250)	Prec@(1,5) (35.3%, 87.5%)	
05/31 12:26:30午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 1.7083 (1.7171)	Prec@(1,5) (35.7%, 87.5%)	
05/31 12:26:43午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 1.8220 (1.7082)	Prec@(1,5) (36.1%, 87.8%)	
05/31 12:26:43午前 searchStage_trainer.py:221 [INFO] Train: [  1/49] Final Prec@1 36.1320%
05/31 12:26:46午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 1.7379	Prec@(1,5) (34.6%, 88.0%)
05/31 12:26:48午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 1.7106	Prec@(1,5) (35.9%, 88.2%)
05/31 12:26:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 1.7070	Prec@(1,5) (36.1%, 88.1%)
05/31 12:26:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 1.7008	Prec@(1,5) (36.3%, 88.2%)
05/31 12:26:54午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 1.7000	Prec@(1,5) (36.4%, 88.2%)
05/31 12:26:57午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 1.7007	Prec@(1,5) (36.6%, 88.1%)
05/31 12:26:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 1.7008	Prec@(1,5) (36.3%, 88.1%)
05/31 12:27:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 1.7002	Prec@(1,5) (36.6%, 88.1%)
05/31 12:27:01午前 searchStage_trainer.py:260 [INFO] Valid: [  1/49] Final Prec@1 36.5840%
05/31 12:27:01午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 4)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('max_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('skip_connect', 3), ('avg_pool_3x3', 5)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:27:01午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 36.5840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2482, 0.2582, 0.2492, 0.2444],
        [0.2442, 0.2614, 0.2543, 0.2401]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2579, 0.2543, 0.2382],
        [0.2494, 0.2582, 0.2532, 0.2393],
        [0.2520, 0.2690, 0.2410, 0.2380]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2566, 0.2485, 0.2406],
        [0.2556, 0.2676, 0.2376, 0.2392],
        [0.2518, 0.2712, 0.2428, 0.2342]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2659, 0.2448, 0.2375],
        [0.2543, 0.2645, 0.2388, 0.2425],
        [0.2510, 0.2670, 0.2422, 0.2399]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2574, 0.2625, 0.2435, 0.2366],
        [0.2554, 0.2627, 0.2413, 0.2406],
        [0.2608, 0.2599, 0.2375, 0.2418]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2622, 0.2421, 0.2434],
        [0.2540, 0.2606, 0.2428, 0.2426],
        [0.2529, 0.2627, 0.2416, 0.2427]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2514, 0.2517, 0.2479],
        [0.2477, 0.2501, 0.2524, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2514, 0.2506, 0.2461],
        [0.2511, 0.2483, 0.2510, 0.2495],
        [0.2498, 0.2519, 0.2505, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2512, 0.2503, 0.2480],
        [0.2515, 0.2517, 0.2478, 0.2490],
        [0.2510, 0.2517, 0.2490, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2511, 0.2489, 0.2493],
        [0.2514, 0.2505, 0.2497, 0.2483],
        [0.2513, 0.2514, 0.2484, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2526, 0.2495, 0.2485],
        [0.2501, 0.2519, 0.2488, 0.2493],
        [0.2504, 0.2511, 0.2494, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2509, 0.2492, 0.2485],
        [0.2506, 0.2505, 0.2489, 0.2501],
        [0.2508, 0.2513, 0.2490, 0.2489]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2496, 0.2504, 0.2504],
        [0.2500, 0.2504, 0.2505, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2511, 0.2508, 0.2489],
        [0.2494, 0.2507, 0.2502, 0.2497],
        [0.2491, 0.2500, 0.2499, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2497, 0.2502, 0.2497],
        [0.2496, 0.2495, 0.2507, 0.2503],
        [0.2494, 0.2497, 0.2509, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2497, 0.2495, 0.2499],
        [0.2511, 0.2499, 0.2501, 0.2488],
        [0.2502, 0.2490, 0.2496, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2520, 0.2538, 0.2485],
        [0.2467, 0.2503, 0.2519, 0.2512],
        [0.2466, 0.2518, 0.2515, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2490, 0.2504, 0.2505],
        [0.2491, 0.2498, 0.2508, 0.2503],
        [0.2475, 0.2509, 0.2524, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:27:18午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 1.5108 (1.6376)	Prec@(1,5) (39.5%, 88.4%)	
05/31 12:27:34午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 1.5050 (1.6259)	Prec@(1,5) (39.8%, 89.0%)	
05/31 12:27:50午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 1.4095 (1.6043)	Prec@(1,5) (40.2%, 89.3%)	
05/31 12:28:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 1.5735 (1.5934)	Prec@(1,5) (40.9%, 89.7%)	
05/31 12:28:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 1.5310 (1.5799)	Prec@(1,5) (41.4%, 89.9%)	
05/31 12:28:39午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 1.5548 (1.5695)	Prec@(1,5) (41.9%, 90.2%)	
05/31 12:28:55午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 1.4001 (1.5611)	Prec@(1,5) (42.2%, 90.4%)	
05/31 12:29:08午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 1.6105 (1.5520)	Prec@(1,5) (42.5%, 90.5%)	
05/31 12:29:08午前 searchStage_trainer.py:221 [INFO] Train: [  2/49] Final Prec@1 42.4920%
05/31 12:29:10午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 1.5100	Prec@(1,5) (43.4%, 91.4%)
05/31 12:29:12午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 1.5285	Prec@(1,5) (44.1%, 91.3%)
05/31 12:29:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 1.5268	Prec@(1,5) (44.0%, 91.5%)
05/31 12:29:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 1.5143	Prec@(1,5) (44.5%, 91.6%)
05/31 12:29:19午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 1.5241	Prec@(1,5) (44.3%, 91.5%)
05/31 12:29:21午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 1.5197	Prec@(1,5) (44.6%, 91.4%)
05/31 12:29:23午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 1.5156	Prec@(1,5) (44.8%, 91.5%)
05/31 12:29:25午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 1.5188	Prec@(1,5) (44.6%, 91.5%)
05/31 12:29:25午前 searchStage_trainer.py:260 [INFO] Valid: [  2/49] Final Prec@1 44.6120%
05/31 12:29:25午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:29:26午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 44.6120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2659, 0.2481, 0.2358],
        [0.2413, 0.2670, 0.2533, 0.2385]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2654, 0.2561, 0.2309],
        [0.2465, 0.2646, 0.2557, 0.2332],
        [0.2515, 0.2824, 0.2364, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2572, 0.2621, 0.2471, 0.2336],
        [0.2603, 0.2749, 0.2313, 0.2336],
        [0.2522, 0.2815, 0.2398, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2742, 0.2445, 0.2294],
        [0.2561, 0.2726, 0.2327, 0.2385],
        [0.2517, 0.2762, 0.2372, 0.2350]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2584, 0.2711, 0.2407, 0.2298],
        [0.2566, 0.2693, 0.2398, 0.2344],
        [0.2621, 0.2682, 0.2322, 0.2376]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2670, 0.2390, 0.2390],
        [0.2556, 0.2666, 0.2377, 0.2401],
        [0.2549, 0.2691, 0.2383, 0.2377]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2518, 0.2513, 0.2479],
        [0.2483, 0.2508, 0.2521, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2519, 0.2507, 0.2456],
        [0.2519, 0.2488, 0.2514, 0.2479],
        [0.2495, 0.2529, 0.2502, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2515, 0.2507, 0.2474],
        [0.2521, 0.2524, 0.2474, 0.2482],
        [0.2513, 0.2521, 0.2487, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2515, 0.2485, 0.2490],
        [0.2520, 0.2510, 0.2492, 0.2479],
        [0.2514, 0.2519, 0.2485, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2533, 0.2502, 0.2471],
        [0.2501, 0.2520, 0.2487, 0.2492],
        [0.2507, 0.2514, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2514, 0.2492, 0.2481],
        [0.2502, 0.2509, 0.2491, 0.2498],
        [0.2509, 0.2517, 0.2488, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2494, 0.2506, 0.2506],
        [0.2500, 0.2506, 0.2506, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2514, 0.2512, 0.2484],
        [0.2491, 0.2508, 0.2503, 0.2498],
        [0.2488, 0.2500, 0.2500, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2494, 0.2503, 0.2499],
        [0.2495, 0.2493, 0.2509, 0.2504],
        [0.2493, 0.2498, 0.2511, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2496, 0.2494, 0.2497],
        [0.2515, 0.2497, 0.2500, 0.2488],
        [0.2503, 0.2487, 0.2496, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2528, 0.2551, 0.2480],
        [0.2455, 0.2506, 0.2525, 0.2514],
        [0.2454, 0.2523, 0.2522, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2487, 0.2504, 0.2507],
        [0.2486, 0.2497, 0.2511, 0.2506],
        [0.2466, 0.2512, 0.2536, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:29:43午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 1.5368 (1.4317)	Prec@(1,5) (47.2%, 92.9%)	
05/31 12:29:59午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 1.5293 (1.4425)	Prec@(1,5) (47.2%, 92.8%)	
05/31 12:30:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 1.3028 (1.4319)	Prec@(1,5) (47.4%, 92.8%)	
05/31 12:30:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 1.3899 (1.4292)	Prec@(1,5) (47.6%, 92.8%)	
05/31 12:30:48午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 1.4743 (1.4226)	Prec@(1,5) (47.9%, 92.7%)	
05/31 12:31:04午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 1.3775 (1.4171)	Prec@(1,5) (48.1%, 92.8%)	
05/31 12:31:20午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 1.3482 (1.4080)	Prec@(1,5) (48.6%, 92.9%)	
05/31 12:31:33午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 1.3340 (1.4019)	Prec@(1,5) (48.8%, 93.0%)	
05/31 12:31:34午前 searchStage_trainer.py:221 [INFO] Train: [  3/49] Final Prec@1 48.8320%
05/31 12:31:36午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 1.5068	Prec@(1,5) (47.4%, 92.7%)
05/31 12:31:38午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 1.4887	Prec@(1,5) (47.1%, 92.8%)
05/31 12:31:40午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 1.4885	Prec@(1,5) (47.3%, 92.7%)
05/31 12:31:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 1.4814	Prec@(1,5) (47.6%, 92.7%)
05/31 12:31:45午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 1.4811	Prec@(1,5) (47.8%, 92.7%)
05/31 12:31:47午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 1.4852	Prec@(1,5) (47.8%, 92.7%)
05/31 12:31:49午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 1.4872	Prec@(1,5) (47.8%, 92.7%)
05/31 12:31:51午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 1.4858	Prec@(1,5) (47.8%, 92.6%)
05/31 12:31:51午前 searchStage_trainer.py:260 [INFO] Valid: [  3/49] Final Prec@1 47.7840%
05/31 12:31:51午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:31:52午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 47.7840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2535, 0.2708, 0.2455, 0.2302],
        [0.2423, 0.2740, 0.2510, 0.2327]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2704, 0.2572, 0.2236],
        [0.2453, 0.2704, 0.2585, 0.2259],
        [0.2524, 0.2929, 0.2334, 0.2213]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2596, 0.2669, 0.2472, 0.2263],
        [0.2642, 0.2824, 0.2248, 0.2286],
        [0.2569, 0.2911, 0.2345, 0.2176]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2809, 0.2407, 0.2228],
        [0.2595, 0.2790, 0.2257, 0.2358],
        [0.2535, 0.2832, 0.2355, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2614, 0.2782, 0.2379, 0.2225],
        [0.2598, 0.2756, 0.2356, 0.2290],
        [0.2632, 0.2755, 0.2286, 0.2327]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2572, 0.2705, 0.2361, 0.2362],
        [0.2590, 0.2728, 0.2336, 0.2346],
        [0.2583, 0.2750, 0.2333, 0.2333]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2517, 0.2514, 0.2476],
        [0.2484, 0.2510, 0.2523, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2525, 0.2511, 0.2448],
        [0.2526, 0.2494, 0.2511, 0.2469],
        [0.2497, 0.2534, 0.2497, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2513, 0.2505, 0.2480],
        [0.2522, 0.2527, 0.2473, 0.2477],
        [0.2518, 0.2524, 0.2487, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2517, 0.2479, 0.2491],
        [0.2519, 0.2513, 0.2492, 0.2476],
        [0.2516, 0.2520, 0.2487, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2540, 0.2499, 0.2465],
        [0.2503, 0.2523, 0.2483, 0.2491],
        [0.2508, 0.2516, 0.2485, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2515, 0.2488, 0.2481],
        [0.2503, 0.2511, 0.2489, 0.2498],
        [0.2510, 0.2521, 0.2489, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2493, 0.2507, 0.2507],
        [0.2499, 0.2507, 0.2507, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2517, 0.2515, 0.2480],
        [0.2491, 0.2508, 0.2503, 0.2498],
        [0.2487, 0.2500, 0.2499, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2492, 0.2504, 0.2500],
        [0.2495, 0.2491, 0.2510, 0.2504],
        [0.2493, 0.2497, 0.2513, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2496, 0.2494, 0.2495],
        [0.2517, 0.2496, 0.2500, 0.2487],
        [0.2503, 0.2485, 0.2495, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2428, 0.2534, 0.2564, 0.2474],
        [0.2445, 0.2507, 0.2531, 0.2516],
        [0.2445, 0.2526, 0.2526, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2484, 0.2505, 0.2509],
        [0.2484, 0.2497, 0.2512, 0.2507],
        [0.2460, 0.2513, 0.2544, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:32:09午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 1.1539 (1.3302)	Prec@(1,5) (51.0%, 93.8%)	
05/31 12:32:25午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 1.4975 (1.3134)	Prec@(1,5) (52.5%, 93.5%)	
05/31 12:32:41午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 1.3775 (1.3136)	Prec@(1,5) (52.5%, 93.4%)	
05/31 12:32:57午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 1.3305 (1.3058)	Prec@(1,5) (53.1%, 93.6%)	
05/31 12:33:13午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 1.3232 (1.3098)	Prec@(1,5) (52.7%, 93.6%)	
05/31 12:33:29午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 1.3249 (1.3103)	Prec@(1,5) (52.6%, 93.5%)	
05/31 12:33:45午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 1.1180 (1.3023)	Prec@(1,5) (52.9%, 93.7%)	
05/31 12:33:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 1.2850 (1.2991)	Prec@(1,5) (53.1%, 93.7%)	
05/31 12:33:58午前 searchStage_trainer.py:221 [INFO] Train: [  4/49] Final Prec@1 53.1160%
05/31 12:34:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 1.3340	Prec@(1,5) (50.2%, 94.1%)
05/31 12:34:03午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 1.3185	Prec@(1,5) (51.3%, 94.1%)
05/31 12:34:05午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 1.3215	Prec@(1,5) (51.2%, 93.9%)
05/31 12:34:07午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 1.3142	Prec@(1,5) (52.0%, 94.1%)
05/31 12:34:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 1.3171	Prec@(1,5) (51.9%, 94.0%)
05/31 12:34:12午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 1.3168	Prec@(1,5) (51.8%, 94.0%)
05/31 12:34:14午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 1.3199	Prec@(1,5) (51.6%, 94.0%)
05/31 12:34:16午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 1.3223	Prec@(1,5) (51.6%, 93.9%)
05/31 12:34:16午前 searchStage_trainer.py:260 [INFO] Valid: [  4/49] Final Prec@1 51.5960%
05/31 12:34:16午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:34:16午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 51.5960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2520, 0.2768, 0.2462, 0.2250],
        [0.2424, 0.2861, 0.2473, 0.2242]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2763, 0.2581, 0.2137],
        [0.2453, 0.2784, 0.2595, 0.2167],
        [0.2552, 0.3038, 0.2254, 0.2156]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2635, 0.2732, 0.2464, 0.2169],
        [0.2687, 0.2903, 0.2194, 0.2216],
        [0.2571, 0.2991, 0.2305, 0.2133]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2580, 0.2867, 0.2388, 0.2165],
        [0.2621, 0.2856, 0.2219, 0.2304],
        [0.2562, 0.2895, 0.2302, 0.2241]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2629, 0.2846, 0.2350, 0.2175],
        [0.2631, 0.2829, 0.2311, 0.2230],
        [0.2648, 0.2814, 0.2256, 0.2282]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2590, 0.2748, 0.2311, 0.2351],
        [0.2605, 0.2788, 0.2317, 0.2290],
        [0.2610, 0.2806, 0.2301, 0.2282]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2524, 0.2512, 0.2469],
        [0.2489, 0.2508, 0.2517, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2528, 0.2511, 0.2448],
        [0.2524, 0.2497, 0.2517, 0.2462],
        [0.2495, 0.2541, 0.2497, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2514, 0.2502, 0.2484],
        [0.2523, 0.2532, 0.2475, 0.2471],
        [0.2520, 0.2527, 0.2483, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2518, 0.2476, 0.2490],
        [0.2521, 0.2514, 0.2491, 0.2474],
        [0.2516, 0.2522, 0.2487, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2543, 0.2502, 0.2460],
        [0.2506, 0.2524, 0.2480, 0.2490],
        [0.2509, 0.2517, 0.2483, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2515, 0.2482, 0.2483],
        [0.2504, 0.2513, 0.2489, 0.2494],
        [0.2513, 0.2523, 0.2487, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2491, 0.2509, 0.2508],
        [0.2498, 0.2507, 0.2508, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2519, 0.2517, 0.2477],
        [0.2489, 0.2508, 0.2504, 0.2498],
        [0.2485, 0.2500, 0.2500, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2489, 0.2505, 0.2502],
        [0.2494, 0.2490, 0.2512, 0.2504],
        [0.2492, 0.2497, 0.2514, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2495, 0.2493, 0.2494],
        [0.2519, 0.2495, 0.2500, 0.2485],
        [0.2504, 0.2484, 0.2494, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2538, 0.2573, 0.2471],
        [0.2438, 0.2509, 0.2535, 0.2517],
        [0.2438, 0.2529, 0.2530, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2482, 0.2505, 0.2512],
        [0.2482, 0.2496, 0.2513, 0.2508],
        [0.2456, 0.2514, 0.2550, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:34:33午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 1.1254 (1.2556)	Prec@(1,5) (55.6%, 94.0%)	
05/31 12:34:49午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 1.0382 (1.2257)	Prec@(1,5) (56.0%, 94.6%)	
05/31 12:35:05午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 1.1456 (1.2136)	Prec@(1,5) (56.2%, 94.9%)	
05/31 12:35:22午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 1.2675 (1.2148)	Prec@(1,5) (56.2%, 94.8%)	
05/31 12:35:38午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 1.1708 (1.2135)	Prec@(1,5) (56.3%, 94.9%)	
05/31 12:35:54午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 1.0333 (1.2089)	Prec@(1,5) (56.4%, 94.9%)	
05/31 12:36:10午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 1.2437 (1.2046)	Prec@(1,5) (56.6%, 94.8%)	
05/31 12:36:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 1.0646 (1.2000)	Prec@(1,5) (56.7%, 94.9%)	
05/31 12:36:23午前 searchStage_trainer.py:221 [INFO] Train: [  5/49] Final Prec@1 56.7280%
05/31 12:36:26午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 1.3100	Prec@(1,5) (53.3%, 94.4%)
05/31 12:36:28午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 1.2929	Prec@(1,5) (53.7%, 94.6%)
05/31 12:36:30午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 1.2889	Prec@(1,5) (53.9%, 94.6%)
05/31 12:36:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 1.2844	Prec@(1,5) (54.0%, 94.6%)
05/31 12:36:35午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 1.2863	Prec@(1,5) (53.9%, 94.7%)
05/31 12:36:37午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 1.2812	Prec@(1,5) (54.0%, 94.8%)
05/31 12:36:39午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 1.2814	Prec@(1,5) (54.0%, 94.8%)
05/31 12:36:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 1.2839	Prec@(1,5) (53.9%, 94.8%)
05/31 12:36:41午前 searchStage_trainer.py:260 [INFO] Valid: [  5/49] Final Prec@1 53.8800%
05/31 12:36:41午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:36:42午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 53.8800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2546, 0.2841, 0.2427, 0.2187],
        [0.2415, 0.2972, 0.2450, 0.2163]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2816, 0.2565, 0.2086],
        [0.2471, 0.2856, 0.2608, 0.2065],
        [0.2601, 0.3142, 0.2187, 0.2070]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2647, 0.2787, 0.2470, 0.2095],
        [0.2701, 0.2977, 0.2166, 0.2156],
        [0.2597, 0.3060, 0.2254, 0.2089]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2592, 0.2921, 0.2356, 0.2131],
        [0.2634, 0.2915, 0.2199, 0.2252],
        [0.2581, 0.2940, 0.2280, 0.2198]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2647, 0.2902, 0.2328, 0.2123],
        [0.2652, 0.2877, 0.2275, 0.2195],
        [0.2674, 0.2863, 0.2228, 0.2235]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2599, 0.2804, 0.2279, 0.2318],
        [0.2612, 0.2839, 0.2299, 0.2249],
        [0.2622, 0.2852, 0.2276, 0.2250]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2526, 0.2516, 0.2467],
        [0.2493, 0.2510, 0.2512, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2531, 0.2506, 0.2447],
        [0.2529, 0.2500, 0.2515, 0.2457],
        [0.2497, 0.2541, 0.2495, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2513, 0.2499, 0.2487],
        [0.2522, 0.2534, 0.2474, 0.2469],
        [0.2520, 0.2529, 0.2484, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2521, 0.2476, 0.2486],
        [0.2522, 0.2514, 0.2488, 0.2476],
        [0.2517, 0.2523, 0.2486, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2545, 0.2506, 0.2456],
        [0.2506, 0.2525, 0.2477, 0.2493],
        [0.2509, 0.2518, 0.2484, 0.2489]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2516, 0.2484, 0.2480],
        [0.2506, 0.2513, 0.2487, 0.2494],
        [0.2514, 0.2525, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2489, 0.2510, 0.2509],
        [0.2498, 0.2507, 0.2510, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2521, 0.2518, 0.2475],
        [0.2487, 0.2509, 0.2506, 0.2498],
        [0.2484, 0.2499, 0.2500, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2488, 0.2506, 0.2502],
        [0.2494, 0.2489, 0.2513, 0.2504],
        [0.2491, 0.2497, 0.2515, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2494, 0.2492, 0.2493],
        [0.2521, 0.2495, 0.2500, 0.2484],
        [0.2504, 0.2482, 0.2493, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2409, 0.2542, 0.2581, 0.2468],
        [0.2433, 0.2510, 0.2538, 0.2519],
        [0.2432, 0.2530, 0.2532, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2480, 0.2505, 0.2513],
        [0.2480, 0.2496, 0.2514, 0.2509],
        [0.2453, 0.2515, 0.2555, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:36:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 1.1236 (1.1629)	Prec@(1,5) (57.8%, 95.4%)	
05/31 12:37:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 0.9095 (1.1466)	Prec@(1,5) (58.5%, 95.7%)	
05/31 12:37:31午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 1.1580 (1.1472)	Prec@(1,5) (58.4%, 95.7%)	
05/31 12:37:47午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 0.9475 (1.1329)	Prec@(1,5) (59.0%, 95.9%)	
05/31 12:38:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 1.2010 (1.1268)	Prec@(1,5) (59.4%, 95.8%)	
05/31 12:38:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 0.9761 (1.1207)	Prec@(1,5) (59.7%, 95.8%)	
05/31 12:38:36午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 0.8911 (1.1219)	Prec@(1,5) (59.6%, 95.8%)	
05/31 12:38:49午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 1.4556 (1.1203)	Prec@(1,5) (59.7%, 95.8%)	
05/31 12:38:49午前 searchStage_trainer.py:221 [INFO] Train: [  6/49] Final Prec@1 59.7320%
05/31 12:38:51午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 1.2577	Prec@(1,5) (55.2%, 95.0%)
05/31 12:38:53午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 1.2586	Prec@(1,5) (55.2%, 95.0%)
05/31 12:38:56午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 1.2539	Prec@(1,5) (55.4%, 95.1%)
05/31 12:38:58午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 1.2449	Prec@(1,5) (55.9%, 95.1%)
05/31 12:39:00午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 1.2474	Prec@(1,5) (55.9%, 95.0%)
05/31 12:39:02午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 1.2419	Prec@(1,5) (55.9%, 95.2%)
05/31 12:39:04午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 1.2366	Prec@(1,5) (55.9%, 95.3%)
05/31 12:39:06午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 1.2412	Prec@(1,5) (55.8%, 95.2%)
05/31 12:39:06午前 searchStage_trainer.py:260 [INFO] Valid: [  6/49] Final Prec@1 55.8680%
05/31 12:39:06午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:39:07午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 55.8680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2559, 0.2919, 0.2412, 0.2109],
        [0.2426, 0.3038, 0.2430, 0.2106]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2573, 0.2872, 0.2554, 0.2001],
        [0.2482, 0.2922, 0.2612, 0.1984],
        [0.2632, 0.3214, 0.2136, 0.2017]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2664, 0.2816, 0.2465, 0.2056],
        [0.2721, 0.3032, 0.2135, 0.2112],
        [0.2624, 0.3110, 0.2231, 0.2035]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2615, 0.2962, 0.2327, 0.2096],
        [0.2649, 0.2955, 0.2187, 0.2210],
        [0.2596, 0.2974, 0.2261, 0.2170]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2670, 0.2942, 0.2312, 0.2076],
        [0.2661, 0.2916, 0.2262, 0.2161],
        [0.2685, 0.2897, 0.2212, 0.2205]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2615, 0.2844, 0.2254, 0.2286],
        [0.2625, 0.2876, 0.2281, 0.2218],
        [0.2637, 0.2884, 0.2257, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2526, 0.2517, 0.2469],
        [0.2495, 0.2513, 0.2510, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2532, 0.2509, 0.2445],
        [0.2531, 0.2498, 0.2516, 0.2455],
        [0.2498, 0.2542, 0.2494, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2517, 0.2501, 0.2482],
        [0.2522, 0.2534, 0.2474, 0.2471],
        [0.2520, 0.2529, 0.2484, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2521, 0.2477, 0.2484],
        [0.2522, 0.2515, 0.2487, 0.2476],
        [0.2517, 0.2523, 0.2486, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2547, 0.2508, 0.2452],
        [0.2505, 0.2525, 0.2475, 0.2495],
        [0.2507, 0.2519, 0.2484, 0.2489]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2518, 0.2486, 0.2477],
        [0.2505, 0.2512, 0.2489, 0.2494],
        [0.2514, 0.2525, 0.2484, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2489, 0.2510, 0.2509],
        [0.2498, 0.2507, 0.2510, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2523, 0.2519, 0.2473],
        [0.2486, 0.2510, 0.2507, 0.2498],
        [0.2484, 0.2499, 0.2500, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2488, 0.2506, 0.2503],
        [0.2494, 0.2488, 0.2514, 0.2504],
        [0.2491, 0.2497, 0.2516, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2493, 0.2492, 0.2492],
        [0.2523, 0.2494, 0.2499, 0.2484],
        [0.2505, 0.2482, 0.2493, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2402, 0.2544, 0.2588, 0.2466],
        [0.2428, 0.2511, 0.2540, 0.2520],
        [0.2429, 0.2532, 0.2534, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2478, 0.2506, 0.2514],
        [0.2479, 0.2496, 0.2515, 0.2510],
        [0.2451, 0.2516, 0.2558, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:39:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 0.9748 (1.0466)	Prec@(1,5) (63.0%, 96.8%)	
05/31 12:39:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 1.1352 (1.0394)	Prec@(1,5) (62.6%, 96.7%)	
05/31 12:39:56午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 1.1118 (1.0597)	Prec@(1,5) (62.2%, 96.1%)	
05/31 12:40:12午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 1.1330 (1.0525)	Prec@(1,5) (62.4%, 96.2%)	
05/31 12:40:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 0.9183 (1.0498)	Prec@(1,5) (62.5%, 96.2%)	
05/31 12:40:44午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 1.1063 (1.0427)	Prec@(1,5) (62.8%, 96.4%)	
05/31 12:41:00午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 1.0213 (1.0411)	Prec@(1,5) (62.8%, 96.4%)	
05/31 12:41:13午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 1.0619 (1.0393)	Prec@(1,5) (62.8%, 96.3%)	
05/31 12:41:14午前 searchStage_trainer.py:221 [INFO] Train: [  7/49] Final Prec@1 62.8240%
05/31 12:41:16午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 1.4647	Prec@(1,5) (50.7%, 95.7%)
05/31 12:41:18午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 1.4844	Prec@(1,5) (50.7%, 95.0%)
05/31 12:41:20午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 1.4881	Prec@(1,5) (50.3%, 95.2%)
05/31 12:41:23午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 1.4806	Prec@(1,5) (50.4%, 95.3%)
05/31 12:41:25午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 1.4770	Prec@(1,5) (50.6%, 95.3%)
05/31 12:41:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 1.4886	Prec@(1,5) (50.3%, 95.2%)
05/31 12:41:29午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 1.4886	Prec@(1,5) (50.2%, 95.2%)
05/31 12:41:31午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 1.4842	Prec@(1,5) (50.2%, 95.2%)
05/31 12:41:31午前 searchStage_trainer.py:260 [INFO] Valid: [  7/49] Final Prec@1 50.2080%
05/31 12:41:31午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:41:31午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 55.8680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2573, 0.2984, 0.2403, 0.2040],
        [0.2427, 0.3104, 0.2413, 0.2056]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2574, 0.2924, 0.2553, 0.1948],
        [0.2487, 0.2978, 0.2597, 0.1939],
        [0.2663, 0.3274, 0.2121, 0.1942]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2679, 0.2861, 0.2466, 0.1994],
        [0.2735, 0.3059, 0.2106, 0.2101],
        [0.2642, 0.3149, 0.2215, 0.1994]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2635, 0.2991, 0.2314, 0.2060],
        [0.2663, 0.2983, 0.2169, 0.2185],
        [0.2602, 0.2994, 0.2251, 0.2152]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2674, 0.2979, 0.2298, 0.2048],
        [0.2675, 0.2946, 0.2249, 0.2130],
        [0.2695, 0.2917, 0.2205, 0.2183]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2625, 0.2869, 0.2240, 0.2266],
        [0.2639, 0.2904, 0.2266, 0.2191],
        [0.2655, 0.2906, 0.2240, 0.2199]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2527, 0.2518, 0.2467],
        [0.2493, 0.2513, 0.2512, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2532, 0.2508, 0.2447],
        [0.2532, 0.2500, 0.2517, 0.2451],
        [0.2498, 0.2542, 0.2494, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2517, 0.2499, 0.2487],
        [0.2522, 0.2535, 0.2473, 0.2470],
        [0.2521, 0.2529, 0.2485, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2520, 0.2475, 0.2486],
        [0.2522, 0.2514, 0.2488, 0.2476],
        [0.2518, 0.2523, 0.2486, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2548, 0.2511, 0.2450],
        [0.2505, 0.2526, 0.2476, 0.2493],
        [0.2506, 0.2520, 0.2483, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2517, 0.2485, 0.2478],
        [0.2506, 0.2511, 0.2488, 0.2494],
        [0.2516, 0.2524, 0.2484, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2488, 0.2510, 0.2510],
        [0.2497, 0.2508, 0.2511, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2523, 0.2521, 0.2471],
        [0.2486, 0.2510, 0.2507, 0.2498],
        [0.2483, 0.2499, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2487, 0.2506, 0.2504],
        [0.2494, 0.2488, 0.2515, 0.2503],
        [0.2491, 0.2497, 0.2516, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2493, 0.2492, 0.2492],
        [0.2524, 0.2494, 0.2499, 0.2483],
        [0.2505, 0.2481, 0.2492, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2397, 0.2546, 0.2593, 0.2464],
        [0.2425, 0.2512, 0.2542, 0.2521],
        [0.2426, 0.2533, 0.2536, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2477, 0.2506, 0.2515],
        [0.2478, 0.2495, 0.2516, 0.2511],
        [0.2450, 0.2516, 0.2560, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:41:48午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 1.0245 (0.9859)	Prec@(1,5) (65.3%, 96.5%)	
05/31 12:42:05午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 0.9752 (1.0040)	Prec@(1,5) (64.1%, 96.5%)	
05/31 12:42:21午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 0.7656 (0.9921)	Prec@(1,5) (64.2%, 96.8%)	
05/31 12:42:37午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 1.1386 (0.9927)	Prec@(1,5) (64.2%, 96.8%)	
05/31 12:42:53午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 0.8641 (0.9933)	Prec@(1,5) (64.4%, 96.8%)	
05/31 12:43:09午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 0.7789 (0.9928)	Prec@(1,5) (64.5%, 96.7%)	
05/31 12:43:25午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 0.7377 (0.9858)	Prec@(1,5) (64.9%, 96.7%)	
05/31 12:43:38午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 1.0033 (0.9839)	Prec@(1,5) (65.0%, 96.8%)	
05/31 12:43:38午前 searchStage_trainer.py:221 [INFO] Train: [  8/49] Final Prec@1 65.0160%
05/31 12:43:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 1.0204	Prec@(1,5) (62.8%, 97.2%)
05/31 12:43:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 1.0289	Prec@(1,5) (62.4%, 96.9%)
05/31 12:43:45午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 1.0294	Prec@(1,5) (62.5%, 96.7%)
05/31 12:43:47午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 1.0259	Prec@(1,5) (62.6%, 96.7%)
05/31 12:43:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 1.0301	Prec@(1,5) (62.5%, 96.8%)
05/31 12:43:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 1.0331	Prec@(1,5) (62.4%, 96.8%)
05/31 12:43:54午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 1.0350	Prec@(1,5) (62.2%, 96.8%)
05/31 12:43:56午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 1.0335	Prec@(1,5) (62.3%, 96.8%)
05/31 12:43:56午前 searchStage_trainer.py:260 [INFO] Valid: [  8/49] Final Prec@1 62.3360%
05/31 12:43:56午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:43:56午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 62.3360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2584, 0.3024, 0.2404, 0.1988],
        [0.2443, 0.3148, 0.2398, 0.2011]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2583, 0.2960, 0.2564, 0.1893],
        [0.2485, 0.3016, 0.2597, 0.1902],
        [0.2678, 0.3312, 0.2110, 0.1900]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2686, 0.2887, 0.2471, 0.1956],
        [0.2744, 0.3076, 0.2100, 0.2080],
        [0.2654, 0.3169, 0.2210, 0.1967]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2650, 0.3009, 0.2299, 0.2042],
        [0.2672, 0.3003, 0.2165, 0.2160],
        [0.2605, 0.3007, 0.2248, 0.2140]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2683, 0.2998, 0.2293, 0.2026],
        [0.2680, 0.2963, 0.2248, 0.2109],
        [0.2701, 0.2931, 0.2201, 0.2166]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2889, 0.2218, 0.2255],
        [0.2656, 0.2919, 0.2254, 0.2171],
        [0.2662, 0.2921, 0.2235, 0.2183]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2526, 0.2518, 0.2468],
        [0.2492, 0.2513, 0.2513, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2533, 0.2508, 0.2444],
        [0.2531, 0.2501, 0.2519, 0.2449],
        [0.2498, 0.2541, 0.2494, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2517, 0.2499, 0.2486],
        [0.2522, 0.2534, 0.2473, 0.2471],
        [0.2521, 0.2529, 0.2485, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2521, 0.2475, 0.2486],
        [0.2523, 0.2514, 0.2487, 0.2477],
        [0.2518, 0.2523, 0.2487, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2549, 0.2510, 0.2451],
        [0.2505, 0.2526, 0.2476, 0.2493],
        [0.2506, 0.2520, 0.2484, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2517, 0.2485, 0.2477],
        [0.2507, 0.2511, 0.2487, 0.2495],
        [0.2516, 0.2524, 0.2484, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2488, 0.2510, 0.2510],
        [0.2497, 0.2508, 0.2511, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2521, 0.2469],
        [0.2485, 0.2510, 0.2507, 0.2498],
        [0.2483, 0.2498, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2486, 0.2507, 0.2504],
        [0.2494, 0.2488, 0.2515, 0.2503],
        [0.2490, 0.2497, 0.2517, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2493, 0.2491, 0.2491],
        [0.2525, 0.2493, 0.2499, 0.2483],
        [0.2506, 0.2480, 0.2492, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2392, 0.2548, 0.2597, 0.2463],
        [0.2423, 0.2513, 0.2543, 0.2522],
        [0.2424, 0.2533, 0.2537, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2476, 0.2506, 0.2516],
        [0.2477, 0.2495, 0.2516, 0.2512],
        [0.2449, 0.2516, 0.2561, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:44:13午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 0.9035 (0.9131)	Prec@(1,5) (68.6%, 97.2%)	
05/31 12:44:29午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 1.0579 (0.9345)	Prec@(1,5) (67.4%, 97.1%)	
05/31 12:44:46午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 1.0771 (0.9342)	Prec@(1,5) (67.4%, 97.2%)	
05/31 12:45:02午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 0.8683 (0.9278)	Prec@(1,5) (67.7%, 97.2%)	
05/31 12:45:18午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 0.9965 (0.9279)	Prec@(1,5) (67.6%, 97.2%)	
05/31 12:45:34午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 0.8155 (0.9223)	Prec@(1,5) (67.6%, 97.3%)	
05/31 12:45:50午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 1.0736 (0.9212)	Prec@(1,5) (67.5%, 97.3%)	
05/31 12:46:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 1.1051 (0.9205)	Prec@(1,5) (67.5%, 97.3%)	
05/31 12:46:04午前 searchStage_trainer.py:221 [INFO] Train: [  9/49] Final Prec@1 67.5320%
05/31 12:46:06午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 1.0746	Prec@(1,5) (62.3%, 96.7%)
05/31 12:46:08午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 1.0516	Prec@(1,5) (63.3%, 96.6%)
05/31 12:46:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 1.0594	Prec@(1,5) (62.6%, 96.6%)
05/31 12:46:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 1.0581	Prec@(1,5) (62.8%, 96.6%)
05/31 12:46:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 1.0600	Prec@(1,5) (62.9%, 96.6%)
05/31 12:46:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 1.0616	Prec@(1,5) (62.7%, 96.5%)
05/31 12:46:20午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 1.0620	Prec@(1,5) (62.7%, 96.6%)
05/31 12:46:21午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 1.0622	Prec@(1,5) (62.7%, 96.5%)
05/31 12:46:21午前 searchStage_trainer.py:260 [INFO] Valid: [  9/49] Final Prec@1 62.7440%
05/31 12:46:21午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:46:22午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 62.7440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2590, 0.3054, 0.2409, 0.1947],
        [0.2452, 0.3177, 0.2386, 0.1985]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2588, 0.2975, 0.2560, 0.1877],
        [0.2491, 0.3042, 0.2603, 0.1864],
        [0.2687, 0.3332, 0.2114, 0.1867]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2692, 0.2911, 0.2468, 0.1929],
        [0.2752, 0.3091, 0.2091, 0.2066],
        [0.2660, 0.3181, 0.2208, 0.1951]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2656, 0.3022, 0.2293, 0.2030],
        [0.2679, 0.3011, 0.2160, 0.2150],
        [0.2612, 0.3013, 0.2244, 0.2130]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2696, 0.3015, 0.2281, 0.2007],
        [0.2681, 0.2976, 0.2250, 0.2093],
        [0.2706, 0.2938, 0.2196, 0.2160]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2908, 0.2209, 0.2246],
        [0.2657, 0.2931, 0.2258, 0.2154],
        [0.2663, 0.2931, 0.2229, 0.2177]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2526, 0.2518, 0.2469],
        [0.2491, 0.2514, 0.2514, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2532, 0.2508, 0.2445],
        [0.2533, 0.2499, 0.2517, 0.2451],
        [0.2498, 0.2542, 0.2495, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2515, 0.2497, 0.2488],
        [0.2523, 0.2534, 0.2472, 0.2471],
        [0.2521, 0.2530, 0.2485, 0.2464]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2521, 0.2475, 0.2484],
        [0.2522, 0.2514, 0.2486, 0.2477],
        [0.2518, 0.2523, 0.2487, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2549, 0.2508, 0.2451],
        [0.2506, 0.2526, 0.2475, 0.2494],
        [0.2506, 0.2520, 0.2484, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2517, 0.2485, 0.2477],
        [0.2507, 0.2511, 0.2487, 0.2496],
        [0.2516, 0.2524, 0.2484, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2488, 0.2510, 0.2511],
        [0.2497, 0.2508, 0.2511, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2526, 0.2523, 0.2467],
        [0.2485, 0.2510, 0.2507, 0.2498],
        [0.2483, 0.2498, 0.2499, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2485, 0.2507, 0.2505],
        [0.2494, 0.2487, 0.2516, 0.2503],
        [0.2490, 0.2497, 0.2517, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2492, 0.2491, 0.2491],
        [0.2526, 0.2493, 0.2499, 0.2482],
        [0.2506, 0.2480, 0.2492, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2389, 0.2550, 0.2600, 0.2461],
        [0.2421, 0.2513, 0.2544, 0.2522],
        [0.2422, 0.2534, 0.2538, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2475, 0.2506, 0.2517],
        [0.2476, 0.2495, 0.2516, 0.2512],
        [0.2449, 0.2516, 0.2562, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:46:39午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 1.0308 (0.8845)	Prec@(1,5) (68.9%, 97.3%)	
05/31 12:46:55午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 1.0509 (0.8828)	Prec@(1,5) (69.2%, 97.3%)	
05/31 12:47:11午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 1.0401 (0.8812)	Prec@(1,5) (68.9%, 97.5%)	
05/31 12:47:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 0.9929 (0.8786)	Prec@(1,5) (68.8%, 97.4%)	
05/31 12:47:44午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 0.8802 (0.8774)	Prec@(1,5) (69.0%, 97.4%)	
05/31 12:48:00午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 0.9355 (0.8771)	Prec@(1,5) (69.0%, 97.4%)	
05/31 12:48:16午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 1.0651 (0.8800)	Prec@(1,5) (68.8%, 97.4%)	
05/31 12:48:29午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 0.8326 (0.8746)	Prec@(1,5) (69.0%, 97.5%)	
05/31 12:48:29午前 searchStage_trainer.py:221 [INFO] Train: [ 10/49] Final Prec@1 69.0280%
05/31 12:48:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 1.1811	Prec@(1,5) (62.1%, 94.5%)
05/31 12:48:34午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 1.1648	Prec@(1,5) (62.5%, 94.7%)
05/31 12:48:36午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 1.1936	Prec@(1,5) (61.5%, 94.6%)
05/31 12:48:38午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 1.1897	Prec@(1,5) (61.5%, 94.6%)
05/31 12:48:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 1.2057	Prec@(1,5) (60.9%, 94.5%)
05/31 12:48:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 1.2066	Prec@(1,5) (60.9%, 94.6%)
05/31 12:48:45午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 1.2085	Prec@(1,5) (60.9%, 94.5%)
05/31 12:48:47午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 1.2079	Prec@(1,5) (60.9%, 94.5%)
05/31 12:48:47午前 searchStage_trainer.py:260 [INFO] Valid: [ 10/49] Final Prec@1 60.8560%
05/31 12:48:47午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:48:47午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 62.7440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2598, 0.3070, 0.2405, 0.1927],
        [0.2452, 0.3198, 0.2387, 0.1963]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2604, 0.2993, 0.2561, 0.1842],
        [0.2494, 0.3067, 0.2610, 0.1828],
        [0.2694, 0.3339, 0.2103, 0.1863]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2689, 0.2922, 0.2473, 0.1917],
        [0.2751, 0.3096, 0.2094, 0.2059],
        [0.2667, 0.3188, 0.2206, 0.1940]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2659, 0.3030, 0.2291, 0.2019],
        [0.2678, 0.3017, 0.2167, 0.2139],
        [0.2612, 0.3015, 0.2242, 0.2132]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2699, 0.3030, 0.2274, 0.1997],
        [0.2687, 0.2983, 0.2242, 0.2088],
        [0.2706, 0.2941, 0.2199, 0.2154]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2645, 0.2918, 0.2198, 0.2239],
        [0.2663, 0.2937, 0.2259, 0.2142],
        [0.2664, 0.2934, 0.2226, 0.2176]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2525, 0.2517, 0.2471],
        [0.2492, 0.2515, 0.2514, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2531, 0.2506, 0.2447],
        [0.2535, 0.2499, 0.2517, 0.2449],
        [0.2498, 0.2542, 0.2494, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2515, 0.2498, 0.2488],
        [0.2523, 0.2534, 0.2472, 0.2471],
        [0.2521, 0.2529, 0.2485, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2474, 0.2485],
        [0.2523, 0.2514, 0.2486, 0.2477],
        [0.2518, 0.2523, 0.2487, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2549, 0.2508, 0.2450],
        [0.2506, 0.2525, 0.2474, 0.2495],
        [0.2505, 0.2520, 0.2484, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2517, 0.2484, 0.2477],
        [0.2507, 0.2510, 0.2487, 0.2495],
        [0.2516, 0.2524, 0.2484, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2488, 0.2510, 0.2511],
        [0.2497, 0.2508, 0.2511, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2527, 0.2523, 0.2466],
        [0.2485, 0.2510, 0.2507, 0.2498],
        [0.2483, 0.2498, 0.2499, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2484, 0.2507, 0.2506],
        [0.2494, 0.2487, 0.2516, 0.2502],
        [0.2490, 0.2497, 0.2517, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2492, 0.2491, 0.2491],
        [0.2527, 0.2493, 0.2498, 0.2482],
        [0.2506, 0.2480, 0.2491, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2386, 0.2551, 0.2603, 0.2460],
        [0.2419, 0.2513, 0.2545, 0.2523],
        [0.2421, 0.2535, 0.2538, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2475, 0.2506, 0.2517],
        [0.2476, 0.2495, 0.2516, 0.2513],
        [0.2448, 0.2516, 0.2563, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:49:04午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 0.9768 (0.8141)	Prec@(1,5) (71.3%, 97.9%)	
05/31 12:49:20午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 0.8246 (0.8045)	Prec@(1,5) (71.7%, 97.9%)	
05/31 12:49:37午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 0.5879 (0.8062)	Prec@(1,5) (71.8%, 97.8%)	
05/31 12:49:53午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 1.0269 (0.8133)	Prec@(1,5) (71.3%, 97.9%)	
05/31 12:50:09午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 0.6155 (0.8199)	Prec@(1,5) (71.1%, 97.8%)	
05/31 12:50:25午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 0.8678 (0.8284)	Prec@(1,5) (70.9%, 97.7%)	
05/31 12:50:41午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 0.8035 (0.8297)	Prec@(1,5) (70.8%, 97.7%)	
05/31 12:50:54午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 0.6602 (0.8293)	Prec@(1,5) (70.7%, 97.7%)	
05/31 12:50:55午前 searchStage_trainer.py:221 [INFO] Train: [ 11/49] Final Prec@1 70.6960%
05/31 12:50:57午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 0.8719	Prec@(1,5) (69.2%, 97.4%)
05/31 12:50:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 0.8974	Prec@(1,5) (68.8%, 97.4%)
05/31 12:51:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 0.8965	Prec@(1,5) (68.4%, 97.4%)
05/31 12:51:04午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 0.8954	Prec@(1,5) (68.5%, 97.4%)
05/31 12:51:06午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 0.8947	Prec@(1,5) (68.6%, 97.4%)
05/31 12:51:08午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 0.8954	Prec@(1,5) (68.5%, 97.3%)
05/31 12:51:10午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 0.8967	Prec@(1,5) (68.5%, 97.3%)
05/31 12:51:12午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 0.8974	Prec@(1,5) (68.4%, 97.3%)
05/31 12:51:12午前 searchStage_trainer.py:260 [INFO] Valid: [ 11/49] Final Prec@1 68.4360%
05/31 12:51:12午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:51:13午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 68.4360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2602, 0.3082, 0.2407, 0.1909],
        [0.2454, 0.3206, 0.2383, 0.1957]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2599, 0.3011, 0.2572, 0.1818],
        [0.2491, 0.3091, 0.2628, 0.1789],
        [0.2689, 0.3340, 0.2103, 0.1868]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2687, 0.2933, 0.2472, 0.1909],
        [0.2761, 0.3098, 0.2097, 0.2045],
        [0.2669, 0.3187, 0.2202, 0.1942]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.3038, 0.2295, 0.2009],
        [0.2676, 0.3017, 0.2166, 0.2141],
        [0.2610, 0.3015, 0.2244, 0.2130]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2701, 0.3037, 0.2272, 0.1990],
        [0.2685, 0.2985, 0.2247, 0.2084],
        [0.2707, 0.2942, 0.2198, 0.2153]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2648, 0.2924, 0.2197, 0.2231],
        [0.2663, 0.2940, 0.2260, 0.2138],
        [0.2665, 0.2935, 0.2222, 0.2177]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2526, 0.2517, 0.2470],
        [0.2492, 0.2514, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2530, 0.2506, 0.2448],
        [0.2535, 0.2498, 0.2518, 0.2449],
        [0.2498, 0.2542, 0.2495, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2516, 0.2499, 0.2485],
        [0.2523, 0.2533, 0.2472, 0.2471],
        [0.2521, 0.2529, 0.2485, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2475, 0.2484],
        [0.2523, 0.2514, 0.2486, 0.2478],
        [0.2518, 0.2523, 0.2487, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2549, 0.2508, 0.2450],
        [0.2505, 0.2525, 0.2474, 0.2495],
        [0.2505, 0.2520, 0.2485, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2517, 0.2484, 0.2476],
        [0.2508, 0.2510, 0.2486, 0.2496],
        [0.2516, 0.2524, 0.2484, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2488, 0.2510, 0.2511],
        [0.2497, 0.2508, 0.2512, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2527, 0.2524, 0.2466],
        [0.2485, 0.2510, 0.2507, 0.2498],
        [0.2483, 0.2498, 0.2499, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2483, 0.2507, 0.2506],
        [0.2495, 0.2487, 0.2516, 0.2502],
        [0.2490, 0.2497, 0.2517, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2492, 0.2490, 0.2490],
        [0.2527, 0.2492, 0.2498, 0.2482],
        [0.2506, 0.2479, 0.2491, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2383, 0.2552, 0.2606, 0.2460],
        [0.2418, 0.2514, 0.2546, 0.2523],
        [0.2420, 0.2535, 0.2539, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2474, 0.2506, 0.2518],
        [0.2475, 0.2495, 0.2517, 0.2513],
        [0.2448, 0.2516, 0.2564, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:51:30午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 0.9148 (0.7585)	Prec@(1,5) (73.3%, 98.5%)	
05/31 12:51:46午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 0.8748 (0.7631)	Prec@(1,5) (73.5%, 98.2%)	
05/31 12:52:02午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 0.6757 (0.7753)	Prec@(1,5) (72.9%, 98.2%)	
05/31 12:52:18午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 0.5414 (0.7746)	Prec@(1,5) (72.9%, 98.1%)	
05/31 12:52:34午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 1.0367 (0.7849)	Prec@(1,5) (72.5%, 98.0%)	
05/31 12:52:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 0.6557 (0.7904)	Prec@(1,5) (72.4%, 97.9%)	
05/31 12:53:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 1.0823 (0.7858)	Prec@(1,5) (72.4%, 98.0%)	
05/31 12:53:20午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 0.6108 (0.7879)	Prec@(1,5) (72.4%, 97.9%)	
05/31 12:53:20午前 searchStage_trainer.py:221 [INFO] Train: [ 12/49] Final Prec@1 72.3560%
05/31 12:53:23午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 0.8833	Prec@(1,5) (67.5%, 97.5%)
05/31 12:53:25午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 0.8748	Prec@(1,5) (68.8%, 97.4%)
05/31 12:53:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 0.8698	Prec@(1,5) (69.3%, 97.6%)
05/31 12:53:29午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 0.8631	Prec@(1,5) (69.7%, 97.6%)
05/31 12:53:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 0.8550	Prec@(1,5) (70.0%, 97.6%)
05/31 12:53:34午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 0.8587	Prec@(1,5) (69.8%, 97.6%)
05/31 12:53:36午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 0.8573	Prec@(1,5) (69.8%, 97.7%)
05/31 12:53:38午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 0.8582	Prec@(1,5) (69.8%, 97.6%)
05/31 12:53:38午前 searchStage_trainer.py:260 [INFO] Valid: [ 12/49] Final Prec@1 69.7680%
05/31 12:53:38午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:53:38午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 69.7680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2610, 0.3086, 0.2405, 0.1899],
        [0.2461, 0.3206, 0.2378, 0.1955]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2606, 0.3019, 0.2573, 0.1803],
        [0.2494, 0.3102, 0.2627, 0.1777],
        [0.2688, 0.3342, 0.2103, 0.1867]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2685, 0.2945, 0.2483, 0.1887],
        [0.2766, 0.3097, 0.2099, 0.2039],
        [0.2669, 0.3185, 0.2198, 0.1949]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.3038, 0.2289, 0.2016],
        [0.2675, 0.3018, 0.2170, 0.2137],
        [0.2608, 0.3014, 0.2249, 0.2129]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.3038, 0.2267, 0.1988],
        [0.2689, 0.2985, 0.2242, 0.2084],
        [0.2708, 0.2940, 0.2200, 0.2153]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2653, 0.2924, 0.2186, 0.2237],
        [0.2666, 0.2942, 0.2260, 0.2133],
        [0.2666, 0.2933, 0.2223, 0.2178]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2526, 0.2517, 0.2470],
        [0.2493, 0.2515, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2530, 0.2508, 0.2447],
        [0.2535, 0.2498, 0.2519, 0.2449],
        [0.2498, 0.2541, 0.2495, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2514, 0.2498, 0.2486],
        [0.2524, 0.2533, 0.2471, 0.2472],
        [0.2521, 0.2529, 0.2485, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2475, 0.2483],
        [0.2523, 0.2514, 0.2485, 0.2478],
        [0.2518, 0.2522, 0.2487, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2550, 0.2509, 0.2451],
        [0.2505, 0.2525, 0.2474, 0.2496],
        [0.2505, 0.2520, 0.2485, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2517, 0.2484, 0.2477],
        [0.2508, 0.2509, 0.2486, 0.2496],
        [0.2517, 0.2524, 0.2484, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2488, 0.2511, 0.2511],
        [0.2497, 0.2508, 0.2512, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2528, 0.2524, 0.2465],
        [0.2485, 0.2510, 0.2507, 0.2499],
        [0.2483, 0.2498, 0.2498, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2483, 0.2507, 0.2507],
        [0.2495, 0.2487, 0.2516, 0.2502],
        [0.2490, 0.2497, 0.2517, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2492, 0.2490, 0.2490],
        [0.2528, 0.2492, 0.2498, 0.2482],
        [0.2506, 0.2479, 0.2491, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2380, 0.2553, 0.2608, 0.2459],
        [0.2417, 0.2514, 0.2546, 0.2524],
        [0.2419, 0.2535, 0.2539, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2473, 0.2506, 0.2518],
        [0.2475, 0.2495, 0.2517, 0.2513],
        [0.2448, 0.2516, 0.2564, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:53:55午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 0.8546 (0.7280)	Prec@(1,5) (73.5%, 98.0%)	
05/31 12:54:11午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 0.7172 (0.7152)	Prec@(1,5) (73.9%, 98.2%)	
05/31 12:54:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 0.9317 (0.7417)	Prec@(1,5) (73.2%, 98.2%)	
05/31 12:54:44午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 0.7103 (0.7441)	Prec@(1,5) (73.3%, 98.2%)	
05/31 12:55:00午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 0.9558 (0.7448)	Prec@(1,5) (73.4%, 98.2%)	
05/31 12:55:16午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 0.7197 (0.7468)	Prec@(1,5) (73.5%, 98.2%)	
05/31 12:55:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 0.9724 (0.7488)	Prec@(1,5) (73.4%, 98.2%)	
05/31 12:55:45午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 0.5312 (0.7495)	Prec@(1,5) (73.5%, 98.2%)	
05/31 12:55:46午前 searchStage_trainer.py:221 [INFO] Train: [ 13/49] Final Prec@1 73.5000%
05/31 12:55:48午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 0.8521	Prec@(1,5) (70.2%, 98.0%)
05/31 12:55:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 0.8508	Prec@(1,5) (70.0%, 98.0%)
05/31 12:55:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 0.8546	Prec@(1,5) (69.7%, 97.9%)
05/31 12:55:55午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 0.8559	Prec@(1,5) (69.7%, 97.9%)
05/31 12:55:57午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 0.8576	Prec@(1,5) (69.7%, 97.9%)
05/31 12:55:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 0.8543	Prec@(1,5) (69.7%, 97.9%)
05/31 12:56:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 0.8497	Prec@(1,5) (69.8%, 97.9%)
05/31 12:56:03午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 0.8511	Prec@(1,5) (69.7%, 97.9%)
05/31 12:56:03午前 searchStage_trainer.py:260 [INFO] Valid: [ 13/49] Final Prec@1 69.7240%
05/31 12:56:03午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:56:03午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 69.7680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2610, 0.3089, 0.2408, 0.1894],
        [0.2458, 0.3208, 0.2381, 0.1954]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2606, 0.3019, 0.2577, 0.1798],
        [0.2502, 0.3105, 0.2624, 0.1769],
        [0.2685, 0.3342, 0.2107, 0.1866]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2686, 0.2953, 0.2487, 0.1874],
        [0.2762, 0.3095, 0.2101, 0.2042],
        [0.2667, 0.3183, 0.2201, 0.1950]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2660, 0.3037, 0.2288, 0.2016],
        [0.2675, 0.3016, 0.2173, 0.2136],
        [0.2608, 0.3010, 0.2249, 0.2133]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2709, 0.3042, 0.2262, 0.1987],
        [0.2689, 0.2983, 0.2243, 0.2085],
        [0.2708, 0.2937, 0.2201, 0.2154]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2653, 0.2924, 0.2183, 0.2240],
        [0.2666, 0.2940, 0.2261, 0.2133],
        [0.2666, 0.2930, 0.2225, 0.2180]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2526, 0.2517, 0.2471],
        [0.2493, 0.2515, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2530, 0.2508, 0.2447],
        [0.2536, 0.2497, 0.2518, 0.2449],
        [0.2498, 0.2541, 0.2495, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2513, 0.2497, 0.2488],
        [0.2524, 0.2533, 0.2471, 0.2472],
        [0.2521, 0.2529, 0.2485, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2475, 0.2483],
        [0.2523, 0.2513, 0.2485, 0.2478],
        [0.2518, 0.2522, 0.2487, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2550, 0.2508, 0.2451],
        [0.2504, 0.2525, 0.2474, 0.2496],
        [0.2505, 0.2520, 0.2485, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2517, 0.2484, 0.2477],
        [0.2508, 0.2509, 0.2486, 0.2497],
        [0.2516, 0.2524, 0.2484, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2488, 0.2511, 0.2511],
        [0.2497, 0.2509, 0.2512, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2528, 0.2524, 0.2464],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2483, 0.2498, 0.2498, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2482, 0.2507, 0.2506],
        [0.2495, 0.2487, 0.2516, 0.2502],
        [0.2490, 0.2497, 0.2517, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2491, 0.2490, 0.2490],
        [0.2528, 0.2492, 0.2498, 0.2481],
        [0.2507, 0.2479, 0.2491, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2378, 0.2554, 0.2610, 0.2458],
        [0.2416, 0.2514, 0.2547, 0.2524],
        [0.2419, 0.2536, 0.2539, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2473, 0.2506, 0.2519],
        [0.2475, 0.2495, 0.2517, 0.2514],
        [0.2448, 0.2516, 0.2564, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:56:20午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 0.5914 (0.6981)	Prec@(1,5) (75.1%, 98.6%)	
05/31 12:56:36午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 0.8595 (0.6995)	Prec@(1,5) (75.1%, 98.6%)	
05/31 12:56:53午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 0.8440 (0.7087)	Prec@(1,5) (74.8%, 98.6%)	
05/31 12:57:09午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 1.0944 (0.7103)	Prec@(1,5) (74.7%, 98.5%)	
05/31 12:57:25午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 0.8974 (0.7156)	Prec@(1,5) (74.5%, 98.5%)	
05/31 12:57:41午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 0.9025 (0.7170)	Prec@(1,5) (74.6%, 98.4%)	
05/31 12:57:57午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 0.6089 (0.7187)	Prec@(1,5) (74.5%, 98.5%)	
05/31 12:58:10午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 0.8297 (0.7188)	Prec@(1,5) (74.6%, 98.4%)	
05/31 12:58:10午前 searchStage_trainer.py:221 [INFO] Train: [ 14/49] Final Prec@1 74.5680%
05/31 12:58:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 0.9265	Prec@(1,5) (68.5%, 96.4%)
05/31 12:58:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 0.9248	Prec@(1,5) (68.1%, 96.7%)
05/31 12:58:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 0.9275	Prec@(1,5) (67.9%, 96.8%)
05/31 12:58:19午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 0.9331	Prec@(1,5) (67.9%, 96.8%)
05/31 12:58:22午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 0.9295	Prec@(1,5) (68.1%, 96.8%)
05/31 12:58:24午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 0.9321	Prec@(1,5) (68.0%, 96.8%)
05/31 12:58:26午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 0.9373	Prec@(1,5) (67.8%, 96.8%)
05/31 12:58:28午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 0.9438	Prec@(1,5) (67.6%, 96.7%)
05/31 12:58:28午前 searchStage_trainer.py:260 [INFO] Valid: [ 14/49] Final Prec@1 67.6000%
05/31 12:58:28午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:58:28午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 69.7680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2607, 0.3093, 0.2411, 0.1889],
        [0.2452, 0.3207, 0.2384, 0.1957]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2607, 0.3021, 0.2580, 0.1792],
        [0.2507, 0.3112, 0.2626, 0.1754],
        [0.2686, 0.3337, 0.2106, 0.1871]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2683, 0.2957, 0.2489, 0.1871],
        [0.2762, 0.3092, 0.2101, 0.2044],
        [0.2665, 0.3181, 0.2203, 0.1951]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2659, 0.3035, 0.2287, 0.2019],
        [0.2674, 0.3014, 0.2174, 0.2137],
        [0.2607, 0.3006, 0.2252, 0.2135]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3040, 0.2261, 0.1991],
        [0.2688, 0.2983, 0.2247, 0.2083],
        [0.2706, 0.2933, 0.2202, 0.2158]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2653, 0.2924, 0.2179, 0.2243],
        [0.2666, 0.2938, 0.2260, 0.2135],
        [0.2665, 0.2926, 0.2227, 0.2182]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2526, 0.2517, 0.2471],
        [0.2493, 0.2515, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2530, 0.2507, 0.2447],
        [0.2535, 0.2497, 0.2518, 0.2450],
        [0.2498, 0.2541, 0.2495, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2513, 0.2497, 0.2487],
        [0.2524, 0.2533, 0.2471, 0.2472],
        [0.2520, 0.2529, 0.2486, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2475, 0.2483],
        [0.2523, 0.2513, 0.2486, 0.2478],
        [0.2517, 0.2522, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2550, 0.2509, 0.2450],
        [0.2504, 0.2525, 0.2474, 0.2496],
        [0.2504, 0.2520, 0.2485, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2516, 0.2484, 0.2477],
        [0.2508, 0.2509, 0.2485, 0.2497],
        [0.2516, 0.2523, 0.2484, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2488, 0.2510, 0.2511],
        [0.2497, 0.2509, 0.2512, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2529, 0.2524, 0.2463],
        [0.2484, 0.2510, 0.2507, 0.2498],
        [0.2483, 0.2498, 0.2498, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2482, 0.2507, 0.2507],
        [0.2495, 0.2487, 0.2516, 0.2502],
        [0.2490, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2491, 0.2490, 0.2490],
        [0.2529, 0.2492, 0.2498, 0.2481],
        [0.2507, 0.2479, 0.2490, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2377, 0.2554, 0.2611, 0.2458],
        [0.2415, 0.2514, 0.2547, 0.2524],
        [0.2418, 0.2536, 0.2540, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2473, 0.2506, 0.2519],
        [0.2474, 0.2495, 0.2517, 0.2514],
        [0.2448, 0.2516, 0.2564, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:58:45午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 0.5598 (0.6958)	Prec@(1,5) (75.4%, 98.2%)	
05/31 12:59:01午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 0.5979 (0.6810)	Prec@(1,5) (76.0%, 98.4%)	
05/31 12:59:18午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 0.7226 (0.6780)	Prec@(1,5) (76.2%, 98.5%)	
05/31 12:59:34午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 0.7811 (0.6788)	Prec@(1,5) (76.2%, 98.5%)	
05/31 12:59:50午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 0.5954 (0.6829)	Prec@(1,5) (76.0%, 98.5%)	
05/31 01:00:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 0.5801 (0.6755)	Prec@(1,5) (76.3%, 98.5%)	
05/31 01:00:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 0.7591 (0.6785)	Prec@(1,5) (76.2%, 98.6%)	
05/31 01:00:36午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 0.5443 (0.6781)	Prec@(1,5) (76.3%, 98.6%)	
05/31 01:00:36午前 searchStage_trainer.py:221 [INFO] Train: [ 15/49] Final Prec@1 76.2840%
05/31 01:00:39午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 0.8308	Prec@(1,5) (70.4%, 97.9%)
05/31 01:00:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 0.8168	Prec@(1,5) (71.0%, 97.9%)
05/31 01:00:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 0.8295	Prec@(1,5) (70.8%, 97.9%)
05/31 01:00:45午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 0.8318	Prec@(1,5) (70.6%, 97.9%)
05/31 01:00:48午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 0.8244	Prec@(1,5) (70.9%, 97.9%)
05/31 01:00:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 0.8191	Prec@(1,5) (71.2%, 98.0%)
05/31 01:00:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 0.8210	Prec@(1,5) (71.1%, 98.0%)
05/31 01:00:54午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 0.8239	Prec@(1,5) (71.0%, 98.0%)
05/31 01:00:54午前 searchStage_trainer.py:260 [INFO] Valid: [ 15/49] Final Prec@1 71.0080%
05/31 01:00:54午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:00:54午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 71.0080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2603, 0.3091, 0.2415, 0.1891],
        [0.2451, 0.3205, 0.2388, 0.1956]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2603, 0.3023, 0.2584, 0.1790],
        [0.2502, 0.3117, 0.2634, 0.1746],
        [0.2684, 0.3333, 0.2108, 0.1874]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2680, 0.2964, 0.2494, 0.1862],
        [0.2761, 0.3089, 0.2103, 0.2048],
        [0.2664, 0.3177, 0.2205, 0.1954]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2660, 0.3035, 0.2288, 0.2018],
        [0.2673, 0.3012, 0.2177, 0.2139],
        [0.2606, 0.3002, 0.2253, 0.2140]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2710, 0.3038, 0.2260, 0.1992],
        [0.2688, 0.2980, 0.2247, 0.2085],
        [0.2706, 0.2930, 0.2203, 0.2161]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2654, 0.2925, 0.2179, 0.2242],
        [0.2666, 0.2935, 0.2261, 0.2137],
        [0.2663, 0.2922, 0.2228, 0.2186]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2526, 0.2517, 0.2471],
        [0.2493, 0.2515, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2529, 0.2507, 0.2448],
        [0.2536, 0.2497, 0.2518, 0.2449],
        [0.2498, 0.2541, 0.2495, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2513, 0.2497, 0.2487],
        [0.2524, 0.2533, 0.2471, 0.2472],
        [0.2520, 0.2529, 0.2485, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2475, 0.2483],
        [0.2523, 0.2513, 0.2486, 0.2478],
        [0.2517, 0.2522, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2550, 0.2510, 0.2450],
        [0.2504, 0.2525, 0.2474, 0.2496],
        [0.2504, 0.2519, 0.2485, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2516, 0.2484, 0.2477],
        [0.2508, 0.2509, 0.2485, 0.2498],
        [0.2516, 0.2523, 0.2485, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2488, 0.2511, 0.2511],
        [0.2497, 0.2509, 0.2512, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2529, 0.2524, 0.2463],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2483, 0.2498, 0.2498, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2481, 0.2507, 0.2507],
        [0.2495, 0.2487, 0.2517, 0.2502],
        [0.2490, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2491, 0.2490, 0.2489],
        [0.2529, 0.2492, 0.2498, 0.2481],
        [0.2507, 0.2479, 0.2490, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2375, 0.2555, 0.2612, 0.2457],
        [0.2414, 0.2514, 0.2547, 0.2524],
        [0.2418, 0.2536, 0.2540, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2472, 0.2506, 0.2520],
        [0.2474, 0.2495, 0.2517, 0.2514],
        [0.2448, 0.2516, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:01:11午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 0.6400 (0.6290)	Prec@(1,5) (78.1%, 99.1%)	
05/31 01:01:27午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 0.6346 (0.6366)	Prec@(1,5) (77.5%, 98.9%)	
05/31 01:01:44午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 0.5342 (0.6441)	Prec@(1,5) (77.3%, 98.9%)	
05/31 01:02:00午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 0.6287 (0.6442)	Prec@(1,5) (77.5%, 98.9%)	
05/31 01:02:16午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 0.8420 (0.6529)	Prec@(1,5) (77.1%, 98.8%)	
05/31 01:02:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 0.8204 (0.6582)	Prec@(1,5) (76.9%, 98.7%)	
05/31 01:02:48午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 0.5324 (0.6559)	Prec@(1,5) (76.9%, 98.7%)	
05/31 01:03:01午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 0.6300 (0.6576)	Prec@(1,5) (76.8%, 98.7%)	
05/31 01:03:02午前 searchStage_trainer.py:221 [INFO] Train: [ 16/49] Final Prec@1 76.8400%
05/31 01:03:04午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 0.8724	Prec@(1,5) (70.7%, 97.6%)
05/31 01:03:06午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 0.8725	Prec@(1,5) (70.9%, 97.7%)
05/31 01:03:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 0.8678	Prec@(1,5) (70.9%, 97.7%)
05/31 01:03:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 0.8654	Prec@(1,5) (71.0%, 97.7%)
05/31 01:03:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 0.8692	Prec@(1,5) (70.9%, 97.6%)
05/31 01:03:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 0.8694	Prec@(1,5) (70.9%, 97.6%)
05/31 01:03:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 0.8649	Prec@(1,5) (71.1%, 97.6%)
05/31 01:03:19午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 0.8632	Prec@(1,5) (71.0%, 97.5%)
05/31 01:03:19午前 searchStage_trainer.py:260 [INFO] Valid: [ 16/49] Final Prec@1 71.0120%
05/31 01:03:19午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:03:20午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 71.0120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2603, 0.3087, 0.2415, 0.1895],
        [0.2453, 0.3201, 0.2390, 0.1955]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2600, 0.3019, 0.2586, 0.1794],
        [0.2504, 0.3121, 0.2637, 0.1738],
        [0.2683, 0.3330, 0.2111, 0.1876]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2685, 0.2962, 0.2490, 0.1863],
        [0.2761, 0.3087, 0.2105, 0.2048],
        [0.2664, 0.3173, 0.2206, 0.1957]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2660, 0.3032, 0.2289, 0.2019],
        [0.2673, 0.3009, 0.2176, 0.2142],
        [0.2604, 0.2998, 0.2256, 0.2142]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2709, 0.3035, 0.2260, 0.1995],
        [0.2688, 0.2977, 0.2249, 0.2087],
        [0.2705, 0.2926, 0.2205, 0.2164]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2656, 0.2923, 0.2178, 0.2244],
        [0.2665, 0.2932, 0.2264, 0.2139],
        [0.2662, 0.2918, 0.2230, 0.2190]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2526, 0.2517, 0.2472],
        [0.2492, 0.2515, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2529, 0.2506, 0.2449],
        [0.2535, 0.2497, 0.2519, 0.2449],
        [0.2498, 0.2541, 0.2495, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2514, 0.2497, 0.2487],
        [0.2524, 0.2533, 0.2471, 0.2472],
        [0.2520, 0.2529, 0.2485, 0.2465]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2475, 0.2483],
        [0.2523, 0.2513, 0.2486, 0.2478],
        [0.2517, 0.2522, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2550, 0.2511, 0.2449],
        [0.2504, 0.2525, 0.2474, 0.2497],
        [0.2504, 0.2519, 0.2485, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2516, 0.2484, 0.2477],
        [0.2508, 0.2509, 0.2485, 0.2498],
        [0.2516, 0.2523, 0.2485, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2488, 0.2510, 0.2511],
        [0.2497, 0.2509, 0.2512, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2530, 0.2524, 0.2463],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2483, 0.2497, 0.2497, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2481, 0.2508, 0.2507],
        [0.2495, 0.2487, 0.2517, 0.2502],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2491, 0.2489, 0.2489],
        [0.2530, 0.2492, 0.2498, 0.2481],
        [0.2507, 0.2479, 0.2490, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2374, 0.2556, 0.2614, 0.2457],
        [0.2414, 0.2514, 0.2547, 0.2524],
        [0.2417, 0.2536, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2472, 0.2506, 0.2520],
        [0.2474, 0.2495, 0.2517, 0.2514],
        [0.2449, 0.2516, 0.2565, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:03:37午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 0.5081 (0.5661)	Prec@(1,5) (80.2%, 99.1%)	
05/31 01:03:53午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 0.5419 (0.5878)	Prec@(1,5) (79.3%, 99.0%)	
05/31 01:04:09午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 0.6686 (0.6048)	Prec@(1,5) (78.7%, 98.9%)	
05/31 01:04:25午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 0.5899 (0.6091)	Prec@(1,5) (78.7%, 98.9%)	
05/31 01:04:41午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 0.4131 (0.6100)	Prec@(1,5) (78.6%, 98.9%)	
05/31 01:04:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 0.7921 (0.6128)	Prec@(1,5) (78.5%, 98.8%)	
05/31 01:05:14午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 0.5662 (0.6196)	Prec@(1,5) (78.2%, 98.8%)	
05/31 01:05:27午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 0.5257 (0.6191)	Prec@(1,5) (78.4%, 98.8%)	
05/31 01:05:27午前 searchStage_trainer.py:221 [INFO] Train: [ 17/49] Final Prec@1 78.3680%
05/31 01:05:30午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 0.7536	Prec@(1,5) (74.8%, 98.0%)
05/31 01:05:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 0.7378	Prec@(1,5) (75.0%, 98.1%)
05/31 01:05:34午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 0.7368	Prec@(1,5) (75.0%, 98.1%)
05/31 01:05:36午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 0.7344	Prec@(1,5) (74.9%, 98.1%)
05/31 01:05:38午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 0.7394	Prec@(1,5) (74.6%, 98.0%)
05/31 01:05:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 0.7372	Prec@(1,5) (74.6%, 98.1%)
05/31 01:05:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 0.7369	Prec@(1,5) (74.5%, 98.0%)
05/31 01:05:45午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 0.7382	Prec@(1,5) (74.5%, 98.0%)
05/31 01:05:45午前 searchStage_trainer.py:260 [INFO] Valid: [ 17/49] Final Prec@1 74.5200%
05/31 01:05:45午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:05:45午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 74.5200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2602, 0.3084, 0.2418, 0.1896],
        [0.2451, 0.3198, 0.2395, 0.1957]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2599, 0.3020, 0.2590, 0.1791],
        [0.2507, 0.3119, 0.2640, 0.1734],
        [0.2685, 0.3325, 0.2110, 0.1881]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2690, 0.2962, 0.2487, 0.1862],
        [0.2760, 0.3084, 0.2107, 0.2049],
        [0.2664, 0.3170, 0.2206, 0.1960]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.3030, 0.2289, 0.2022],
        [0.2673, 0.3006, 0.2177, 0.2145],
        [0.2603, 0.2994, 0.2258, 0.2145]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2709, 0.3034, 0.2259, 0.1998],
        [0.2688, 0.2973, 0.2250, 0.2088],
        [0.2704, 0.2922, 0.2207, 0.2167]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2655, 0.2921, 0.2177, 0.2248],
        [0.2664, 0.2929, 0.2265, 0.2142],
        [0.2661, 0.2914, 0.2232, 0.2193]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2525, 0.2517, 0.2472],
        [0.2492, 0.2515, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2529, 0.2507, 0.2449],
        [0.2535, 0.2497, 0.2519, 0.2449],
        [0.2498, 0.2541, 0.2495, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2514, 0.2498, 0.2487],
        [0.2524, 0.2532, 0.2471, 0.2473],
        [0.2520, 0.2529, 0.2485, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2475, 0.2483],
        [0.2523, 0.2513, 0.2486, 0.2479],
        [0.2517, 0.2522, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2550, 0.2511, 0.2449],
        [0.2504, 0.2525, 0.2474, 0.2497],
        [0.2504, 0.2519, 0.2485, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2516, 0.2484, 0.2477],
        [0.2508, 0.2509, 0.2485, 0.2498],
        [0.2516, 0.2523, 0.2485, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2488, 0.2510, 0.2511],
        [0.2497, 0.2509, 0.2512, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2530, 0.2524, 0.2462],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2483, 0.2497, 0.2497, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2481, 0.2508, 0.2507],
        [0.2495, 0.2487, 0.2517, 0.2502],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2491, 0.2489, 0.2489],
        [0.2530, 0.2492, 0.2498, 0.2481],
        [0.2507, 0.2479, 0.2490, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2373, 0.2556, 0.2615, 0.2457],
        [0.2413, 0.2514, 0.2548, 0.2525],
        [0.2417, 0.2536, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2471, 0.2506, 0.2520],
        [0.2474, 0.2495, 0.2517, 0.2514],
        [0.2449, 0.2516, 0.2565, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:06:02午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 0.6227 (0.5693)	Prec@(1,5) (81.1%, 98.8%)	
05/31 01:06:18午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 0.7410 (0.5706)	Prec@(1,5) (80.5%, 98.9%)	
05/31 01:06:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 0.4575 (0.5713)	Prec@(1,5) (80.4%, 98.8%)	
05/31 01:06:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 0.4947 (0.5879)	Prec@(1,5) (79.6%, 98.8%)	
05/31 01:07:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 0.5265 (0.5865)	Prec@(1,5) (79.5%, 98.8%)	
05/31 01:07:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 0.6487 (0.5861)	Prec@(1,5) (79.4%, 98.8%)	
05/31 01:07:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 0.6684 (0.5886)	Prec@(1,5) (79.4%, 98.9%)	
05/31 01:07:53午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 0.5306 (0.5918)	Prec@(1,5) (79.3%, 98.9%)	
05/31 01:07:53午前 searchStage_trainer.py:221 [INFO] Train: [ 18/49] Final Prec@1 79.3320%
05/31 01:07:55午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 0.7361	Prec@(1,5) (74.9%, 97.9%)
05/31 01:07:58午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 0.7171	Prec@(1,5) (75.5%, 98.1%)
05/31 01:08:00午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 0.7148	Prec@(1,5) (75.4%, 98.1%)
05/31 01:08:02午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 0.7174	Prec@(1,5) (75.5%, 98.3%)
05/31 01:08:04午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 0.7277	Prec@(1,5) (75.0%, 98.1%)
05/31 01:08:06午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 0.7272	Prec@(1,5) (74.9%, 98.2%)
05/31 01:08:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 0.7244	Prec@(1,5) (75.0%, 98.2%)
05/31 01:08:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 0.7277	Prec@(1,5) (74.9%, 98.2%)
05/31 01:08:11午前 searchStage_trainer.py:260 [INFO] Valid: [ 18/49] Final Prec@1 74.9360%
05/31 01:08:11午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:08:11午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 74.9360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2602, 0.3081, 0.2419, 0.1897],
        [0.2450, 0.3194, 0.2397, 0.1960]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2600, 0.3019, 0.2591, 0.1789],
        [0.2508, 0.3118, 0.2639, 0.1735],
        [0.2685, 0.3321, 0.2111, 0.1883]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2690, 0.2962, 0.2489, 0.1858],
        [0.2760, 0.3081, 0.2107, 0.2052],
        [0.2664, 0.3166, 0.2207, 0.1963]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2659, 0.3027, 0.2288, 0.2026],
        [0.2672, 0.3003, 0.2178, 0.2147],
        [0.2602, 0.2991, 0.2259, 0.2148]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2710, 0.3031, 0.2257, 0.2003],
        [0.2688, 0.2971, 0.2252, 0.2089],
        [0.2704, 0.2918, 0.2208, 0.2170]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2655, 0.2918, 0.2174, 0.2252],
        [0.2663, 0.2926, 0.2267, 0.2144],
        [0.2660, 0.2910, 0.2234, 0.2196]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2472],
        [0.2492, 0.2515, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2528, 0.2507, 0.2449],
        [0.2535, 0.2497, 0.2519, 0.2449],
        [0.2497, 0.2541, 0.2495, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2514, 0.2498, 0.2486],
        [0.2524, 0.2532, 0.2471, 0.2473],
        [0.2520, 0.2528, 0.2485, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2475, 0.2483],
        [0.2523, 0.2513, 0.2486, 0.2479],
        [0.2517, 0.2521, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2550, 0.2511, 0.2449],
        [0.2503, 0.2525, 0.2474, 0.2498],
        [0.2504, 0.2519, 0.2486, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2515, 0.2484, 0.2477],
        [0.2508, 0.2508, 0.2485, 0.2499],
        [0.2516, 0.2523, 0.2485, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2511],
        [0.2497, 0.2509, 0.2512, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2530, 0.2524, 0.2462],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2483, 0.2497, 0.2497, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2480, 0.2508, 0.2507],
        [0.2495, 0.2487, 0.2517, 0.2502],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2531, 0.2490, 0.2489, 0.2489],
        [0.2530, 0.2491, 0.2498, 0.2481],
        [0.2507, 0.2479, 0.2490, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2372, 0.2557, 0.2616, 0.2456],
        [0.2413, 0.2515, 0.2548, 0.2525],
        [0.2417, 0.2536, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2471, 0.2506, 0.2520],
        [0.2474, 0.2495, 0.2517, 0.2514],
        [0.2449, 0.2516, 0.2565, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:08:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 0.8066 (0.5307)	Prec@(1,5) (81.3%, 99.1%)	
05/31 01:08:45午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 0.4328 (0.5384)	Prec@(1,5) (81.0%, 99.0%)	
05/31 01:09:00午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 0.4770 (0.5543)	Prec@(1,5) (80.2%, 99.0%)	
05/31 01:09:17午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 0.5818 (0.5546)	Prec@(1,5) (80.3%, 99.0%)	
05/31 01:09:33午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 0.3771 (0.5555)	Prec@(1,5) (80.4%, 99.0%)	
05/31 01:09:49午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 0.3835 (0.5540)	Prec@(1,5) (80.5%, 99.0%)	
05/31 01:10:06午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 0.4572 (0.5559)	Prec@(1,5) (80.3%, 99.0%)	
05/31 01:10:18午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 0.7705 (0.5591)	Prec@(1,5) (80.3%, 99.0%)	
05/31 01:10:19午前 searchStage_trainer.py:221 [INFO] Train: [ 19/49] Final Prec@1 80.2840%
05/31 01:10:21午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 0.6750	Prec@(1,5) (77.0%, 98.3%)
05/31 01:10:23午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 0.6920	Prec@(1,5) (76.6%, 98.2%)
05/31 01:10:26午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 0.6848	Prec@(1,5) (76.9%, 98.2%)
05/31 01:10:28午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 0.6829	Prec@(1,5) (76.8%, 98.4%)
05/31 01:10:30午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 0.6904	Prec@(1,5) (76.5%, 98.3%)
05/31 01:10:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 0.6878	Prec@(1,5) (76.6%, 98.3%)
05/31 01:10:34午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 0.6857	Prec@(1,5) (76.7%, 98.3%)
05/31 01:10:36午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 0.6843	Prec@(1,5) (76.7%, 98.3%)
05/31 01:10:36午前 searchStage_trainer.py:260 [INFO] Valid: [ 19/49] Final Prec@1 76.6840%
05/31 01:10:36午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:10:37午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 76.6840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2601, 0.3078, 0.2421, 0.1899],
        [0.2449, 0.3192, 0.2398, 0.1961]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2599, 0.3019, 0.2596, 0.1785],
        [0.2507, 0.3118, 0.2642, 0.1734],
        [0.2683, 0.3317, 0.2113, 0.1887]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2687, 0.2961, 0.2494, 0.1857],
        [0.2760, 0.3079, 0.2108, 0.2053],
        [0.2663, 0.3163, 0.2208, 0.1966]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2659, 0.3026, 0.2289, 0.2027],
        [0.2672, 0.3000, 0.2179, 0.2149],
        [0.2601, 0.2987, 0.2261, 0.2151]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2710, 0.3028, 0.2255, 0.2006],
        [0.2687, 0.2968, 0.2255, 0.2090],
        [0.2702, 0.2915, 0.2209, 0.2173]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2655, 0.2916, 0.2173, 0.2256],
        [0.2663, 0.2923, 0.2269, 0.2146],
        [0.2659, 0.2906, 0.2235, 0.2199]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2472],
        [0.2492, 0.2515, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2528, 0.2506, 0.2449],
        [0.2535, 0.2497, 0.2519, 0.2449],
        [0.2497, 0.2540, 0.2495, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2514, 0.2498, 0.2487],
        [0.2524, 0.2532, 0.2471, 0.2473],
        [0.2520, 0.2528, 0.2485, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2475, 0.2484],
        [0.2523, 0.2513, 0.2486, 0.2479],
        [0.2517, 0.2521, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2550, 0.2512, 0.2449],
        [0.2503, 0.2525, 0.2474, 0.2498],
        [0.2504, 0.2519, 0.2486, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2515, 0.2484, 0.2477],
        [0.2508, 0.2508, 0.2485, 0.2499],
        [0.2516, 0.2522, 0.2485, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2497, 0.2509, 0.2512, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2530, 0.2524, 0.2462],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2483, 0.2497, 0.2497, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2480, 0.2508, 0.2507],
        [0.2495, 0.2487, 0.2517, 0.2502],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2490, 0.2489, 0.2489],
        [0.2530, 0.2491, 0.2498, 0.2481],
        [0.2508, 0.2479, 0.2490, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2371, 0.2557, 0.2617, 0.2456],
        [0.2413, 0.2515, 0.2548, 0.2525],
        [0.2417, 0.2536, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2471, 0.2506, 0.2521],
        [0.2473, 0.2495, 0.2517, 0.2515],
        [0.2449, 0.2516, 0.2565, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:10:54午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 0.5156 (0.5267)	Prec@(1,5) (81.4%, 99.0%)	
05/31 01:11:10午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 0.6568 (0.5302)	Prec@(1,5) (81.6%, 99.1%)	
05/31 01:11:26午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 0.7143 (0.5300)	Prec@(1,5) (81.4%, 99.1%)	
05/31 01:11:43午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 0.6178 (0.5329)	Prec@(1,5) (81.4%, 99.2%)	
05/31 01:11:59午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 0.4727 (0.5378)	Prec@(1,5) (81.2%, 99.2%)	
05/31 01:12:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 0.4158 (0.5419)	Prec@(1,5) (81.1%, 99.1%)	
05/31 01:12:31午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 0.9057 (0.5429)	Prec@(1,5) (81.1%, 99.1%)	
05/31 01:12:44午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 0.4999 (0.5405)	Prec@(1,5) (81.2%, 99.1%)	
05/31 01:12:44午前 searchStage_trainer.py:221 [INFO] Train: [ 20/49] Final Prec@1 81.1960%
05/31 01:12:46午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 0.7254	Prec@(1,5) (75.9%, 98.0%)
05/31 01:12:49午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 0.7398	Prec@(1,5) (75.1%, 98.1%)
05/31 01:12:51午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 0.7354	Prec@(1,5) (75.1%, 98.2%)
05/31 01:12:53午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 0.7355	Prec@(1,5) (75.1%, 98.2%)
05/31 01:12:55午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 0.7395	Prec@(1,5) (75.0%, 98.1%)
05/31 01:12:58午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 0.7395	Prec@(1,5) (75.0%, 98.1%)
05/31 01:13:00午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 0.7372	Prec@(1,5) (75.1%, 98.2%)
05/31 01:13:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 0.7360	Prec@(1,5) (75.1%, 98.2%)
05/31 01:13:02午前 searchStage_trainer.py:260 [INFO] Valid: [ 20/49] Final Prec@1 75.0480%
05/31 01:13:02午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:13:02午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 76.6840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2600, 0.3076, 0.2424, 0.1901],
        [0.2448, 0.3190, 0.2399, 0.1963]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2597, 0.3021, 0.2599, 0.1783],
        [0.2506, 0.3116, 0.2642, 0.1736],
        [0.2683, 0.3314, 0.2114, 0.1890]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2686, 0.2965, 0.2498, 0.1851],
        [0.2760, 0.3076, 0.2109, 0.2055],
        [0.2663, 0.3159, 0.2209, 0.1970]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2660, 0.3024, 0.2289, 0.2028],
        [0.2671, 0.2998, 0.2179, 0.2153],
        [0.2600, 0.2984, 0.2262, 0.2154]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3026, 0.2258, 0.2007],
        [0.2687, 0.2965, 0.2255, 0.2094],
        [0.2702, 0.2912, 0.2211, 0.2176]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2654, 0.2914, 0.2172, 0.2259],
        [0.2662, 0.2921, 0.2268, 0.2149],
        [0.2658, 0.2903, 0.2237, 0.2202]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2472],
        [0.2492, 0.2514, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2535, 0.2497, 0.2519, 0.2449],
        [0.2497, 0.2540, 0.2495, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2514, 0.2498, 0.2487],
        [0.2524, 0.2532, 0.2471, 0.2473],
        [0.2520, 0.2528, 0.2485, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2475, 0.2484],
        [0.2523, 0.2513, 0.2486, 0.2479],
        [0.2517, 0.2521, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2550, 0.2512, 0.2449],
        [0.2503, 0.2525, 0.2474, 0.2498],
        [0.2504, 0.2519, 0.2486, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2515, 0.2484, 0.2478],
        [0.2507, 0.2508, 0.2485, 0.2499],
        [0.2516, 0.2522, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2497, 0.2509, 0.2512, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2530, 0.2524, 0.2461],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2483, 0.2497, 0.2497, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2480, 0.2508, 0.2507],
        [0.2495, 0.2487, 0.2517, 0.2501],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2490, 0.2489, 0.2489],
        [0.2531, 0.2491, 0.2497, 0.2481],
        [0.2508, 0.2479, 0.2490, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2370, 0.2557, 0.2617, 0.2456],
        [0.2412, 0.2515, 0.2548, 0.2525],
        [0.2417, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2471, 0.2506, 0.2521],
        [0.2473, 0.2495, 0.2517, 0.2515],
        [0.2449, 0.2516, 0.2565, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:13:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 0.6594 (0.5045)	Prec@(1,5) (82.6%, 99.5%)	
05/31 01:13:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 0.5933 (0.4922)	Prec@(1,5) (83.1%, 99.4%)	
05/31 01:13:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 0.3527 (0.5038)	Prec@(1,5) (82.5%, 99.4%)	
05/31 01:14:08午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 0.6311 (0.5053)	Prec@(1,5) (82.5%, 99.3%)	
05/31 01:14:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 0.5952 (0.5097)	Prec@(1,5) (82.3%, 99.2%)	
05/31 01:14:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 0.5103 (0.5142)	Prec@(1,5) (82.2%, 99.2%)	
05/31 01:14:56午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 0.4683 (0.5192)	Prec@(1,5) (82.1%, 99.2%)	
05/31 01:15:09午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 0.4421 (0.5159)	Prec@(1,5) (82.2%, 99.2%)	
05/31 01:15:09午前 searchStage_trainer.py:221 [INFO] Train: [ 21/49] Final Prec@1 82.1960%
05/31 01:15:12午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 0.6525	Prec@(1,5) (78.2%, 98.8%)
05/31 01:15:14午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 0.6530	Prec@(1,5) (78.4%, 98.7%)
05/31 01:15:16午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 0.6541	Prec@(1,5) (78.4%, 98.7%)
05/31 01:15:18午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 0.6623	Prec@(1,5) (77.8%, 98.6%)
05/31 01:15:20午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 0.6631	Prec@(1,5) (77.8%, 98.6%)
05/31 01:15:23午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 0.6595	Prec@(1,5) (77.8%, 98.6%)
05/31 01:15:25午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 0.6595	Prec@(1,5) (77.8%, 98.6%)
05/31 01:15:26午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 0.6593	Prec@(1,5) (77.7%, 98.6%)
05/31 01:15:27午前 searchStage_trainer.py:260 [INFO] Valid: [ 21/49] Final Prec@1 77.7000%
05/31 01:15:27午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:15:27午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.7000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2598, 0.3073, 0.2426, 0.1904],
        [0.2448, 0.3188, 0.2400, 0.1964]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2596, 0.3018, 0.2603, 0.1783],
        [0.2508, 0.3112, 0.2642, 0.1738],
        [0.2683, 0.3311, 0.2115, 0.1892]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2691, 0.2965, 0.2497, 0.1847],
        [0.2760, 0.3073, 0.2109, 0.2058],
        [0.2662, 0.3156, 0.2210, 0.1972]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2660, 0.3022, 0.2290, 0.2028],
        [0.2670, 0.2995, 0.2179, 0.2156],
        [0.2600, 0.2981, 0.2264, 0.2156]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3024, 0.2258, 0.2010],
        [0.2685, 0.2963, 0.2256, 0.2096],
        [0.2701, 0.2909, 0.2212, 0.2178]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2655, 0.2913, 0.2173, 0.2259],
        [0.2661, 0.2918, 0.2270, 0.2151],
        [0.2657, 0.2900, 0.2238, 0.2205]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2472],
        [0.2492, 0.2514, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2529, 0.2507, 0.2449],
        [0.2535, 0.2497, 0.2519, 0.2450],
        [0.2497, 0.2540, 0.2495, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2514, 0.2497, 0.2488],
        [0.2524, 0.2532, 0.2471, 0.2473],
        [0.2520, 0.2528, 0.2486, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2521, 0.2475, 0.2484],
        [0.2523, 0.2513, 0.2486, 0.2479],
        [0.2517, 0.2521, 0.2487, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2550, 0.2512, 0.2449],
        [0.2503, 0.2525, 0.2474, 0.2498],
        [0.2504, 0.2519, 0.2486, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2515, 0.2484, 0.2477],
        [0.2507, 0.2508, 0.2485, 0.2499],
        [0.2516, 0.2522, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2497, 0.2509, 0.2512, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2524, 0.2461],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2484, 0.2497, 0.2497, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2480, 0.2508, 0.2507],
        [0.2495, 0.2486, 0.2517, 0.2501],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2490, 0.2489, 0.2489],
        [0.2531, 0.2491, 0.2497, 0.2481],
        [0.2508, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2369, 0.2558, 0.2618, 0.2455],
        [0.2412, 0.2515, 0.2548, 0.2525],
        [0.2417, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2471, 0.2506, 0.2521],
        [0.2473, 0.2495, 0.2517, 0.2515],
        [0.2449, 0.2516, 0.2565, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:15:44午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 0.4144 (0.4741)	Prec@(1,5) (83.1%, 99.4%)	
05/31 01:16:00午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 0.5306 (0.4837)	Prec@(1,5) (82.8%, 99.4%)	
05/31 01:16:16午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 0.2973 (0.4930)	Prec@(1,5) (82.6%, 99.4%)	
05/31 01:16:33午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 0.4596 (0.4926)	Prec@(1,5) (82.6%, 99.4%)	
05/31 01:16:49午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 0.5684 (0.4928)	Prec@(1,5) (82.7%, 99.3%)	
05/31 01:17:05午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 0.4522 (0.4970)	Prec@(1,5) (82.5%, 99.3%)	
05/31 01:17:21午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 0.5563 (0.5001)	Prec@(1,5) (82.5%, 99.3%)	
05/31 01:17:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 0.5160 (0.4991)	Prec@(1,5) (82.5%, 99.3%)	
05/31 01:17:35午前 searchStage_trainer.py:221 [INFO] Train: [ 22/49] Final Prec@1 82.5200%
05/31 01:17:37午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 0.6290	Prec@(1,5) (77.7%, 99.0%)
05/31 01:17:40午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 0.6237	Prec@(1,5) (78.5%, 98.9%)
05/31 01:17:42午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 0.6357	Prec@(1,5) (78.2%, 98.7%)
05/31 01:17:44午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 0.6419	Prec@(1,5) (78.2%, 98.7%)
05/31 01:17:46午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 0.6493	Prec@(1,5) (77.9%, 98.7%)
05/31 01:17:48午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 0.6507	Prec@(1,5) (77.9%, 98.7%)
05/31 01:17:51午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 0.6503	Prec@(1,5) (77.9%, 98.6%)
05/31 01:17:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 0.6546	Prec@(1,5) (77.7%, 98.6%)
05/31 01:17:52午前 searchStage_trainer.py:260 [INFO] Valid: [ 22/49] Final Prec@1 77.7520%
05/31 01:17:52午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:17:53午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.7520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2596, 0.3070, 0.2428, 0.1905],
        [0.2447, 0.3185, 0.2401, 0.1966]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2596, 0.3016, 0.2605, 0.1784],
        [0.2508, 0.3109, 0.2643, 0.1739],
        [0.2682, 0.3308, 0.2116, 0.1894]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2690, 0.2965, 0.2497, 0.1847],
        [0.2759, 0.3070, 0.2110, 0.2060],
        [0.2662, 0.3153, 0.2211, 0.1974]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2660, 0.3021, 0.2291, 0.2029],
        [0.2669, 0.2992, 0.2180, 0.2159],
        [0.2599, 0.2978, 0.2265, 0.2158]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3022, 0.2260, 0.2010],
        [0.2685, 0.2960, 0.2257, 0.2098],
        [0.2700, 0.2906, 0.2213, 0.2181]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2653, 0.2911, 0.2174, 0.2262],
        [0.2661, 0.2916, 0.2270, 0.2153],
        [0.2656, 0.2897, 0.2240, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2472],
        [0.2492, 0.2514, 0.2513, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2529, 0.2506, 0.2449],
        [0.2534, 0.2496, 0.2519, 0.2450],
        [0.2497, 0.2540, 0.2495, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2514, 0.2497, 0.2489],
        [0.2524, 0.2532, 0.2471, 0.2473],
        [0.2520, 0.2528, 0.2486, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2475, 0.2484],
        [0.2523, 0.2513, 0.2486, 0.2479],
        [0.2517, 0.2521, 0.2487, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2550, 0.2513, 0.2448],
        [0.2503, 0.2524, 0.2474, 0.2499],
        [0.2504, 0.2519, 0.2486, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2515, 0.2484, 0.2478],
        [0.2507, 0.2508, 0.2485, 0.2499],
        [0.2516, 0.2522, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2497, 0.2509, 0.2512, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2461],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2484, 0.2497, 0.2497, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2480, 0.2508, 0.2507],
        [0.2495, 0.2486, 0.2517, 0.2501],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2490, 0.2488, 0.2489],
        [0.2531, 0.2491, 0.2497, 0.2480],
        [0.2508, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2368, 0.2558, 0.2619, 0.2455],
        [0.2412, 0.2515, 0.2548, 0.2525],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2470, 0.2506, 0.2521],
        [0.2473, 0.2495, 0.2517, 0.2515],
        [0.2449, 0.2516, 0.2565, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:18:10午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 0.4302 (0.4527)	Prec@(1,5) (83.8%, 99.4%)	
05/31 01:18:26午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 0.4084 (0.4416)	Prec@(1,5) (84.2%, 99.5%)	
05/31 01:18:42午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 0.3873 (0.4584)	Prec@(1,5) (84.0%, 99.4%)	
05/31 01:18:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 0.5652 (0.4610)	Prec@(1,5) (83.8%, 99.4%)	
05/31 01:19:14午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 0.4303 (0.4620)	Prec@(1,5) (83.9%, 99.4%)	
05/31 01:19:30午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 0.7070 (0.4703)	Prec@(1,5) (83.7%, 99.4%)	
05/31 01:19:47午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 0.4207 (0.4671)	Prec@(1,5) (83.8%, 99.4%)	
05/31 01:19:59午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 0.5561 (0.4672)	Prec@(1,5) (83.8%, 99.4%)	
05/31 01:20:00午前 searchStage_trainer.py:221 [INFO] Train: [ 23/49] Final Prec@1 83.7840%
05/31 01:20:02午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 0.7374	Prec@(1,5) (75.3%, 98.3%)
05/31 01:20:04午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 0.7261	Prec@(1,5) (76.0%, 98.3%)
05/31 01:20:07午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 0.7214	Prec@(1,5) (76.1%, 98.4%)
05/31 01:20:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 0.7263	Prec@(1,5) (75.9%, 98.4%)
05/31 01:20:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 0.7153	Prec@(1,5) (76.2%, 98.5%)
05/31 01:20:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 0.7199	Prec@(1,5) (76.1%, 98.5%)
05/31 01:20:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 0.7238	Prec@(1,5) (76.0%, 98.4%)
05/31 01:20:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 0.7253	Prec@(1,5) (76.0%, 98.4%)
05/31 01:20:17午前 searchStage_trainer.py:260 [INFO] Valid: [ 23/49] Final Prec@1 76.0080%
05/31 01:20:17午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:20:18午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.7520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2596, 0.3069, 0.2430, 0.1906],
        [0.2447, 0.3183, 0.2402, 0.1969]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2593, 0.3015, 0.2606, 0.1786],
        [0.2507, 0.3109, 0.2643, 0.1741],
        [0.2682, 0.3305, 0.2118, 0.1895]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2689, 0.2963, 0.2497, 0.1852],
        [0.2760, 0.3068, 0.2111, 0.2061],
        [0.2661, 0.3151, 0.2212, 0.1976]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2659, 0.3019, 0.2292, 0.2030],
        [0.2668, 0.2990, 0.2181, 0.2161],
        [0.2598, 0.2975, 0.2266, 0.2161]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2709, 0.3021, 0.2259, 0.2011],
        [0.2685, 0.2958, 0.2257, 0.2100],
        [0.2700, 0.2903, 0.2214, 0.2183]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2652, 0.2909, 0.2177, 0.2263],
        [0.2660, 0.2914, 0.2271, 0.2155],
        [0.2655, 0.2894, 0.2241, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2472],
        [0.2492, 0.2514, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2449],
        [0.2534, 0.2496, 0.2519, 0.2450],
        [0.2497, 0.2540, 0.2496, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2513, 0.2496, 0.2490],
        [0.2524, 0.2532, 0.2471, 0.2473],
        [0.2520, 0.2528, 0.2486, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2474, 0.2484],
        [0.2523, 0.2512, 0.2486, 0.2479],
        [0.2517, 0.2521, 0.2487, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2550, 0.2513, 0.2448],
        [0.2503, 0.2524, 0.2474, 0.2499],
        [0.2503, 0.2518, 0.2486, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2515, 0.2484, 0.2478],
        [0.2507, 0.2508, 0.2486, 0.2499],
        [0.2516, 0.2522, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2497, 0.2509, 0.2512, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2461],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2484, 0.2497, 0.2497, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2480, 0.2508, 0.2507],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2490, 0.2488, 0.2489],
        [0.2531, 0.2491, 0.2497, 0.2480],
        [0.2508, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2367, 0.2558, 0.2619, 0.2455],
        [0.2412, 0.2515, 0.2548, 0.2525],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2470, 0.2506, 0.2521],
        [0.2473, 0.2495, 0.2517, 0.2515],
        [0.2449, 0.2515, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:20:34午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 0.2717 (0.4133)	Prec@(1,5) (85.6%, 99.7%)	
05/31 01:20:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 0.3362 (0.4211)	Prec@(1,5) (85.1%, 99.5%)	
05/31 01:21:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 0.3790 (0.4257)	Prec@(1,5) (85.0%, 99.5%)	
05/31 01:21:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 0.4310 (0.4352)	Prec@(1,5) (84.8%, 99.5%)	
05/31 01:21:39午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 0.4110 (0.4457)	Prec@(1,5) (84.5%, 99.4%)	
05/31 01:21:55午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 0.4735 (0.4451)	Prec@(1,5) (84.6%, 99.4%)	
05/31 01:22:11午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 0.5101 (0.4476)	Prec@(1,5) (84.5%, 99.4%)	
05/31 01:22:25午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 0.5114 (0.4511)	Prec@(1,5) (84.4%, 99.4%)	
05/31 01:22:25午前 searchStage_trainer.py:221 [INFO] Train: [ 24/49] Final Prec@1 84.3880%
05/31 01:22:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 0.6717	Prec@(1,5) (79.2%, 98.4%)
05/31 01:22:29午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 0.6756	Prec@(1,5) (78.5%, 98.5%)
05/31 01:22:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 0.6813	Prec@(1,5) (78.0%, 98.5%)
05/31 01:22:34午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 0.6880	Prec@(1,5) (77.7%, 98.4%)
05/31 01:22:36午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 0.6916	Prec@(1,5) (77.6%, 98.4%)
05/31 01:22:38午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 0.6918	Prec@(1,5) (77.4%, 98.5%)
05/31 01:22:40午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 0.6916	Prec@(1,5) (77.4%, 98.5%)
05/31 01:22:42午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 0.6901	Prec@(1,5) (77.5%, 98.5%)
05/31 01:22:42午前 searchStage_trainer.py:260 [INFO] Valid: [ 24/49] Final Prec@1 77.4800%
05/31 01:22:42午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:22:42午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.7520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2595, 0.3066, 0.2431, 0.1907],
        [0.2446, 0.3180, 0.2403, 0.1971]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2593, 0.3014, 0.2608, 0.1785],
        [0.2507, 0.3110, 0.2644, 0.1740],
        [0.2682, 0.3302, 0.2118, 0.1898]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2686, 0.2964, 0.2497, 0.1853],
        [0.2758, 0.3067, 0.2113, 0.2062],
        [0.2661, 0.3148, 0.2213, 0.1978]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.3018, 0.2292, 0.2031],
        [0.2668, 0.2987, 0.2182, 0.2162],
        [0.2597, 0.2973, 0.2267, 0.2163]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3020, 0.2261, 0.2011],
        [0.2684, 0.2956, 0.2257, 0.2103],
        [0.2699, 0.2900, 0.2215, 0.2185]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2652, 0.2907, 0.2177, 0.2264],
        [0.2659, 0.2911, 0.2272, 0.2157],
        [0.2654, 0.2891, 0.2242, 0.2213]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2473],
        [0.2492, 0.2514, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2449],
        [0.2534, 0.2496, 0.2519, 0.2451],
        [0.2497, 0.2540, 0.2496, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2514, 0.2497, 0.2489],
        [0.2524, 0.2532, 0.2471, 0.2473],
        [0.2520, 0.2528, 0.2486, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2474, 0.2484],
        [0.2523, 0.2512, 0.2486, 0.2479],
        [0.2517, 0.2521, 0.2487, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2550, 0.2513, 0.2448],
        [0.2503, 0.2524, 0.2474, 0.2499],
        [0.2503, 0.2518, 0.2486, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2508, 0.2486, 0.2499],
        [0.2516, 0.2522, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2461],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2484, 0.2497, 0.2497, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2479, 0.2508, 0.2508],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2490, 0.2488, 0.2489],
        [0.2531, 0.2491, 0.2497, 0.2480],
        [0.2508, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2367, 0.2558, 0.2620, 0.2455],
        [0.2412, 0.2515, 0.2548, 0.2525],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2470, 0.2506, 0.2521],
        [0.2473, 0.2495, 0.2518, 0.2515],
        [0.2449, 0.2515, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:22:59午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 0.4189 (0.3896)	Prec@(1,5) (86.8%, 99.6%)	
05/31 01:23:16午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 0.4191 (0.3961)	Prec@(1,5) (86.2%, 99.5%)	
05/31 01:23:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 0.5864 (0.4158)	Prec@(1,5) (85.6%, 99.4%)	
05/31 01:23:48午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 0.4490 (0.4087)	Prec@(1,5) (85.9%, 99.4%)	
05/31 01:24:05午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 0.4633 (0.4136)	Prec@(1,5) (85.7%, 99.4%)	
05/31 01:24:21午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 0.3637 (0.4182)	Prec@(1,5) (85.6%, 99.4%)	
05/31 01:24:37午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 0.4760 (0.4245)	Prec@(1,5) (85.3%, 99.4%)	
05/31 01:24:50午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 0.4150 (0.4286)	Prec@(1,5) (85.1%, 99.4%)	
05/31 01:24:50午前 searchStage_trainer.py:221 [INFO] Train: [ 25/49] Final Prec@1 85.0760%
05/31 01:24:53午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 0.7981	Prec@(1,5) (74.7%, 98.1%)
05/31 01:24:55午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 0.7925	Prec@(1,5) (74.3%, 98.1%)
05/31 01:24:57午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 0.8052	Prec@(1,5) (74.3%, 98.0%)
05/31 01:24:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 0.8121	Prec@(1,5) (74.3%, 98.0%)
05/31 01:25:02午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 0.8108	Prec@(1,5) (74.2%, 98.0%)
05/31 01:25:04午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 0.8095	Prec@(1,5) (74.2%, 98.0%)
05/31 01:25:06午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 0.8030	Prec@(1,5) (74.4%, 98.1%)
05/31 01:25:08午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 0.8017	Prec@(1,5) (74.4%, 98.0%)
05/31 01:25:08午前 searchStage_trainer.py:260 [INFO] Valid: [ 25/49] Final Prec@1 74.4680%
05/31 01:25:08午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:25:08午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.7520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2594, 0.3064, 0.2434, 0.1908],
        [0.2445, 0.3177, 0.2405, 0.1973]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.3012, 0.2608, 0.1786],
        [0.2508, 0.3109, 0.2645, 0.1738],
        [0.2682, 0.3300, 0.2119, 0.1900]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2689, 0.2962, 0.2498, 0.1851],
        [0.2758, 0.3065, 0.2113, 0.2063],
        [0.2660, 0.3146, 0.2214, 0.1980]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.3018, 0.2292, 0.2032],
        [0.2668, 0.2986, 0.2183, 0.2164],
        [0.2596, 0.2970, 0.2268, 0.2165]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3018, 0.2262, 0.2012],
        [0.2683, 0.2953, 0.2258, 0.2105],
        [0.2699, 0.2898, 0.2216, 0.2187]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2652, 0.2906, 0.2177, 0.2266],
        [0.2659, 0.2909, 0.2273, 0.2159],
        [0.2654, 0.2888, 0.2243, 0.2215]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2472],
        [0.2492, 0.2514, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2534, 0.2496, 0.2519, 0.2451],
        [0.2497, 0.2540, 0.2496, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2513, 0.2497, 0.2489],
        [0.2524, 0.2532, 0.2471, 0.2473],
        [0.2520, 0.2528, 0.2486, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2474, 0.2484],
        [0.2523, 0.2512, 0.2486, 0.2479],
        [0.2517, 0.2521, 0.2487, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2550, 0.2513, 0.2448],
        [0.2503, 0.2524, 0.2474, 0.2499],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2508, 0.2486, 0.2499],
        [0.2516, 0.2522, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2461],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2479, 0.2508, 0.2508],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2490, 0.2488, 0.2489],
        [0.2532, 0.2491, 0.2497, 0.2480],
        [0.2508, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2366, 0.2559, 0.2620, 0.2455],
        [0.2411, 0.2515, 0.2548, 0.2525],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2470, 0.2506, 0.2521],
        [0.2473, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:25:25午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 0.5659 (0.4085)	Prec@(1,5) (85.8%, 99.6%)	
05/31 01:25:41午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 0.3745 (0.3974)	Prec@(1,5) (86.0%, 99.6%)	
05/31 01:25:57午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 0.5051 (0.4003)	Prec@(1,5) (86.0%, 99.5%)	
05/31 01:26:13午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.3065 (0.4048)	Prec@(1,5) (86.0%, 99.5%)	
05/31 01:26:30午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 0.2735 (0.4063)	Prec@(1,5) (86.0%, 99.5%)	
05/31 01:26:46午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 0.3802 (0.4108)	Prec@(1,5) (85.8%, 99.5%)	
05/31 01:27:02午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 0.4208 (0.4087)	Prec@(1,5) (85.8%, 99.5%)	
05/31 01:27:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 0.3274 (0.4053)	Prec@(1,5) (86.0%, 99.5%)	
05/31 01:27:16午前 searchStage_trainer.py:221 [INFO] Train: [ 26/49] Final Prec@1 86.0120%
05/31 01:27:18午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 0.5917	Prec@(1,5) (80.3%, 98.9%)
05/31 01:27:20午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 0.6223	Prec@(1,5) (79.4%, 98.7%)
05/31 01:27:23午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 0.6110	Prec@(1,5) (80.0%, 98.8%)
05/31 01:27:25午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 0.6085	Prec@(1,5) (80.1%, 98.7%)
05/31 01:27:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 0.6124	Prec@(1,5) (80.1%, 98.7%)
05/31 01:27:29午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 0.6104	Prec@(1,5) (80.1%, 98.7%)
05/31 01:27:31午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 0.6087	Prec@(1,5) (80.2%, 98.8%)
05/31 01:27:33午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 0.6075	Prec@(1,5) (80.2%, 98.8%)
05/31 01:27:33午前 searchStage_trainer.py:260 [INFO] Valid: [ 26/49] Final Prec@1 80.1880%
05/31 01:27:33午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:27:34午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.1880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2594, 0.3062, 0.2435, 0.1910],
        [0.2445, 0.3175, 0.2406, 0.1974]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2592, 0.3011, 0.2610, 0.1787],
        [0.2506, 0.3108, 0.2647, 0.1738],
        [0.2682, 0.3297, 0.2119, 0.1902]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.2962, 0.2499, 0.1851],
        [0.2758, 0.3063, 0.2114, 0.2065],
        [0.2660, 0.3143, 0.2215, 0.1982]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.3017, 0.2293, 0.2032],
        [0.2667, 0.2984, 0.2183, 0.2166],
        [0.2596, 0.2968, 0.2269, 0.2168]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2709, 0.3016, 0.2261, 0.2014],
        [0.2683, 0.2951, 0.2259, 0.2107],
        [0.2698, 0.2896, 0.2217, 0.2189]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2651, 0.2904, 0.2178, 0.2267],
        [0.2658, 0.2908, 0.2273, 0.2161],
        [0.2653, 0.2886, 0.2244, 0.2217]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2473],
        [0.2492, 0.2514, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2534, 0.2496, 0.2519, 0.2451],
        [0.2497, 0.2540, 0.2496, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2513, 0.2497, 0.2490],
        [0.2524, 0.2532, 0.2471, 0.2473],
        [0.2520, 0.2528, 0.2485, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2474, 0.2484],
        [0.2523, 0.2512, 0.2486, 0.2479],
        [0.2517, 0.2521, 0.2487, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2550, 0.2513, 0.2448],
        [0.2503, 0.2524, 0.2474, 0.2499],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2508, 0.2486, 0.2499],
        [0.2516, 0.2522, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2507, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2479, 0.2508, 0.2508],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2490, 0.2488, 0.2489],
        [0.2532, 0.2491, 0.2497, 0.2480],
        [0.2508, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2366, 0.2559, 0.2621, 0.2454],
        [0.2411, 0.2515, 0.2548, 0.2525],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2470, 0.2506, 0.2522],
        [0.2473, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:27:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 0.4042 (0.3636)	Prec@(1,5) (87.4%, 99.7%)	
05/31 01:28:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 0.5965 (0.3761)	Prec@(1,5) (86.8%, 99.5%)	
05/31 01:28:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 0.3484 (0.3810)	Prec@(1,5) (86.5%, 99.5%)	
05/31 01:28:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 0.3425 (0.3790)	Prec@(1,5) (86.5%, 99.6%)	
05/31 01:28:56午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 0.4734 (0.3880)	Prec@(1,5) (86.3%, 99.5%)	
05/31 01:29:12午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 0.3754 (0.3836)	Prec@(1,5) (86.5%, 99.5%)	
05/31 01:29:29午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 0.3400 (0.3823)	Prec@(1,5) (86.5%, 99.5%)	
05/31 01:29:42午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 0.3364 (0.3821)	Prec@(1,5) (86.6%, 99.5%)	
05/31 01:29:42午前 searchStage_trainer.py:221 [INFO] Train: [ 27/49] Final Prec@1 86.5840%
05/31 01:29:44午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 0.6066	Prec@(1,5) (79.2%, 99.0%)
05/31 01:29:47午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 0.5997	Prec@(1,5) (79.3%, 98.9%)
05/31 01:29:49午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 0.5980	Prec@(1,5) (79.1%, 98.9%)
05/31 01:29:51午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 0.5925	Prec@(1,5) (79.5%, 98.8%)
05/31 01:29:53午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 0.5913	Prec@(1,5) (79.6%, 98.8%)
05/31 01:29:56午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 0.5900	Prec@(1,5) (79.9%, 98.8%)
05/31 01:29:58午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 0.5913	Prec@(1,5) (79.9%, 98.9%)
05/31 01:29:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 0.5907	Prec@(1,5) (80.0%, 98.9%)
05/31 01:30:00午前 searchStage_trainer.py:260 [INFO] Valid: [ 27/49] Final Prec@1 79.9800%
05/31 01:30:00午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:30:00午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.1880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2593, 0.3060, 0.2436, 0.1911],
        [0.2445, 0.3173, 0.2407, 0.1976]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2591, 0.3011, 0.2611, 0.1787],
        [0.2505, 0.3108, 0.2647, 0.1739],
        [0.2681, 0.3295, 0.2120, 0.1903]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2689, 0.2961, 0.2502, 0.1848],
        [0.2758, 0.3062, 0.2114, 0.2066],
        [0.2660, 0.3141, 0.2215, 0.1984]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.3015, 0.2294, 0.2032],
        [0.2667, 0.2982, 0.2184, 0.2168],
        [0.2595, 0.2966, 0.2269, 0.2169]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3016, 0.2263, 0.2014],
        [0.2683, 0.2949, 0.2259, 0.2109],
        [0.2698, 0.2894, 0.2218, 0.2191]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2651, 0.2903, 0.2178, 0.2268],
        [0.2658, 0.2906, 0.2274, 0.2162],
        [0.2652, 0.2884, 0.2245, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2473],
        [0.2492, 0.2514, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2534, 0.2496, 0.2519, 0.2451],
        [0.2497, 0.2540, 0.2496, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2513, 0.2497, 0.2490],
        [0.2524, 0.2532, 0.2471, 0.2473],
        [0.2520, 0.2528, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2474, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2479],
        [0.2517, 0.2521, 0.2487, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2550, 0.2513, 0.2448],
        [0.2503, 0.2524, 0.2474, 0.2499],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2484, 0.2479],
        [0.2507, 0.2508, 0.2486, 0.2499],
        [0.2516, 0.2522, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2507, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2479, 0.2508, 0.2508],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2490, 0.2488, 0.2489],
        [0.2532, 0.2491, 0.2497, 0.2480],
        [0.2508, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2365, 0.2559, 0.2621, 0.2454],
        [0.2411, 0.2515, 0.2548, 0.2525],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2470, 0.2506, 0.2522],
        [0.2473, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:30:17午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 0.3701 (0.3256)	Prec@(1,5) (88.2%, 99.7%)	
05/31 01:30:33午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 0.4116 (0.3640)	Prec@(1,5) (87.0%, 99.6%)	
05/31 01:30:49午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 0.3083 (0.3645)	Prec@(1,5) (87.1%, 99.6%)	
05/31 01:31:05午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.4048 (0.3706)	Prec@(1,5) (86.9%, 99.6%)	
05/31 01:31:22午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 0.3370 (0.3659)	Prec@(1,5) (87.2%, 99.6%)	
05/31 01:31:38午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 0.3423 (0.3649)	Prec@(1,5) (87.3%, 99.6%)	
05/31 01:31:54午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][350/390]	Step 11298	lr 0.01075	Loss 0.3035 (0.3669)	Prec@(1,5) (87.2%, 99.6%)	
05/31 01:32:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 0.2834 (0.3688)	Prec@(1,5) (87.1%, 99.6%)	
05/31 01:32:07午前 searchStage_trainer.py:221 [INFO] Train: [ 28/49] Final Prec@1 87.1400%
05/31 01:32:10午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][50/391]	Step 11339	Loss 0.6673	Prec@(1,5) (78.3%, 98.8%)
05/31 01:32:12午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 0.6458	Prec@(1,5) (79.2%, 98.8%)
05/31 01:32:14午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][150/391]	Step 11339	Loss 0.6499	Prec@(1,5) (79.1%, 98.8%)
05/31 01:32:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 0.6392	Prec@(1,5) (79.3%, 98.8%)
05/31 01:32:19午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][250/391]	Step 11339	Loss 0.6426	Prec@(1,5) (79.0%, 98.8%)
05/31 01:32:21午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 0.6402	Prec@(1,5) (79.0%, 98.8%)
05/31 01:32:23午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][350/391]	Step 11339	Loss 0.6378	Prec@(1,5) (79.1%, 98.8%)
05/31 01:32:25午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 0.6409	Prec@(1,5) (79.1%, 98.8%)
05/31 01:32:25午前 searchStage_trainer.py:260 [INFO] Valid: [ 28/49] Final Prec@1 79.0760%
05/31 01:32:25午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:32:25午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.1880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2593, 0.3058, 0.2437, 0.1912],
        [0.2445, 0.3171, 0.2407, 0.1977]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2590, 0.3010, 0.2612, 0.1787],
        [0.2504, 0.3107, 0.2648, 0.1741],
        [0.2681, 0.3293, 0.2121, 0.1905]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2690, 0.2960, 0.2503, 0.1847],
        [0.2758, 0.3060, 0.2114, 0.2068],
        [0.2659, 0.3139, 0.2216, 0.1986]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.3014, 0.2294, 0.2033],
        [0.2666, 0.2980, 0.2184, 0.2169],
        [0.2595, 0.2964, 0.2270, 0.2171]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3014, 0.2263, 0.2014],
        [0.2683, 0.2947, 0.2260, 0.2110],
        [0.2697, 0.2891, 0.2219, 0.2193]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2651, 0.2901, 0.2179, 0.2269],
        [0.2658, 0.2904, 0.2274, 0.2164],
        [0.2652, 0.2882, 0.2245, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2473],
        [0.2492, 0.2514, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2534, 0.2496, 0.2519, 0.2451],
        [0.2497, 0.2540, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2513, 0.2497, 0.2490],
        [0.2524, 0.2532, 0.2472, 0.2473],
        [0.2520, 0.2528, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2474, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2479],
        [0.2517, 0.2520, 0.2487, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2550, 0.2513, 0.2448],
        [0.2503, 0.2524, 0.2474, 0.2499],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2508, 0.2486, 0.2499],
        [0.2516, 0.2521, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2507, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2479, 0.2507, 0.2508],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2491, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2490, 0.2488, 0.2489],
        [0.2532, 0.2491, 0.2497, 0.2480],
        [0.2509, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2365, 0.2559, 0.2622, 0.2454],
        [0.2411, 0.2515, 0.2548, 0.2525],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2470, 0.2506, 0.2522],
        [0.2473, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:32:42午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][50/390]	Step 11389	lr 0.01002	Loss 0.3069 (0.3090)	Prec@(1,5) (89.3%, 99.7%)	
05/31 01:32:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 0.3490 (0.3288)	Prec@(1,5) (88.7%, 99.7%)	
05/31 01:33:14午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][150/390]	Step 11489	lr 0.01002	Loss 0.4469 (0.3359)	Prec@(1,5) (88.4%, 99.7%)	
05/31 01:33:30午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.3873 (0.3439)	Prec@(1,5) (88.1%, 99.7%)	
05/31 01:33:47午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][250/390]	Step 11589	lr 0.01002	Loss 0.2870 (0.3440)	Prec@(1,5) (88.1%, 99.7%)	
05/31 01:34:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 0.2720 (0.3485)	Prec@(1,5) (87.9%, 99.7%)	
05/31 01:34:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][350/390]	Step 11689	lr 0.01002	Loss 0.2533 (0.3488)	Prec@(1,5) (87.8%, 99.7%)	
05/31 01:34:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 0.2925 (0.3477)	Prec@(1,5) (87.8%, 99.7%)	
05/31 01:34:32午前 searchStage_trainer.py:221 [INFO] Train: [ 29/49] Final Prec@1 87.8320%
05/31 01:34:35午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][50/391]	Step 11730	Loss 0.5470	Prec@(1,5) (81.6%, 98.9%)
05/31 01:34:37午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 0.5534	Prec@(1,5) (81.6%, 98.6%)
05/31 01:34:39午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][150/391]	Step 11730	Loss 0.5615	Prec@(1,5) (81.5%, 98.7%)
05/31 01:34:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 0.5612	Prec@(1,5) (81.3%, 98.8%)
05/31 01:34:44午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][250/391]	Step 11730	Loss 0.5661	Prec@(1,5) (81.2%, 98.8%)
05/31 01:34:46午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 0.5667	Prec@(1,5) (81.4%, 98.8%)
05/31 01:34:48午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][350/391]	Step 11730	Loss 0.5620	Prec@(1,5) (81.6%, 98.8%)
05/31 01:34:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 0.5620	Prec@(1,5) (81.5%, 98.8%)
05/31 01:34:50午前 searchStage_trainer.py:260 [INFO] Valid: [ 29/49] Final Prec@1 81.5160%
05/31 01:34:50午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:34:51午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.5160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2593, 0.3056, 0.2439, 0.1913],
        [0.2444, 0.3168, 0.2408, 0.1979]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2590, 0.3009, 0.2612, 0.1788],
        [0.2503, 0.3105, 0.2649, 0.1742],
        [0.2681, 0.3291, 0.2122, 0.1906]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2691, 0.2961, 0.2504, 0.1845],
        [0.2758, 0.3058, 0.2114, 0.2069],
        [0.2659, 0.3137, 0.2217, 0.1987]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.3013, 0.2295, 0.2034],
        [0.2666, 0.2978, 0.2185, 0.2170],
        [0.2594, 0.2962, 0.2271, 0.2173]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2708, 0.3013, 0.2263, 0.2016],
        [0.2682, 0.2946, 0.2261, 0.2111],
        [0.2697, 0.2889, 0.2219, 0.2194]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2650, 0.2899, 0.2180, 0.2271],
        [0.2657, 0.2902, 0.2275, 0.2165],
        [0.2651, 0.2880, 0.2246, 0.2223]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2517, 0.2473],
        [0.2492, 0.2514, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2534, 0.2496, 0.2520, 0.2451],
        [0.2497, 0.2540, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2513, 0.2497, 0.2489],
        [0.2524, 0.2532, 0.2472, 0.2473],
        [0.2520, 0.2528, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2474, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2479],
        [0.2517, 0.2520, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2550, 0.2513, 0.2448],
        [0.2503, 0.2524, 0.2474, 0.2499],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2507, 0.2486, 0.2499],
        [0.2516, 0.2521, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2507, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2478, 0.2507, 0.2508],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2489, 0.2488, 0.2489],
        [0.2532, 0.2491, 0.2497, 0.2480],
        [0.2509, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2364, 0.2560, 0.2622, 0.2454],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2470, 0.2506, 0.2522],
        [0.2473, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:35:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][50/390]	Step 11780	lr 0.00929	Loss 0.2139 (0.3057)	Prec@(1,5) (88.9%, 99.7%)	
05/31 01:35:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 0.2983 (0.3096)	Prec@(1,5) (88.9%, 99.8%)	
05/31 01:35:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][150/390]	Step 11880	lr 0.00929	Loss 0.3268 (0.3089)	Prec@(1,5) (89.0%, 99.8%)	
05/31 01:35:56午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 0.2995 (0.3064)	Prec@(1,5) (89.1%, 99.8%)	
05/31 01:36:12午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][250/390]	Step 11980	lr 0.00929	Loss 0.3567 (0.3062)	Prec@(1,5) (89.1%, 99.8%)	
05/31 01:36:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 0.3491 (0.3148)	Prec@(1,5) (88.9%, 99.8%)	
05/31 01:36:45午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][350/390]	Step 12080	lr 0.00929	Loss 0.3568 (0.3188)	Prec@(1,5) (88.8%, 99.8%)	
05/31 01:36:57午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 0.2748 (0.3222)	Prec@(1,5) (88.7%, 99.8%)	
05/31 01:36:58午前 searchStage_trainer.py:221 [INFO] Train: [ 30/49] Final Prec@1 88.6800%
05/31 01:37:00午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][50/391]	Step 12121	Loss 0.5777	Prec@(1,5) (81.1%, 99.1%)
05/31 01:37:02午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 0.5859	Prec@(1,5) (80.8%, 98.9%)
05/31 01:37:05午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][150/391]	Step 12121	Loss 0.5870	Prec@(1,5) (80.6%, 98.9%)
05/31 01:37:07午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 0.5804	Prec@(1,5) (80.6%, 99.0%)
05/31 01:37:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][250/391]	Step 12121	Loss 0.5788	Prec@(1,5) (80.6%, 99.0%)
05/31 01:37:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 0.5825	Prec@(1,5) (80.5%, 99.0%)
05/31 01:37:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][350/391]	Step 12121	Loss 0.5800	Prec@(1,5) (80.6%, 99.0%)
05/31 01:37:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 0.5750	Prec@(1,5) (80.8%, 99.0%)
05/31 01:37:15午前 searchStage_trainer.py:260 [INFO] Valid: [ 30/49] Final Prec@1 80.8440%
05/31 01:37:15午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:37:16午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.5160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2592, 0.3054, 0.2440, 0.1914],
        [0.2444, 0.3166, 0.2409, 0.1980]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2590, 0.3007, 0.2612, 0.1791],
        [0.2503, 0.3104, 0.2651, 0.1742],
        [0.2681, 0.3289, 0.2123, 0.1907]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2691, 0.2959, 0.2503, 0.1846],
        [0.2758, 0.3057, 0.2115, 0.2070],
        [0.2659, 0.3135, 0.2218, 0.1989]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.3012, 0.2295, 0.2035],
        [0.2666, 0.2977, 0.2185, 0.2172],
        [0.2594, 0.2960, 0.2272, 0.2174]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2707, 0.3012, 0.2264, 0.2017],
        [0.2682, 0.2945, 0.2261, 0.2112],
        [0.2696, 0.2888, 0.2220, 0.2196]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2650, 0.2899, 0.2180, 0.2272],
        [0.2657, 0.2901, 0.2276, 0.2167],
        [0.2651, 0.2878, 0.2247, 0.2225]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2449],
        [0.2534, 0.2496, 0.2520, 0.2451],
        [0.2497, 0.2540, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2513, 0.2498, 0.2490],
        [0.2524, 0.2532, 0.2472, 0.2473],
        [0.2520, 0.2528, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2474, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2479],
        [0.2517, 0.2520, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2550, 0.2514, 0.2448],
        [0.2503, 0.2524, 0.2474, 0.2499],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2460],
        [0.2484, 0.2510, 0.2507, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2478, 0.2507, 0.2508],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2489, 0.2488, 0.2489],
        [0.2532, 0.2491, 0.2497, 0.2480],
        [0.2509, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2364, 0.2560, 0.2622, 0.2454],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2470, 0.2506, 0.2522],
        [0.2473, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:37:33午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][50/390]	Step 12171	lr 0.00858	Loss 0.2648 (0.3052)	Prec@(1,5) (89.3%, 99.8%)	
05/31 01:37:49午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.2431 (0.2929)	Prec@(1,5) (89.9%, 99.9%)	
05/31 01:38:05午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][150/390]	Step 12271	lr 0.00858	Loss 0.2227 (0.2942)	Prec@(1,5) (89.8%, 99.8%)	
05/31 01:38:22午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.3618 (0.2945)	Prec@(1,5) (89.6%, 99.8%)	
05/31 01:38:38午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][250/390]	Step 12371	lr 0.00858	Loss 0.2229 (0.2924)	Prec@(1,5) (89.7%, 99.8%)	
05/31 01:38:54午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 0.3482 (0.2947)	Prec@(1,5) (89.5%, 99.8%)	
05/31 01:39:11午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][350/390]	Step 12471	lr 0.00858	Loss 0.1727 (0.2969)	Prec@(1,5) (89.3%, 99.8%)	
05/31 01:39:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 0.2638 (0.2995)	Prec@(1,5) (89.2%, 99.8%)	
05/31 01:39:24午前 searchStage_trainer.py:221 [INFO] Train: [ 31/49] Final Prec@1 89.2280%
05/31 01:39:26午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][50/391]	Step 12512	Loss 0.7704	Prec@(1,5) (76.2%, 98.4%)
05/31 01:39:29午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 0.7553	Prec@(1,5) (76.9%, 98.4%)
05/31 01:39:31午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][150/391]	Step 12512	Loss 0.7346	Prec@(1,5) (77.0%, 98.5%)
05/31 01:39:33午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 0.7316	Prec@(1,5) (77.2%, 98.4%)
05/31 01:39:35午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][250/391]	Step 12512	Loss 0.7292	Prec@(1,5) (77.3%, 98.5%)
05/31 01:39:38午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 0.7271	Prec@(1,5) (77.4%, 98.5%)
05/31 01:39:40午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][350/391]	Step 12512	Loss 0.7272	Prec@(1,5) (77.4%, 98.5%)
05/31 01:39:42午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 0.7207	Prec@(1,5) (77.6%, 98.5%)
05/31 01:39:42午前 searchStage_trainer.py:260 [INFO] Valid: [ 31/49] Final Prec@1 77.5560%
05/31 01:39:42午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:39:42午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.5160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2592, 0.3053, 0.2440, 0.1915],
        [0.2444, 0.3165, 0.2410, 0.1982]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2589, 0.3006, 0.2613, 0.1792],
        [0.2503, 0.3104, 0.2652, 0.1742],
        [0.2681, 0.3288, 0.2123, 0.1909]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2690, 0.2959, 0.2505, 0.1846],
        [0.2757, 0.3056, 0.2115, 0.2072],
        [0.2658, 0.3133, 0.2218, 0.1990]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2658, 0.3011, 0.2296, 0.2036],
        [0.2665, 0.2975, 0.2186, 0.2173],
        [0.2593, 0.2958, 0.2273, 0.2176]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2707, 0.3010, 0.2265, 0.2018],
        [0.2682, 0.2943, 0.2262, 0.2113],
        [0.2696, 0.2886, 0.2221, 0.2198]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2650, 0.2897, 0.2181, 0.2272],
        [0.2657, 0.2899, 0.2276, 0.2168],
        [0.2650, 0.2876, 0.2248, 0.2226]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2525, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2534, 0.2496, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2513, 0.2497, 0.2490],
        [0.2524, 0.2532, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2474, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2479],
        [0.2517, 0.2520, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2550, 0.2514, 0.2448],
        [0.2503, 0.2524, 0.2474, 0.2499],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2485, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2506, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2478, 0.2507, 0.2508],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2489, 0.2487, 0.2489],
        [0.2532, 0.2491, 0.2497, 0.2480],
        [0.2509, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2364, 0.2560, 0.2623, 0.2454],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2470, 0.2506, 0.2522],
        [0.2473, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:39:59午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][50/390]	Step 12562	lr 0.00789	Loss 0.3243 (0.2621)	Prec@(1,5) (91.5%, 99.7%)	
05/31 01:40:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.2346 (0.2557)	Prec@(1,5) (91.4%, 99.8%)	
05/31 01:40:31午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][150/390]	Step 12662	lr 0.00789	Loss 0.3024 (0.2582)	Prec@(1,5) (91.2%, 99.9%)	
05/31 01:40:47午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.3641 (0.2613)	Prec@(1,5) (91.0%, 99.9%)	
05/31 01:41:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][250/390]	Step 12762	lr 0.00789	Loss 0.2745 (0.2646)	Prec@(1,5) (90.8%, 99.8%)	
05/31 01:41:20午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.2437 (0.2693)	Prec@(1,5) (90.7%, 99.8%)	
05/31 01:41:36午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][350/390]	Step 12862	lr 0.00789	Loss 0.1302 (0.2733)	Prec@(1,5) (90.4%, 99.8%)	
05/31 01:41:49午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 0.2896 (0.2745)	Prec@(1,5) (90.4%, 99.8%)	
05/31 01:41:49午前 searchStage_trainer.py:221 [INFO] Train: [ 32/49] Final Prec@1 90.3520%
05/31 01:41:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][50/391]	Step 12903	Loss 0.5448	Prec@(1,5) (82.9%, 99.0%)
05/31 01:41:54午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 0.5447	Prec@(1,5) (82.6%, 99.1%)
05/31 01:41:56午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][150/391]	Step 12903	Loss 0.5574	Prec@(1,5) (82.6%, 99.0%)
05/31 01:41:58午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 0.5582	Prec@(1,5) (82.4%, 98.9%)
05/31 01:42:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][250/391]	Step 12903	Loss 0.5609	Prec@(1,5) (82.4%, 98.9%)
05/31 01:42:03午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 0.5656	Prec@(1,5) (82.2%, 98.9%)
05/31 01:42:05午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][350/391]	Step 12903	Loss 0.5639	Prec@(1,5) (82.2%, 99.0%)
05/31 01:42:07午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 0.5583	Prec@(1,5) (82.3%, 99.0%)
05/31 01:42:07午前 searchStage_trainer.py:260 [INFO] Valid: [ 32/49] Final Prec@1 82.2920%
05/31 01:42:07午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:42:07午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 82.2920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2592, 0.3051, 0.2441, 0.1916],
        [0.2443, 0.3164, 0.2411, 0.1982]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2589, 0.3004, 0.2613, 0.1794],
        [0.2503, 0.3103, 0.2652, 0.1741],
        [0.2680, 0.3286, 0.2124, 0.1910]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2689, 0.2959, 0.2507, 0.1845],
        [0.2757, 0.3055, 0.2116, 0.2073],
        [0.2658, 0.3132, 0.2219, 0.1991]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2657, 0.3010, 0.2296, 0.2037],
        [0.2665, 0.2974, 0.2186, 0.2175],
        [0.2593, 0.2956, 0.2273, 0.2177]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2707, 0.3009, 0.2265, 0.2019],
        [0.2681, 0.2942, 0.2263, 0.2114],
        [0.2696, 0.2884, 0.2221, 0.2199]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2650, 0.2896, 0.2181, 0.2273],
        [0.2656, 0.2898, 0.2277, 0.2169],
        [0.2650, 0.2874, 0.2248, 0.2228]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2449],
        [0.2533, 0.2496, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2490],
        [0.2524, 0.2532, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2473, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2550, 0.2514, 0.2448],
        [0.2502, 0.2524, 0.2474, 0.2499],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2485, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2506, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2478, 0.2507, 0.2509],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2489, 0.2487, 0.2488],
        [0.2532, 0.2491, 0.2497, 0.2480],
        [0.2509, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2363, 0.2560, 0.2623, 0.2454],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2470, 0.2506, 0.2522],
        [0.2473, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:42:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][50/390]	Step 12953	lr 0.00722	Loss 0.3840 (0.2356)	Prec@(1,5) (91.7%, 99.8%)	
05/31 01:42:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.2724 (0.2409)	Prec@(1,5) (91.4%, 99.8%)	
05/31 01:42:56午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][150/390]	Step 13053	lr 0.00722	Loss 0.4004 (0.2492)	Prec@(1,5) (91.2%, 99.9%)	
05/31 01:43:13午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.2866 (0.2563)	Prec@(1,5) (90.9%, 99.9%)	
05/31 01:43:29午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][250/390]	Step 13153	lr 0.00722	Loss 0.1903 (0.2586)	Prec@(1,5) (90.8%, 99.9%)	
05/31 01:43:45午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.2669 (0.2617)	Prec@(1,5) (90.7%, 99.8%)	
05/31 01:44:01午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][350/390]	Step 13253	lr 0.00722	Loss 0.1621 (0.2614)	Prec@(1,5) (90.7%, 99.9%)	
05/31 01:44:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 0.1845 (0.2636)	Prec@(1,5) (90.6%, 99.8%)	
05/31 01:44:15午前 searchStage_trainer.py:221 [INFO] Train: [ 33/49] Final Prec@1 90.6280%
05/31 01:44:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][50/391]	Step 13294	Loss 0.5543	Prec@(1,5) (82.4%, 99.2%)
05/31 01:44:20午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 0.5498	Prec@(1,5) (82.4%, 99.3%)
05/31 01:44:22午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][150/391]	Step 13294	Loss 0.5597	Prec@(1,5) (82.3%, 99.2%)
05/31 01:44:24午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 0.5601	Prec@(1,5) (82.2%, 99.1%)
05/31 01:44:26午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][250/391]	Step 13294	Loss 0.5601	Prec@(1,5) (82.2%, 99.2%)
05/31 01:44:28午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 0.5564	Prec@(1,5) (82.3%, 99.1%)
05/31 01:44:31午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][350/391]	Step 13294	Loss 0.5525	Prec@(1,5) (82.4%, 99.1%)
05/31 01:44:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 0.5602	Prec@(1,5) (82.3%, 99.1%)
05/31 01:44:32午前 searchStage_trainer.py:260 [INFO] Valid: [ 33/49] Final Prec@1 82.3040%
05/31 01:44:32午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:44:33午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 82.3040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2591, 0.3050, 0.2442, 0.1917],
        [0.2442, 0.3162, 0.2412, 0.1983]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2589, 0.3004, 0.2613, 0.1794],
        [0.2503, 0.3102, 0.2653, 0.1742],
        [0.2680, 0.3284, 0.2125, 0.1911]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.2960, 0.2509, 0.1843],
        [0.2757, 0.3053, 0.2116, 0.2074],
        [0.2658, 0.3130, 0.2219, 0.1993]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2657, 0.3009, 0.2296, 0.2038],
        [0.2665, 0.2972, 0.2187, 0.2176],
        [0.2592, 0.2955, 0.2274, 0.2179]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2707, 0.3008, 0.2265, 0.2021],
        [0.2681, 0.2941, 0.2263, 0.2115],
        [0.2695, 0.2883, 0.2222, 0.2200]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2650, 0.2895, 0.2181, 0.2275],
        [0.2656, 0.2896, 0.2278, 0.2170],
        [0.2649, 0.2873, 0.2249, 0.2229]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2496, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2497, 0.2490],
        [0.2524, 0.2532, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2521, 0.2473, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2550, 0.2514, 0.2448],
        [0.2502, 0.2524, 0.2474, 0.2500],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2485, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2531, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2506, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2478, 0.2507, 0.2509],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2491, 0.2497, 0.2480],
        [0.2509, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2363, 0.2560, 0.2623, 0.2454],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2506, 0.2522],
        [0.2473, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:44:50午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][50/390]	Step 13344	lr 0.00657	Loss 0.1841 (0.2381)	Prec@(1,5) (91.2%, 99.8%)	
05/31 01:45:06午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.4585 (0.2343)	Prec@(1,5) (91.7%, 99.8%)	
05/31 01:45:22午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][150/390]	Step 13444	lr 0.00657	Loss 0.2228 (0.2367)	Prec@(1,5) (91.5%, 99.9%)	
05/31 01:45:38午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.3859 (0.2444)	Prec@(1,5) (91.3%, 99.8%)	
05/31 01:45:55午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][250/390]	Step 13544	lr 0.00657	Loss 0.2243 (0.2475)	Prec@(1,5) (91.2%, 99.8%)	
05/31 01:46:11午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.2099 (0.2466)	Prec@(1,5) (91.2%, 99.8%)	
05/31 01:46:27午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][350/390]	Step 13644	lr 0.00657	Loss 0.2020 (0.2464)	Prec@(1,5) (91.3%, 99.8%)	
05/31 01:46:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 0.2121 (0.2479)	Prec@(1,5) (91.2%, 99.8%)	
05/31 01:46:40午前 searchStage_trainer.py:221 [INFO] Train: [ 34/49] Final Prec@1 91.2160%
05/31 01:46:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][50/391]	Step 13685	Loss 0.5842	Prec@(1,5) (81.6%, 98.7%)
05/31 01:46:45午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 0.5787	Prec@(1,5) (82.0%, 98.8%)
05/31 01:46:47午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][150/391]	Step 13685	Loss 0.5678	Prec@(1,5) (82.3%, 98.9%)
05/31 01:46:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 0.5625	Prec@(1,5) (82.3%, 98.9%)
05/31 01:46:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][250/391]	Step 13685	Loss 0.5605	Prec@(1,5) (82.3%, 98.9%)
05/31 01:46:54午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 0.5652	Prec@(1,5) (82.2%, 98.9%)
05/31 01:46:56午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][350/391]	Step 13685	Loss 0.5652	Prec@(1,5) (82.2%, 98.9%)
05/31 01:46:58午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 0.5659	Prec@(1,5) (82.2%, 98.9%)
05/31 01:46:58午前 searchStage_trainer.py:260 [INFO] Valid: [ 34/49] Final Prec@1 82.1680%
05/31 01:46:58午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:46:58午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 82.3040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2591, 0.3048, 0.2443, 0.1918],
        [0.2442, 0.3161, 0.2413, 0.1984]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2588, 0.3003, 0.2614, 0.1795],
        [0.2503, 0.3101, 0.2654, 0.1742],
        [0.2680, 0.3283, 0.2125, 0.1912]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.2959, 0.2510, 0.1843],
        [0.2757, 0.3052, 0.2116, 0.2075],
        [0.2657, 0.3129, 0.2220, 0.1994]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2657, 0.3008, 0.2297, 0.2038],
        [0.2665, 0.2971, 0.2187, 0.2177],
        [0.2592, 0.2953, 0.2275, 0.2180]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.3007, 0.2265, 0.2021],
        [0.2681, 0.2939, 0.2264, 0.2116],
        [0.2695, 0.2881, 0.2222, 0.2202]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2649, 0.2894, 0.2182, 0.2275],
        [0.2656, 0.2895, 0.2278, 0.2171],
        [0.2649, 0.2871, 0.2249, 0.2231]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2496, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2489],
        [0.2524, 0.2532, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2448],
        [0.2502, 0.2524, 0.2474, 0.2500],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2506, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2478, 0.2507, 0.2509],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2491, 0.2497, 0.2480],
        [0.2509, 0.2478, 0.2489, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2363, 0.2560, 0.2623, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2506, 0.2522],
        [0.2473, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:47:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][50/390]	Step 13735	lr 0.00595	Loss 0.1331 (0.2163)	Prec@(1,5) (92.4%, 99.9%)	
05/31 01:47:31午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.1187 (0.2085)	Prec@(1,5) (92.8%, 99.9%)	
05/31 01:47:48午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][150/390]	Step 13835	lr 0.00595	Loss 0.3751 (0.2075)	Prec@(1,5) (92.8%, 99.9%)	
05/31 01:48:04午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 0.2620 (0.2156)	Prec@(1,5) (92.5%, 99.9%)	
05/31 01:48:20午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][250/390]	Step 13935	lr 0.00595	Loss 0.1530 (0.2159)	Prec@(1,5) (92.5%, 99.9%)	
05/31 01:48:37午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 0.1893 (0.2178)	Prec@(1,5) (92.4%, 99.9%)	
05/31 01:48:53午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][350/390]	Step 14035	lr 0.00595	Loss 0.2052 (0.2218)	Prec@(1,5) (92.3%, 99.9%)	
05/31 01:49:06午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 0.2564 (0.2237)	Prec@(1,5) (92.2%, 99.9%)	
05/31 01:49:06午前 searchStage_trainer.py:221 [INFO] Train: [ 35/49] Final Prec@1 92.2240%
05/31 01:49:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][50/391]	Step 14076	Loss 0.5534	Prec@(1,5) (82.7%, 98.8%)
05/31 01:49:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 0.5514	Prec@(1,5) (82.7%, 99.1%)
05/31 01:49:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][150/391]	Step 14076	Loss 0.5558	Prec@(1,5) (82.7%, 99.0%)
05/31 01:49:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 0.5454	Prec@(1,5) (83.2%, 99.1%)
05/31 01:49:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][250/391]	Step 14076	Loss 0.5465	Prec@(1,5) (83.1%, 99.1%)
05/31 01:49:20午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 0.5432	Prec@(1,5) (83.2%, 99.1%)
05/31 01:49:22午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][350/391]	Step 14076	Loss 0.5479	Prec@(1,5) (83.1%, 99.1%)
05/31 01:49:24午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 0.5481	Prec@(1,5) (83.1%, 99.1%)
05/31 01:49:24午前 searchStage_trainer.py:260 [INFO] Valid: [ 35/49] Final Prec@1 83.1320%
05/31 01:49:24午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:49:24午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.1320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2590, 0.3047, 0.2444, 0.1919],
        [0.2442, 0.3159, 0.2414, 0.1985]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2588, 0.3001, 0.2615, 0.1796],
        [0.2503, 0.3100, 0.2654, 0.1742],
        [0.2680, 0.3281, 0.2126, 0.1913]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.2959, 0.2510, 0.1843],
        [0.2757, 0.3051, 0.2117, 0.2076],
        [0.2657, 0.3127, 0.2221, 0.1995]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2657, 0.3008, 0.2297, 0.2039],
        [0.2664, 0.2970, 0.2187, 0.2178],
        [0.2592, 0.2952, 0.2275, 0.2181]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.3006, 0.2265, 0.2022],
        [0.2680, 0.2938, 0.2264, 0.2117],
        [0.2695, 0.2880, 0.2223, 0.2203]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2648, 0.2893, 0.2182, 0.2276],
        [0.2656, 0.2894, 0.2279, 0.2172],
        [0.2649, 0.2870, 0.2250, 0.2232]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2513, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2496, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2489],
        [0.2524, 0.2532, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2524, 0.2474, 0.2500],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2506, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2478, 0.2507, 0.2509],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2491, 0.2497, 0.2480],
        [0.2509, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2363, 0.2560, 0.2624, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2522],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:49:41午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][50/390]	Step 14126	lr 0.00535	Loss 0.2108 (0.1990)	Prec@(1,5) (92.7%, 99.9%)	
05/31 01:49:57午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.1329 (0.1895)	Prec@(1,5) (93.3%, 100.0%)	
05/31 01:50:14午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][150/390]	Step 14226	lr 0.00535	Loss 0.1895 (0.1874)	Prec@(1,5) (93.4%, 100.0%)	
05/31 01:50:30午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.2533 (0.1921)	Prec@(1,5) (93.2%, 99.9%)	
05/31 01:50:46午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][250/390]	Step 14326	lr 0.00535	Loss 0.2200 (0.1937)	Prec@(1,5) (93.1%, 99.9%)	
05/31 01:51:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.2834 (0.1974)	Prec@(1,5) (93.0%, 99.9%)	
05/31 01:51:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][350/390]	Step 14426	lr 0.00535	Loss 0.3127 (0.1998)	Prec@(1,5) (92.9%, 99.9%)	
05/31 01:51:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.0638 (0.2000)	Prec@(1,5) (92.9%, 99.9%)	
05/31 01:51:32午前 searchStage_trainer.py:221 [INFO] Train: [ 36/49] Final Prec@1 92.8800%
05/31 01:51:35午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][50/391]	Step 14467	Loss 0.5461	Prec@(1,5) (83.8%, 98.7%)
05/31 01:51:37午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 0.5656	Prec@(1,5) (83.2%, 98.9%)
05/31 01:51:39午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][150/391]	Step 14467	Loss 0.5752	Prec@(1,5) (82.9%, 98.8%)
05/31 01:51:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 0.5809	Prec@(1,5) (82.9%, 98.8%)
05/31 01:51:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][250/391]	Step 14467	Loss 0.5768	Prec@(1,5) (83.0%, 98.9%)
05/31 01:51:46午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 0.5741	Prec@(1,5) (83.2%, 98.9%)
05/31 01:51:48午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][350/391]	Step 14467	Loss 0.5722	Prec@(1,5) (83.2%, 98.9%)
05/31 01:51:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 0.5693	Prec@(1,5) (83.3%, 98.9%)
05/31 01:51:50午前 searchStage_trainer.py:260 [INFO] Valid: [ 36/49] Final Prec@1 83.2760%
05/31 01:51:50午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:51:50午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.2760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2590, 0.3046, 0.2445, 0.1920],
        [0.2441, 0.3158, 0.2414, 0.1986]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2588, 0.3000, 0.2615, 0.1796],
        [0.2502, 0.3100, 0.2656, 0.1743],
        [0.2679, 0.3280, 0.2126, 0.1914]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2689, 0.2958, 0.2511, 0.1842],
        [0.2757, 0.3050, 0.2117, 0.2076],
        [0.2657, 0.3126, 0.2221, 0.1996]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2657, 0.3007, 0.2297, 0.2039],
        [0.2664, 0.2969, 0.2188, 0.2179],
        [0.2591, 0.2951, 0.2276, 0.2182]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.3005, 0.2266, 0.2023],
        [0.2680, 0.2937, 0.2265, 0.2118],
        [0.2694, 0.2878, 0.2224, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2648, 0.2892, 0.2183, 0.2277],
        [0.2655, 0.2892, 0.2279, 0.2173],
        [0.2648, 0.2868, 0.2251, 0.2233]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2513, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2489],
        [0.2524, 0.2532, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2479],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2506, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2478, 0.2507, 0.2509],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2491, 0.2497, 0.2480],
        [0.2509, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2362, 0.2560, 0.2624, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2522],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:52:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][50/390]	Step 14517	lr 0.00479	Loss 0.0912 (0.1791)	Prec@(1,5) (93.7%, 99.9%)	
05/31 01:52:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.2899 (0.1826)	Prec@(1,5) (93.6%, 99.9%)	
05/31 01:52:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][150/390]	Step 14617	lr 0.00479	Loss 0.0931 (0.1830)	Prec@(1,5) (93.6%, 99.9%)	
05/31 01:52:56午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.1326 (0.1812)	Prec@(1,5) (93.6%, 99.9%)	
05/31 01:53:12午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][250/390]	Step 14717	lr 0.00479	Loss 0.1222 (0.1805)	Prec@(1,5) (93.6%, 99.9%)	
05/31 01:53:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.0984 (0.1836)	Prec@(1,5) (93.6%, 99.9%)	
05/31 01:53:45午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][350/390]	Step 14817	lr 0.00479	Loss 0.2800 (0.1840)	Prec@(1,5) (93.5%, 99.9%)	
05/31 01:53:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.1660 (0.1850)	Prec@(1,5) (93.5%, 99.9%)	
05/31 01:53:58午前 searchStage_trainer.py:221 [INFO] Train: [ 37/49] Final Prec@1 93.5280%
05/31 01:54:00午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][50/391]	Step 14858	Loss 0.5785	Prec@(1,5) (83.7%, 99.1%)
05/31 01:54:03午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 0.5530	Prec@(1,5) (84.0%, 99.2%)
05/31 01:54:05午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][150/391]	Step 14858	Loss 0.5506	Prec@(1,5) (83.7%, 99.1%)
05/31 01:54:07午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 0.5522	Prec@(1,5) (83.9%, 99.1%)
05/31 01:54:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][250/391]	Step 14858	Loss 0.5570	Prec@(1,5) (83.7%, 99.1%)
05/31 01:54:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 0.5634	Prec@(1,5) (83.5%, 99.1%)
05/31 01:54:14午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][350/391]	Step 14858	Loss 0.5573	Prec@(1,5) (83.6%, 99.0%)
05/31 01:54:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 0.5561	Prec@(1,5) (83.6%, 99.0%)
05/31 01:54:15午前 searchStage_trainer.py:260 [INFO] Valid: [ 37/49] Final Prec@1 83.5800%
05/31 01:54:15午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:54:16午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.5800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2590, 0.3045, 0.2445, 0.1921],
        [0.2441, 0.3157, 0.2415, 0.1987]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2588, 0.3000, 0.2615, 0.1797],
        [0.2501, 0.3099, 0.2657, 0.1743],
        [0.2679, 0.3279, 0.2127, 0.1915]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2689, 0.2958, 0.2511, 0.1843],
        [0.2757, 0.3049, 0.2117, 0.2077],
        [0.2657, 0.3124, 0.2222, 0.1997]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2657, 0.3006, 0.2298, 0.2040],
        [0.2664, 0.2968, 0.2188, 0.2180],
        [0.2591, 0.2949, 0.2276, 0.2183]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.3005, 0.2266, 0.2024],
        [0.2680, 0.2936, 0.2265, 0.2119],
        [0.2694, 0.2877, 0.2224, 0.2205]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2648, 0.2891, 0.2184, 0.2278],
        [0.2655, 0.2891, 0.2279, 0.2174],
        [0.2648, 0.2867, 0.2251, 0.2234]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2513, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2489],
        [0.2524, 0.2531, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2518, 0.2486, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2506, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2477, 0.2507, 0.2509],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2491, 0.2497, 0.2480],
        [0.2509, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2362, 0.2561, 0.2624, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2522],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:54:33午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][50/390]	Step 14908	lr 0.00425	Loss 0.1130 (0.1742)	Prec@(1,5) (93.6%, 100.0%)	
05/31 01:54:49午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.0707 (0.1586)	Prec@(1,5) (94.3%, 100.0%)	
05/31 01:55:06午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][150/390]	Step 15008	lr 0.00425	Loss 0.2495 (0.1566)	Prec@(1,5) (94.5%, 99.9%)	
05/31 01:55:22午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.2572 (0.1607)	Prec@(1,5) (94.4%, 99.9%)	
05/31 01:55:38午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][250/390]	Step 15108	lr 0.00425	Loss 0.1381 (0.1629)	Prec@(1,5) (94.3%, 99.9%)	
05/31 01:55:54午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.1474 (0.1656)	Prec@(1,5) (94.2%, 99.9%)	
05/31 01:56:10午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][350/390]	Step 15208	lr 0.00425	Loss 0.2212 (0.1696)	Prec@(1,5) (94.0%, 99.9%)	
05/31 01:56:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.2797 (0.1701)	Prec@(1,5) (94.0%, 99.9%)	
05/31 01:56:24午前 searchStage_trainer.py:221 [INFO] Train: [ 38/49] Final Prec@1 93.9680%
05/31 01:56:26午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][50/391]	Step 15249	Loss 0.5552	Prec@(1,5) (83.3%, 99.3%)
05/31 01:56:28午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 0.5483	Prec@(1,5) (83.5%, 99.3%)
05/31 01:56:31午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][150/391]	Step 15249	Loss 0.5388	Prec@(1,5) (83.6%, 99.3%)
05/31 01:56:33午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 0.5406	Prec@(1,5) (83.6%, 99.2%)
05/31 01:56:35午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][250/391]	Step 15249	Loss 0.5425	Prec@(1,5) (83.6%, 99.1%)
05/31 01:56:37午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 0.5448	Prec@(1,5) (83.6%, 99.1%)
05/31 01:56:39午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][350/391]	Step 15249	Loss 0.5389	Prec@(1,5) (83.9%, 99.2%)
05/31 01:56:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 0.5378	Prec@(1,5) (83.9%, 99.2%)
05/31 01:56:41午前 searchStage_trainer.py:260 [INFO] Valid: [ 38/49] Final Prec@1 83.8320%
05/31 01:56:41午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:56:42午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.8320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2589, 0.3044, 0.2446, 0.1921],
        [0.2441, 0.3156, 0.2416, 0.1988]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2588, 0.3000, 0.2615, 0.1797],
        [0.2501, 0.3098, 0.2657, 0.1743],
        [0.2679, 0.3277, 0.2127, 0.1916]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.2958, 0.2512, 0.1843],
        [0.2756, 0.3049, 0.2118, 0.2078],
        [0.2657, 0.3123, 0.2222, 0.1998]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2656, 0.3005, 0.2298, 0.2041],
        [0.2664, 0.2967, 0.2189, 0.2181],
        [0.2591, 0.2948, 0.2277, 0.2184]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.3003, 0.2266, 0.2025],
        [0.2680, 0.2935, 0.2266, 0.2120],
        [0.2694, 0.2876, 0.2225, 0.2206]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2648, 0.2890, 0.2184, 0.2279],
        [0.2655, 0.2890, 0.2280, 0.2175],
        [0.2647, 0.2866, 0.2252, 0.2235]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2489],
        [0.2524, 0.2531, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2550, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2517, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2514, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2460],
        [0.2484, 0.2511, 0.2506, 0.2499],
        [0.2484, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2477, 0.2507, 0.2509],
        [0.2496, 0.2486, 0.2517, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2491, 0.2497, 0.2480],
        [0.2509, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2362, 0.2561, 0.2624, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2522],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2450, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:56:59午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][50/390]	Step 15299	lr 0.00375	Loss 0.1415 (0.1371)	Prec@(1,5) (95.5%, 99.9%)	
05/31 01:57:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.0817 (0.1337)	Prec@(1,5) (95.2%, 100.0%)	
05/31 01:57:31午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][150/390]	Step 15399	lr 0.00375	Loss 0.1363 (0.1395)	Prec@(1,5) (95.0%, 100.0%)	
05/31 01:57:47午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.0740 (0.1447)	Prec@(1,5) (94.8%, 99.9%)	
05/31 01:58:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][250/390]	Step 15499	lr 0.00375	Loss 0.0768 (0.1463)	Prec@(1,5) (94.8%, 100.0%)	
05/31 01:58:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.0662 (0.1486)	Prec@(1,5) (94.7%, 99.9%)	
05/31 01:58:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][350/390]	Step 15599	lr 0.00375	Loss 0.0682 (0.1487)	Prec@(1,5) (94.7%, 99.9%)	
05/31 01:58:48午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.0893 (0.1494)	Prec@(1,5) (94.7%, 99.9%)	
05/31 01:58:49午前 searchStage_trainer.py:221 [INFO] Train: [ 39/49] Final Prec@1 94.6920%
05/31 01:58:51午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][50/391]	Step 15640	Loss 0.6320	Prec@(1,5) (81.5%, 98.8%)
05/31 01:58:53午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 0.6179	Prec@(1,5) (82.2%, 98.7%)
05/31 01:58:56午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][150/391]	Step 15640	Loss 0.5993	Prec@(1,5) (82.5%, 99.0%)
05/31 01:58:58午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 0.5935	Prec@(1,5) (82.7%, 99.0%)
05/31 01:59:00午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][250/391]	Step 15640	Loss 0.5865	Prec@(1,5) (82.9%, 99.0%)
05/31 01:59:02午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 0.5786	Prec@(1,5) (83.1%, 99.1%)
05/31 01:59:05午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][350/391]	Step 15640	Loss 0.5752	Prec@(1,5) (83.2%, 99.1%)
05/31 01:59:06午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 0.5716	Prec@(1,5) (83.3%, 99.1%)
05/31 01:59:06午前 searchStage_trainer.py:260 [INFO] Valid: [ 39/49] Final Prec@1 83.2880%
05/31 01:59:06午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 01:59:07午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.8320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2589, 0.3042, 0.2447, 0.1922],
        [0.2440, 0.3155, 0.2416, 0.1989]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2588, 0.2999, 0.2616, 0.1797],
        [0.2502, 0.3097, 0.2657, 0.1744],
        [0.2679, 0.3276, 0.2128, 0.1917]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2687, 0.2958, 0.2512, 0.1843],
        [0.2756, 0.3048, 0.2118, 0.2078],
        [0.2656, 0.3122, 0.2223, 0.1999]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2656, 0.3005, 0.2298, 0.2041],
        [0.2663, 0.2966, 0.2189, 0.2182],
        [0.2590, 0.2947, 0.2277, 0.2185]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2706, 0.3002, 0.2266, 0.2027],
        [0.2679, 0.2934, 0.2266, 0.2121],
        [0.2693, 0.2875, 0.2225, 0.2207]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2647, 0.2889, 0.2184, 0.2279],
        [0.2655, 0.2889, 0.2280, 0.2176],
        [0.2647, 0.2864, 0.2252, 0.2237]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2489],
        [0.2524, 0.2531, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2550, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2517, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2513, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2510, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2459],
        [0.2484, 0.2511, 0.2506, 0.2499],
        [0.2485, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2477, 0.2507, 0.2509],
        [0.2496, 0.2486, 0.2516, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2490, 0.2496, 0.2480],
        [0.2509, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2362, 0.2561, 0.2624, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2522],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2451, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 01:59:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][50/390]	Step 15690	lr 0.00329	Loss 0.1435 (0.1304)	Prec@(1,5) (95.9%, 100.0%)	
05/31 01:59:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.0855 (0.1242)	Prec@(1,5) (96.0%, 100.0%)	
05/31 01:59:57午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][150/390]	Step 15790	lr 0.00329	Loss 0.3403 (0.1195)	Prec@(1,5) (96.0%, 100.0%)	
05/31 02:00:13午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.1281 (0.1191)	Prec@(1,5) (96.0%, 100.0%)	
05/31 02:00:29午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][250/390]	Step 15890	lr 0.00329	Loss 0.2014 (0.1245)	Prec@(1,5) (95.7%, 100.0%)	
05/31 02:00:46午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.1903 (0.1228)	Prec@(1,5) (95.8%, 100.0%)	
05/31 02:01:02午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][350/390]	Step 15990	lr 0.00329	Loss 0.1346 (0.1242)	Prec@(1,5) (95.7%, 100.0%)	
05/31 02:01:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.0366 (0.1263)	Prec@(1,5) (95.6%, 100.0%)	
05/31 02:01:15午前 searchStage_trainer.py:221 [INFO] Train: [ 40/49] Final Prec@1 95.6320%
05/31 02:01:18午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][50/391]	Step 16031	Loss 0.5710	Prec@(1,5) (83.9%, 98.8%)
05/31 02:01:20午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 0.5746	Prec@(1,5) (84.1%, 98.9%)
05/31 02:01:22午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][150/391]	Step 16031	Loss 0.5778	Prec@(1,5) (84.0%, 98.9%)
05/31 02:01:24午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 0.5880	Prec@(1,5) (83.9%, 98.9%)
05/31 02:01:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][250/391]	Step 16031	Loss 0.5855	Prec@(1,5) (83.9%, 98.9%)
05/31 02:01:29午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 0.5785	Prec@(1,5) (84.2%, 98.9%)
05/31 02:01:31午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][350/391]	Step 16031	Loss 0.5745	Prec@(1,5) (84.2%, 99.0%)
05/31 02:01:33午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 0.5700	Prec@(1,5) (84.2%, 99.0%)
05/31 02:01:33午前 searchStage_trainer.py:260 [INFO] Valid: [ 40/49] Final Prec@1 84.2240%
05/31 02:01:33午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 02:01:33午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.2240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2589, 0.3041, 0.2447, 0.1923],
        [0.2440, 0.3154, 0.2417, 0.1990]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2588, 0.2998, 0.2616, 0.1798],
        [0.2502, 0.3096, 0.2658, 0.1744],
        [0.2679, 0.3275, 0.2128, 0.1918]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.2957, 0.2513, 0.1842],
        [0.2756, 0.3047, 0.2118, 0.2079],
        [0.2656, 0.3121, 0.2223, 0.2000]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2656, 0.3004, 0.2299, 0.2041],
        [0.2663, 0.2965, 0.2189, 0.2183],
        [0.2590, 0.2946, 0.2278, 0.2186]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3002, 0.2266, 0.2027],
        [0.2679, 0.2932, 0.2266, 0.2122],
        [0.2693, 0.2874, 0.2225, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2648, 0.2888, 0.2184, 0.2280],
        [0.2654, 0.2888, 0.2281, 0.2177],
        [0.2647, 0.2863, 0.2253, 0.2238]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2490],
        [0.2524, 0.2531, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2517, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2513, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2509, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2459],
        [0.2484, 0.2511, 0.2506, 0.2499],
        [0.2485, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2477, 0.2507, 0.2509],
        [0.2496, 0.2486, 0.2516, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2490, 0.2496, 0.2480],
        [0.2509, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2362, 0.2561, 0.2625, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2522],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2451, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 02:01:50午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][50/390]	Step 16081	lr 0.00287	Loss 0.1405 (0.1061)	Prec@(1,5) (96.2%, 100.0%)	
05/31 02:02:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.2608 (0.1027)	Prec@(1,5) (96.5%, 100.0%)	
05/31 02:02:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][150/390]	Step 16181	lr 0.00287	Loss 0.1319 (0.1030)	Prec@(1,5) (96.5%, 100.0%)	
05/31 02:02:39午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.1364 (0.1073)	Prec@(1,5) (96.5%, 99.9%)	
05/31 02:02:55午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][250/390]	Step 16281	lr 0.00287	Loss 0.0658 (0.1083)	Prec@(1,5) (96.5%, 99.9%)	
05/31 02:03:12午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.0457 (0.1089)	Prec@(1,5) (96.4%, 100.0%)	
05/31 02:03:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][350/390]	Step 16381	lr 0.00287	Loss 0.1567 (0.1094)	Prec@(1,5) (96.4%, 100.0%)	
05/31 02:03:41午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.1011 (0.1116)	Prec@(1,5) (96.3%, 100.0%)	
05/31 02:03:41午前 searchStage_trainer.py:221 [INFO] Train: [ 41/49] Final Prec@1 96.2880%
05/31 02:03:44午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][50/391]	Step 16422	Loss 0.5633	Prec@(1,5) (85.0%, 98.9%)
05/31 02:03:46午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 0.5539	Prec@(1,5) (84.9%, 98.9%)
05/31 02:03:48午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][150/391]	Step 16422	Loss 0.5504	Prec@(1,5) (85.1%, 99.0%)
05/31 02:03:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 0.5679	Prec@(1,5) (84.7%, 99.1%)
05/31 02:03:53午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][250/391]	Step 16422	Loss 0.5697	Prec@(1,5) (84.6%, 99.1%)
05/31 02:03:55午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 0.5628	Prec@(1,5) (84.7%, 99.1%)
05/31 02:03:57午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][350/391]	Step 16422	Loss 0.5628	Prec@(1,5) (84.6%, 99.1%)
05/31 02:03:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 0.5655	Prec@(1,5) (84.6%, 99.1%)
05/31 02:03:59午前 searchStage_trainer.py:260 [INFO] Valid: [ 41/49] Final Prec@1 84.6280%
05/31 02:03:59午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 02:03:59午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.6280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2588, 0.3040, 0.2448, 0.1923],
        [0.2440, 0.3153, 0.2417, 0.1990]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2587, 0.2998, 0.2617, 0.1798],
        [0.2501, 0.3096, 0.2659, 0.1744],
        [0.2678, 0.3274, 0.2129, 0.1919]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.2957, 0.2514, 0.1841],
        [0.2756, 0.3046, 0.2118, 0.2080],
        [0.2656, 0.3120, 0.2223, 0.2001]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2656, 0.3004, 0.2299, 0.2041],
        [0.2663, 0.2964, 0.2189, 0.2184],
        [0.2590, 0.2945, 0.2278, 0.2187]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3001, 0.2267, 0.2027],
        [0.2679, 0.2931, 0.2267, 0.2123],
        [0.2693, 0.2873, 0.2226, 0.2209]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2647, 0.2887, 0.2184, 0.2281],
        [0.2654, 0.2887, 0.2281, 0.2178],
        [0.2646, 0.2862, 0.2253, 0.2239]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2490],
        [0.2523, 0.2531, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2512, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2517, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2513, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2509, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2459],
        [0.2485, 0.2511, 0.2506, 0.2499],
        [0.2485, 0.2497, 0.2496, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2477, 0.2507, 0.2509],
        [0.2497, 0.2486, 0.2516, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2490, 0.2496, 0.2480],
        [0.2509, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2361, 0.2561, 0.2625, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2522],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2451, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 02:04:16午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][50/390]	Step 16472	lr 0.00248	Loss 0.1314 (0.1039)	Prec@(1,5) (96.5%, 100.0%)	
05/31 02:04:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.0758 (0.0988)	Prec@(1,5) (96.7%, 100.0%)	
05/31 02:04:49午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][150/390]	Step 16572	lr 0.00248	Loss 0.0910 (0.1039)	Prec@(1,5) (96.5%, 100.0%)	
05/31 02:05:05午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.0969 (0.1047)	Prec@(1,5) (96.4%, 100.0%)	
05/31 02:05:21午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][250/390]	Step 16672	lr 0.00248	Loss 0.1378 (0.1054)	Prec@(1,5) (96.3%, 100.0%)	
05/31 02:05:38午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.0989 (0.1045)	Prec@(1,5) (96.4%, 100.0%)	
05/31 02:05:54午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][350/390]	Step 16772	lr 0.00248	Loss 0.1285 (0.1054)	Prec@(1,5) (96.4%, 100.0%)	
05/31 02:06:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.2226 (0.1056)	Prec@(1,5) (96.3%, 100.0%)	
05/31 02:06:07午前 searchStage_trainer.py:221 [INFO] Train: [ 42/49] Final Prec@1 96.3440%
05/31 02:06:10午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][50/391]	Step 16813	Loss 0.5971	Prec@(1,5) (83.6%, 99.1%)
05/31 02:06:12午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 0.5839	Prec@(1,5) (84.1%, 99.2%)
05/31 02:06:14午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][150/391]	Step 16813	Loss 0.5739	Prec@(1,5) (84.6%, 99.2%)
05/31 02:06:16午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 0.5769	Prec@(1,5) (84.4%, 99.2%)
05/31 02:06:18午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][250/391]	Step 16813	Loss 0.5894	Prec@(1,5) (84.3%, 99.1%)
05/31 02:06:21午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 0.5854	Prec@(1,5) (84.3%, 99.2%)
05/31 02:06:23午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][350/391]	Step 16813	Loss 0.5820	Prec@(1,5) (84.4%, 99.2%)
05/31 02:06:25午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 0.5797	Prec@(1,5) (84.6%, 99.1%)
05/31 02:06:25午前 searchStage_trainer.py:260 [INFO] Valid: [ 42/49] Final Prec@1 84.5520%
05/31 02:06:25午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 02:06:25午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.6280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2588, 0.3039, 0.2449, 0.1924],
        [0.2439, 0.3152, 0.2418, 0.1991]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2587, 0.2997, 0.2617, 0.1798],
        [0.2501, 0.3095, 0.2659, 0.1744],
        [0.2678, 0.3273, 0.2129, 0.1920]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.2957, 0.2514, 0.1841],
        [0.2756, 0.3045, 0.2119, 0.2081],
        [0.2656, 0.3118, 0.2224, 0.2002]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2656, 0.3003, 0.2299, 0.2042],
        [0.2663, 0.2963, 0.2190, 0.2185],
        [0.2589, 0.2944, 0.2279, 0.2188]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3001, 0.2267, 0.2027],
        [0.2679, 0.2931, 0.2267, 0.2123],
        [0.2693, 0.2872, 0.2226, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2647, 0.2886, 0.2185, 0.2282],
        [0.2654, 0.2886, 0.2281, 0.2178],
        [0.2646, 0.2861, 0.2253, 0.2240]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2489],
        [0.2523, 0.2531, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2467]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2511, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2517, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2513, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2509, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2459],
        [0.2485, 0.2511, 0.2506, 0.2499],
        [0.2485, 0.2497, 0.2495, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2477, 0.2507, 0.2509],
        [0.2497, 0.2486, 0.2516, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2490, 0.2496, 0.2480],
        [0.2509, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2361, 0.2561, 0.2625, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2522],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2451, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 02:06:42午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][50/390]	Step 16863	lr 0.00214	Loss 0.1687 (0.0886)	Prec@(1,5) (97.1%, 100.0%)	
05/31 02:06:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.0699 (0.0911)	Prec@(1,5) (96.8%, 100.0%)	
05/31 02:07:14午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][150/390]	Step 16963	lr 0.00214	Loss 0.0533 (0.0879)	Prec@(1,5) (96.9%, 100.0%)	
05/31 02:07:30午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.0650 (0.0881)	Prec@(1,5) (96.9%, 100.0%)	
05/31 02:07:47午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][250/390]	Step 17063	lr 0.00214	Loss 0.2072 (0.0874)	Prec@(1,5) (96.9%, 100.0%)	
05/31 02:08:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.0911 (0.0885)	Prec@(1,5) (96.9%, 100.0%)	
05/31 02:08:20午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][350/390]	Step 17163	lr 0.00214	Loss 0.0272 (0.0898)	Prec@(1,5) (96.8%, 100.0%)	
05/31 02:08:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.0221 (0.0896)	Prec@(1,5) (96.8%, 100.0%)	
05/31 02:08:33午前 searchStage_trainer.py:221 [INFO] Train: [ 43/49] Final Prec@1 96.8040%
05/31 02:08:35午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][50/391]	Step 17204	Loss 0.5962	Prec@(1,5) (84.8%, 98.9%)
05/31 02:08:38午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 0.5872	Prec@(1,5) (84.6%, 99.0%)
05/31 02:08:40午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][150/391]	Step 17204	Loss 0.5949	Prec@(1,5) (84.5%, 99.0%)
05/31 02:08:42午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 0.6027	Prec@(1,5) (84.2%, 99.1%)
05/31 02:08:44午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][250/391]	Step 17204	Loss 0.5954	Prec@(1,5) (84.3%, 99.1%)
05/31 02:08:46午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 0.5924	Prec@(1,5) (84.3%, 99.1%)
05/31 02:08:49午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][350/391]	Step 17204	Loss 0.5969	Prec@(1,5) (84.2%, 99.0%)
05/31 02:08:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 0.5937	Prec@(1,5) (84.3%, 99.1%)
05/31 02:08:50午前 searchStage_trainer.py:260 [INFO] Valid: [ 43/49] Final Prec@1 84.2360%
05/31 02:08:50午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 02:08:51午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.6280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2588, 0.3039, 0.2449, 0.1925],
        [0.2439, 0.3151, 0.2418, 0.1992]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2587, 0.2996, 0.2618, 0.1799],
        [0.2501, 0.3094, 0.2660, 0.1744],
        [0.2678, 0.3272, 0.2130, 0.1920]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2687, 0.2957, 0.2515, 0.1841],
        [0.2755, 0.3044, 0.2119, 0.2081],
        [0.2656, 0.3117, 0.2224, 0.2003]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2656, 0.3002, 0.2300, 0.2042],
        [0.2663, 0.2962, 0.2190, 0.2185],
        [0.2589, 0.2942, 0.2279, 0.2189]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.3000, 0.2267, 0.2028],
        [0.2679, 0.2930, 0.2268, 0.2124],
        [0.2692, 0.2870, 0.2227, 0.2211]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2647, 0.2885, 0.2185, 0.2283],
        [0.2654, 0.2885, 0.2282, 0.2179],
        [0.2646, 0.2860, 0.2254, 0.2240]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2452],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2490],
        [0.2524, 0.2531, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2511, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2517, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2513, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2521, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2509, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2459],
        [0.2485, 0.2511, 0.2506, 0.2499],
        [0.2485, 0.2497, 0.2495, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2477, 0.2507, 0.2509],
        [0.2497, 0.2486, 0.2516, 0.2501],
        [0.2492, 0.2497, 0.2517, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2490, 0.2496, 0.2480],
        [0.2509, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2361, 0.2561, 0.2625, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2522],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2451, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 02:09:08午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][50/390]	Step 17254	lr 0.00184	Loss 0.0862 (0.0794)	Prec@(1,5) (97.1%, 100.0%)	
05/31 02:09:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.0746 (0.0768)	Prec@(1,5) (97.3%, 100.0%)	
05/31 02:09:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][150/390]	Step 17354	lr 0.00184	Loss 0.0237 (0.0790)	Prec@(1,5) (97.1%, 100.0%)	
05/31 02:09:56午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.0200 (0.0783)	Prec@(1,5) (97.2%, 100.0%)	
05/31 02:10:13午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][250/390]	Step 17454	lr 0.00184	Loss 0.0571 (0.0781)	Prec@(1,5) (97.2%, 100.0%)	
05/31 02:10:29午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.0731 (0.0791)	Prec@(1,5) (97.2%, 100.0%)	
05/31 02:10:45午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][350/390]	Step 17554	lr 0.00184	Loss 0.0741 (0.0792)	Prec@(1,5) (97.2%, 100.0%)	
05/31 02:10:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.1139 (0.0792)	Prec@(1,5) (97.2%, 100.0%)	
05/31 02:10:58午前 searchStage_trainer.py:221 [INFO] Train: [ 44/49] Final Prec@1 97.2280%
05/31 02:11:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][50/391]	Step 17595	Loss 0.5662	Prec@(1,5) (85.4%, 99.5%)
05/31 02:11:03午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 0.5833	Prec@(1,5) (84.8%, 99.2%)
05/31 02:11:05午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][150/391]	Step 17595	Loss 0.5958	Prec@(1,5) (84.5%, 99.2%)
05/31 02:11:07午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 0.5996	Prec@(1,5) (84.5%, 99.1%)
05/31 02:11:10午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][250/391]	Step 17595	Loss 0.6028	Prec@(1,5) (84.5%, 99.1%)
05/31 02:11:12午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 0.6065	Prec@(1,5) (84.5%, 99.1%)
05/31 02:11:14午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][350/391]	Step 17595	Loss 0.6024	Prec@(1,5) (84.7%, 99.1%)
05/31 02:11:16午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 0.5999	Prec@(1,5) (84.8%, 99.1%)
05/31 02:11:16午前 searchStage_trainer.py:260 [INFO] Valid: [ 44/49] Final Prec@1 84.7440%
05/31 02:11:16午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 02:11:16午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.7440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2588, 0.3038, 0.2450, 0.1925],
        [0.2439, 0.3150, 0.2419, 0.1992]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2587, 0.2996, 0.2618, 0.1799],
        [0.2501, 0.3094, 0.2660, 0.1745],
        [0.2678, 0.3271, 0.2130, 0.1921]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2687, 0.2957, 0.2516, 0.1840],
        [0.2755, 0.3044, 0.2119, 0.2082],
        [0.2655, 0.3117, 0.2225, 0.2003]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2655, 0.3002, 0.2300, 0.2043],
        [0.2663, 0.2961, 0.2190, 0.2186],
        [0.2589, 0.2942, 0.2280, 0.2190]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.2999, 0.2268, 0.2028],
        [0.2678, 0.2929, 0.2268, 0.2125],
        [0.2692, 0.2870, 0.2227, 0.2211]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2647, 0.2885, 0.2185, 0.2283],
        [0.2653, 0.2884, 0.2282, 0.2180],
        [0.2645, 0.2859, 0.2254, 0.2241]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2490],
        [0.2523, 0.2531, 0.2472, 0.2473],
        [0.2520, 0.2527, 0.2486, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2511, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2517, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2513, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2520, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2509, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2459],
        [0.2485, 0.2511, 0.2506, 0.2499],
        [0.2485, 0.2497, 0.2495, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2477, 0.2507, 0.2509],
        [0.2497, 0.2486, 0.2516, 0.2501],
        [0.2492, 0.2497, 0.2516, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2490, 0.2496, 0.2480],
        [0.2509, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2361, 0.2561, 0.2625, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2522],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2451, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 02:11:34午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][50/390]	Step 17645	lr 0.00159	Loss 0.1079 (0.0753)	Prec@(1,5) (97.6%, 99.9%)	
05/31 02:11:50午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.0226 (0.0704)	Prec@(1,5) (97.7%, 100.0%)	
05/31 02:12:06午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][150/390]	Step 17745	lr 0.00159	Loss 0.0119 (0.0720)	Prec@(1,5) (97.7%, 100.0%)	
05/31 02:12:22午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.0193 (0.0711)	Prec@(1,5) (97.7%, 100.0%)	
05/31 02:12:39午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][250/390]	Step 17845	lr 0.00159	Loss 0.1370 (0.0711)	Prec@(1,5) (97.7%, 100.0%)	
05/31 02:12:55午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.1540 (0.0716)	Prec@(1,5) (97.6%, 100.0%)	
05/31 02:13:11午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][350/390]	Step 17945	lr 0.00159	Loss 0.1761 (0.0730)	Prec@(1,5) (97.6%, 100.0%)	
05/31 02:13:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.1427 (0.0725)	Prec@(1,5) (97.6%, 100.0%)	
05/31 02:13:24午前 searchStage_trainer.py:221 [INFO] Train: [ 45/49] Final Prec@1 97.5640%
05/31 02:13:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][50/391]	Step 17986	Loss 0.5979	Prec@(1,5) (84.8%, 99.3%)
05/31 02:13:29午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 0.5883	Prec@(1,5) (84.5%, 99.3%)
05/31 02:13:31午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][150/391]	Step 17986	Loss 0.5955	Prec@(1,5) (84.5%, 99.3%)
05/31 02:13:33午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 0.5946	Prec@(1,5) (84.7%, 99.2%)
05/31 02:13:35午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][250/391]	Step 17986	Loss 0.5911	Prec@(1,5) (84.8%, 99.2%)
05/31 02:13:38午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 0.5965	Prec@(1,5) (84.7%, 99.2%)
05/31 02:13:40午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][350/391]	Step 17986	Loss 0.5939	Prec@(1,5) (84.7%, 99.2%)
05/31 02:13:42午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 0.5933	Prec@(1,5) (84.7%, 99.2%)
05/31 02:13:42午前 searchStage_trainer.py:260 [INFO] Valid: [ 45/49] Final Prec@1 84.7440%
05/31 02:13:42午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 02:13:42午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.7440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2587, 0.3037, 0.2450, 0.1926],
        [0.2439, 0.3149, 0.2419, 0.1993]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2587, 0.2995, 0.2619, 0.1799],
        [0.2500, 0.3093, 0.2661, 0.1746],
        [0.2678, 0.3270, 0.2130, 0.1922]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2686, 0.2957, 0.2517, 0.1840],
        [0.2755, 0.3043, 0.2120, 0.2082],
        [0.2655, 0.3116, 0.2225, 0.2004]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2655, 0.3001, 0.2300, 0.2043],
        [0.2662, 0.2960, 0.2190, 0.2187],
        [0.2589, 0.2941, 0.2280, 0.2191]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.2998, 0.2268, 0.2029],
        [0.2678, 0.2928, 0.2268, 0.2126],
        [0.2692, 0.2869, 0.2227, 0.2212]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2646, 0.2884, 0.2185, 0.2284],
        [0.2653, 0.2884, 0.2283, 0.2180],
        [0.2645, 0.2858, 0.2254, 0.2242]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2473],
        [0.2492, 0.2513, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2451],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2490],
        [0.2523, 0.2531, 0.2472, 0.2474],
        [0.2520, 0.2527, 0.2486, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2511, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2517, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2513, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2520, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2509, 0.2512],
        [0.2498, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2459],
        [0.2485, 0.2511, 0.2506, 0.2499],
        [0.2485, 0.2497, 0.2495, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2477, 0.2507, 0.2509],
        [0.2497, 0.2486, 0.2516, 0.2501],
        [0.2492, 0.2497, 0.2516, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2490, 0.2496, 0.2480],
        [0.2510, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2360, 0.2561, 0.2626, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2523],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2451, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 02:13:59午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][50/390]	Step 18036	lr 0.00138	Loss 0.0528 (0.0656)	Prec@(1,5) (97.9%, 100.0%)	
05/31 02:14:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.0090 (0.0641)	Prec@(1,5) (98.0%, 100.0%)	
05/31 02:14:31午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][150/390]	Step 18136	lr 0.00138	Loss 0.0662 (0.0656)	Prec@(1,5) (97.9%, 100.0%)	
05/31 02:14:48午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.0780 (0.0655)	Prec@(1,5) (98.0%, 100.0%)	
05/31 02:15:04午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][250/390]	Step 18236	lr 0.00138	Loss 0.0442 (0.0656)	Prec@(1,5) (97.9%, 100.0%)	
05/31 02:15:20午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.1054 (0.0642)	Prec@(1,5) (98.0%, 100.0%)	
05/31 02:15:36午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][350/390]	Step 18336	lr 0.00138	Loss 0.0695 (0.0652)	Prec@(1,5) (97.9%, 100.0%)	
05/31 02:15:49午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.0429 (0.0642)	Prec@(1,5) (97.9%, 100.0%)	
05/31 02:15:50午前 searchStage_trainer.py:221 [INFO] Train: [ 46/49] Final Prec@1 97.9240%
05/31 02:15:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][50/391]	Step 18377	Loss 0.6021	Prec@(1,5) (85.0%, 99.3%)
05/31 02:15:54午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 0.5905	Prec@(1,5) (84.8%, 99.2%)
05/31 02:15:56午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][150/391]	Step 18377	Loss 0.5928	Prec@(1,5) (84.8%, 99.1%)
05/31 02:15:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 0.5936	Prec@(1,5) (84.9%, 99.2%)
05/31 02:16:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][250/391]	Step 18377	Loss 0.5892	Prec@(1,5) (84.9%, 99.2%)
05/31 02:16:03午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 0.5910	Prec@(1,5) (84.9%, 99.2%)
05/31 02:16:05午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][350/391]	Step 18377	Loss 0.5971	Prec@(1,5) (84.8%, 99.2%)
05/31 02:16:07午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 0.5973	Prec@(1,5) (84.8%, 99.2%)
05/31 02:16:07午前 searchStage_trainer.py:260 [INFO] Valid: [ 46/49] Final Prec@1 84.8040%
05/31 02:16:07午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 02:16:08午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.8040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2587, 0.3036, 0.2451, 0.1926],
        [0.2439, 0.3148, 0.2420, 0.1994]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2587, 0.2995, 0.2619, 0.1799],
        [0.2500, 0.3092, 0.2661, 0.1746],
        [0.2678, 0.3269, 0.2131, 0.1922]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2686, 0.2957, 0.2517, 0.1840],
        [0.2755, 0.3042, 0.2120, 0.2083],
        [0.2655, 0.3114, 0.2225, 0.2005]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2655, 0.3001, 0.2301, 0.2043],
        [0.2662, 0.2960, 0.2191, 0.2188],
        [0.2589, 0.2940, 0.2280, 0.2191]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.2998, 0.2268, 0.2029],
        [0.2678, 0.2927, 0.2269, 0.2126],
        [0.2692, 0.2868, 0.2228, 0.2213]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2646, 0.2883, 0.2185, 0.2285],
        [0.2653, 0.2883, 0.2283, 0.2181],
        [0.2645, 0.2857, 0.2255, 0.2243]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2474],
        [0.2492, 0.2513, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2452],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2514, 0.2498, 0.2490],
        [0.2523, 0.2531, 0.2472, 0.2474],
        [0.2520, 0.2527, 0.2486, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2511, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2517, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2513, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2520, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2509, 0.2512],
        [0.2499, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2459],
        [0.2485, 0.2511, 0.2506, 0.2499],
        [0.2485, 0.2497, 0.2495, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2477, 0.2507, 0.2509],
        [0.2497, 0.2486, 0.2516, 0.2501],
        [0.2492, 0.2497, 0.2516, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2490, 0.2496, 0.2480],
        [0.2510, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2360, 0.2561, 0.2626, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2523],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2451, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 02:16:25午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][50/390]	Step 18427	lr 0.00121	Loss 0.0286 (0.0524)	Prec@(1,5) (98.3%, 100.0%)	
05/31 02:16:41午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.0742 (0.0580)	Prec@(1,5) (98.2%, 100.0%)	
05/31 02:16:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][150/390]	Step 18527	lr 0.00121	Loss 0.0589 (0.0598)	Prec@(1,5) (98.1%, 100.0%)	
05/31 02:17:14午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.0572 (0.0596)	Prec@(1,5) (98.1%, 100.0%)	
05/31 02:17:30午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][250/390]	Step 18627	lr 0.00121	Loss 0.0115 (0.0579)	Prec@(1,5) (98.2%, 100.0%)	
05/31 02:17:46午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.0340 (0.0575)	Prec@(1,5) (98.2%, 100.0%)	
05/31 02:18:03午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][350/390]	Step 18727	lr 0.00121	Loss 0.0138 (0.0582)	Prec@(1,5) (98.2%, 100.0%)	
05/31 02:18:15午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.0977 (0.0586)	Prec@(1,5) (98.2%, 100.0%)	
05/31 02:18:16午前 searchStage_trainer.py:221 [INFO] Train: [ 47/49] Final Prec@1 98.1520%
05/31 02:18:18午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][50/391]	Step 18768	Loss 0.6076	Prec@(1,5) (84.7%, 99.0%)
05/31 02:18:21午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 0.6187	Prec@(1,5) (84.5%, 99.0%)
05/31 02:18:23午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][150/391]	Step 18768	Loss 0.6345	Prec@(1,5) (84.2%, 99.0%)
05/31 02:18:25午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 0.6199	Prec@(1,5) (84.5%, 99.1%)
05/31 02:18:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][250/391]	Step 18768	Loss 0.6096	Prec@(1,5) (84.7%, 99.1%)
05/31 02:18:29午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 0.6058	Prec@(1,5) (84.9%, 99.1%)
05/31 02:18:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][350/391]	Step 18768	Loss 0.6067	Prec@(1,5) (84.9%, 99.1%)
05/31 02:18:33午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 0.6086	Prec@(1,5) (84.8%, 99.1%)
05/31 02:18:34午前 searchStage_trainer.py:260 [INFO] Valid: [ 47/49] Final Prec@1 84.7760%
05/31 02:18:34午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 02:18:34午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.8040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2587, 0.3035, 0.2451, 0.1927],
        [0.2439, 0.3147, 0.2420, 0.1994]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2587, 0.2995, 0.2619, 0.1799],
        [0.2500, 0.3092, 0.2662, 0.1747],
        [0.2678, 0.3268, 0.2131, 0.1923]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2686, 0.2956, 0.2517, 0.1841],
        [0.2755, 0.3042, 0.2120, 0.2083],
        [0.2655, 0.3114, 0.2226, 0.2006]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2655, 0.3001, 0.2301, 0.2044],
        [0.2662, 0.2959, 0.2191, 0.2188],
        [0.2588, 0.2939, 0.2281, 0.2192]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.2997, 0.2268, 0.2030],
        [0.2678, 0.2926, 0.2269, 0.2127],
        [0.2691, 0.2867, 0.2228, 0.2214]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2646, 0.2883, 0.2185, 0.2286],
        [0.2653, 0.2882, 0.2283, 0.2182],
        [0.2645, 0.2856, 0.2255, 0.2244]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2474],
        [0.2492, 0.2513, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2452],
        [0.2497, 0.2539, 0.2496, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2514, 0.2498, 0.2490],
        [0.2523, 0.2531, 0.2472, 0.2474],
        [0.2520, 0.2527, 0.2486, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2511, 0.2486, 0.2480],
        [0.2517, 0.2520, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2517, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2513, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2520, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2509, 0.2512],
        [0.2499, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2459],
        [0.2485, 0.2511, 0.2506, 0.2499],
        [0.2485, 0.2497, 0.2495, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2477, 0.2507, 0.2509],
        [0.2497, 0.2486, 0.2516, 0.2501],
        [0.2492, 0.2497, 0.2516, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2533, 0.2490, 0.2496, 0.2480],
        [0.2510, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2360, 0.2561, 0.2626, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2523],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2451, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 02:18:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][50/390]	Step 18818	lr 0.00109	Loss 0.0238 (0.0533)	Prec@(1,5) (98.3%, 100.0%)	
05/31 02:19:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.0450 (0.0518)	Prec@(1,5) (98.4%, 100.0%)	
05/31 02:19:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][150/390]	Step 18918	lr 0.00109	Loss 0.1580 (0.0507)	Prec@(1,5) (98.4%, 100.0%)	
05/31 02:19:39午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.0417 (0.0511)	Prec@(1,5) (98.4%, 100.0%)	
05/31 02:19:55午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][250/390]	Step 19018	lr 0.00109	Loss 0.1522 (0.0523)	Prec@(1,5) (98.4%, 100.0%)	
05/31 02:20:12午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.0402 (0.0526)	Prec@(1,5) (98.4%, 100.0%)	
05/31 02:20:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][350/390]	Step 19118	lr 0.00109	Loss 0.1237 (0.0528)	Prec@(1,5) (98.4%, 100.0%)	
05/31 02:20:41午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.0449 (0.0528)	Prec@(1,5) (98.4%, 100.0%)	
05/31 02:20:41午前 searchStage_trainer.py:221 [INFO] Train: [ 48/49] Final Prec@1 98.3520%
05/31 02:20:44午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][50/391]	Step 19159	Loss 0.5840	Prec@(1,5) (85.9%, 99.3%)
05/31 02:20:46午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 0.6081	Prec@(1,5) (85.2%, 99.2%)
05/31 02:20:48午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][150/391]	Step 19159	Loss 0.6051	Prec@(1,5) (85.4%, 99.1%)
05/31 02:20:50午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 0.6209	Prec@(1,5) (85.1%, 99.1%)
05/31 02:20:53午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][250/391]	Step 19159	Loss 0.6230	Prec@(1,5) (85.1%, 99.1%)
05/31 02:20:55午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 0.6160	Prec@(1,5) (85.2%, 99.1%)
05/31 02:20:57午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][350/391]	Step 19159	Loss 0.6113	Prec@(1,5) (85.2%, 99.1%)
05/31 02:20:59午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 0.6126	Prec@(1,5) (85.1%, 99.1%)
05/31 02:20:59午前 searchStage_trainer.py:260 [INFO] Valid: [ 48/49] Final Prec@1 85.1040%
05/31 02:20:59午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 02:20:59午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.1040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2587, 0.3034, 0.2452, 0.1927],
        [0.2438, 0.3146, 0.2421, 0.1995]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2994, 0.2620, 0.1800],
        [0.2500, 0.3091, 0.2662, 0.1747],
        [0.2678, 0.3268, 0.2131, 0.1923]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2686, 0.2956, 0.2517, 0.1842],
        [0.2755, 0.3041, 0.2120, 0.2084],
        [0.2655, 0.3113, 0.2226, 0.2006]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2655, 0.3000, 0.2301, 0.2044],
        [0.2662, 0.2958, 0.2191, 0.2189],
        [0.2588, 0.2938, 0.2281, 0.2193]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2705, 0.2997, 0.2268, 0.2030],
        [0.2678, 0.2925, 0.2269, 0.2128],
        [0.2691, 0.2866, 0.2228, 0.2214]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2646, 0.2882, 0.2186, 0.2286],
        [0.2653, 0.2881, 0.2284, 0.2182],
        [0.2644, 0.2855, 0.2256, 0.2245]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2524, 0.2518, 0.2474],
        [0.2492, 0.2513, 0.2514, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2507, 0.2450],
        [0.2533, 0.2495, 0.2520, 0.2452],
        [0.2497, 0.2539, 0.2496, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2514, 0.2498, 0.2490],
        [0.2523, 0.2531, 0.2472, 0.2474],
        [0.2520, 0.2527, 0.2486, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2520, 0.2473, 0.2485],
        [0.2523, 0.2511, 0.2486, 0.2480],
        [0.2517, 0.2519, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2551, 0.2514, 0.2447],
        [0.2502, 0.2523, 0.2474, 0.2500],
        [0.2503, 0.2517, 0.2486, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2513, 0.2483, 0.2480],
        [0.2507, 0.2507, 0.2486, 0.2500],
        [0.2516, 0.2520, 0.2486, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2487, 0.2509, 0.2512],
        [0.2499, 0.2509, 0.2511, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2532, 0.2525, 0.2459],
        [0.2485, 0.2511, 0.2506, 0.2499],
        [0.2485, 0.2497, 0.2495, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2477, 0.2507, 0.2509],
        [0.2497, 0.2486, 0.2516, 0.2501],
        [0.2492, 0.2497, 0.2516, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2489, 0.2487, 0.2488],
        [0.2534, 0.2490, 0.2496, 0.2480],
        [0.2510, 0.2478, 0.2488, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2360, 0.2561, 0.2626, 0.2453],
        [0.2411, 0.2515, 0.2548, 0.2526],
        [0.2416, 0.2537, 0.2540, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2505, 0.2523],
        [0.2472, 0.2495, 0.2518, 0.2515],
        [0.2451, 0.2515, 0.2564, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 02:21:17午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][50/390]	Step 19209	lr 0.00102	Loss 0.0336 (0.0443)	Prec@(1,5) (98.6%, 100.0%)	
05/31 02:21:33午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.0182 (0.0459)	Prec@(1,5) (98.6%, 100.0%)	
05/31 02:21:49午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][150/390]	Step 19309	lr 0.00102	Loss 0.0487 (0.0478)	Prec@(1,5) (98.5%, 100.0%)	
05/31 02:22:05午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.0116 (0.0488)	Prec@(1,5) (98.5%, 100.0%)	
05/31 02:22:21午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][250/390]	Step 19409	lr 0.00102	Loss 0.0057 (0.0495)	Prec@(1,5) (98.5%, 100.0%)	
05/31 02:22:37午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.0207 (0.0497)	Prec@(1,5) (98.5%, 100.0%)	
05/31 02:22:53午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][350/390]	Step 19509	lr 0.00102	Loss 0.0241 (0.0509)	Prec@(1,5) (98.4%, 100.0%)	
05/31 02:23:06午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.0311 (0.0501)	Prec@(1,5) (98.4%, 100.0%)	
05/31 02:23:07午前 searchStage_trainer.py:221 [INFO] Train: [ 49/49] Final Prec@1 98.4480%
05/31 02:23:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][50/391]	Step 19550	Loss 0.6231	Prec@(1,5) (85.2%, 99.2%)
05/31 02:23:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 0.6102	Prec@(1,5) (85.2%, 99.1%)
05/31 02:23:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][150/391]	Step 19550	Loss 0.6045	Prec@(1,5) (85.1%, 99.1%)
05/31 02:23:16午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 0.6069	Prec@(1,5) (85.1%, 99.0%)
05/31 02:23:18午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][250/391]	Step 19550	Loss 0.6066	Prec@(1,5) (85.1%, 99.1%)
05/31 02:23:20午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 0.6061	Prec@(1,5) (85.1%, 99.2%)
05/31 02:23:22午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][350/391]	Step 19550	Loss 0.6073	Prec@(1,5) (85.0%, 99.2%)
05/31 02:23:24午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 0.6080	Prec@(1,5) (85.1%, 99.1%)
05/31 02:23:24午前 searchStage_trainer.py:260 [INFO] Valid: [ 49/49] Final Prec@1 85.0560%
05/31 02:23:24午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 02:23:25午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.1040%
05/31 02:23:25午前 searchStage_main.py:84 [INFO] Final best Prec@1 = 85.1040%
05/31 02:23:25午前 searchStage_main.py:85 [INFO] Final Best Genotype = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('max_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('max_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
