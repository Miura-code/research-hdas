05/30 10:20:32PM parser.py:28 [INFO] 
05/30 10:20:32PM parser.py:29 [INFO] Parameters:
05/30 10:20:32PM parser.py:31 [INFO] DAG_PATH=results/search_Stage/CIFAR10/SCRATCH_PCDARTS_STAGESPEC/EXP-20240530-222032/DAG
05/30 10:20:32PM parser.py:31 [INFO] ALPHA_LR=0.0003
05/30 10:20:32PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
05/30 10:20:32PM parser.py:31 [INFO] BATCH_SIZE=64
05/30 10:20:32PM parser.py:31 [INFO] CHECKPOINT_RESET=False
05/30 10:20:32PM parser.py:31 [INFO] DATA_PATH=../data/
05/30 10:20:32PM parser.py:31 [INFO] DATASET=CIFAR10
05/30 10:20:32PM parser.py:31 [INFO] EPOCHS=50
05/30 10:20:32PM parser.py:31 [INFO] EXP_NAME=EXP-20240530-222032
05/30 10:20:32PM parser.py:31 [INFO] GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 0), ('skip_connect', 1)], [('skip_connect', 1), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 3)], [('sep_conv_3x3', 1), ('sep_conv_5x5', 4)]], normal_concat=range(2, 6), reduce=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('none', 1), ('avg_pool_3x3', 2)], [('skip_connect', 2), ('sep_conv_5x5', 3)], [('none', 3), ('avg_pool_3x3', 4)]], reduce_concat=range(2, 6))
05/30 10:20:32PM parser.py:31 [INFO] GPUS=[0]
05/30 10:20:32PM parser.py:31 [INFO] INIT_CHANNELS=16
05/30 10:20:32PM parser.py:31 [INFO] LAYERS=20
05/30 10:20:32PM parser.py:31 [INFO] LOCAL_RANK=0
05/30 10:20:32PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/30 10:20:32PM parser.py:31 [INFO] NAME=SCRATCH_PCDARTS_STAGESPEC
05/30 10:20:32PM parser.py:31 [INFO] PATH=results/search_Stage/CIFAR10/SCRATCH_PCDARTS_STAGESPEC/EXP-20240530-222032
05/30 10:20:32PM parser.py:31 [INFO] PRINT_FREQ=50
05/30 10:20:32PM parser.py:31 [INFO] RESUME_PATH=None
05/30 10:20:32PM parser.py:31 [INFO] SAVE=EXP
05/30 10:20:32PM parser.py:31 [INFO] SEED=2
05/30 10:20:32PM parser.py:31 [INFO] SHARE_STAGE=False
05/30 10:20:32PM parser.py:31 [INFO] SPEC_CELL=False
05/30 10:20:32PM parser.py:31 [INFO] TRAIN_PORTION=0.5
05/30 10:20:32PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
05/30 10:20:32PM parser.py:31 [INFO] W_LR=0.025
05/30 10:20:32PM parser.py:31 [INFO] W_LR_MIN=0.001
05/30 10:20:32PM parser.py:31 [INFO] W_MOMENTUM=0.9
05/30 10:20:32PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
05/30 10:20:32PM parser.py:31 [INFO] WORKERS=4
05/30 10:20:32PM parser.py:32 [INFO] 
05/30 10:20:34PM searchStage_trainer.py:103 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2500, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2500, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2497, 0.2503, 0.2501],
        [0.2499, 0.2499, 0.2502, 0.2500],
        [0.2502, 0.2499, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2498, 0.2503, 0.2498],
        [0.2499, 0.2501, 0.2501, 0.2500],
        [0.2500, 0.2501, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2501, 0.2499],
        [0.2497, 0.2504, 0.2499, 0.2501],
        [0.2501, 0.2501, 0.2501, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2501, 0.2499, 0.2502],
        [0.2502, 0.2499, 0.2499, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2496, 0.2502],
        [0.2504, 0.2499, 0.2499, 0.2498],
        [0.2501, 0.2501, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2500, 0.2501, 0.2503],
        [0.2501, 0.2498, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2497, 0.2503, 0.2502],
        [0.2497, 0.2501, 0.2500, 0.2502],
        [0.2504, 0.2500, 0.2498, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2500, 0.2502],
        [0.2500, 0.2503, 0.2497, 0.2499],
        [0.2497, 0.2498, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2503, 0.2498, 0.2500],
        [0.2497, 0.2501, 0.2501, 0.2501],
        [0.2498, 0.2499, 0.2500, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2502, 0.2501],
        [0.2503, 0.2500, 0.2499, 0.2498],
        [0.2502, 0.2501, 0.2497, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2505, 0.2499],
        [0.2497, 0.2500, 0.2501, 0.2501],
        [0.2501, 0.2503, 0.2496, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2497, 0.2502, 0.2501],
        [0.2497, 0.2500, 0.2502, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2498, 0.2502, 0.2502],
        [0.2500, 0.2500, 0.2498, 0.2502],
        [0.2496, 0.2503, 0.2503, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2502, 0.2498],
        [0.2497, 0.2498, 0.2504, 0.2501],
        [0.2501, 0.2500, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2503, 0.2497, 0.2498],
        [0.2501, 0.2495, 0.2503, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2501, 0.2499, 0.2498],
        [0.2502, 0.2500, 0.2496, 0.2502],
        [0.2498, 0.2502, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2499, 0.2502],
        [0.2499, 0.2504, 0.2497, 0.2500],
        [0.2497, 0.2500, 0.2501, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:20:51PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 2.4044 (2.3337)	Prec@(1,5) (13.7%, 58.3%)	
05/30 10:21:07PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 2.0471 (2.2780)	Prec@(1,5) (16.3%, 65.6%)	
05/30 10:21:22PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 1.9034 (2.1770)	Prec@(1,5) (19.5%, 70.7%)	
05/30 10:21:38PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 1.7605 (2.0993)	Prec@(1,5) (21.8%, 74.1%)	
05/30 10:21:54PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 1.7793 (2.0478)	Prec@(1,5) (23.2%, 76.3%)	
05/30 10:22:09PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 1.6854 (2.0015)	Prec@(1,5) (24.9%, 78.0%)	
05/30 10:22:25PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 1.5453 (1.9625)	Prec@(1,5) (26.1%, 79.3%)	
05/30 10:22:37PM searchStage_trainer.py:211 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 1.7947 (1.9371)	Prec@(1,5) (27.0%, 80.2%)	
05/30 10:22:38PM searchStage_trainer.py:221 [INFO] Train: [  0/49] Final Prec@1 26.9880%
05/30 10:22:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 1.9619	Prec@(1,5) (29.6%, 84.5%)
05/30 10:22:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 1.9660	Prec@(1,5) (30.2%, 84.9%)
05/30 10:22:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 1.9765	Prec@(1,5) (30.1%, 85.0%)
05/30 10:22:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 1.9731	Prec@(1,5) (30.0%, 85.0%)
05/30 10:22:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 1.9822	Prec@(1,5) (29.9%, 84.8%)
05/30 10:22:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 1.9847	Prec@(1,5) (29.9%, 84.7%)
05/30 10:22:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 1.9839	Prec@(1,5) (29.8%, 84.8%)
05/30 10:22:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 1.9800	Prec@(1,5) (29.9%, 84.9%)
05/30 10:22:56PM searchStage_trainer.py:260 [INFO] Valid: [  0/49] Final Prec@1 29.8480%
05/30 10:22:56PM searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('max_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 1)], [('max_pool_3x3', 2), ('max_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('max_pool_3x3', 3), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 4), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:22:56PM searchStage_main.py:79 [INFO] Until now, best Prec@1 = 29.8480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2522, 0.2516, 0.2510, 0.2452],
        [0.2520, 0.2528, 0.2506, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2549, 0.2529, 0.2449],
        [0.2521, 0.2521, 0.2532, 0.2427],
        [0.2558, 0.2565, 0.2429, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2524, 0.2532, 0.2465],
        [0.2525, 0.2598, 0.2460, 0.2417],
        [0.2522, 0.2594, 0.2460, 0.2423]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2578, 0.2443, 0.2464],
        [0.2530, 0.2576, 0.2481, 0.2412],
        [0.2525, 0.2587, 0.2469, 0.2419]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2573, 0.2465, 0.2430],
        [0.2526, 0.2574, 0.2459, 0.2441],
        [0.2527, 0.2573, 0.2460, 0.2441]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2573, 0.2491, 0.2445],
        [0.2499, 0.2582, 0.2480, 0.2438],
        [0.2523, 0.2547, 0.2462, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2493, 0.2509, 0.2508],
        [0.2502, 0.2509, 0.2505, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2503, 0.2510, 0.2481],
        [0.2503, 0.2496, 0.2508, 0.2492],
        [0.2520, 0.2509, 0.2479, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2502, 0.2506, 0.2487],
        [0.2511, 0.2509, 0.2487, 0.2492],
        [0.2509, 0.2506, 0.2492, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2507, 0.2497, 0.2491],
        [0.2500, 0.2506, 0.2496, 0.2497],
        [0.2504, 0.2507, 0.2493, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2506, 0.2491, 0.2495],
        [0.2508, 0.2507, 0.2498, 0.2487],
        [0.2506, 0.2508, 0.2487, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2503, 0.2502, 0.2496],
        [0.2497, 0.2503, 0.2500, 0.2500],
        [0.2503, 0.2506, 0.2497, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2494, 0.2501, 0.2503],
        [0.2501, 0.2499, 0.2502, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2495, 0.2501, 0.2505],
        [0.2501, 0.2498, 0.2497, 0.2504],
        [0.2497, 0.2504, 0.2507, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2510, 0.2507, 0.2486],
        [0.2498, 0.2494, 0.2500, 0.2508],
        [0.2498, 0.2500, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2506, 0.2493],
        [0.2502, 0.2493, 0.2509, 0.2496],
        [0.2499, 0.2498, 0.2496, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2499, 0.2510, 0.2500],
        [0.2494, 0.2502, 0.2501, 0.2502],
        [0.2483, 0.2505, 0.2514, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2505, 0.2512, 0.2483],
        [0.2494, 0.2505, 0.2510, 0.2490],
        [0.2495, 0.2488, 0.2489, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:23:14午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 1.7691 (1.6807)	Prec@(1,5) (36.6%, 88.5%)	
05/30 10:23:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 1.7825 (1.6593)	Prec@(1,5) (37.9%, 88.8%)	
05/30 10:23:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 1.7900 (1.6543)	Prec@(1,5) (37.8%, 89.1%)	
05/30 10:24:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 1.5130 (1.6408)	Prec@(1,5) (38.4%, 89.2%)	
05/30 10:24:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 1.4486 (1.6308)	Prec@(1,5) (39.0%, 89.3%)	
05/30 10:24:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 1.7445 (1.6177)	Prec@(1,5) (39.7%, 89.6%)	
05/30 10:24:50午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 1.2675 (1.6100)	Prec@(1,5) (40.0%, 89.7%)	
05/30 10:25:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 1.4607 (1.5990)	Prec@(1,5) (40.4%, 89.9%)	
05/30 10:25:04午後 searchStage_trainer.py:221 [INFO] Train: [  1/49] Final Prec@1 40.3960%
05/30 10:25:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 1.5241	Prec@(1,5) (41.9%, 91.6%)
05/30 10:25:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 1.5359	Prec@(1,5) (41.5%, 91.2%)
05/30 10:25:11午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 1.5395	Prec@(1,5) (41.2%, 91.0%)
05/30 10:25:13午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 1.5356	Prec@(1,5) (41.6%, 91.1%)
05/30 10:25:15午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 1.5375	Prec@(1,5) (41.6%, 91.1%)
05/30 10:25:17午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 1.5395	Prec@(1,5) (41.4%, 91.1%)
05/30 10:25:19午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 1.5433	Prec@(1,5) (41.2%, 91.0%)
05/30 10:25:21午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 1.5429	Prec@(1,5) (41.2%, 91.1%)
05/30 10:25:21午後 searchStage_trainer.py:260 [INFO] Valid: [  1/49] Final Prec@1 41.1960%
05/30 10:25:21午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)]], DAG3_concat=range(6, 8))
05/30 10:25:22午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 41.1960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2564, 0.2507, 0.2439],
        [0.2510, 0.2610, 0.2536, 0.2344]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2581, 0.2523, 0.2432],
        [0.2521, 0.2547, 0.2560, 0.2372],
        [0.2600, 0.2674, 0.2394, 0.2332]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2555, 0.2556, 0.2404],
        [0.2559, 0.2700, 0.2421, 0.2320],
        [0.2546, 0.2695, 0.2417, 0.2342]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2659, 0.2425, 0.2396],
        [0.2559, 0.2654, 0.2446, 0.2341],
        [0.2557, 0.2681, 0.2427, 0.2336]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2568, 0.2652, 0.2406, 0.2374],
        [0.2573, 0.2665, 0.2406, 0.2356],
        [0.2580, 0.2650, 0.2413, 0.2356]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2641, 0.2449, 0.2396],
        [0.2539, 0.2651, 0.2432, 0.2378],
        [0.2555, 0.2621, 0.2418, 0.2405]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2493, 0.2517, 0.2513],
        [0.2506, 0.2516, 0.2509, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2509, 0.2518, 0.2468],
        [0.2498, 0.2498, 0.2513, 0.2490],
        [0.2519, 0.2523, 0.2481, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2508, 0.2502, 0.2482],
        [0.2511, 0.2514, 0.2486, 0.2489],
        [0.2516, 0.2512, 0.2492, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2510, 0.2489, 0.2495],
        [0.2505, 0.2512, 0.2494, 0.2489],
        [0.2510, 0.2512, 0.2496, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2506, 0.2493, 0.2488],
        [0.2513, 0.2508, 0.2488, 0.2491],
        [0.2509, 0.2516, 0.2489, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2505, 0.2491, 0.2500],
        [0.2499, 0.2505, 0.2498, 0.2498],
        [0.2505, 0.2511, 0.2502, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2494, 0.2503, 0.2502],
        [0.2502, 0.2497, 0.2502, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2494, 0.2501, 0.2506],
        [0.2501, 0.2493, 0.2497, 0.2509],
        [0.2499, 0.2503, 0.2510, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2516, 0.2512, 0.2478],
        [0.2497, 0.2494, 0.2499, 0.2510],
        [0.2494, 0.2500, 0.2501, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2509, 0.2513, 0.2489],
        [0.2502, 0.2491, 0.2513, 0.2494],
        [0.2499, 0.2496, 0.2493, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2496, 0.2519, 0.2501],
        [0.2489, 0.2502, 0.2503, 0.2507],
        [0.2472, 0.2510, 0.2525, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2512, 0.2524, 0.2466],
        [0.2489, 0.2507, 0.2521, 0.2484],
        [0.2491, 0.2479, 0.2480, 0.2549]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:25:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 1.4998 (1.4623)	Prec@(1,5) (46.2%, 91.8%)	
05/30 10:25:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 1.1931 (1.4811)	Prec@(1,5) (45.0%, 91.5%)	
05/30 10:26:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 1.6955 (1.4814)	Prec@(1,5) (45.2%, 91.5%)	
05/30 10:26:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 1.4139 (1.4738)	Prec@(1,5) (45.4%, 91.7%)	
05/30 10:26:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 1.0759 (1.4572)	Prec@(1,5) (46.2%, 91.9%)	
05/30 10:26:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 1.2876 (1.4494)	Prec@(1,5) (46.6%, 92.0%)	
05/30 10:27:16午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 1.3572 (1.4447)	Prec@(1,5) (46.9%, 92.2%)	
05/30 10:27:29午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 1.5862 (1.4358)	Prec@(1,5) (47.2%, 92.3%)	
05/30 10:27:29午後 searchStage_trainer.py:221 [INFO] Train: [  2/49] Final Prec@1 47.2280%
05/30 10:27:31午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 1.5457	Prec@(1,5) (43.9%, 92.2%)
05/30 10:27:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 1.5101	Prec@(1,5) (45.4%, 92.5%)
05/30 10:27:36午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 1.5042	Prec@(1,5) (45.8%, 92.5%)
05/30 10:27:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 1.5110	Prec@(1,5) (45.7%, 92.2%)
05/30 10:27:40午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 1.5194	Prec@(1,5) (45.3%, 92.2%)
05/30 10:27:42午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 1.5152	Prec@(1,5) (45.5%, 92.2%)
05/30 10:27:45午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 1.5157	Prec@(1,5) (45.6%, 92.3%)
05/30 10:27:47午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 1.5157	Prec@(1,5) (45.5%, 92.3%)
05/30 10:27:47午後 searchStage_trainer.py:260 [INFO] Valid: [  2/49] Final Prec@1 45.5200%
05/30 10:27:47午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)]], DAG3_concat=range(6, 8))
05/30 10:27:47午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 45.5200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2475, 0.2601, 0.2526, 0.2398],
        [0.2525, 0.2689, 0.2539, 0.2247]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2428, 0.2643, 0.2546, 0.2382],
        [0.2531, 0.2614, 0.2553, 0.2302],
        [0.2611, 0.2798, 0.2354, 0.2237]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2596, 0.2595, 0.2325],
        [0.2605, 0.2813, 0.2369, 0.2213],
        [0.2564, 0.2803, 0.2364, 0.2269]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2739, 0.2405, 0.2312],
        [0.2601, 0.2734, 0.2380, 0.2285],
        [0.2593, 0.2780, 0.2395, 0.2232]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2605, 0.2746, 0.2343, 0.2306],
        [0.2582, 0.2758, 0.2375, 0.2284],
        [0.2610, 0.2745, 0.2381, 0.2263]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2715, 0.2448, 0.2305],
        [0.2566, 0.2737, 0.2382, 0.2315],
        [0.2565, 0.2692, 0.2370, 0.2374]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2501, 0.2522, 0.2503],
        [0.2505, 0.2513, 0.2510, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2508, 0.2531, 0.2461],
        [0.2505, 0.2505, 0.2512, 0.2478],
        [0.2523, 0.2530, 0.2475, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2508, 0.2504, 0.2481],
        [0.2511, 0.2518, 0.2480, 0.2490],
        [0.2518, 0.2520, 0.2499, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2514, 0.2489, 0.2491],
        [0.2508, 0.2515, 0.2492, 0.2485],
        [0.2513, 0.2515, 0.2495, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2510, 0.2487, 0.2488],
        [0.2512, 0.2514, 0.2493, 0.2482],
        [0.2515, 0.2520, 0.2483, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2506, 0.2491, 0.2496],
        [0.2502, 0.2505, 0.2495, 0.2498],
        [0.2511, 0.2514, 0.2498, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2493, 0.2504, 0.2503],
        [0.2504, 0.2496, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2495, 0.2501, 0.2506],
        [0.2502, 0.2488, 0.2496, 0.2514],
        [0.2500, 0.2504, 0.2513, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2521, 0.2518, 0.2471],
        [0.2497, 0.2494, 0.2497, 0.2512],
        [0.2492, 0.2499, 0.2501, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2511, 0.2519, 0.2484],
        [0.2502, 0.2490, 0.2515, 0.2493],
        [0.2499, 0.2494, 0.2490, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2493, 0.2527, 0.2502],
        [0.2484, 0.2502, 0.2504, 0.2509],
        [0.2463, 0.2512, 0.2534, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2516, 0.2533, 0.2453],
        [0.2484, 0.2508, 0.2530, 0.2478],
        [0.2489, 0.2472, 0.2472, 0.2567]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:28:04午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 1.5571 (1.3448)	Prec@(1,5) (50.7%, 93.1%)	
05/30 10:28:20午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 1.3375 (1.3372)	Prec@(1,5) (51.3%, 93.2%)	
05/30 10:28:36午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 1.2504 (1.3236)	Prec@(1,5) (51.8%, 93.5%)	
05/30 10:28:52午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 1.2156 (1.3180)	Prec@(1,5) (52.1%, 93.6%)	
05/30 10:29:09午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 1.3813 (1.3198)	Prec@(1,5) (52.2%, 93.6%)	
05/30 10:29:25午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 1.1592 (1.3122)	Prec@(1,5) (52.4%, 93.7%)	
05/30 10:29:41午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 1.3825 (1.3074)	Prec@(1,5) (52.5%, 93.7%)	
05/30 10:29:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 1.3573 (1.3015)	Prec@(1,5) (52.7%, 93.8%)	
05/30 10:29:55午後 searchStage_trainer.py:221 [INFO] Train: [  3/49] Final Prec@1 52.7000%
05/30 10:29:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 1.6050	Prec@(1,5) (46.7%, 91.1%)
05/30 10:30:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 1.5883	Prec@(1,5) (47.1%, 91.5%)
05/30 10:30:02午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 1.5896	Prec@(1,5) (47.0%, 91.7%)
05/30 10:30:04午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 1.5977	Prec@(1,5) (46.8%, 91.6%)
05/30 10:30:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 1.6014	Prec@(1,5) (46.9%, 91.6%)
05/30 10:30:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 1.5977	Prec@(1,5) (46.8%, 91.7%)
05/30 10:30:11午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 1.6055	Prec@(1,5) (46.6%, 91.7%)
05/30 10:30:12午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 1.6080	Prec@(1,5) (46.6%, 91.6%)
05/30 10:30:12午後 searchStage_trainer.py:260 [INFO] Valid: [  3/49] Final Prec@1 46.5520%
05/30 10:30:12午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 4)]], DAG3_concat=range(6, 8))
05/30 10:30:13午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 46.5520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2485, 0.2678, 0.2508, 0.2328],
        [0.2541, 0.2758, 0.2544, 0.2157]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2720, 0.2543, 0.2288],
        [0.2509, 0.2660, 0.2572, 0.2260],
        [0.2621, 0.2939, 0.2311, 0.2129]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2609, 0.2595, 0.2285],
        [0.2642, 0.2906, 0.2330, 0.2123],
        [0.2607, 0.2915, 0.2329, 0.2149]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2577, 0.2814, 0.2370, 0.2239],
        [0.2622, 0.2821, 0.2368, 0.2190],
        [0.2644, 0.2850, 0.2342, 0.2164]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2630, 0.2826, 0.2319, 0.2225],
        [0.2611, 0.2847, 0.2332, 0.2210],
        [0.2638, 0.2831, 0.2339, 0.2193]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2564, 0.2773, 0.2406, 0.2257],
        [0.2600, 0.2791, 0.2348, 0.2261],
        [0.2600, 0.2770, 0.2339, 0.2291]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2499, 0.2524, 0.2503],
        [0.2503, 0.2520, 0.2511, 0.2466]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2511, 0.2527, 0.2462],
        [0.2510, 0.2508, 0.2514, 0.2468],
        [0.2527, 0.2536, 0.2475, 0.2463]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2512, 0.2507, 0.2477],
        [0.2508, 0.2524, 0.2481, 0.2487],
        [0.2518, 0.2523, 0.2500, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2515, 0.2486, 0.2487],
        [0.2510, 0.2517, 0.2492, 0.2481],
        [0.2512, 0.2517, 0.2494, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2513, 0.2484, 0.2484],
        [0.2515, 0.2514, 0.2488, 0.2483],
        [0.2516, 0.2523, 0.2485, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2506, 0.2490, 0.2493],
        [0.2506, 0.2506, 0.2491, 0.2497],
        [0.2513, 0.2516, 0.2498, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2504, 0.2503],
        [0.2505, 0.2494, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2492, 0.2501, 0.2508],
        [0.2502, 0.2487, 0.2495, 0.2516],
        [0.2501, 0.2504, 0.2514, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2527, 0.2523, 0.2463],
        [0.2497, 0.2492, 0.2496, 0.2514],
        [0.2490, 0.2498, 0.2501, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2513, 0.2525, 0.2480],
        [0.2502, 0.2490, 0.2517, 0.2492],
        [0.2500, 0.2492, 0.2487, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2491, 0.2532, 0.2504],
        [0.2480, 0.2502, 0.2505, 0.2512],
        [0.2457, 0.2514, 0.2542, 0.2488]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2520, 0.2542, 0.2440],
        [0.2481, 0.2508, 0.2537, 0.2474],
        [0.2488, 0.2465, 0.2464, 0.2583]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:30:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 1.0214 (1.2425)	Prec@(1,5) (54.8%, 94.8%)	
05/30 10:30:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 1.4213 (1.2247)	Prec@(1,5) (55.5%, 94.6%)	
05/30 10:31:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 1.0432 (1.2252)	Prec@(1,5) (55.7%, 94.5%)	
05/30 10:31:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 1.0439 (1.2148)	Prec@(1,5) (56.1%, 94.6%)	
05/30 10:31:35午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 1.2134 (1.2162)	Prec@(1,5) (55.9%, 94.6%)	
05/30 10:31:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 1.2998 (1.2078)	Prec@(1,5) (56.3%, 94.7%)	
05/30 10:32:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 1.0477 (1.2003)	Prec@(1,5) (56.6%, 94.8%)	
05/30 10:32:20午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 1.0689 (1.1939)	Prec@(1,5) (57.0%, 94.8%)	
05/30 10:32:20午後 searchStage_trainer.py:221 [INFO] Train: [  4/49] Final Prec@1 57.0240%
05/30 10:32:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 1.1675	Prec@(1,5) (58.6%, 94.7%)
05/30 10:32:25午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 1.1781	Prec@(1,5) (58.4%, 95.0%)
05/30 10:32:27午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 1.1839	Prec@(1,5) (58.1%, 95.1%)
05/30 10:32:30午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 1.1892	Prec@(1,5) (57.7%, 95.1%)
05/30 10:32:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 1.1860	Prec@(1,5) (57.8%, 95.1%)
05/30 10:32:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 1.1837	Prec@(1,5) (58.0%, 95.1%)
05/30 10:32:36午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 1.1820	Prec@(1,5) (58.0%, 95.2%)
05/30 10:32:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 1.1792	Prec@(1,5) (58.1%, 95.2%)
05/30 10:32:38午後 searchStage_trainer.py:260 [INFO] Valid: [  4/49] Final Prec@1 58.0880%
05/30 10:32:38午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)]], DAG3_concat=range(6, 8))
05/30 10:32:38午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 58.0880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2472, 0.2755, 0.2557, 0.2215],
        [0.2558, 0.2842, 0.2514, 0.2086]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2764, 0.2554, 0.2235],
        [0.2538, 0.2728, 0.2584, 0.2150],
        [0.2668, 0.3054, 0.2243, 0.2035]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2657, 0.2626, 0.2197],
        [0.2679, 0.2992, 0.2266, 0.2063],
        [0.2657, 0.3014, 0.2287, 0.2042]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2600, 0.2891, 0.2334, 0.2176],
        [0.2651, 0.2895, 0.2330, 0.2123],
        [0.2686, 0.2931, 0.2310, 0.2073]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2668, 0.2894, 0.2288, 0.2151],
        [0.2638, 0.2925, 0.2292, 0.2145],
        [0.2685, 0.2901, 0.2299, 0.2114]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2594, 0.2836, 0.2389, 0.2181],
        [0.2615, 0.2850, 0.2308, 0.2228],
        [0.2628, 0.2845, 0.2312, 0.2215]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2495, 0.2522, 0.2511],
        [0.2508, 0.2523, 0.2515, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2512, 0.2528, 0.2455],
        [0.2509, 0.2502, 0.2515, 0.2475],
        [0.2528, 0.2541, 0.2477, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2512, 0.2508, 0.2476],
        [0.2511, 0.2526, 0.2477, 0.2485],
        [0.2517, 0.2525, 0.2503, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2512, 0.2481, 0.2491],
        [0.2514, 0.2518, 0.2489, 0.2480],
        [0.2515, 0.2521, 0.2494, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2516, 0.2489, 0.2476],
        [0.2513, 0.2515, 0.2486, 0.2487],
        [0.2514, 0.2525, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2508, 0.2488, 0.2492],
        [0.2507, 0.2509, 0.2492, 0.2492],
        [0.2513, 0.2517, 0.2496, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2491, 0.2504, 0.2503],
        [0.2507, 0.2493, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2491, 0.2502, 0.2509],
        [0.2501, 0.2484, 0.2495, 0.2519],
        [0.2501, 0.2504, 0.2516, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2531, 0.2527, 0.2457],
        [0.2498, 0.2492, 0.2495, 0.2515],
        [0.2489, 0.2497, 0.2501, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2515, 0.2529, 0.2478],
        [0.2502, 0.2489, 0.2518, 0.2491],
        [0.2500, 0.2491, 0.2485, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2488, 0.2536, 0.2506],
        [0.2478, 0.2502, 0.2507, 0.2513],
        [0.2452, 0.2515, 0.2546, 0.2487]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2522, 0.2547, 0.2431],
        [0.2479, 0.2509, 0.2543, 0.2470],
        [0.2487, 0.2460, 0.2459, 0.2594]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:32:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 1.3308 (1.1326)	Prec@(1,5) (60.4%, 95.7%)	
05/30 10:33:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 1.1710 (1.1127)	Prec@(1,5) (60.7%, 95.7%)	
05/30 10:33:28午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 1.0421 (1.1090)	Prec@(1,5) (60.5%, 95.8%)	
05/30 10:33:44午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 0.8368 (1.1129)	Prec@(1,5) (60.4%, 95.7%)	
05/30 10:34:00午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 1.0828 (1.1090)	Prec@(1,5) (60.4%, 95.8%)	
05/30 10:34:17午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 1.0203 (1.1109)	Prec@(1,5) (60.3%, 95.8%)	
05/30 10:34:33午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 0.9946 (1.1032)	Prec@(1,5) (60.6%, 95.8%)	
05/30 10:34:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 0.7976 (1.0993)	Prec@(1,5) (60.7%, 95.8%)	
05/30 10:34:46午後 searchStage_trainer.py:221 [INFO] Train: [  5/49] Final Prec@1 60.7240%
05/30 10:34:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 1.2471	Prec@(1,5) (57.3%, 95.2%)
05/30 10:34:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 1.2316	Prec@(1,5) (57.9%, 95.4%)
05/30 10:34:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 1.2401	Prec@(1,5) (57.8%, 95.3%)
05/30 10:34:55午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 1.2309	Prec@(1,5) (57.9%, 95.4%)
05/30 10:34:58午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 1.2278	Prec@(1,5) (58.0%, 95.4%)
05/30 10:35:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 1.2264	Prec@(1,5) (58.0%, 95.4%)
05/30 10:35:02午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 1.2212	Prec@(1,5) (58.1%, 95.4%)
05/30 10:35:04午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 1.2247	Prec@(1,5) (58.0%, 95.4%)
05/30 10:35:04午後 searchStage_trainer.py:260 [INFO] Valid: [  5/49] Final Prec@1 58.0280%
05/30 10:35:04午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 5), ('max_pool_3x3', 6)]], DAG3_concat=range(6, 8))
05/30 10:35:04午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 58.0880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2486, 0.2846, 0.2545, 0.2123],
        [0.2541, 0.2941, 0.2530, 0.1988]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2855, 0.2566, 0.2131],
        [0.2543, 0.2803, 0.2580, 0.2073],
        [0.2689, 0.3156, 0.2197, 0.1958]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2685, 0.2635, 0.2144],
        [0.2701, 0.3074, 0.2247, 0.1979],
        [0.2691, 0.3096, 0.2252, 0.1961]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2621, 0.2955, 0.2304, 0.2120],
        [0.2684, 0.2961, 0.2314, 0.2041],
        [0.2720, 0.2988, 0.2281, 0.2011]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2697, 0.2959, 0.2249, 0.2094],
        [0.2667, 0.2991, 0.2273, 0.2068],
        [0.2696, 0.2964, 0.2284, 0.2056]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2612, 0.2894, 0.2382, 0.2112],
        [0.2633, 0.2901, 0.2288, 0.2178],
        [0.2658, 0.2896, 0.2281, 0.2165]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2494, 0.2523, 0.2512],
        [0.2508, 0.2525, 0.2517, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2515, 0.2528, 0.2451],
        [0.2508, 0.2504, 0.2517, 0.2470],
        [0.2529, 0.2541, 0.2475, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2514, 0.2509, 0.2474],
        [0.2511, 0.2529, 0.2478, 0.2483],
        [0.2517, 0.2526, 0.2502, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2511, 0.2480, 0.2492],
        [0.2516, 0.2518, 0.2489, 0.2477],
        [0.2517, 0.2521, 0.2493, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2519, 0.2485, 0.2476],
        [0.2512, 0.2517, 0.2483, 0.2488],
        [0.2515, 0.2526, 0.2490, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2508, 0.2486, 0.2491],
        [0.2507, 0.2508, 0.2492, 0.2493],
        [0.2513, 0.2519, 0.2496, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2489, 0.2505, 0.2504],
        [0.2509, 0.2491, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2491, 0.2501, 0.2510],
        [0.2501, 0.2484, 0.2495, 0.2521],
        [0.2501, 0.2504, 0.2516, 0.2478]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2535, 0.2529, 0.2452],
        [0.2498, 0.2491, 0.2494, 0.2517],
        [0.2488, 0.2497, 0.2501, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2515, 0.2532, 0.2476],
        [0.2502, 0.2488, 0.2519, 0.2491],
        [0.2501, 0.2490, 0.2483, 0.2526]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2487, 0.2540, 0.2507],
        [0.2476, 0.2502, 0.2507, 0.2514],
        [0.2448, 0.2516, 0.2550, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2525, 0.2553, 0.2424],
        [0.2477, 0.2509, 0.2546, 0.2467],
        [0.2486, 0.2456, 0.2455, 0.2602]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:35:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 0.9998 (1.0574)	Prec@(1,5) (62.2%, 96.0%)	
05/30 10:35:38午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 0.9948 (1.0414)	Prec@(1,5) (63.0%, 96.1%)	
05/30 10:35:54午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 0.8283 (1.0318)	Prec@(1,5) (63.3%, 96.3%)	
05/30 10:36:10午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 0.8985 (1.0354)	Prec@(1,5) (63.2%, 96.3%)	
05/30 10:36:26午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 0.9505 (1.0252)	Prec@(1,5) (63.8%, 96.2%)	
05/30 10:36:42午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 0.9242 (1.0224)	Prec@(1,5) (63.7%, 96.2%)	
05/30 10:36:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 0.9043 (1.0177)	Prec@(1,5) (63.8%, 96.4%)	
05/30 10:37:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 1.0681 (1.0147)	Prec@(1,5) (63.9%, 96.4%)	
05/30 10:37:12午後 searchStage_trainer.py:221 [INFO] Train: [  6/49] Final Prec@1 63.9520%
05/30 10:37:14午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 1.2374	Prec@(1,5) (58.3%, 94.4%)
05/30 10:37:17午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 1.2290	Prec@(1,5) (58.4%, 94.3%)
05/30 10:37:19午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 1.2147	Prec@(1,5) (59.1%, 94.6%)
05/30 10:37:21午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 1.2111	Prec@(1,5) (59.2%, 94.6%)
05/30 10:37:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 1.2170	Prec@(1,5) (59.1%, 94.6%)
05/30 10:37:25午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 1.2224	Prec@(1,5) (59.0%, 94.7%)
05/30 10:37:28午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 1.2265	Prec@(1,5) (59.0%, 94.7%)
05/30 10:37:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 1.2236	Prec@(1,5) (59.0%, 94.7%)
05/30 10:37:29午後 searchStage_trainer.py:260 [INFO] Valid: [  6/49] Final Prec@1 59.0040%
05/30 10:37:30午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:37:30午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 59.0040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2488, 0.2953, 0.2547, 0.2012],
        [0.2570, 0.3014, 0.2500, 0.1916]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2909, 0.2556, 0.2069],
        [0.2560, 0.2850, 0.2594, 0.1997],
        [0.2719, 0.3237, 0.2167, 0.1877]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2726, 0.2650, 0.2088],
        [0.2716, 0.3142, 0.2233, 0.1909],
        [0.2705, 0.3158, 0.2236, 0.1901]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2642, 0.2998, 0.2281, 0.2079],
        [0.2712, 0.3012, 0.2288, 0.1988],
        [0.2742, 0.3039, 0.2276, 0.1943]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2726, 0.3006, 0.2226, 0.2043],
        [0.2701, 0.3037, 0.2245, 0.2016],
        [0.2718, 0.3007, 0.2278, 0.1997]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2622, 0.2942, 0.2375, 0.2061],
        [0.2647, 0.2947, 0.2282, 0.2124],
        [0.2669, 0.2938, 0.2265, 0.2128]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2496, 0.2526, 0.2507],
        [0.2507, 0.2525, 0.2515, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2515, 0.2530, 0.2447],
        [0.2506, 0.2503, 0.2519, 0.2472],
        [0.2530, 0.2543, 0.2475, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2513, 0.2510, 0.2475],
        [0.2512, 0.2529, 0.2479, 0.2479],
        [0.2517, 0.2526, 0.2501, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2512, 0.2479, 0.2492],
        [0.2517, 0.2518, 0.2489, 0.2476],
        [0.2518, 0.2522, 0.2492, 0.2468]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2521, 0.2489, 0.2472],
        [0.2511, 0.2519, 0.2483, 0.2487],
        [0.2514, 0.2526, 0.2488, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2508, 0.2484, 0.2492],
        [0.2509, 0.2508, 0.2493, 0.2490],
        [0.2513, 0.2520, 0.2496, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2489, 0.2505, 0.2504],
        [0.2509, 0.2491, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2490, 0.2501, 0.2511],
        [0.2501, 0.2483, 0.2494, 0.2522],
        [0.2501, 0.2504, 0.2517, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2538, 0.2531, 0.2449],
        [0.2497, 0.2491, 0.2494, 0.2518],
        [0.2487, 0.2496, 0.2501, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2516, 0.2534, 0.2475],
        [0.2502, 0.2487, 0.2520, 0.2490],
        [0.2501, 0.2489, 0.2482, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2486, 0.2543, 0.2508],
        [0.2474, 0.2502, 0.2509, 0.2515],
        [0.2446, 0.2516, 0.2553, 0.2485]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2527, 0.2557, 0.2417],
        [0.2475, 0.2509, 0.2549, 0.2466],
        [0.2486, 0.2454, 0.2452, 0.2608]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:37:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 1.0328 (0.9613)	Prec@(1,5) (66.1%, 96.9%)	
05/30 10:38:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 1.0471 (0.9718)	Prec@(1,5) (65.6%, 96.9%)	
05/30 10:38:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 0.7035 (0.9542)	Prec@(1,5) (66.1%, 96.9%)	
05/30 10:38:36午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 0.8198 (0.9531)	Prec@(1,5) (66.1%, 97.0%)	
05/30 10:38:52午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 0.9334 (0.9563)	Prec@(1,5) (66.0%, 96.9%)	
05/30 10:39:08午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 0.9481 (0.9557)	Prec@(1,5) (66.3%, 96.9%)	
05/30 10:39:24午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 1.0797 (0.9491)	Prec@(1,5) (66.7%, 96.9%)	
05/30 10:39:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 0.7941 (0.9430)	Prec@(1,5) (66.9%, 96.9%)	
05/30 10:39:38午後 searchStage_trainer.py:221 [INFO] Train: [  7/49] Final Prec@1 66.8840%
05/30 10:39:40午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 1.0612	Prec@(1,5) (63.1%, 96.1%)
05/30 10:39:42午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 1.0784	Prec@(1,5) (62.7%, 96.0%)
05/30 10:39:44午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 1.0773	Prec@(1,5) (62.6%, 96.0%)
05/30 10:39:47午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 1.0820	Prec@(1,5) (62.4%, 96.0%)
05/30 10:39:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 1.0868	Prec@(1,5) (62.5%, 95.9%)
05/30 10:39:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 1.0876	Prec@(1,5) (62.4%, 95.8%)
05/30 10:39:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 1.0916	Prec@(1,5) (62.2%, 95.8%)
05/30 10:39:55午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 1.0929	Prec@(1,5) (62.2%, 95.8%)
05/30 10:39:55午後 searchStage_trainer.py:260 [INFO] Valid: [  7/49] Final Prec@1 62.1640%
05/30 10:39:55午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:39:56午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 62.1640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2484, 0.3029, 0.2572, 0.1915],
        [0.2592, 0.3077, 0.2480, 0.1851]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2966, 0.2567, 0.1994],
        [0.2573, 0.2879, 0.2602, 0.1946],
        [0.2741, 0.3280, 0.2156, 0.1823]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2766, 0.2650, 0.2032],
        [0.2742, 0.3176, 0.2221, 0.1860],
        [0.2724, 0.3196, 0.2224, 0.1856]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2670, 0.3034, 0.2260, 0.2036],
        [0.2731, 0.3037, 0.2272, 0.1961],
        [0.2770, 0.3066, 0.2275, 0.1890]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2749, 0.3046, 0.2212, 0.1993],
        [0.2725, 0.3070, 0.2232, 0.1973],
        [0.2735, 0.3034, 0.2269, 0.1962]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2980, 0.2358, 0.2024],
        [0.2661, 0.2979, 0.2273, 0.2088],
        [0.2684, 0.2969, 0.2260, 0.2087]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2496, 0.2526, 0.2508],
        [0.2509, 0.2524, 0.2516, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2516, 0.2529, 0.2444],
        [0.2506, 0.2502, 0.2521, 0.2472],
        [0.2530, 0.2543, 0.2474, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2516, 0.2508, 0.2472],
        [0.2513, 0.2529, 0.2476, 0.2482],
        [0.2518, 0.2527, 0.2502, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2512, 0.2478, 0.2493],
        [0.2517, 0.2520, 0.2489, 0.2474],
        [0.2518, 0.2521, 0.2492, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2521, 0.2490, 0.2471],
        [0.2512, 0.2519, 0.2482, 0.2487],
        [0.2514, 0.2526, 0.2487, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2508, 0.2485, 0.2491],
        [0.2510, 0.2507, 0.2492, 0.2491],
        [0.2513, 0.2521, 0.2496, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2489, 0.2505, 0.2504],
        [0.2510, 0.2490, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2489, 0.2500, 0.2512],
        [0.2501, 0.2481, 0.2494, 0.2523],
        [0.2502, 0.2504, 0.2517, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2540, 0.2533, 0.2445],
        [0.2497, 0.2490, 0.2493, 0.2519],
        [0.2487, 0.2496, 0.2501, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2517, 0.2536, 0.2473],
        [0.2502, 0.2487, 0.2521, 0.2490],
        [0.2501, 0.2488, 0.2482, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2485, 0.2545, 0.2509],
        [0.2473, 0.2502, 0.2509, 0.2515],
        [0.2444, 0.2517, 0.2555, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2529, 0.2561, 0.2411],
        [0.2474, 0.2509, 0.2552, 0.2465],
        [0.2486, 0.2452, 0.2450, 0.2613]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:40:13午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 0.8500 (0.9252)	Prec@(1,5) (67.8%, 97.0%)	
05/30 10:40:29午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 0.9531 (0.9096)	Prec@(1,5) (68.0%, 97.3%)	
05/30 10:40:45午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 0.8366 (0.9013)	Prec@(1,5) (68.2%, 97.4%)	
05/30 10:41:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 0.7439 (0.9022)	Prec@(1,5) (68.3%, 97.2%)	
05/30 10:41:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 0.9117 (0.8946)	Prec@(1,5) (68.7%, 97.3%)	
05/30 10:41:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 0.7852 (0.8888)	Prec@(1,5) (68.9%, 97.4%)	
05/30 10:41:50午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 0.8769 (0.8925)	Prec@(1,5) (68.8%, 97.3%)	
05/30 10:42:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 0.8854 (0.8929)	Prec@(1,5) (68.8%, 97.3%)	
05/30 10:42:04午後 searchStage_trainer.py:221 [INFO] Train: [  8/49] Final Prec@1 68.7880%
05/30 10:42:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 0.8634	Prec@(1,5) (69.2%, 97.7%)
05/30 10:42:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 0.8627	Prec@(1,5) (69.7%, 97.5%)
05/30 10:42:10午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 0.8639	Prec@(1,5) (69.7%, 97.6%)
05/30 10:42:13午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 0.8670	Prec@(1,5) (69.8%, 97.4%)
05/30 10:42:15午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 0.8720	Prec@(1,5) (69.6%, 97.4%)
05/30 10:42:17午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 0.8737	Prec@(1,5) (69.4%, 97.4%)
05/30 10:42:19午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 0.8734	Prec@(1,5) (69.4%, 97.4%)
05/30 10:42:21午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 0.8724	Prec@(1,5) (69.4%, 97.3%)
05/30 10:42:21午後 searchStage_trainer.py:260 [INFO] Valid: [  8/49] Final Prec@1 69.4280%
05/30 10:42:21午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:42:22午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 69.4280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2505, 0.3073, 0.2568, 0.1854],
        [0.2618, 0.3116, 0.2476, 0.1790]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.3007, 0.2584, 0.1941],
        [0.2580, 0.2917, 0.2604, 0.1899],
        [0.2757, 0.3308, 0.2148, 0.1787]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2782, 0.2669, 0.1993],
        [0.2754, 0.3199, 0.2221, 0.1826],
        [0.2737, 0.3217, 0.2222, 0.1825]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2683, 0.3061, 0.2260, 0.1996],
        [0.2746, 0.3061, 0.2269, 0.1924],
        [0.2784, 0.3081, 0.2264, 0.1871]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2770, 0.3075, 0.2210, 0.1945],
        [0.2742, 0.3093, 0.2229, 0.1937],
        [0.2746, 0.3048, 0.2257, 0.1950]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2651, 0.3006, 0.2354, 0.1989],
        [0.2666, 0.3000, 0.2267, 0.2067],
        [0.2691, 0.2989, 0.2261, 0.2060]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2494, 0.2526, 0.2507],
        [0.2509, 0.2523, 0.2516, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2519, 0.2529, 0.2441],
        [0.2507, 0.2499, 0.2522, 0.2473],
        [0.2531, 0.2542, 0.2474, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2518, 0.2509, 0.2466],
        [0.2512, 0.2528, 0.2477, 0.2482],
        [0.2517, 0.2526, 0.2500, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2512, 0.2477, 0.2494],
        [0.2517, 0.2520, 0.2490, 0.2472],
        [0.2518, 0.2521, 0.2491, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2522, 0.2490, 0.2470],
        [0.2512, 0.2519, 0.2481, 0.2488],
        [0.2514, 0.2527, 0.2488, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2508, 0.2484, 0.2491],
        [0.2510, 0.2508, 0.2492, 0.2491],
        [0.2514, 0.2520, 0.2495, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2488, 0.2505, 0.2504],
        [0.2510, 0.2490, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2489, 0.2500, 0.2513],
        [0.2501, 0.2480, 0.2495, 0.2524],
        [0.2502, 0.2505, 0.2518, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2542, 0.2535, 0.2442],
        [0.2497, 0.2490, 0.2493, 0.2520],
        [0.2486, 0.2496, 0.2501, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2518, 0.2538, 0.2473],
        [0.2502, 0.2487, 0.2521, 0.2490],
        [0.2501, 0.2488, 0.2481, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2484, 0.2547, 0.2510],
        [0.2472, 0.2503, 0.2510, 0.2515],
        [0.2443, 0.2517, 0.2556, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2530, 0.2564, 0.2408],
        [0.2473, 0.2510, 0.2554, 0.2463],
        [0.2486, 0.2450, 0.2448, 0.2616]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:42:38午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 0.8144 (0.8486)	Prec@(1,5) (70.4%, 97.8%)	
05/30 10:42:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 0.8975 (0.8297)	Prec@(1,5) (70.8%, 97.7%)	
05/30 10:43:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 0.7368 (0.8255)	Prec@(1,5) (71.2%, 97.7%)	
05/30 10:43:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 1.0214 (0.8232)	Prec@(1,5) (71.3%, 97.7%)	
05/30 10:43:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 0.7606 (0.8283)	Prec@(1,5) (71.1%, 97.7%)	
05/30 10:44:00午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 0.8507 (0.8229)	Prec@(1,5) (71.3%, 97.7%)	
05/30 10:44:16午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 0.7805 (0.8236)	Prec@(1,5) (71.2%, 97.7%)	
05/30 10:44:29午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 0.9063 (0.8285)	Prec@(1,5) (71.1%, 97.7%)	
05/30 10:44:30午後 searchStage_trainer.py:221 [INFO] Train: [  9/49] Final Prec@1 71.1040%
05/30 10:44:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 0.9115	Prec@(1,5) (68.2%, 96.9%)
05/30 10:44:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 0.9282	Prec@(1,5) (67.7%, 97.0%)
05/30 10:44:37午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 0.9343	Prec@(1,5) (67.4%, 97.1%)
05/30 10:44:39午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 0.9390	Prec@(1,5) (67.4%, 97.1%)
05/30 10:44:41午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 0.9377	Prec@(1,5) (67.3%, 97.0%)
05/30 10:44:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 0.9379	Prec@(1,5) (67.3%, 97.0%)
05/30 10:44:46午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 0.9387	Prec@(1,5) (67.3%, 97.0%)
05/30 10:44:47午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 0.9385	Prec@(1,5) (67.3%, 97.0%)
05/30 10:44:47午後 searchStage_trainer.py:260 [INFO] Valid: [  9/49] Final Prec@1 67.2560%
05/30 10:44:47午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:44:48午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 69.4280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2508, 0.3103, 0.2575, 0.1815],
        [0.2630, 0.3151, 0.2478, 0.1742]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.3035, 0.2590, 0.1894],
        [0.2585, 0.2949, 0.2612, 0.1855],
        [0.2765, 0.3322, 0.2140, 0.1772]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2575, 0.2800, 0.2674, 0.1951],
        [0.2770, 0.3209, 0.2213, 0.1808],
        [0.2744, 0.3231, 0.2217, 0.1808]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2700, 0.3080, 0.2248, 0.1972],
        [0.2749, 0.3077, 0.2275, 0.1899],
        [0.2792, 0.3089, 0.2258, 0.1861]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2779, 0.3093, 0.2199, 0.1929],
        [0.2750, 0.3111, 0.2230, 0.1909],
        [0.2747, 0.3057, 0.2261, 0.1934]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2660, 0.3023, 0.2350, 0.1967],
        [0.2673, 0.3015, 0.2265, 0.2047],
        [0.2698, 0.3001, 0.2256, 0.2044]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2492, 0.2527, 0.2509],
        [0.2510, 0.2524, 0.2517, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2519, 0.2529, 0.2441],
        [0.2509, 0.2499, 0.2520, 0.2472],
        [0.2530, 0.2542, 0.2474, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2519, 0.2509, 0.2465],
        [0.2512, 0.2528, 0.2477, 0.2483],
        [0.2517, 0.2526, 0.2501, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2511, 0.2476, 0.2495],
        [0.2518, 0.2520, 0.2489, 0.2472],
        [0.2518, 0.2521, 0.2492, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2522, 0.2491, 0.2469],
        [0.2512, 0.2519, 0.2481, 0.2488],
        [0.2514, 0.2526, 0.2487, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2507, 0.2484, 0.2493],
        [0.2510, 0.2507, 0.2492, 0.2490],
        [0.2514, 0.2520, 0.2495, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2488, 0.2505, 0.2504],
        [0.2511, 0.2489, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2488, 0.2500, 0.2514],
        [0.2501, 0.2480, 0.2494, 0.2525],
        [0.2502, 0.2505, 0.2518, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2544, 0.2537, 0.2440],
        [0.2497, 0.2490, 0.2492, 0.2521],
        [0.2486, 0.2496, 0.2501, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2518, 0.2539, 0.2472],
        [0.2502, 0.2486, 0.2521, 0.2490],
        [0.2501, 0.2488, 0.2480, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2484, 0.2549, 0.2511],
        [0.2471, 0.2503, 0.2511, 0.2515],
        [0.2442, 0.2518, 0.2557, 0.2484]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2531, 0.2566, 0.2404],
        [0.2473, 0.2510, 0.2555, 0.2462],
        [0.2486, 0.2449, 0.2447, 0.2619]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:45:04午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 0.8380 (0.7454)	Prec@(1,5) (74.1%, 97.8%)	
05/30 10:45:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 0.9530 (0.7604)	Prec@(1,5) (73.5%, 97.8%)	
05/30 10:45:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 0.7506 (0.7691)	Prec@(1,5) (73.1%, 97.9%)	
05/30 10:45:53午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 0.7154 (0.7739)	Prec@(1,5) (72.8%, 97.9%)	
05/30 10:46:09午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 0.7031 (0.7662)	Prec@(1,5) (73.2%, 98.0%)	
05/30 10:46:25午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 0.7470 (0.7749)	Prec@(1,5) (72.9%, 98.0%)	
05/30 10:46:41午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 0.7493 (0.7774)	Prec@(1,5) (72.9%, 97.9%)	
05/30 10:46:54午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 0.7555 (0.7789)	Prec@(1,5) (72.9%, 98.0%)	
05/30 10:46:55午後 searchStage_trainer.py:221 [INFO] Train: [ 10/49] Final Prec@1 72.8960%
05/30 10:46:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 0.9834	Prec@(1,5) (67.5%, 96.0%)
05/30 10:46:59午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 0.9674	Prec@(1,5) (67.9%, 96.0%)
05/30 10:47:01午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 0.9485	Prec@(1,5) (68.4%, 96.3%)
05/30 10:47:04午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 0.9465	Prec@(1,5) (68.3%, 96.3%)
05/30 10:47:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 0.9490	Prec@(1,5) (68.2%, 96.3%)
05/30 10:47:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 0.9400	Prec@(1,5) (68.4%, 96.4%)
05/30 10:47:10午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 0.9371	Prec@(1,5) (68.5%, 96.4%)
05/30 10:47:12午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 0.9407	Prec@(1,5) (68.3%, 96.4%)
05/30 10:47:12午後 searchStage_trainer.py:260 [INFO] Valid: [ 10/49] Final Prec@1 68.3240%
05/30 10:47:12午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:47:13午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 69.4280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2519, 0.3119, 0.2579, 0.1783],
        [0.2637, 0.3166, 0.2482, 0.1715]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.3056, 0.2602, 0.1859],
        [0.2604, 0.2962, 0.2611, 0.1824],
        [0.2772, 0.3328, 0.2136, 0.1764]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2596, 0.2823, 0.2675, 0.1906],
        [0.2775, 0.3212, 0.2212, 0.1801],
        [0.2751, 0.3235, 0.2212, 0.1802]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2709, 0.3091, 0.2247, 0.1953],
        [0.2759, 0.3080, 0.2267, 0.1894],
        [0.2796, 0.3094, 0.2259, 0.1851]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2788, 0.3107, 0.2199, 0.1905],
        [0.2758, 0.3117, 0.2222, 0.1903],
        [0.2749, 0.3060, 0.2267, 0.1924]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2669, 0.3034, 0.2346, 0.1950],
        [0.2675, 0.3024, 0.2267, 0.2034],
        [0.2699, 0.3004, 0.2259, 0.2038]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2491, 0.2527, 0.2511],
        [0.2509, 0.2526, 0.2518, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2521, 0.2529, 0.2440],
        [0.2509, 0.2499, 0.2521, 0.2471],
        [0.2531, 0.2542, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2518, 0.2508, 0.2465],
        [0.2512, 0.2528, 0.2477, 0.2483],
        [0.2517, 0.2526, 0.2502, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2510, 0.2476, 0.2496],
        [0.2518, 0.2520, 0.2490, 0.2472],
        [0.2518, 0.2521, 0.2492, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2491, 0.2469],
        [0.2512, 0.2519, 0.2481, 0.2488],
        [0.2514, 0.2526, 0.2487, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2507, 0.2485, 0.2492],
        [0.2510, 0.2507, 0.2493, 0.2490],
        [0.2514, 0.2520, 0.2495, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2488, 0.2505, 0.2504],
        [0.2511, 0.2489, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2488, 0.2499, 0.2515],
        [0.2502, 0.2479, 0.2494, 0.2525],
        [0.2502, 0.2505, 0.2518, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2546, 0.2538, 0.2437],
        [0.2497, 0.2490, 0.2492, 0.2521],
        [0.2486, 0.2496, 0.2501, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2519, 0.2541, 0.2471],
        [0.2503, 0.2486, 0.2521, 0.2490],
        [0.2502, 0.2487, 0.2480, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2483, 0.2550, 0.2511],
        [0.2470, 0.2503, 0.2511, 0.2515],
        [0.2441, 0.2518, 0.2558, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2532, 0.2568, 0.2401],
        [0.2472, 0.2510, 0.2556, 0.2462],
        [0.2486, 0.2448, 0.2446, 0.2620]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:47:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 0.7958 (0.7435)	Prec@(1,5) (74.4%, 98.2%)	
05/30 10:47:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 0.7191 (0.7200)	Prec@(1,5) (75.3%, 98.3%)	
05/30 10:48:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 0.8388 (0.7232)	Prec@(1,5) (75.0%, 98.3%)	
05/30 10:48:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 0.6131 (0.7245)	Prec@(1,5) (75.0%, 98.3%)	
05/30 10:48:35午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 0.8635 (0.7335)	Prec@(1,5) (74.6%, 98.2%)	
05/30 10:48:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 0.5925 (0.7338)	Prec@(1,5) (74.6%, 98.2%)	
05/30 10:49:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 0.7218 (0.7368)	Prec@(1,5) (74.5%, 98.2%)	
05/30 10:49:20午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 1.0817 (0.7357)	Prec@(1,5) (74.5%, 98.2%)	
05/30 10:49:20午後 searchStage_trainer.py:221 [INFO] Train: [ 11/49] Final Prec@1 74.5120%
05/30 10:49:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 0.7748	Prec@(1,5) (73.3%, 97.5%)
05/30 10:49:25午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 0.7933	Prec@(1,5) (72.3%, 97.6%)
05/30 10:49:27午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 0.7928	Prec@(1,5) (72.5%, 97.7%)
05/30 10:49:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 0.7999	Prec@(1,5) (72.3%, 97.7%)
05/30 10:49:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 0.7952	Prec@(1,5) (72.6%, 97.7%)
05/30 10:49:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 0.7928	Prec@(1,5) (72.7%, 97.7%)
05/30 10:49:36午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 0.7883	Prec@(1,5) (72.9%, 97.7%)
05/30 10:49:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 0.7891	Prec@(1,5) (72.9%, 97.8%)
05/30 10:49:38午後 searchStage_trainer.py:260 [INFO] Valid: [ 11/49] Final Prec@1 72.8640%
05/30 10:49:38午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:49:39午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 72.8640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2528, 0.3131, 0.2577, 0.1763],
        [0.2640, 0.3174, 0.2488, 0.1699]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.3064, 0.2608, 0.1840],
        [0.2612, 0.2970, 0.2618, 0.1800],
        [0.2773, 0.3329, 0.2138, 0.1759]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2839, 0.2682, 0.1870],
        [0.2780, 0.3212, 0.2210, 0.1798],
        [0.2756, 0.3232, 0.2210, 0.1802]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2714, 0.3095, 0.2247, 0.1944],
        [0.2765, 0.3082, 0.2267, 0.1886],
        [0.2797, 0.3095, 0.2259, 0.1850]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2792, 0.3117, 0.2201, 0.1890],
        [0.2758, 0.3122, 0.2217, 0.1903],
        [0.2750, 0.3062, 0.2271, 0.1918]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2676, 0.3045, 0.2349, 0.1931],
        [0.2676, 0.3023, 0.2266, 0.2035],
        [0.2701, 0.3004, 0.2261, 0.2035]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2491, 0.2528, 0.2511],
        [0.2509, 0.2525, 0.2518, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2521, 0.2527, 0.2441],
        [0.2509, 0.2500, 0.2521, 0.2471],
        [0.2531, 0.2542, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2519, 0.2510, 0.2464],
        [0.2512, 0.2528, 0.2477, 0.2482],
        [0.2516, 0.2526, 0.2501, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2511, 0.2476, 0.2497],
        [0.2518, 0.2520, 0.2490, 0.2472],
        [0.2518, 0.2521, 0.2492, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2524, 0.2491, 0.2468],
        [0.2512, 0.2519, 0.2481, 0.2488],
        [0.2514, 0.2526, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2508, 0.2485, 0.2492],
        [0.2510, 0.2507, 0.2493, 0.2490],
        [0.2514, 0.2520, 0.2495, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2488, 0.2505, 0.2504],
        [0.2512, 0.2488, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2487, 0.2498, 0.2516],
        [0.2502, 0.2479, 0.2494, 0.2526],
        [0.2502, 0.2505, 0.2518, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2547, 0.2540, 0.2435],
        [0.2497, 0.2490, 0.2491, 0.2522],
        [0.2486, 0.2496, 0.2500, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2520, 0.2542, 0.2470],
        [0.2503, 0.2486, 0.2521, 0.2490],
        [0.2502, 0.2487, 0.2479, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2483, 0.2551, 0.2511],
        [0.2470, 0.2503, 0.2512, 0.2515],
        [0.2440, 0.2518, 0.2559, 0.2483]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2533, 0.2570, 0.2398],
        [0.2472, 0.2510, 0.2557, 0.2461],
        [0.2486, 0.2448, 0.2445, 0.2622]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:49:56午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 0.6663 (0.6935)	Prec@(1,5) (76.0%, 98.2%)	
05/30 10:50:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 0.7024 (0.7008)	Prec@(1,5) (75.5%, 98.2%)	
05/30 10:50:28午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 0.6991 (0.7042)	Prec@(1,5) (75.4%, 98.1%)	
05/30 10:50:44午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 0.8915 (0.7085)	Prec@(1,5) (75.3%, 98.1%)	
05/30 10:51:00午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 0.5752 (0.7094)	Prec@(1,5) (75.2%, 98.1%)	
05/30 10:51:17午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 0.5265 (0.7066)	Prec@(1,5) (75.3%, 98.2%)	
05/30 10:51:33午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 0.5652 (0.7045)	Prec@(1,5) (75.4%, 98.2%)	
05/30 10:51:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 0.5506 (0.7033)	Prec@(1,5) (75.5%, 98.3%)	
05/30 10:51:46午後 searchStage_trainer.py:221 [INFO] Train: [ 12/49] Final Prec@1 75.5440%
05/30 10:51:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 0.8437	Prec@(1,5) (70.5%, 97.6%)
05/30 10:51:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 0.8539	Prec@(1,5) (70.6%, 97.6%)
05/30 10:51:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 0.8672	Prec@(1,5) (70.3%, 97.5%)
05/30 10:51:55午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 0.8736	Prec@(1,5) (70.2%, 97.5%)
05/30 10:51:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 0.8690	Prec@(1,5) (70.3%, 97.6%)
05/30 10:52:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 0.8650	Prec@(1,5) (70.5%, 97.6%)
05/30 10:52:02午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 0.8650	Prec@(1,5) (70.4%, 97.5%)
05/30 10:52:04午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 0.8637	Prec@(1,5) (70.4%, 97.5%)
05/30 10:52:04午後 searchStage_trainer.py:260 [INFO] Valid: [ 12/49] Final Prec@1 70.4040%
05/30 10:52:04午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:52:04午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 72.8640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2531, 0.3133, 0.2581, 0.1755],
        [0.2643, 0.3177, 0.2496, 0.1684]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.3074, 0.2616, 0.1820],
        [0.2615, 0.2972, 0.2619, 0.1795],
        [0.2775, 0.3326, 0.2140, 0.1759]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2621, 0.2845, 0.2686, 0.1848],
        [0.2783, 0.3209, 0.2202, 0.1806],
        [0.2758, 0.3231, 0.2215, 0.1796]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2717, 0.3095, 0.2241, 0.1947],
        [0.2768, 0.3082, 0.2270, 0.1880],
        [0.2798, 0.3092, 0.2261, 0.1849]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2800, 0.3121, 0.2201, 0.1878],
        [0.2758, 0.3121, 0.2218, 0.1903],
        [0.2750, 0.3059, 0.2273, 0.1918]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2685, 0.3053, 0.2346, 0.1916],
        [0.2679, 0.3024, 0.2268, 0.2030],
        [0.2700, 0.3000, 0.2259, 0.2042]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2491, 0.2528, 0.2511],
        [0.2509, 0.2525, 0.2518, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2521, 0.2527, 0.2443],
        [0.2508, 0.2500, 0.2521, 0.2471],
        [0.2530, 0.2542, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2520, 0.2510, 0.2463],
        [0.2512, 0.2528, 0.2476, 0.2483],
        [0.2516, 0.2525, 0.2502, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2511, 0.2476, 0.2496],
        [0.2518, 0.2520, 0.2490, 0.2472],
        [0.2518, 0.2521, 0.2492, 0.2469]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2524, 0.2491, 0.2468],
        [0.2512, 0.2519, 0.2481, 0.2488],
        [0.2514, 0.2526, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2508, 0.2485, 0.2492],
        [0.2510, 0.2507, 0.2493, 0.2490],
        [0.2514, 0.2519, 0.2495, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2488, 0.2505, 0.2505],
        [0.2512, 0.2488, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2487, 0.2498, 0.2516],
        [0.2502, 0.2478, 0.2494, 0.2526],
        [0.2502, 0.2505, 0.2518, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2548, 0.2541, 0.2434],
        [0.2497, 0.2489, 0.2491, 0.2522],
        [0.2486, 0.2496, 0.2500, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2520, 0.2542, 0.2469],
        [0.2503, 0.2486, 0.2521, 0.2491],
        [0.2502, 0.2487, 0.2479, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2454, 0.2482, 0.2552, 0.2512],
        [0.2470, 0.2503, 0.2512, 0.2515],
        [0.2440, 0.2518, 0.2559, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2534, 0.2571, 0.2396],
        [0.2472, 0.2510, 0.2558, 0.2461],
        [0.2486, 0.2447, 0.2444, 0.2623]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:52:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 0.6870 (0.6510)	Prec@(1,5) (77.2%, 98.3%)	
05/30 10:52:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 0.6022 (0.6481)	Prec@(1,5) (77.3%, 98.4%)	
05/30 10:52:53午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 0.7793 (0.6544)	Prec@(1,5) (77.1%, 98.4%)	
05/30 10:53:10午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 0.6730 (0.6620)	Prec@(1,5) (77.0%, 98.4%)	
05/30 10:53:26午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 0.9523 (0.6669)	Prec@(1,5) (76.8%, 98.4%)	
05/30 10:53:42午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 0.6010 (0.6687)	Prec@(1,5) (76.7%, 98.5%)	
05/30 10:53:58午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 0.8094 (0.6673)	Prec@(1,5) (76.7%, 98.5%)	
05/30 10:54:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 0.6900 (0.6724)	Prec@(1,5) (76.5%, 98.5%)	
05/30 10:54:12午後 searchStage_trainer.py:221 [INFO] Train: [ 13/49] Final Prec@1 76.5280%
05/30 10:54:14午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 0.7347	Prec@(1,5) (74.6%, 98.2%)
05/30 10:54:16午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 0.7371	Prec@(1,5) (74.7%, 98.2%)
05/30 10:54:18午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 0.7420	Prec@(1,5) (74.2%, 98.2%)
05/30 10:54:21午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 0.7376	Prec@(1,5) (74.2%, 98.3%)
05/30 10:54:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 0.7401	Prec@(1,5) (74.2%, 98.1%)
05/30 10:54:25午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 0.7411	Prec@(1,5) (74.0%, 98.1%)
05/30 10:54:27午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 0.7403	Prec@(1,5) (74.1%, 98.1%)
05/30 10:54:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 0.7390	Prec@(1,5) (74.2%, 98.1%)
05/30 10:54:29午後 searchStage_trainer.py:260 [INFO] Valid: [ 13/49] Final Prec@1 74.2200%
05/30 10:54:29午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:54:29午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 74.2200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2536, 0.3135, 0.2579, 0.1751],
        [0.2648, 0.3179, 0.2495, 0.1678]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.3077, 0.2620, 0.1806],
        [0.2620, 0.2969, 0.2618, 0.1793],
        [0.2777, 0.3322, 0.2142, 0.1759]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2627, 0.2851, 0.2690, 0.1831],
        [0.2784, 0.3206, 0.2200, 0.1810],
        [0.2759, 0.3228, 0.2216, 0.1796]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2719, 0.3095, 0.2243, 0.1942],
        [0.2767, 0.3081, 0.2271, 0.1881],
        [0.2797, 0.3089, 0.2263, 0.1851]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2800, 0.3123, 0.2204, 0.1874],
        [0.2759, 0.3119, 0.2221, 0.1901],
        [0.2750, 0.3055, 0.2274, 0.1921]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.3053, 0.2346, 0.1913],
        [0.2678, 0.3021, 0.2269, 0.2032],
        [0.2699, 0.2996, 0.2262, 0.2043]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2490, 0.2528, 0.2511],
        [0.2509, 0.2525, 0.2518, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2520, 0.2527, 0.2444],
        [0.2508, 0.2500, 0.2522, 0.2470],
        [0.2530, 0.2542, 0.2474, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2521, 0.2510, 0.2462],
        [0.2513, 0.2527, 0.2476, 0.2484],
        [0.2516, 0.2525, 0.2502, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2511, 0.2476, 0.2496],
        [0.2518, 0.2520, 0.2490, 0.2472],
        [0.2518, 0.2521, 0.2492, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2490, 0.2468],
        [0.2511, 0.2518, 0.2481, 0.2489],
        [0.2514, 0.2526, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2507, 0.2485, 0.2492],
        [0.2510, 0.2507, 0.2494, 0.2490],
        [0.2514, 0.2519, 0.2495, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2488, 0.2505, 0.2505],
        [0.2512, 0.2488, 0.2501, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2487, 0.2498, 0.2516],
        [0.2502, 0.2477, 0.2494, 0.2527],
        [0.2502, 0.2505, 0.2518, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2549, 0.2542, 0.2432],
        [0.2497, 0.2489, 0.2491, 0.2522],
        [0.2486, 0.2496, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2520, 0.2543, 0.2469],
        [0.2503, 0.2485, 0.2521, 0.2490],
        [0.2502, 0.2487, 0.2479, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2482, 0.2553, 0.2512],
        [0.2469, 0.2503, 0.2512, 0.2515],
        [0.2440, 0.2519, 0.2559, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2535, 0.2573, 0.2394],
        [0.2471, 0.2510, 0.2558, 0.2460],
        [0.2486, 0.2447, 0.2443, 0.2624]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:54:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 0.4562 (0.6326)	Prec@(1,5) (78.3%, 98.8%)	
05/30 10:55:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 0.7047 (0.6313)	Prec@(1,5) (78.3%, 98.6%)	
05/30 10:55:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 0.6322 (0.6343)	Prec@(1,5) (77.9%, 98.6%)	
05/30 10:55:35午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 0.5987 (0.6387)	Prec@(1,5) (78.1%, 98.5%)	
05/30 10:55:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 0.8014 (0.6428)	Prec@(1,5) (77.8%, 98.6%)	
05/30 10:56:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 0.7128 (0.6432)	Prec@(1,5) (77.8%, 98.6%)	
05/30 10:56:24午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 0.8487 (0.6425)	Prec@(1,5) (77.7%, 98.6%)	
05/30 10:56:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 0.7289 (0.6403)	Prec@(1,5) (77.8%, 98.6%)	
05/30 10:56:37午後 searchStage_trainer.py:221 [INFO] Train: [ 14/49] Final Prec@1 77.8360%
05/30 10:56:40午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 0.8138	Prec@(1,5) (71.0%, 97.6%)
05/30 10:56:42午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 0.8281	Prec@(1,5) (71.5%, 97.3%)
05/30 10:56:44午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 0.8250	Prec@(1,5) (71.8%, 97.4%)
05/30 10:56:46午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 0.8254	Prec@(1,5) (72.0%, 97.4%)
05/30 10:56:48午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 0.8296	Prec@(1,5) (71.9%, 97.4%)
05/30 10:56:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 0.8276	Prec@(1,5) (72.1%, 97.3%)
05/30 10:56:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 0.8234	Prec@(1,5) (72.2%, 97.3%)
05/30 10:56:55午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 0.8268	Prec@(1,5) (72.1%, 97.2%)
05/30 10:56:55午後 searchStage_trainer.py:260 [INFO] Valid: [ 14/49] Final Prec@1 72.1640%
05/30 10:56:55午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:56:55午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 74.2200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2538, 0.3132, 0.2582, 0.1748],
        [0.2646, 0.3176, 0.2501, 0.1677]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.3077, 0.2622, 0.1804],
        [0.2620, 0.2970, 0.2623, 0.1788],
        [0.2775, 0.3318, 0.2146, 0.1762]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2635, 0.2861, 0.2695, 0.1808],
        [0.2783, 0.3202, 0.2199, 0.1815],
        [0.2758, 0.3225, 0.2217, 0.1801]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2724, 0.3092, 0.2239, 0.1945],
        [0.2768, 0.3078, 0.2274, 0.1880],
        [0.2796, 0.3085, 0.2265, 0.1855]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2801, 0.3122, 0.2202, 0.1876],
        [0.2758, 0.3118, 0.2225, 0.1899],
        [0.2749, 0.3050, 0.2275, 0.1925]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2691, 0.3052, 0.2346, 0.1911],
        [0.2680, 0.3018, 0.2269, 0.2034],
        [0.2698, 0.2991, 0.2264, 0.2047]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2490, 0.2528, 0.2511],
        [0.2508, 0.2525, 0.2518, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2520, 0.2526, 0.2445],
        [0.2508, 0.2500, 0.2522, 0.2470],
        [0.2530, 0.2542, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2520, 0.2510, 0.2463],
        [0.2512, 0.2528, 0.2476, 0.2484],
        [0.2516, 0.2525, 0.2502, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2511, 0.2476, 0.2496],
        [0.2518, 0.2519, 0.2490, 0.2472],
        [0.2518, 0.2521, 0.2492, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2490, 0.2468],
        [0.2511, 0.2518, 0.2480, 0.2490],
        [0.2514, 0.2526, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2507, 0.2485, 0.2492],
        [0.2509, 0.2507, 0.2494, 0.2490],
        [0.2514, 0.2519, 0.2495, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2488, 0.2505, 0.2505],
        [0.2512, 0.2488, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2487, 0.2498, 0.2516],
        [0.2502, 0.2477, 0.2494, 0.2527],
        [0.2502, 0.2505, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2550, 0.2543, 0.2431],
        [0.2497, 0.2489, 0.2491, 0.2522],
        [0.2486, 0.2496, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2520, 0.2543, 0.2469],
        [0.2503, 0.2485, 0.2521, 0.2490],
        [0.2502, 0.2486, 0.2478, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2482, 0.2554, 0.2512],
        [0.2469, 0.2503, 0.2512, 0.2515],
        [0.2439, 0.2519, 0.2560, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2535, 0.2574, 0.2392],
        [0.2471, 0.2510, 0.2559, 0.2460],
        [0.2486, 0.2446, 0.2443, 0.2625]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:57:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 0.5835 (0.5856)	Prec@(1,5) (79.9%, 99.1%)	
05/30 10:57:28午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 0.5564 (0.5918)	Prec@(1,5) (80.1%, 98.9%)	
05/30 10:57:44午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 0.4227 (0.5943)	Prec@(1,5) (79.8%, 98.8%)	
05/30 10:58:01午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 0.7212 (0.5992)	Prec@(1,5) (79.6%, 98.9%)	
05/30 10:58:17午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 0.6468 (0.6007)	Prec@(1,5) (79.4%, 98.9%)	
05/30 10:58:33午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 0.5024 (0.6040)	Prec@(1,5) (79.3%, 98.8%)	
05/30 10:58:49午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 0.8117 (0.6066)	Prec@(1,5) (79.3%, 98.8%)	
05/30 10:59:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 0.7369 (0.6081)	Prec@(1,5) (79.2%, 98.8%)	
05/30 10:59:03午後 searchStage_trainer.py:221 [INFO] Train: [ 15/49] Final Prec@1 79.1560%
05/30 10:59:05午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 0.6940	Prec@(1,5) (75.7%, 98.5%)
05/30 10:59:07午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 0.7093	Prec@(1,5) (75.1%, 98.5%)
05/30 10:59:09午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 0.7000	Prec@(1,5) (75.5%, 98.5%)
05/30 10:59:12午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 0.6992	Prec@(1,5) (75.7%, 98.4%)
05/30 10:59:14午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 0.7043	Prec@(1,5) (75.5%, 98.4%)
05/30 10:59:16午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 0.7050	Prec@(1,5) (75.6%, 98.3%)
05/30 10:59:18午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 0.7034	Prec@(1,5) (75.6%, 98.3%)
05/30 10:59:20午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 0.7045	Prec@(1,5) (75.6%, 98.3%)
05/30 10:59:20午後 searchStage_trainer.py:260 [INFO] Valid: [ 15/49] Final Prec@1 75.5640%
05/30 10:59:20午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 10:59:21午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 75.5640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2539, 0.3130, 0.2580, 0.1750],
        [0.2647, 0.3174, 0.2505, 0.1674]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.3078, 0.2626, 0.1798],
        [0.2625, 0.2966, 0.2626, 0.1782],
        [0.2774, 0.3313, 0.2146, 0.1767]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2635, 0.2868, 0.2699, 0.1798],
        [0.2782, 0.3199, 0.2200, 0.1819],
        [0.2757, 0.3221, 0.2218, 0.1804]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2727, 0.3091, 0.2237, 0.1944],
        [0.2767, 0.3076, 0.2275, 0.1882],
        [0.2797, 0.3081, 0.2264, 0.1858]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2803, 0.3120, 0.2202, 0.1876],
        [0.2757, 0.3115, 0.2228, 0.1900],
        [0.2749, 0.3046, 0.2276, 0.1929]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2690, 0.3051, 0.2344, 0.1916],
        [0.2680, 0.3014, 0.2269, 0.2037],
        [0.2696, 0.2987, 0.2267, 0.2050]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2490, 0.2528, 0.2511],
        [0.2508, 0.2524, 0.2519, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2519, 0.2525, 0.2446],
        [0.2509, 0.2500, 0.2521, 0.2470],
        [0.2530, 0.2542, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2521, 0.2510, 0.2462],
        [0.2512, 0.2527, 0.2476, 0.2484],
        [0.2516, 0.2525, 0.2502, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2511, 0.2477, 0.2496],
        [0.2518, 0.2519, 0.2491, 0.2472],
        [0.2518, 0.2520, 0.2492, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2491, 0.2468],
        [0.2511, 0.2518, 0.2480, 0.2490],
        [0.2513, 0.2525, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2507, 0.2485, 0.2492],
        [0.2509, 0.2506, 0.2494, 0.2491],
        [0.2514, 0.2519, 0.2495, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2488, 0.2504, 0.2505],
        [0.2513, 0.2488, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2487, 0.2498, 0.2517],
        [0.2502, 0.2477, 0.2493, 0.2528],
        [0.2502, 0.2505, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2551, 0.2543, 0.2430],
        [0.2497, 0.2489, 0.2491, 0.2522],
        [0.2486, 0.2496, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2521, 0.2544, 0.2468],
        [0.2503, 0.2485, 0.2521, 0.2490],
        [0.2502, 0.2486, 0.2478, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2481, 0.2555, 0.2512],
        [0.2469, 0.2503, 0.2512, 0.2515],
        [0.2439, 0.2519, 0.2560, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2536, 0.2575, 0.2391],
        [0.2471, 0.2510, 0.2559, 0.2460],
        [0.2486, 0.2446, 0.2442, 0.2626]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 10:59:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 0.5579 (0.5554)	Prec@(1,5) (80.8%, 99.1%)	
05/30 10:59:54午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 0.3984 (0.5569)	Prec@(1,5) (80.6%, 99.3%)	
05/30 11:00:10午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 0.6558 (0.5719)	Prec@(1,5) (80.1%, 99.2%)	
05/30 11:00:26午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 0.5082 (0.5771)	Prec@(1,5) (80.0%, 99.1%)	
05/30 11:00:42午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 0.8454 (0.5820)	Prec@(1,5) (80.0%, 99.0%)	
05/30 11:00:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 0.6265 (0.5796)	Prec@(1,5) (80.1%, 99.0%)	
05/30 11:01:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 0.5639 (0.5790)	Prec@(1,5) (80.1%, 99.0%)	
05/30 11:01:28午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 0.4492 (0.5804)	Prec@(1,5) (80.1%, 99.0%)	
05/30 11:01:29午後 searchStage_trainer.py:221 [INFO] Train: [ 16/49] Final Prec@1 80.0640%
05/30 11:01:31午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 0.7101	Prec@(1,5) (75.7%, 97.8%)
05/30 11:01:33午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 0.6973	Prec@(1,5) (76.5%, 98.0%)
05/30 11:01:35午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 0.7030	Prec@(1,5) (76.3%, 98.2%)
05/30 11:01:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 0.6916	Prec@(1,5) (76.5%, 98.2%)
05/30 11:01:40午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 0.6835	Prec@(1,5) (76.8%, 98.3%)
05/30 11:01:42午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 0.6836	Prec@(1,5) (76.8%, 98.3%)
05/30 11:01:44午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 0.6834	Prec@(1,5) (76.8%, 98.3%)
05/30 11:01:46午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 0.6801	Prec@(1,5) (76.9%, 98.3%)
05/30 11:01:46午後 searchStage_trainer.py:260 [INFO] Valid: [ 16/49] Final Prec@1 76.9080%
05/30 11:01:46午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:01:47午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 76.9080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2537, 0.3128, 0.2586, 0.1749],
        [0.2644, 0.3172, 0.2507, 0.1677]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.3080, 0.2630, 0.1792],
        [0.2624, 0.2965, 0.2632, 0.1779],
        [0.2773, 0.3308, 0.2146, 0.1773]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2871, 0.2704, 0.1787],
        [0.2782, 0.3196, 0.2200, 0.1822],
        [0.2757, 0.3217, 0.2218, 0.1809]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2728, 0.3088, 0.2235, 0.1948],
        [0.2766, 0.3073, 0.2276, 0.1885],
        [0.2796, 0.3077, 0.2266, 0.1861]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2801, 0.3119, 0.2201, 0.1879],
        [0.2757, 0.3113, 0.2229, 0.1901],
        [0.2748, 0.3042, 0.2277, 0.1933]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2691, 0.3050, 0.2345, 0.1914],
        [0.2679, 0.3010, 0.2269, 0.2042],
        [0.2694, 0.2982, 0.2269, 0.2055]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2490, 0.2528, 0.2511],
        [0.2507, 0.2524, 0.2519, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2519, 0.2526, 0.2447],
        [0.2509, 0.2499, 0.2522, 0.2470],
        [0.2530, 0.2542, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2521, 0.2511, 0.2462],
        [0.2512, 0.2527, 0.2477, 0.2484],
        [0.2516, 0.2525, 0.2502, 0.2457]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2510, 0.2477, 0.2496],
        [0.2518, 0.2519, 0.2491, 0.2473],
        [0.2518, 0.2520, 0.2492, 0.2470]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2524, 0.2491, 0.2468],
        [0.2511, 0.2518, 0.2480, 0.2491],
        [0.2513, 0.2525, 0.2488, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2507, 0.2485, 0.2492],
        [0.2509, 0.2506, 0.2494, 0.2491],
        [0.2514, 0.2519, 0.2495, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2487, 0.2504, 0.2505],
        [0.2513, 0.2488, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2498, 0.2517],
        [0.2502, 0.2476, 0.2493, 0.2528],
        [0.2502, 0.2505, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2552, 0.2544, 0.2429],
        [0.2497, 0.2489, 0.2491, 0.2523],
        [0.2486, 0.2496, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2521, 0.2544, 0.2468],
        [0.2503, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2478, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2481, 0.2556, 0.2512],
        [0.2469, 0.2503, 0.2513, 0.2515],
        [0.2439, 0.2519, 0.2560, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2536, 0.2576, 0.2389],
        [0.2471, 0.2510, 0.2560, 0.2459],
        [0.2486, 0.2446, 0.2442, 0.2626]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:02:04午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 0.4890 (0.5203)	Prec@(1,5) (81.8%, 98.9%)	
05/30 11:02:20午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 0.5122 (0.5288)	Prec@(1,5) (81.4%, 99.0%)	
05/30 11:02:36午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 0.5569 (0.5498)	Prec@(1,5) (80.9%, 98.9%)	
05/30 11:02:52午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 0.4840 (0.5554)	Prec@(1,5) (80.6%, 99.0%)	
05/30 11:03:08午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 0.3907 (0.5612)	Prec@(1,5) (80.3%, 99.0%)	
05/30 11:03:24午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 0.4395 (0.5666)	Prec@(1,5) (80.2%, 99.0%)	
05/30 11:03:41午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 0.4508 (0.5655)	Prec@(1,5) (80.3%, 99.0%)	
05/30 11:03:54午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 0.5899 (0.5646)	Prec@(1,5) (80.4%, 99.0%)	
05/30 11:03:54午後 searchStage_trainer.py:221 [INFO] Train: [ 17/49] Final Prec@1 80.4440%
05/30 11:03:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 0.6585	Prec@(1,5) (77.8%, 98.7%)
05/30 11:03:59午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 0.6757	Prec@(1,5) (77.4%, 98.5%)
05/30 11:04:01午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 0.6808	Prec@(1,5) (77.2%, 98.3%)
05/30 11:04:03午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 0.6840	Prec@(1,5) (77.0%, 98.3%)
05/30 11:04:05午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 0.6924	Prec@(1,5) (76.8%, 98.3%)
05/30 11:04:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 0.6952	Prec@(1,5) (76.7%, 98.3%)
05/30 11:04:10午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 0.6934	Prec@(1,5) (76.7%, 98.3%)
05/30 11:04:12午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 0.6916	Prec@(1,5) (76.8%, 98.3%)
05/30 11:04:12午後 searchStage_trainer.py:260 [INFO] Valid: [ 17/49] Final Prec@1 76.7600%
05/30 11:04:12午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:04:12午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 76.9080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2536, 0.3126, 0.2586, 0.1752],
        [0.2645, 0.3169, 0.2509, 0.1677]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.3079, 0.2634, 0.1791],
        [0.2627, 0.2962, 0.2633, 0.1778],
        [0.2772, 0.3303, 0.2147, 0.1778]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2640, 0.2874, 0.2706, 0.1780],
        [0.2782, 0.3192, 0.2200, 0.1825],
        [0.2756, 0.3212, 0.2219, 0.1813]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2728, 0.3088, 0.2235, 0.1949],
        [0.2766, 0.3070, 0.2275, 0.1889],
        [0.2795, 0.3073, 0.2267, 0.1865]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2801, 0.3118, 0.2198, 0.1883],
        [0.2755, 0.3109, 0.2231, 0.1904],
        [0.2747, 0.3038, 0.2279, 0.1936]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2690, 0.3048, 0.2347, 0.1915],
        [0.2677, 0.3008, 0.2270, 0.2045],
        [0.2693, 0.2978, 0.2270, 0.2059]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2490, 0.2528, 0.2511],
        [0.2507, 0.2524, 0.2520, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2518, 0.2526, 0.2447],
        [0.2509, 0.2499, 0.2522, 0.2470],
        [0.2530, 0.2542, 0.2475, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2520, 0.2511, 0.2464],
        [0.2512, 0.2527, 0.2477, 0.2484],
        [0.2515, 0.2525, 0.2502, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2510, 0.2476, 0.2497],
        [0.2518, 0.2519, 0.2491, 0.2472],
        [0.2518, 0.2520, 0.2492, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2490, 0.2468],
        [0.2511, 0.2518, 0.2481, 0.2491],
        [0.2513, 0.2525, 0.2488, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2506, 0.2486, 0.2492],
        [0.2509, 0.2506, 0.2494, 0.2491],
        [0.2513, 0.2519, 0.2495, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2487, 0.2504, 0.2505],
        [0.2513, 0.2488, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2517],
        [0.2502, 0.2476, 0.2493, 0.2529],
        [0.2502, 0.2505, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2552, 0.2545, 0.2428],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2495, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2521, 0.2545, 0.2468],
        [0.2503, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2478, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2481, 0.2556, 0.2512],
        [0.2469, 0.2504, 0.2513, 0.2515],
        [0.2439, 0.2519, 0.2560, 0.2482]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2536, 0.2577, 0.2388],
        [0.2471, 0.2510, 0.2560, 0.2459],
        [0.2486, 0.2446, 0.2442, 0.2627]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:04:29午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 0.4590 (0.5074)	Prec@(1,5) (82.3%, 99.1%)	
05/30 11:04:45午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 0.4829 (0.5146)	Prec@(1,5) (82.0%, 99.1%)	
05/30 11:05:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 0.4433 (0.5331)	Prec@(1,5) (81.7%, 99.0%)	
05/30 11:05:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 0.6224 (0.5317)	Prec@(1,5) (81.8%, 99.0%)	
05/30 11:05:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 0.8062 (0.5388)	Prec@(1,5) (81.7%, 99.0%)	
05/30 11:05:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 0.5540 (0.5376)	Prec@(1,5) (81.6%, 99.0%)	
05/30 11:06:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 0.6233 (0.5412)	Prec@(1,5) (81.4%, 99.0%)	
05/30 11:06:20午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 0.6938 (0.5438)	Prec@(1,5) (81.2%, 99.0%)	
05/30 11:06:20午後 searchStage_trainer.py:221 [INFO] Train: [ 18/49] Final Prec@1 81.2240%
05/30 11:06:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 0.6640	Prec@(1,5) (77.1%, 98.7%)
05/30 11:06:25午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 0.6712	Prec@(1,5) (77.2%, 98.5%)
05/30 11:06:27午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 0.6700	Prec@(1,5) (77.6%, 98.5%)
05/30 11:06:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 0.6714	Prec@(1,5) (77.6%, 98.5%)
05/30 11:06:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 0.6657	Prec@(1,5) (77.7%, 98.5%)
05/30 11:06:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 0.6654	Prec@(1,5) (77.7%, 98.5%)
05/30 11:06:36午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 0.6615	Prec@(1,5) (77.9%, 98.5%)
05/30 11:06:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 0.6601	Prec@(1,5) (77.9%, 98.5%)
05/30 11:06:38午後 searchStage_trainer.py:260 [INFO] Valid: [ 18/49] Final Prec@1 77.8320%
05/30 11:06:38午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:06:38午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.8320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2532, 0.3123, 0.2589, 0.1755],
        [0.2643, 0.3166, 0.2512, 0.1679]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.3080, 0.2636, 0.1789],
        [0.2626, 0.2963, 0.2633, 0.1778],
        [0.2771, 0.3299, 0.2148, 0.1782]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2643, 0.2873, 0.2705, 0.1778],
        [0.2783, 0.3189, 0.2201, 0.1828],
        [0.2755, 0.3208, 0.2220, 0.1817]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2729, 0.3085, 0.2232, 0.1954],
        [0.2766, 0.3067, 0.2276, 0.1891],
        [0.2795, 0.3069, 0.2268, 0.1868]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2800, 0.3117, 0.2200, 0.1883],
        [0.2754, 0.3106, 0.2232, 0.1908],
        [0.2745, 0.3034, 0.2281, 0.1940]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2691, 0.3046, 0.2348, 0.1915],
        [0.2677, 0.3005, 0.2271, 0.2047],
        [0.2692, 0.2973, 0.2270, 0.2065]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2490, 0.2528, 0.2511],
        [0.2507, 0.2524, 0.2520, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2518, 0.2526, 0.2447],
        [0.2510, 0.2499, 0.2522, 0.2469],
        [0.2530, 0.2542, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2520, 0.2511, 0.2463],
        [0.2512, 0.2527, 0.2477, 0.2484],
        [0.2515, 0.2525, 0.2502, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2510, 0.2476, 0.2497],
        [0.2518, 0.2519, 0.2491, 0.2472],
        [0.2517, 0.2520, 0.2492, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2491, 0.2467],
        [0.2511, 0.2517, 0.2480, 0.2491],
        [0.2513, 0.2525, 0.2487, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2506, 0.2486, 0.2492],
        [0.2509, 0.2506, 0.2494, 0.2491],
        [0.2513, 0.2519, 0.2495, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2487, 0.2504, 0.2505],
        [0.2513, 0.2488, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2517],
        [0.2502, 0.2476, 0.2493, 0.2529],
        [0.2502, 0.2505, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2553, 0.2545, 0.2427],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2495, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2521, 0.2545, 0.2467],
        [0.2503, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2478, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2481, 0.2557, 0.2512],
        [0.2468, 0.2504, 0.2513, 0.2515],
        [0.2439, 0.2519, 0.2561, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2537, 0.2577, 0.2387],
        [0.2471, 0.2510, 0.2560, 0.2459],
        [0.2487, 0.2445, 0.2441, 0.2627]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:06:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 0.5451 (0.4803)	Prec@(1,5) (83.2%, 99.3%)	
05/30 11:07:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 0.5530 (0.5015)	Prec@(1,5) (82.6%, 99.2%)	
05/30 11:07:28午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 0.5356 (0.4977)	Prec@(1,5) (82.7%, 99.2%)	
05/30 11:07:44午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 0.4938 (0.5011)	Prec@(1,5) (82.6%, 99.2%)	
05/30 11:08:00午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 0.3854 (0.5058)	Prec@(1,5) (82.6%, 99.1%)	
05/30 11:08:17午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 0.4817 (0.5142)	Prec@(1,5) (82.3%, 99.1%)	
05/30 11:08:33午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 0.4712 (0.5137)	Prec@(1,5) (82.4%, 99.1%)	
05/30 11:08:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 0.4235 (0.5134)	Prec@(1,5) (82.4%, 99.1%)	
05/30 11:08:47午後 searchStage_trainer.py:221 [INFO] Train: [ 19/49] Final Prec@1 82.3640%
05/30 11:08:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 0.6712	Prec@(1,5) (76.6%, 98.7%)
05/30 11:08:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 0.6530	Prec@(1,5) (77.8%, 98.6%)
05/30 11:08:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 0.6450	Prec@(1,5) (78.0%, 98.7%)
05/30 11:08:56午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 0.6578	Prec@(1,5) (77.7%, 98.5%)
05/30 11:08:58午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 0.6622	Prec@(1,5) (77.6%, 98.4%)
05/30 11:09:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 0.6601	Prec@(1,5) (77.5%, 98.5%)
05/30 11:09:02午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 0.6595	Prec@(1,5) (77.6%, 98.4%)
05/30 11:09:04午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 0.6632	Prec@(1,5) (77.6%, 98.4%)
05/30 11:09:04午後 searchStage_trainer.py:260 [INFO] Valid: [ 19/49] Final Prec@1 77.5680%
05/30 11:09:04午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:09:04午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 77.8320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2532, 0.3120, 0.2593, 0.1756],
        [0.2641, 0.3163, 0.2514, 0.1683]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.3078, 0.2638, 0.1789],
        [0.2627, 0.2958, 0.2632, 0.1783],
        [0.2770, 0.3295, 0.2149, 0.1785]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2642, 0.2872, 0.2708, 0.1778],
        [0.2782, 0.3186, 0.2201, 0.1831],
        [0.2754, 0.3205, 0.2222, 0.1820]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2730, 0.3084, 0.2230, 0.1956],
        [0.2766, 0.3064, 0.2276, 0.1895],
        [0.2794, 0.3066, 0.2269, 0.1871]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2800, 0.3115, 0.2200, 0.1885],
        [0.2754, 0.3103, 0.2233, 0.1910],
        [0.2744, 0.3031, 0.2282, 0.1943]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2690, 0.3044, 0.2351, 0.1915],
        [0.2676, 0.3001, 0.2271, 0.2051],
        [0.2690, 0.2969, 0.2271, 0.2069]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2490, 0.2528, 0.2512],
        [0.2507, 0.2524, 0.2520, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2518, 0.2526, 0.2447],
        [0.2510, 0.2499, 0.2522, 0.2469],
        [0.2530, 0.2542, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2520, 0.2511, 0.2464],
        [0.2512, 0.2527, 0.2477, 0.2484],
        [0.2515, 0.2525, 0.2502, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2510, 0.2476, 0.2497],
        [0.2518, 0.2519, 0.2491, 0.2473],
        [0.2517, 0.2520, 0.2492, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2491, 0.2467],
        [0.2511, 0.2517, 0.2480, 0.2492],
        [0.2513, 0.2525, 0.2488, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2506, 0.2486, 0.2492],
        [0.2509, 0.2506, 0.2494, 0.2492],
        [0.2513, 0.2518, 0.2495, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2487, 0.2504, 0.2505],
        [0.2513, 0.2487, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2517],
        [0.2503, 0.2476, 0.2492, 0.2529],
        [0.2502, 0.2505, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2553, 0.2546, 0.2427],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2495, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2521, 0.2545, 0.2467],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2478, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2481, 0.2557, 0.2512],
        [0.2468, 0.2504, 0.2513, 0.2515],
        [0.2439, 0.2519, 0.2561, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2537, 0.2578, 0.2386],
        [0.2471, 0.2510, 0.2560, 0.2459],
        [0.2487, 0.2445, 0.2441, 0.2627]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:09:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 0.6214 (0.4789)	Prec@(1,5) (83.7%, 99.3%)	
05/30 11:09:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 0.4986 (0.4855)	Prec@(1,5) (83.3%, 99.1%)	
05/30 11:09:54午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 0.5893 (0.4803)	Prec@(1,5) (83.5%, 99.1%)	
05/30 11:10:10午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 0.4413 (0.4869)	Prec@(1,5) (83.2%, 99.1%)	
05/30 11:10:26午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 0.4481 (0.4885)	Prec@(1,5) (83.1%, 99.1%)	
05/30 11:10:43午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 0.5551 (0.4946)	Prec@(1,5) (83.0%, 99.1%)	
05/30 11:10:59午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 0.4071 (0.4938)	Prec@(1,5) (83.0%, 99.1%)	
05/30 11:11:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 0.4401 (0.4929)	Prec@(1,5) (83.1%, 99.1%)	
05/30 11:11:12午後 searchStage_trainer.py:221 [INFO] Train: [ 20/49] Final Prec@1 83.1000%
05/30 11:11:14午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 0.5806	Prec@(1,5) (80.4%, 98.8%)
05/30 11:11:17午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 0.5962	Prec@(1,5) (79.3%, 98.8%)
05/30 11:11:19午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 0.5998	Prec@(1,5) (79.3%, 98.7%)
05/30 11:11:21午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 0.6063	Prec@(1,5) (79.1%, 98.8%)
05/30 11:11:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 0.6133	Prec@(1,5) (79.0%, 98.7%)
05/30 11:11:25午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 0.6161	Prec@(1,5) (78.9%, 98.6%)
05/30 11:11:28午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 0.6140	Prec@(1,5) (79.0%, 98.7%)
05/30 11:11:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 0.6128	Prec@(1,5) (79.1%, 98.7%)
05/30 11:11:30午後 searchStage_trainer.py:260 [INFO] Valid: [ 20/49] Final Prec@1 79.1120%
05/30 11:11:30午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:11:30午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 79.1120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2531, 0.3115, 0.2594, 0.1760],
        [0.2640, 0.3161, 0.2516, 0.1683]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.3077, 0.2636, 0.1792],
        [0.2625, 0.2958, 0.2635, 0.1782],
        [0.2769, 0.3291, 0.2151, 0.1789]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2872, 0.2713, 0.1778],
        [0.2781, 0.3183, 0.2203, 0.1833],
        [0.2753, 0.3202, 0.2222, 0.1823]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2727, 0.3083, 0.2233, 0.1957],
        [0.2766, 0.3061, 0.2276, 0.1897],
        [0.2793, 0.3062, 0.2270, 0.1875]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2801, 0.3113, 0.2201, 0.1885],
        [0.2754, 0.3100, 0.2232, 0.1914],
        [0.2743, 0.3027, 0.2283, 0.1947]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.3042, 0.2355, 0.1915],
        [0.2676, 0.2998, 0.2271, 0.2055],
        [0.2689, 0.2965, 0.2272, 0.2073]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2490, 0.2528, 0.2512],
        [0.2507, 0.2524, 0.2520, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2518, 0.2526, 0.2447],
        [0.2510, 0.2499, 0.2522, 0.2469],
        [0.2530, 0.2541, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2520, 0.2511, 0.2464],
        [0.2512, 0.2527, 0.2477, 0.2484],
        [0.2515, 0.2525, 0.2502, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2510, 0.2476, 0.2497],
        [0.2518, 0.2519, 0.2490, 0.2473],
        [0.2517, 0.2520, 0.2492, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2491, 0.2467],
        [0.2511, 0.2517, 0.2480, 0.2492],
        [0.2513, 0.2525, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2506, 0.2485, 0.2492],
        [0.2509, 0.2506, 0.2494, 0.2492],
        [0.2513, 0.2518, 0.2495, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2504, 0.2505],
        [0.2513, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2517],
        [0.2503, 0.2476, 0.2492, 0.2529],
        [0.2502, 0.2506, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2554, 0.2546, 0.2426],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2495, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2522, 0.2546, 0.2467],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2481, 0.2558, 0.2512],
        [0.2468, 0.2504, 0.2513, 0.2515],
        [0.2439, 0.2519, 0.2561, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2538, 0.2579, 0.2385],
        [0.2470, 0.2510, 0.2561, 0.2459],
        [0.2487, 0.2445, 0.2441, 0.2627]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:11:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 0.3348 (0.4749)	Prec@(1,5) (83.4%, 99.4%)	
05/30 11:12:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 0.5030 (0.4676)	Prec@(1,5) (83.8%, 99.4%)	
05/30 11:12:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 0.6263 (0.4679)	Prec@(1,5) (83.8%, 99.4%)	
05/30 11:12:36午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 0.5754 (0.4665)	Prec@(1,5) (83.9%, 99.4%)	
05/30 11:12:52午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 0.4505 (0.4718)	Prec@(1,5) (83.8%, 99.4%)	
05/30 11:13:09午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 0.5479 (0.4717)	Prec@(1,5) (83.8%, 99.3%)	
05/30 11:13:25午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 0.3697 (0.4718)	Prec@(1,5) (83.8%, 99.3%)	
05/30 11:13:38午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 0.4295 (0.4737)	Prec@(1,5) (83.7%, 99.3%)	
05/30 11:13:38午後 searchStage_trainer.py:221 [INFO] Train: [ 21/49] Final Prec@1 83.6840%
05/30 11:13:41午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 0.6576	Prec@(1,5) (77.8%, 98.6%)
05/30 11:13:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 0.6757	Prec@(1,5) (77.1%, 98.3%)
05/30 11:13:45午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 0.6819	Prec@(1,5) (77.3%, 98.3%)
05/30 11:13:47午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 0.6770	Prec@(1,5) (77.4%, 98.3%)
05/30 11:13:50午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 0.6762	Prec@(1,5) (77.4%, 98.4%)
05/30 11:13:52午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 0.6738	Prec@(1,5) (77.4%, 98.4%)
05/30 11:13:54午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 0.6702	Prec@(1,5) (77.6%, 98.4%)
05/30 11:13:56午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 0.6678	Prec@(1,5) (77.7%, 98.5%)
05/30 11:13:56午後 searchStage_trainer.py:260 [INFO] Valid: [ 21/49] Final Prec@1 77.6560%
05/30 11:13:56午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:13:56午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 79.1120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2529, 0.3113, 0.2598, 0.1761],
        [0.2639, 0.3158, 0.2517, 0.1686]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.3076, 0.2638, 0.1793],
        [0.2623, 0.2956, 0.2636, 0.1785],
        [0.2768, 0.3288, 0.2152, 0.1792]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2639, 0.2870, 0.2710, 0.1781],
        [0.2780, 0.3180, 0.2203, 0.1837],
        [0.2752, 0.3199, 0.2224, 0.1825]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2726, 0.3081, 0.2232, 0.1960],
        [0.2765, 0.3058, 0.2277, 0.1900],
        [0.2792, 0.3059, 0.2272, 0.1877]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2803, 0.3111, 0.2200, 0.1886],
        [0.2753, 0.3097, 0.2233, 0.1917],
        [0.2742, 0.3024, 0.2284, 0.1950]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.3041, 0.2355, 0.1916],
        [0.2675, 0.2995, 0.2272, 0.2058],
        [0.2687, 0.2962, 0.2274, 0.2077]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2489, 0.2528, 0.2512],
        [0.2507, 0.2524, 0.2520, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2518, 0.2526, 0.2447],
        [0.2510, 0.2498, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2520, 0.2511, 0.2464],
        [0.2512, 0.2527, 0.2476, 0.2485],
        [0.2515, 0.2525, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2510, 0.2476, 0.2497],
        [0.2518, 0.2519, 0.2490, 0.2473],
        [0.2517, 0.2520, 0.2492, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2524, 0.2492, 0.2466],
        [0.2511, 0.2517, 0.2480, 0.2492],
        [0.2513, 0.2525, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2485, 0.2493],
        [0.2509, 0.2506, 0.2494, 0.2492],
        [0.2513, 0.2518, 0.2495, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2504, 0.2505],
        [0.2513, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2517],
        [0.2503, 0.2475, 0.2492, 0.2530],
        [0.2502, 0.2506, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2554, 0.2546, 0.2425],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2495, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2522, 0.2546, 0.2467],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2481, 0.2558, 0.2512],
        [0.2468, 0.2504, 0.2513, 0.2515],
        [0.2439, 0.2519, 0.2561, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2538, 0.2579, 0.2385],
        [0.2470, 0.2510, 0.2561, 0.2458],
        [0.2487, 0.2445, 0.2441, 0.2628]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:14:13午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 0.4148 (0.4427)	Prec@(1,5) (84.2%, 99.4%)	
05/30 11:14:29午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 0.1997 (0.4441)	Prec@(1,5) (84.6%, 99.2%)	
05/30 11:14:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 0.3152 (0.4457)	Prec@(1,5) (84.5%, 99.2%)	
05/30 11:15:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 0.7446 (0.4493)	Prec@(1,5) (84.4%, 99.2%)	
05/30 11:15:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 0.4172 (0.4496)	Prec@(1,5) (84.4%, 99.2%)	
05/30 11:15:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 0.3409 (0.4498)	Prec@(1,5) (84.4%, 99.2%)	
05/30 11:15:50午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 0.5311 (0.4496)	Prec@(1,5) (84.4%, 99.3%)	
05/30 11:16:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 0.5931 (0.4518)	Prec@(1,5) (84.4%, 99.3%)	
05/30 11:16:03午後 searchStage_trainer.py:221 [INFO] Train: [ 22/49] Final Prec@1 84.4000%
05/30 11:16:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 0.5970	Prec@(1,5) (79.4%, 98.8%)
05/30 11:16:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 0.6044	Prec@(1,5) (79.7%, 98.8%)
05/30 11:16:10午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 0.6069	Prec@(1,5) (79.4%, 98.9%)
05/30 11:16:13午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 0.6108	Prec@(1,5) (79.3%, 98.8%)
05/30 11:16:15午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 0.6167	Prec@(1,5) (79.2%, 98.7%)
05/30 11:16:17午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 0.6200	Prec@(1,5) (79.2%, 98.7%)
05/30 11:16:19午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 0.6162	Prec@(1,5) (79.3%, 98.7%)
05/30 11:16:21午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 0.6158	Prec@(1,5) (79.3%, 98.7%)
05/30 11:16:21午後 searchStage_trainer.py:260 [INFO] Valid: [ 22/49] Final Prec@1 79.2840%
05/30 11:16:21午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:16:22午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 79.2840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2527, 0.3110, 0.2598, 0.1764],
        [0.2637, 0.3156, 0.2519, 0.1688]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.3074, 0.2640, 0.1794],
        [0.2622, 0.2953, 0.2634, 0.1790],
        [0.2767, 0.3285, 0.2153, 0.1794]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2868, 0.2710, 0.1784],
        [0.2779, 0.3178, 0.2204, 0.1839],
        [0.2751, 0.3196, 0.2226, 0.1828]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2727, 0.3079, 0.2232, 0.1962],
        [0.2763, 0.3056, 0.2279, 0.1902],
        [0.2791, 0.3056, 0.2272, 0.1881]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2803, 0.3109, 0.2202, 0.1886],
        [0.2752, 0.3094, 0.2234, 0.1920],
        [0.2741, 0.3021, 0.2285, 0.1954]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2688, 0.3039, 0.2355, 0.1918],
        [0.2674, 0.2992, 0.2273, 0.2061],
        [0.2686, 0.2958, 0.2274, 0.2081]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2489, 0.2528, 0.2512],
        [0.2506, 0.2524, 0.2521, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2518, 0.2526, 0.2448],
        [0.2510, 0.2498, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2520, 0.2510, 0.2464],
        [0.2512, 0.2527, 0.2476, 0.2485],
        [0.2515, 0.2525, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2510, 0.2476, 0.2498],
        [0.2518, 0.2519, 0.2490, 0.2473],
        [0.2517, 0.2520, 0.2492, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2491, 0.2467],
        [0.2511, 0.2517, 0.2480, 0.2492],
        [0.2513, 0.2525, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2485, 0.2493],
        [0.2509, 0.2505, 0.2494, 0.2492],
        [0.2513, 0.2518, 0.2495, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2504, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2517],
        [0.2503, 0.2475, 0.2492, 0.2530],
        [0.2502, 0.2506, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2555, 0.2547, 0.2425],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2495, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2522, 0.2546, 0.2467],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2481, 0.2559, 0.2512],
        [0.2468, 0.2504, 0.2513, 0.2515],
        [0.2438, 0.2520, 0.2561, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2538, 0.2580, 0.2384],
        [0.2470, 0.2510, 0.2561, 0.2458],
        [0.2487, 0.2445, 0.2440, 0.2628]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:16:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 0.4664 (0.4092)	Prec@(1,5) (86.2%, 99.6%)	
05/30 11:16:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 0.5023 (0.4277)	Prec@(1,5) (85.2%, 99.5%)	
05/30 11:17:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 0.4319 (0.4279)	Prec@(1,5) (85.3%, 99.5%)	
05/30 11:17:27午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 0.2738 (0.4311)	Prec@(1,5) (85.2%, 99.4%)	
05/30 11:17:44午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 0.4720 (0.4290)	Prec@(1,5) (85.3%, 99.4%)	
05/30 11:18:00午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 0.2254 (0.4305)	Prec@(1,5) (85.3%, 99.3%)	
05/30 11:18:16午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 0.7266 (0.4354)	Prec@(1,5) (85.1%, 99.3%)	
05/30 11:18:29午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 0.3284 (0.4382)	Prec@(1,5) (85.0%, 99.3%)	
05/30 11:18:29午後 searchStage_trainer.py:221 [INFO] Train: [ 23/49] Final Prec@1 85.0040%
05/30 11:18:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 0.5669	Prec@(1,5) (80.1%, 99.0%)
05/30 11:18:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 0.5704	Prec@(1,5) (80.2%, 98.9%)
05/30 11:18:36午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 0.5751	Prec@(1,5) (80.1%, 98.9%)
05/30 11:18:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 0.5713	Prec@(1,5) (80.5%, 98.9%)
05/30 11:18:41午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 0.5694	Prec@(1,5) (80.5%, 98.9%)
05/30 11:18:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 0.5673	Prec@(1,5) (80.6%, 98.9%)
05/30 11:18:45午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 0.5718	Prec@(1,5) (80.5%, 98.9%)
05/30 11:18:47午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 0.5728	Prec@(1,5) (80.4%, 98.8%)
05/30 11:18:47午後 searchStage_trainer.py:260 [INFO] Valid: [ 23/49] Final Prec@1 80.3960%
05/30 11:18:47午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('skip_connect', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:18:47午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.3960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2527, 0.3107, 0.2600, 0.1766],
        [0.2637, 0.3153, 0.2520, 0.1690]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.3072, 0.2640, 0.1796],
        [0.2622, 0.2952, 0.2634, 0.1792],
        [0.2767, 0.3282, 0.2155, 0.1796]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2635, 0.2869, 0.2711, 0.1786],
        [0.2778, 0.3175, 0.2205, 0.1842],
        [0.2750, 0.3193, 0.2227, 0.1830]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2726, 0.3077, 0.2233, 0.1964],
        [0.2763, 0.3054, 0.2280, 0.1903],
        [0.2790, 0.3053, 0.2273, 0.1884]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2803, 0.3109, 0.2203, 0.1886],
        [0.2751, 0.3092, 0.2236, 0.1921],
        [0.2740, 0.3018, 0.2285, 0.1958]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2687, 0.3037, 0.2357, 0.1919],
        [0.2673, 0.2989, 0.2273, 0.2064],
        [0.2685, 0.2955, 0.2275, 0.2084]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2489, 0.2529, 0.2512],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2518, 0.2526, 0.2448],
        [0.2510, 0.2498, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2465],
        [0.2512, 0.2527, 0.2476, 0.2485],
        [0.2515, 0.2525, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2510, 0.2476, 0.2498],
        [0.2518, 0.2519, 0.2490, 0.2473],
        [0.2517, 0.2519, 0.2492, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2492, 0.2467],
        [0.2511, 0.2517, 0.2480, 0.2492],
        [0.2513, 0.2525, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2485, 0.2493],
        [0.2509, 0.2505, 0.2494, 0.2492],
        [0.2513, 0.2518, 0.2495, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2504, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2517],
        [0.2503, 0.2475, 0.2492, 0.2530],
        [0.2502, 0.2506, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2555, 0.2547, 0.2424],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2495, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2522, 0.2546, 0.2466],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2480, 0.2559, 0.2512],
        [0.2468, 0.2504, 0.2513, 0.2515],
        [0.2438, 0.2520, 0.2561, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2538, 0.2580, 0.2383],
        [0.2470, 0.2510, 0.2561, 0.2458],
        [0.2487, 0.2445, 0.2440, 0.2628]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:19:04午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 0.2878 (0.4179)	Prec@(1,5) (86.4%, 99.2%)	
05/30 11:19:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 0.4092 (0.4049)	Prec@(1,5) (86.9%, 99.2%)	
05/30 11:19:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 0.4447 (0.4092)	Prec@(1,5) (86.4%, 99.3%)	
05/30 11:19:53午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 0.5078 (0.4095)	Prec@(1,5) (86.4%, 99.4%)	
05/30 11:20:09午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 0.5756 (0.4118)	Prec@(1,5) (86.3%, 99.4%)	
05/30 11:20:26午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 0.2000 (0.4148)	Prec@(1,5) (86.1%, 99.3%)	
05/30 11:20:42午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 0.4699 (0.4152)	Prec@(1,5) (86.1%, 99.4%)	
05/30 11:20:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 0.5952 (0.4154)	Prec@(1,5) (86.1%, 99.4%)	
05/30 11:20:55午後 searchStage_trainer.py:221 [INFO] Train: [ 24/49] Final Prec@1 86.0520%
05/30 11:20:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 0.6302	Prec@(1,5) (78.3%, 98.6%)
05/30 11:21:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 0.6357	Prec@(1,5) (78.3%, 98.6%)
05/30 11:21:02午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 0.6327	Prec@(1,5) (78.4%, 98.8%)
05/30 11:21:04午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 0.6294	Prec@(1,5) (78.5%, 98.9%)
05/30 11:21:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 0.6272	Prec@(1,5) (78.9%, 98.9%)
05/30 11:21:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 0.6218	Prec@(1,5) (79.1%, 98.9%)
05/30 11:21:11午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 0.6204	Prec@(1,5) (79.2%, 98.8%)
05/30 11:21:12午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 0.6145	Prec@(1,5) (79.3%, 98.8%)
05/30 11:21:12午後 searchStage_trainer.py:260 [INFO] Valid: [ 24/49] Final Prec@1 79.3400%
05/30 11:21:12午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:21:13午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.3960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2527, 0.3104, 0.2601, 0.1768],
        [0.2637, 0.3150, 0.2521, 0.1692]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.3069, 0.2640, 0.1798],
        [0.2622, 0.2950, 0.2635, 0.1794],
        [0.2766, 0.3279, 0.2156, 0.1799]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2631, 0.2868, 0.2713, 0.1787],
        [0.2778, 0.3173, 0.2206, 0.1843],
        [0.2749, 0.3190, 0.2227, 0.1833]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2726, 0.3076, 0.2233, 0.1965],
        [0.2763, 0.3051, 0.2280, 0.1906],
        [0.2789, 0.3050, 0.2274, 0.1887]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2803, 0.3107, 0.2201, 0.1889],
        [0.2750, 0.3090, 0.2237, 0.1923],
        [0.2739, 0.3015, 0.2286, 0.1960]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2686, 0.3036, 0.2358, 0.1920],
        [0.2672, 0.2987, 0.2274, 0.2067],
        [0.2684, 0.2952, 0.2277, 0.2087]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2489, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2517, 0.2526, 0.2448],
        [0.2510, 0.2498, 0.2522, 0.2469],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2527, 0.2476, 0.2485],
        [0.2515, 0.2524, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2510, 0.2476, 0.2498],
        [0.2518, 0.2519, 0.2490, 0.2473],
        [0.2517, 0.2519, 0.2492, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2511, 0.2517, 0.2480, 0.2492],
        [0.2513, 0.2525, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2485, 0.2493],
        [0.2509, 0.2505, 0.2494, 0.2492],
        [0.2513, 0.2518, 0.2495, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2504, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2518],
        [0.2503, 0.2475, 0.2492, 0.2530],
        [0.2502, 0.2506, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2556, 0.2547, 0.2423],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2495, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2522, 0.2546, 0.2466],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2480, 0.2559, 0.2512],
        [0.2468, 0.2504, 0.2513, 0.2515],
        [0.2438, 0.2520, 0.2561, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2538, 0.2581, 0.2383],
        [0.2470, 0.2510, 0.2561, 0.2458],
        [0.2487, 0.2445, 0.2440, 0.2628]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:21:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 0.3299 (0.3628)	Prec@(1,5) (87.3%, 99.7%)	
05/30 11:21:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 0.3509 (0.3715)	Prec@(1,5) (87.0%, 99.6%)	
05/30 11:22:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 0.4272 (0.3790)	Prec@(1,5) (86.6%, 99.5%)	
05/30 11:22:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 0.2404 (0.3857)	Prec@(1,5) (86.5%, 99.5%)	
05/30 11:22:35午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 0.4965 (0.3906)	Prec@(1,5) (86.3%, 99.5%)	
05/30 11:22:51午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 0.5228 (0.3941)	Prec@(1,5) (86.1%, 99.5%)	
05/30 11:23:08午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 0.4156 (0.3958)	Prec@(1,5) (86.1%, 99.5%)	
05/30 11:23:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 0.3400 (0.3963)	Prec@(1,5) (86.0%, 99.5%)	
05/30 11:23:21午後 searchStage_trainer.py:221 [INFO] Train: [ 25/49] Final Prec@1 86.0280%
05/30 11:23:24午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 0.5982	Prec@(1,5) (80.0%, 98.7%)
05/30 11:23:26午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 0.5887	Prec@(1,5) (80.1%, 98.9%)
05/30 11:23:28午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 0.5920	Prec@(1,5) (80.0%, 98.9%)
05/30 11:23:30午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 0.5928	Prec@(1,5) (80.0%, 99.0%)
05/30 11:23:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 0.5919	Prec@(1,5) (80.0%, 98.9%)
05/30 11:23:35午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 0.5846	Prec@(1,5) (80.3%, 98.9%)
05/30 11:23:37午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 0.5811	Prec@(1,5) (80.4%, 98.9%)
05/30 11:23:39午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 0.5824	Prec@(1,5) (80.3%, 98.9%)
05/30 11:23:39午後 searchStage_trainer.py:260 [INFO] Valid: [ 25/49] Final Prec@1 80.2880%
05/30 11:23:39午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:23:39午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 80.3960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2526, 0.3102, 0.2602, 0.1770],
        [0.2636, 0.3148, 0.2522, 0.1694]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.3068, 0.2641, 0.1800],
        [0.2621, 0.2949, 0.2636, 0.1794],
        [0.2765, 0.3277, 0.2156, 0.1801]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2629, 0.2867, 0.2714, 0.1789],
        [0.2777, 0.3171, 0.2206, 0.1845],
        [0.2749, 0.3187, 0.2228, 0.1836]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2725, 0.3074, 0.2233, 0.1967],
        [0.2762, 0.3049, 0.2280, 0.1908],
        [0.2789, 0.3048, 0.2275, 0.1889]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2802, 0.3106, 0.2201, 0.1890],
        [0.2749, 0.3087, 0.2237, 0.1926],
        [0.2738, 0.3012, 0.2287, 0.1963]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2686, 0.3034, 0.2358, 0.1922],
        [0.2671, 0.2985, 0.2274, 0.2070],
        [0.2683, 0.2949, 0.2277, 0.2090]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2489, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2517, 0.2526, 0.2448],
        [0.2510, 0.2498, 0.2522, 0.2469],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2465],
        [0.2512, 0.2527, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2510, 0.2476, 0.2498],
        [0.2518, 0.2518, 0.2490, 0.2473],
        [0.2517, 0.2519, 0.2492, 0.2471]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2492, 0.2467],
        [0.2511, 0.2517, 0.2480, 0.2492],
        [0.2513, 0.2524, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2485, 0.2493],
        [0.2509, 0.2505, 0.2494, 0.2492],
        [0.2513, 0.2518, 0.2495, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2504, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2518],
        [0.2503, 0.2475, 0.2492, 0.2530],
        [0.2503, 0.2506, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2556, 0.2548, 0.2423],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2495, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2522, 0.2547, 0.2466],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2480, 0.2560, 0.2512],
        [0.2468, 0.2504, 0.2513, 0.2515],
        [0.2438, 0.2520, 0.2561, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2539, 0.2581, 0.2382],
        [0.2470, 0.2510, 0.2561, 0.2458],
        [0.2487, 0.2445, 0.2440, 0.2628]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:23:56午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 0.2742 (0.3787)	Prec@(1,5) (86.9%, 99.5%)	
05/30 11:24:13午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 0.5854 (0.3748)	Prec@(1,5) (87.2%, 99.5%)	
05/30 11:24:29午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 0.4433 (0.3739)	Prec@(1,5) (87.1%, 99.5%)	
05/30 11:24:45午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.2386 (0.3783)	Prec@(1,5) (86.9%, 99.5%)	
05/30 11:25:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 0.4045 (0.3757)	Prec@(1,5) (86.8%, 99.5%)	
05/30 11:25:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 0.4965 (0.3778)	Prec@(1,5) (86.8%, 99.5%)	
05/30 11:25:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 0.4044 (0.3793)	Prec@(1,5) (86.7%, 99.5%)	
05/30 11:25:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 0.2840 (0.3826)	Prec@(1,5) (86.6%, 99.5%)	
05/30 11:25:48午後 searchStage_trainer.py:221 [INFO] Train: [ 26/49] Final Prec@1 86.6280%
05/30 11:25:50午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 0.5596	Prec@(1,5) (82.1%, 98.9%)
05/30 11:25:52午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 0.5586	Prec@(1,5) (81.6%, 98.9%)
05/30 11:25:54午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 0.5709	Prec@(1,5) (81.3%, 98.8%)
05/30 11:25:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 0.5628	Prec@(1,5) (81.5%, 98.8%)
05/30 11:25:59午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 0.5587	Prec@(1,5) (81.6%, 98.8%)
05/30 11:26:01午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 0.5638	Prec@(1,5) (81.3%, 98.8%)
05/30 11:26:03午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 0.5647	Prec@(1,5) (81.2%, 98.8%)
05/30 11:26:05午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 0.5684	Prec@(1,5) (81.1%, 98.8%)
05/30 11:26:05午後 searchStage_trainer.py:260 [INFO] Valid: [ 26/49] Final Prec@1 81.1440%
05/30 11:26:05午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:26:06午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.1440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2525, 0.3099, 0.2603, 0.1773],
        [0.2635, 0.3146, 0.2524, 0.1695]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.3067, 0.2642, 0.1801],
        [0.2620, 0.2948, 0.2637, 0.1794],
        [0.2765, 0.3274, 0.2157, 0.1804]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2867, 0.2717, 0.1790],
        [0.2776, 0.3170, 0.2208, 0.1847],
        [0.2748, 0.3185, 0.2229, 0.1838]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2724, 0.3073, 0.2233, 0.1970],
        [0.2762, 0.3047, 0.2281, 0.1910],
        [0.2788, 0.3045, 0.2276, 0.1891]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2801, 0.3105, 0.2204, 0.1890],
        [0.2749, 0.3085, 0.2238, 0.1928],
        [0.2737, 0.3010, 0.2288, 0.1965]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2685, 0.3033, 0.2358, 0.1924],
        [0.2671, 0.2982, 0.2275, 0.2072],
        [0.2682, 0.2947, 0.2278, 0.2093]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2489, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2517, 0.2526, 0.2449],
        [0.2510, 0.2498, 0.2522, 0.2469],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2510, 0.2466],
        [0.2512, 0.2527, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2510, 0.2476, 0.2498],
        [0.2518, 0.2518, 0.2490, 0.2473],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2466],
        [0.2511, 0.2517, 0.2480, 0.2492],
        [0.2513, 0.2524, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2486, 0.2493],
        [0.2509, 0.2505, 0.2494, 0.2492],
        [0.2513, 0.2518, 0.2495, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2504, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2518],
        [0.2503, 0.2475, 0.2492, 0.2530],
        [0.2503, 0.2506, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2557, 0.2548, 0.2423],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2495, 0.2500, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2522, 0.2547, 0.2466],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2480, 0.2560, 0.2512],
        [0.2468, 0.2504, 0.2513, 0.2515],
        [0.2438, 0.2520, 0.2561, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2539, 0.2581, 0.2382],
        [0.2470, 0.2511, 0.2561, 0.2458],
        [0.2487, 0.2445, 0.2440, 0.2628]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:26:22午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 0.3924 (0.3364)	Prec@(1,5) (88.5%, 99.8%)	
05/30 11:26:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 0.3168 (0.3437)	Prec@(1,5) (87.8%, 99.7%)	
05/30 11:26:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 0.5507 (0.3440)	Prec@(1,5) (88.1%, 99.6%)	
05/30 11:27:11午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 0.5102 (0.3443)	Prec@(1,5) (88.1%, 99.6%)	
05/30 11:27:28午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 0.3457 (0.3517)	Prec@(1,5) (87.9%, 99.6%)	
05/30 11:27:44午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 0.2786 (0.3576)	Prec@(1,5) (87.7%, 99.5%)	
05/30 11:28:00午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 0.4471 (0.3605)	Prec@(1,5) (87.6%, 99.5%)	
05/30 11:28:13午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 0.3274 (0.3624)	Prec@(1,5) (87.6%, 99.5%)	
05/30 11:28:14午後 searchStage_trainer.py:221 [INFO] Train: [ 27/49] Final Prec@1 87.5440%
05/30 11:28:16午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 0.5708	Prec@(1,5) (81.5%, 98.9%)
05/30 11:28:18午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 0.5640	Prec@(1,5) (81.5%, 99.1%)
05/30 11:28:20午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 0.5665	Prec@(1,5) (81.2%, 99.0%)
05/30 11:28:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 0.5614	Prec@(1,5) (81.5%, 99.0%)
05/30 11:28:25午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 0.5577	Prec@(1,5) (81.7%, 99.0%)
05/30 11:28:27午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 0.5515	Prec@(1,5) (81.7%, 99.0%)
05/30 11:28:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 0.5522	Prec@(1,5) (81.7%, 99.0%)
05/30 11:28:31午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 0.5533	Prec@(1,5) (81.7%, 99.0%)
05/30 11:28:31午後 searchStage_trainer.py:260 [INFO] Valid: [ 27/49] Final Prec@1 81.6360%
05/30 11:28:31午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:28:32午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.6360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2524, 0.3097, 0.2604, 0.1774],
        [0.2635, 0.3144, 0.2525, 0.1697]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.3065, 0.2643, 0.1803],
        [0.2620, 0.2946, 0.2639, 0.1796],
        [0.2764, 0.3272, 0.2158, 0.1806]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2623, 0.2866, 0.2719, 0.1792],
        [0.2776, 0.3168, 0.2208, 0.1848],
        [0.2748, 0.3183, 0.2230, 0.1840]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2725, 0.3071, 0.2232, 0.1972],
        [0.2761, 0.3046, 0.2282, 0.1912],
        [0.2787, 0.3043, 0.2276, 0.1893]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2800, 0.3104, 0.2205, 0.1892],
        [0.2748, 0.3083, 0.2238, 0.1930],
        [0.2736, 0.3007, 0.2288, 0.1968]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.3031, 0.2358, 0.1927],
        [0.2670, 0.2980, 0.2275, 0.2074],
        [0.2681, 0.2944, 0.2279, 0.2096]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2489, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2517, 0.2526, 0.2449],
        [0.2510, 0.2498, 0.2522, 0.2469],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2510, 0.2466],
        [0.2512, 0.2527, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2510, 0.2476, 0.2498],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2511, 0.2517, 0.2480, 0.2493],
        [0.2513, 0.2524, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2485, 0.2493],
        [0.2509, 0.2505, 0.2494, 0.2492],
        [0.2513, 0.2518, 0.2495, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2504, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2518],
        [0.2503, 0.2475, 0.2492, 0.2530],
        [0.2503, 0.2506, 0.2518, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2557, 0.2548, 0.2422],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2522, 0.2547, 0.2466],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2480, 0.2560, 0.2512],
        [0.2468, 0.2504, 0.2513, 0.2515],
        [0.2438, 0.2520, 0.2561, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2539, 0.2582, 0.2381],
        [0.2470, 0.2511, 0.2562, 0.2458],
        [0.2487, 0.2445, 0.2440, 0.2628]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:28:48午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 0.2332 (0.3075)	Prec@(1,5) (89.6%, 99.7%)	
05/30 11:29:05午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 0.1757 (0.3085)	Prec@(1,5) (89.2%, 99.7%)	
05/30 11:29:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 0.3033 (0.3232)	Prec@(1,5) (88.7%, 99.7%)	
05/30 11:29:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.3452 (0.3261)	Prec@(1,5) (88.7%, 99.7%)	
05/30 11:29:54午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 0.3672 (0.3335)	Prec@(1,5) (88.4%, 99.6%)	
05/30 11:30:10午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 0.3090 (0.3365)	Prec@(1,5) (88.4%, 99.6%)	
05/30 11:30:26午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][350/390]	Step 11298	lr 0.01075	Loss 0.4470 (0.3391)	Prec@(1,5) (88.3%, 99.6%)	
05/30 11:30:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 0.3137 (0.3379)	Prec@(1,5) (88.4%, 99.6%)	
05/30 11:30:39午後 searchStage_trainer.py:221 [INFO] Train: [ 28/49] Final Prec@1 88.3560%
05/30 11:30:42午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][50/391]	Step 11339	Loss 0.5997	Prec@(1,5) (80.0%, 98.6%)
05/30 11:30:44午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 0.6079	Prec@(1,5) (80.2%, 98.6%)
05/30 11:30:46午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][150/391]	Step 11339	Loss 0.5957	Prec@(1,5) (80.5%, 98.7%)
05/30 11:30:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 0.5852	Prec@(1,5) (80.9%, 98.8%)
05/30 11:30:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][250/391]	Step 11339	Loss 0.5934	Prec@(1,5) (80.7%, 98.7%)
05/30 11:30:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 0.5911	Prec@(1,5) (80.7%, 98.8%)
05/30 11:30:55午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][350/391]	Step 11339	Loss 0.5911	Prec@(1,5) (80.7%, 98.7%)
05/30 11:30:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 0.5916	Prec@(1,5) (80.7%, 98.8%)
05/30 11:30:57午後 searchStage_trainer.py:260 [INFO] Valid: [ 28/49] Final Prec@1 80.7360%
05/30 11:30:57午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:30:58午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.6360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2523, 0.3095, 0.2605, 0.1776],
        [0.2633, 0.3143, 0.2526, 0.1698]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.3064, 0.2645, 0.1802],
        [0.2620, 0.2944, 0.2639, 0.1797],
        [0.2763, 0.3270, 0.2158, 0.1809]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2622, 0.2865, 0.2719, 0.1794],
        [0.2776, 0.3166, 0.2208, 0.1850],
        [0.2747, 0.3180, 0.2230, 0.1842]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2724, 0.3070, 0.2233, 0.1973],
        [0.2760, 0.3044, 0.2283, 0.1913],
        [0.2787, 0.3041, 0.2277, 0.1896]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2800, 0.3102, 0.2204, 0.1894],
        [0.2748, 0.3081, 0.2239, 0.1932],
        [0.2736, 0.3005, 0.2289, 0.1970]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.3030, 0.2358, 0.1928],
        [0.2670, 0.2978, 0.2276, 0.2076],
        [0.2680, 0.2942, 0.2280, 0.2098]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2489, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2517, 0.2526, 0.2449],
        [0.2510, 0.2498, 0.2522, 0.2469],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2510, 0.2465],
        [0.2512, 0.2527, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2510, 0.2476, 0.2498],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2511, 0.2517, 0.2480, 0.2493],
        [0.2513, 0.2524, 0.2488, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2486, 0.2493],
        [0.2509, 0.2505, 0.2494, 0.2492],
        [0.2513, 0.2518, 0.2496, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2504, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2518],
        [0.2503, 0.2475, 0.2492, 0.2530],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2557, 0.2548, 0.2422],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2522, 0.2547, 0.2466],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2503, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2480, 0.2560, 0.2512],
        [0.2468, 0.2504, 0.2514, 0.2515],
        [0.2438, 0.2520, 0.2561, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2539, 0.2582, 0.2381],
        [0.2470, 0.2511, 0.2562, 0.2458],
        [0.2487, 0.2445, 0.2440, 0.2628]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:31:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][50/390]	Step 11389	lr 0.01002	Loss 0.1934 (0.3293)	Prec@(1,5) (89.1%, 99.6%)	
05/30 11:31:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 0.3262 (0.3210)	Prec@(1,5) (89.3%, 99.7%)	
05/30 11:31:48午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][150/390]	Step 11489	lr 0.01002	Loss 0.3038 (0.3083)	Prec@(1,5) (89.7%, 99.7%)	
05/30 11:32:04午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.3222 (0.3095)	Prec@(1,5) (89.6%, 99.7%)	
05/30 11:32:20午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][250/390]	Step 11589	lr 0.01002	Loss 0.3290 (0.3172)	Prec@(1,5) (89.3%, 99.7%)	
05/30 11:32:36午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 0.2133 (0.3214)	Prec@(1,5) (89.1%, 99.7%)	
05/30 11:32:53午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][350/390]	Step 11689	lr 0.01002	Loss 0.1815 (0.3190)	Prec@(1,5) (89.2%, 99.7%)	
05/30 11:33:06午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 0.2558 (0.3186)	Prec@(1,5) (89.2%, 99.7%)	
05/30 11:33:06午後 searchStage_trainer.py:221 [INFO] Train: [ 29/49] Final Prec@1 89.1680%
05/30 11:33:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][50/391]	Step 11730	Loss 0.5566	Prec@(1,5) (81.3%, 98.9%)
05/30 11:33:11午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 0.5532	Prec@(1,5) (81.5%, 99.0%)
05/30 11:33:13午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][150/391]	Step 11730	Loss 0.5522	Prec@(1,5) (81.7%, 98.9%)
05/30 11:33:15午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 0.5484	Prec@(1,5) (81.8%, 99.0%)
05/30 11:33:17午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][250/391]	Step 11730	Loss 0.5493	Prec@(1,5) (81.8%, 98.9%)
05/30 11:33:19午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 0.5503	Prec@(1,5) (81.9%, 98.8%)
05/30 11:33:22午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][350/391]	Step 11730	Loss 0.5509	Prec@(1,5) (81.9%, 98.9%)
05/30 11:33:23午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 0.5536	Prec@(1,5) (81.8%, 98.9%)
05/30 11:33:24午後 searchStage_trainer.py:260 [INFO] Valid: [ 29/49] Final Prec@1 81.7560%
05/30 11:33:24午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:33:24午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.7560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2523, 0.3093, 0.2606, 0.1778],
        [0.2633, 0.3141, 0.2527, 0.1699]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.3063, 0.2647, 0.1802],
        [0.2619, 0.2943, 0.2639, 0.1799],
        [0.2763, 0.3268, 0.2159, 0.1811]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2624, 0.2866, 0.2719, 0.1792],
        [0.2775, 0.3165, 0.2208, 0.1852],
        [0.2747, 0.3178, 0.2231, 0.1844]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.3069, 0.2234, 0.1974],
        [0.2760, 0.3042, 0.2283, 0.1915],
        [0.2786, 0.3039, 0.2277, 0.1898]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2799, 0.3101, 0.2205, 0.1895],
        [0.2748, 0.3080, 0.2239, 0.1933],
        [0.2735, 0.3003, 0.2290, 0.1972]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2684, 0.3028, 0.2358, 0.1930],
        [0.2669, 0.2976, 0.2276, 0.2078],
        [0.2680, 0.2940, 0.2280, 0.2100]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2489, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2517, 0.2526, 0.2449],
        [0.2511, 0.2498, 0.2522, 0.2469],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2510, 0.2465],
        [0.2512, 0.2527, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2498],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2492, 0.2467],
        [0.2511, 0.2517, 0.2480, 0.2493],
        [0.2513, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2486, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2492],
        [0.2513, 0.2518, 0.2496, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2504, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2497, 0.2518],
        [0.2503, 0.2475, 0.2492, 0.2530],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2558, 0.2549, 0.2421],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2522, 0.2547, 0.2466],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2480, 0.2561, 0.2512],
        [0.2468, 0.2504, 0.2514, 0.2515],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2539, 0.2582, 0.2380],
        [0.2470, 0.2511, 0.2562, 0.2458],
        [0.2487, 0.2445, 0.2440, 0.2628]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:33:41午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][50/390]	Step 11780	lr 0.00929	Loss 0.4081 (0.3179)	Prec@(1,5) (88.8%, 99.7%)	
05/30 11:33:57午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 0.5545 (0.3007)	Prec@(1,5) (89.7%, 99.7%)	
05/30 11:34:13午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][150/390]	Step 11880	lr 0.00929	Loss 0.1937 (0.2911)	Prec@(1,5) (90.0%, 99.7%)	
05/30 11:34:29午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 0.3439 (0.2933)	Prec@(1,5) (89.9%, 99.7%)	
05/30 11:34:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][250/390]	Step 11980	lr 0.00929	Loss 0.2333 (0.2941)	Prec@(1,5) (89.9%, 99.7%)	
05/30 11:35:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 0.3834 (0.3046)	Prec@(1,5) (89.5%, 99.7%)	
05/30 11:35:18午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][350/390]	Step 12080	lr 0.00929	Loss 0.2082 (0.3045)	Prec@(1,5) (89.7%, 99.7%)	
05/30 11:35:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 0.4054 (0.3086)	Prec@(1,5) (89.5%, 99.7%)	
05/30 11:35:32午後 searchStage_trainer.py:221 [INFO] Train: [ 30/49] Final Prec@1 89.4800%
05/30 11:35:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][50/391]	Step 12121	Loss 0.6387	Prec@(1,5) (80.0%, 98.4%)
05/30 11:35:37午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 0.6130	Prec@(1,5) (80.7%, 98.6%)
05/30 11:35:39午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][150/391]	Step 12121	Loss 0.6164	Prec@(1,5) (80.7%, 98.7%)
05/30 11:35:41午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 0.6023	Prec@(1,5) (80.9%, 98.7%)
05/30 11:35:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][250/391]	Step 12121	Loss 0.6067	Prec@(1,5) (80.7%, 98.7%)
05/30 11:35:45午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 0.6126	Prec@(1,5) (80.6%, 98.7%)
05/30 11:35:48午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][350/391]	Step 12121	Loss 0.6085	Prec@(1,5) (80.7%, 98.7%)
05/30 11:35:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 0.6074	Prec@(1,5) (80.7%, 98.7%)
05/30 11:35:50午後 searchStage_trainer.py:260 [INFO] Valid: [ 30/49] Final Prec@1 80.7000%
05/30 11:35:50午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:35:50午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.7560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2522, 0.3091, 0.2607, 0.1780],
        [0.2632, 0.3139, 0.2528, 0.1701]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.3061, 0.2647, 0.1804],
        [0.2619, 0.2942, 0.2639, 0.1800],
        [0.2762, 0.3266, 0.2160, 0.1813]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2623, 0.2865, 0.2719, 0.1793],
        [0.2775, 0.3163, 0.2209, 0.1853],
        [0.2746, 0.3176, 0.2231, 0.1846]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.3068, 0.2233, 0.1976],
        [0.2760, 0.3041, 0.2284, 0.1916],
        [0.2785, 0.3037, 0.2278, 0.1900]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2799, 0.3100, 0.2204, 0.1897],
        [0.2747, 0.3078, 0.2240, 0.1935],
        [0.2735, 0.3001, 0.2290, 0.1974]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2683, 0.3027, 0.2358, 0.1931],
        [0.2669, 0.2975, 0.2276, 0.2080],
        [0.2679, 0.2937, 0.2281, 0.2103]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2489, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2517, 0.2526, 0.2449],
        [0.2511, 0.2498, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2510, 0.2465],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2517, 0.2480, 0.2493],
        [0.2513, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2485, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2503, 0.2475, 0.2492, 0.2530],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2558, 0.2549, 0.2421],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2522, 0.2547, 0.2466],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2480, 0.2561, 0.2512],
        [0.2468, 0.2504, 0.2514, 0.2515],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2539, 0.2582, 0.2380],
        [0.2470, 0.2511, 0.2562, 0.2458],
        [0.2487, 0.2445, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:36:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][50/390]	Step 12171	lr 0.00858	Loss 0.3827 (0.2689)	Prec@(1,5) (90.3%, 99.8%)	
05/30 11:36:23午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.2700 (0.2658)	Prec@(1,5) (90.4%, 99.8%)	
05/30 11:36:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][150/390]	Step 12271	lr 0.00858	Loss 0.2805 (0.2719)	Prec@(1,5) (90.3%, 99.8%)	
05/30 11:36:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.2695 (0.2748)	Prec@(1,5) (90.3%, 99.8%)	
05/30 11:37:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][250/390]	Step 12371	lr 0.00858	Loss 0.2050 (0.2762)	Prec@(1,5) (90.2%, 99.8%)	
05/30 11:37:28午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 0.1372 (0.2796)	Prec@(1,5) (90.1%, 99.8%)	
05/30 11:37:44午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][350/390]	Step 12471	lr 0.00858	Loss 0.3361 (0.2833)	Prec@(1,5) (90.0%, 99.8%)	
05/30 11:37:57午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 0.3323 (0.2889)	Prec@(1,5) (89.8%, 99.8%)	
05/30 11:37:57午後 searchStage_trainer.py:221 [INFO] Train: [ 31/49] Final Prec@1 89.8480%
05/30 11:38:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][50/391]	Step 12512	Loss 0.5814	Prec@(1,5) (82.0%, 98.9%)
05/30 11:38:02午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 0.5800	Prec@(1,5) (81.5%, 98.9%)
05/30 11:38:04午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][150/391]	Step 12512	Loss 0.5708	Prec@(1,5) (81.5%, 98.9%)
05/30 11:38:06午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 0.5563	Prec@(1,5) (81.7%, 99.0%)
05/30 11:38:09午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][250/391]	Step 12512	Loss 0.5610	Prec@(1,5) (81.6%, 98.9%)
05/30 11:38:11午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 0.5577	Prec@(1,5) (81.7%, 98.9%)
05/30 11:38:13午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][350/391]	Step 12512	Loss 0.5585	Prec@(1,5) (81.7%, 98.9%)
05/30 11:38:15午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 0.5597	Prec@(1,5) (81.7%, 98.9%)
05/30 11:38:15午後 searchStage_trainer.py:260 [INFO] Valid: [ 31/49] Final Prec@1 81.6800%
05/30 11:38:15午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:38:15午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 81.7560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2521, 0.3089, 0.2608, 0.1781],
        [0.2632, 0.3137, 0.2529, 0.1703]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.3061, 0.2648, 0.1804],
        [0.2618, 0.2941, 0.2640, 0.1802],
        [0.2762, 0.3264, 0.2160, 0.1814]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2625, 0.2864, 0.2718, 0.1794],
        [0.2774, 0.3162, 0.2209, 0.1855],
        [0.2746, 0.3175, 0.2232, 0.1848]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2724, 0.3067, 0.2232, 0.1977],
        [0.2759, 0.3039, 0.2284, 0.1917],
        [0.2785, 0.3035, 0.2278, 0.1902]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2798, 0.3099, 0.2205, 0.1898],
        [0.2746, 0.3077, 0.2241, 0.1936],
        [0.2734, 0.2999, 0.2291, 0.1976]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2683, 0.3026, 0.2358, 0.1933],
        [0.2668, 0.2973, 0.2277, 0.2082],
        [0.2678, 0.2935, 0.2282, 0.2105]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2489, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2517, 0.2526, 0.2449],
        [0.2511, 0.2498, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2510, 0.2465],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2493],
        [0.2513, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2485, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2503, 0.2475, 0.2492, 0.2530],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2558, 0.2549, 0.2421],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2480, 0.2561, 0.2512],
        [0.2468, 0.2504, 0.2514, 0.2515],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2539, 0.2583, 0.2380],
        [0.2470, 0.2511, 0.2562, 0.2458],
        [0.2487, 0.2445, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:38:32午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][50/390]	Step 12562	lr 0.00789	Loss 0.3929 (0.2702)	Prec@(1,5) (89.8%, 99.9%)	
05/30 11:38:49午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.1780 (0.2611)	Prec@(1,5) (90.7%, 99.9%)	
05/30 11:39:05午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][150/390]	Step 12662	lr 0.00789	Loss 0.3385 (0.2560)	Prec@(1,5) (90.9%, 99.8%)	
05/30 11:39:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.2380 (0.2566)	Prec@(1,5) (90.8%, 99.9%)	
05/30 11:39:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][250/390]	Step 12762	lr 0.00789	Loss 0.2750 (0.2612)	Prec@(1,5) (90.6%, 99.8%)	
05/30 11:39:54午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.2152 (0.2639)	Prec@(1,5) (90.5%, 99.8%)	
05/30 11:40:10午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][350/390]	Step 12862	lr 0.00789	Loss 0.4021 (0.2716)	Prec@(1,5) (90.3%, 99.8%)	
05/30 11:40:23午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 0.2782 (0.2726)	Prec@(1,5) (90.3%, 99.8%)	
05/30 11:40:23午後 searchStage_trainer.py:221 [INFO] Train: [ 32/49] Final Prec@1 90.3160%
05/30 11:40:26午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][50/391]	Step 12903	Loss 0.5402	Prec@(1,5) (82.9%, 99.1%)
05/30 11:40:28午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 0.5306	Prec@(1,5) (83.3%, 99.0%)
05/30 11:40:30午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][150/391]	Step 12903	Loss 0.5370	Prec@(1,5) (83.3%, 99.0%)
05/30 11:40:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 0.5406	Prec@(1,5) (83.2%, 99.0%)
05/30 11:40:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][250/391]	Step 12903	Loss 0.5365	Prec@(1,5) (83.2%, 98.9%)
05/30 11:40:37午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 0.5367	Prec@(1,5) (83.2%, 99.0%)
05/30 11:40:39午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][350/391]	Step 12903	Loss 0.5389	Prec@(1,5) (83.1%, 99.0%)
05/30 11:40:41午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 0.5340	Prec@(1,5) (83.2%, 99.0%)
05/30 11:40:41午後 searchStage_trainer.py:260 [INFO] Valid: [ 32/49] Final Prec@1 83.2200%
05/30 11:40:41午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:40:41午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.2200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2521, 0.3088, 0.2609, 0.1782],
        [0.2631, 0.3136, 0.2529, 0.1704]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.3059, 0.2649, 0.1805],
        [0.2618, 0.2939, 0.2641, 0.1803],
        [0.2761, 0.3262, 0.2161, 0.1816]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2864, 0.2718, 0.1792],
        [0.2774, 0.3160, 0.2210, 0.1856],
        [0.2745, 0.3173, 0.2232, 0.1850]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2724, 0.3066, 0.2231, 0.1979],
        [0.2759, 0.3038, 0.2285, 0.1918],
        [0.2784, 0.3033, 0.2279, 0.1903]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2799, 0.3097, 0.2204, 0.1900],
        [0.2746, 0.3075, 0.2241, 0.1938],
        [0.2733, 0.2998, 0.2291, 0.1978]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2683, 0.3024, 0.2359, 0.1934],
        [0.2668, 0.2971, 0.2277, 0.2084],
        [0.2678, 0.2934, 0.2282, 0.2107]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2489, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2517, 0.2526, 0.2449],
        [0.2511, 0.2498, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2519, 0.2510, 0.2465],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2493],
        [0.2513, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2485, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2503, 0.2475, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2558, 0.2549, 0.2420],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2477, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2480, 0.2561, 0.2512],
        [0.2468, 0.2504, 0.2514, 0.2514],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2583, 0.2379],
        [0.2470, 0.2511, 0.2562, 0.2458],
        [0.2488, 0.2445, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:40:58午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][50/390]	Step 12953	lr 0.00722	Loss 0.3950 (0.2488)	Prec@(1,5) (91.4%, 99.7%)	
05/30 11:41:14午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.2282 (0.2409)	Prec@(1,5) (91.5%, 99.8%)	
05/30 11:41:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][150/390]	Step 13053	lr 0.00722	Loss 0.2998 (0.2391)	Prec@(1,5) (91.6%, 99.8%)	
05/30 11:41:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.2181 (0.2411)	Prec@(1,5) (91.4%, 99.9%)	
05/30 11:42:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][250/390]	Step 13153	lr 0.00722	Loss 0.2711 (0.2402)	Prec@(1,5) (91.5%, 99.9%)	
05/30 11:42:19午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.1233 (0.2411)	Prec@(1,5) (91.5%, 99.8%)	
05/30 11:42:36午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][350/390]	Step 13253	lr 0.00722	Loss 0.1085 (0.2474)	Prec@(1,5) (91.2%, 99.8%)	
05/30 11:42:49午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 0.2947 (0.2476)	Prec@(1,5) (91.2%, 99.8%)	
05/30 11:42:49午後 searchStage_trainer.py:221 [INFO] Train: [ 33/49] Final Prec@1 91.2520%
05/30 11:42:52午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][50/391]	Step 13294	Loss 0.5123	Prec@(1,5) (83.6%, 99.1%)
05/30 11:42:54午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 0.4978	Prec@(1,5) (83.9%, 99.2%)
05/30 11:42:56午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][150/391]	Step 13294	Loss 0.4950	Prec@(1,5) (83.9%, 99.2%)
05/30 11:42:58午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 0.4996	Prec@(1,5) (83.9%, 99.2%)
05/30 11:43:00午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][250/391]	Step 13294	Loss 0.5039	Prec@(1,5) (83.8%, 99.1%)
05/30 11:43:03午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 0.5014	Prec@(1,5) (83.8%, 99.2%)
05/30 11:43:05午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][350/391]	Step 13294	Loss 0.5012	Prec@(1,5) (83.9%, 99.1%)
05/30 11:43:07午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 0.5039	Prec@(1,5) (83.8%, 99.1%)
05/30 11:43:07午後 searchStage_trainer.py:260 [INFO] Valid: [ 33/49] Final Prec@1 83.8120%
05/30 11:43:07午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:43:07午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.8120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2520, 0.3086, 0.2610, 0.1784],
        [0.2631, 0.3134, 0.2530, 0.1705]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.3057, 0.2650, 0.1806],
        [0.2617, 0.2938, 0.2642, 0.1803],
        [0.2761, 0.3260, 0.2161, 0.1818]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2627, 0.2863, 0.2718, 0.1792],
        [0.2774, 0.3159, 0.2210, 0.1858],
        [0.2745, 0.3171, 0.2233, 0.1851]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2724, 0.3065, 0.2231, 0.1980],
        [0.2759, 0.3036, 0.2285, 0.1920],
        [0.2784, 0.3032, 0.2279, 0.1905]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2798, 0.3096, 0.2204, 0.1901],
        [0.2746, 0.3073, 0.2241, 0.1940],
        [0.2733, 0.2996, 0.2292, 0.1979]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2683, 0.3023, 0.2359, 0.1936],
        [0.2667, 0.2970, 0.2278, 0.2086],
        [0.2677, 0.2932, 0.2283, 0.2109]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2449],
        [0.2511, 0.2498, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2493],
        [0.2513, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2505, 0.2485, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2503, 0.2475, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2559, 0.2549, 0.2420],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2480, 0.2561, 0.2512],
        [0.2467, 0.2504, 0.2514, 0.2514],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2583, 0.2379],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2445, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:43:24午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][50/390]	Step 13344	lr 0.00657	Loss 0.2458 (0.2017)	Prec@(1,5) (93.2%, 99.8%)	
05/30 11:43:40午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.0888 (0.2047)	Prec@(1,5) (93.2%, 99.9%)	
05/30 11:43:57午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][150/390]	Step 13444	lr 0.00657	Loss 0.1278 (0.2110)	Prec@(1,5) (93.2%, 99.9%)	
05/30 11:44:13午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.3431 (0.2122)	Prec@(1,5) (93.0%, 99.9%)	
05/30 11:44:30午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][250/390]	Step 13544	lr 0.00657	Loss 0.1833 (0.2184)	Prec@(1,5) (92.7%, 99.9%)	
05/30 11:44:46午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.2557 (0.2222)	Prec@(1,5) (92.4%, 99.9%)	
05/30 11:45:02午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][350/390]	Step 13644	lr 0.00657	Loss 0.1706 (0.2250)	Prec@(1,5) (92.3%, 99.9%)	
05/30 11:45:15午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 0.0882 (0.2259)	Prec@(1,5) (92.2%, 99.9%)	
05/30 11:45:15午後 searchStage_trainer.py:221 [INFO] Train: [ 34/49] Final Prec@1 92.2400%
05/30 11:45:18午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][50/391]	Step 13685	Loss 0.5344	Prec@(1,5) (84.0%, 98.7%)
05/30 11:45:20午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 0.5419	Prec@(1,5) (83.5%, 99.0%)
05/30 11:45:22午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][150/391]	Step 13685	Loss 0.5359	Prec@(1,5) (83.6%, 99.0%)
05/30 11:45:24午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 0.5332	Prec@(1,5) (83.5%, 99.1%)
05/30 11:45:27午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][250/391]	Step 13685	Loss 0.5246	Prec@(1,5) (83.7%, 99.1%)
05/30 11:45:29午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 0.5176	Prec@(1,5) (83.9%, 99.1%)
05/30 11:45:31午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][350/391]	Step 13685	Loss 0.5133	Prec@(1,5) (84.0%, 99.1%)
05/30 11:45:33午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 0.5139	Prec@(1,5) (84.0%, 99.1%)
05/30 11:45:33午後 searchStage_trainer.py:260 [INFO] Valid: [ 34/49] Final Prec@1 83.9880%
05/30 11:45:33午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:45:33午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.9880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2519, 0.3085, 0.2611, 0.1785],
        [0.2630, 0.3132, 0.2531, 0.1707]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.3057, 0.2651, 0.1807],
        [0.2616, 0.2937, 0.2643, 0.1804],
        [0.2760, 0.3258, 0.2162, 0.1819]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2627, 0.2861, 0.2719, 0.1794],
        [0.2774, 0.3157, 0.2210, 0.1859],
        [0.2744, 0.3170, 0.2234, 0.1852]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2724, 0.3064, 0.2232, 0.1981],
        [0.2759, 0.3035, 0.2286, 0.1921],
        [0.2784, 0.3030, 0.2280, 0.1907]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2798, 0.3095, 0.2205, 0.1901],
        [0.2746, 0.3072, 0.2241, 0.1941],
        [0.2732, 0.2994, 0.2292, 0.1981]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2683, 0.3022, 0.2359, 0.1937],
        [0.2667, 0.2968, 0.2278, 0.2087],
        [0.2676, 0.2930, 0.2283, 0.2110]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2449],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2458]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2493],
        [0.2512, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2485, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2503, 0.2475, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2559, 0.2550, 0.2420],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2480, 0.2562, 0.2512],
        [0.2467, 0.2504, 0.2514, 0.2514],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2583, 0.2379],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2445, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:45:50午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][50/390]	Step 13735	lr 0.00595	Loss 0.2271 (0.1917)	Prec@(1,5) (93.5%, 99.8%)	
05/30 11:46:06午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.1858 (0.1926)	Prec@(1,5) (93.5%, 99.9%)	
05/30 11:46:23午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][150/390]	Step 13835	lr 0.00595	Loss 0.2239 (0.1950)	Prec@(1,5) (93.4%, 99.9%)	
05/30 11:46:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 0.0854 (0.1987)	Prec@(1,5) (93.2%, 99.9%)	
05/30 11:46:55午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][250/390]	Step 13935	lr 0.00595	Loss 0.1876 (0.2004)	Prec@(1,5) (93.1%, 99.9%)	
05/30 11:47:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 0.2044 (0.2017)	Prec@(1,5) (93.0%, 99.8%)	
05/30 11:47:28午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][350/390]	Step 14035	lr 0.00595	Loss 0.2813 (0.2021)	Prec@(1,5) (93.0%, 99.9%)	
05/30 11:47:41午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 0.1475 (0.2031)	Prec@(1,5) (92.9%, 99.9%)	
05/30 11:47:41午後 searchStage_trainer.py:221 [INFO] Train: [ 35/49] Final Prec@1 92.9040%
05/30 11:47:44午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][50/391]	Step 14076	Loss 0.5816	Prec@(1,5) (82.7%, 99.0%)
05/30 11:47:46午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 0.5635	Prec@(1,5) (83.3%, 98.9%)
05/30 11:47:48午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][150/391]	Step 14076	Loss 0.5405	Prec@(1,5) (83.9%, 98.9%)
05/30 11:47:50午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 0.5417	Prec@(1,5) (83.6%, 99.0%)
05/30 11:47:53午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][250/391]	Step 14076	Loss 0.5368	Prec@(1,5) (83.7%, 99.0%)
05/30 11:47:55午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 0.5346	Prec@(1,5) (83.6%, 99.1%)
05/30 11:47:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][350/391]	Step 14076	Loss 0.5300	Prec@(1,5) (83.6%, 99.1%)
05/30 11:47:59午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 0.5317	Prec@(1,5) (83.7%, 99.1%)
05/30 11:47:59午後 searchStage_trainer.py:260 [INFO] Valid: [ 35/49] Final Prec@1 83.6600%
05/30 11:47:59午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:47:59午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 83.9880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2519, 0.3084, 0.2612, 0.1786],
        [0.2629, 0.3131, 0.2531, 0.1708]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.3056, 0.2651, 0.1807],
        [0.2616, 0.2937, 0.2644, 0.1804],
        [0.2760, 0.3257, 0.2162, 0.1821]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2625, 0.2861, 0.2720, 0.1794],
        [0.2773, 0.3156, 0.2210, 0.1861],
        [0.2744, 0.3168, 0.2234, 0.1854]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2724, 0.3063, 0.2232, 0.1981],
        [0.2758, 0.3034, 0.2286, 0.1922],
        [0.2783, 0.3028, 0.2280, 0.1908]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2798, 0.3094, 0.2205, 0.1903],
        [0.2745, 0.3071, 0.2242, 0.1942],
        [0.2732, 0.2993, 0.2293, 0.1983]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2682, 0.3020, 0.2359, 0.1938],
        [0.2666, 0.2967, 0.2278, 0.2088],
        [0.2676, 0.2928, 0.2284, 0.2112]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2513],
        [0.2506, 0.2523, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2449],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2466],
        [0.2510, 0.2516, 0.2480, 0.2493],
        [0.2512, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2485, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2503, 0.2475, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2559, 0.2550, 0.2420],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2480, 0.2562, 0.2512],
        [0.2467, 0.2504, 0.2514, 0.2514],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2583, 0.2379],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2445, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:48:16午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][50/390]	Step 14126	lr 0.00535	Loss 0.0965 (0.1901)	Prec@(1,5) (93.5%, 99.7%)	
05/30 11:48:32午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.3147 (0.1843)	Prec@(1,5) (93.7%, 99.8%)	
05/30 11:48:49午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][150/390]	Step 14226	lr 0.00535	Loss 0.1121 (0.1793)	Prec@(1,5) (93.7%, 99.9%)	
05/30 11:49:05午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.2372 (0.1856)	Prec@(1,5) (93.5%, 99.9%)	
05/30 11:49:21午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][250/390]	Step 14326	lr 0.00535	Loss 0.1336 (0.1874)	Prec@(1,5) (93.5%, 99.9%)	
05/30 11:49:37午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.2819 (0.1893)	Prec@(1,5) (93.4%, 99.9%)	
05/30 11:49:53午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][350/390]	Step 14426	lr 0.00535	Loss 0.2948 (0.1931)	Prec@(1,5) (93.2%, 99.9%)	
05/30 11:50:06午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.1235 (0.1913)	Prec@(1,5) (93.3%, 99.9%)	
05/30 11:50:07午後 searchStage_trainer.py:221 [INFO] Train: [ 36/49] Final Prec@1 93.3120%
05/30 11:50:09午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][50/391]	Step 14467	Loss 0.5217	Prec@(1,5) (83.9%, 99.5%)
05/30 11:50:12午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 0.5163	Prec@(1,5) (84.2%, 99.3%)
05/30 11:50:14午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][150/391]	Step 14467	Loss 0.5196	Prec@(1,5) (84.3%, 99.2%)
05/30 11:50:16午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 0.5229	Prec@(1,5) (84.2%, 99.2%)
05/30 11:50:18午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][250/391]	Step 14467	Loss 0.5225	Prec@(1,5) (84.4%, 99.1%)
05/30 11:50:20午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 0.5211	Prec@(1,5) (84.5%, 99.2%)
05/30 11:50:22午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][350/391]	Step 14467	Loss 0.5174	Prec@(1,5) (84.6%, 99.2%)
05/30 11:50:24午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 0.5145	Prec@(1,5) (84.6%, 99.2%)
05/30 11:50:24午後 searchStage_trainer.py:260 [INFO] Valid: [ 36/49] Final Prec@1 84.6280%
05/30 11:50:24午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:50:25午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.6280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2518, 0.3082, 0.2612, 0.1787],
        [0.2629, 0.3129, 0.2532, 0.1710]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.3055, 0.2652, 0.1808],
        [0.2615, 0.2936, 0.2644, 0.1805],
        [0.2760, 0.3255, 0.2162, 0.1822]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2625, 0.2859, 0.2720, 0.1795],
        [0.2773, 0.3155, 0.2210, 0.1862],
        [0.2744, 0.3167, 0.2234, 0.1855]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2724, 0.3062, 0.2232, 0.1982],
        [0.2758, 0.3033, 0.2286, 0.1924],
        [0.2783, 0.3027, 0.2281, 0.1910]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2798, 0.3093, 0.2205, 0.1904],
        [0.2745, 0.3070, 0.2242, 0.1944],
        [0.2731, 0.2991, 0.2293, 0.1984]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2682, 0.3020, 0.2359, 0.1939],
        [0.2666, 0.2966, 0.2279, 0.2090],
        [0.2675, 0.2927, 0.2284, 0.2114]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2513],
        [0.2506, 0.2522, 0.2521, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2449],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2519, 0.2511, 0.2465],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2493],
        [0.2512, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2485, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2503, 0.2475, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2559, 0.2550, 0.2419],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2480, 0.2562, 0.2512],
        [0.2467, 0.2504, 0.2514, 0.2514],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2583, 0.2378],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2445, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:50:41午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][50/390]	Step 14517	lr 0.00479	Loss 0.1871 (0.1645)	Prec@(1,5) (94.2%, 100.0%)	
05/30 11:50:58午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.1914 (0.1601)	Prec@(1,5) (94.3%, 100.0%)	
05/30 11:51:14午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][150/390]	Step 14617	lr 0.00479	Loss 0.1274 (0.1584)	Prec@(1,5) (94.3%, 99.9%)	
05/30 11:51:31午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.1277 (0.1622)	Prec@(1,5) (94.2%, 99.9%)	
05/30 11:51:47午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][250/390]	Step 14717	lr 0.00479	Loss 0.2342 (0.1670)	Prec@(1,5) (94.0%, 99.9%)	
05/30 11:52:03午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.2225 (0.1679)	Prec@(1,5) (94.1%, 99.9%)	
05/30 11:52:20午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][350/390]	Step 14817	lr 0.00479	Loss 0.2730 (0.1711)	Prec@(1,5) (93.9%, 99.9%)	
05/30 11:52:33午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.1637 (0.1720)	Prec@(1,5) (93.9%, 99.9%)	
05/30 11:52:33午後 searchStage_trainer.py:221 [INFO] Train: [ 37/49] Final Prec@1 93.8720%
05/30 11:52:35午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][50/391]	Step 14858	Loss 0.5361	Prec@(1,5) (84.8%, 98.7%)
05/30 11:52:38午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 0.5229	Prec@(1,5) (84.9%, 98.9%)
05/30 11:52:40午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][150/391]	Step 14858	Loss 0.5213	Prec@(1,5) (84.9%, 99.0%)
05/30 11:52:42午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 0.5228	Prec@(1,5) (84.7%, 99.1%)
05/30 11:52:44午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][250/391]	Step 14858	Loss 0.5170	Prec@(1,5) (84.7%, 99.1%)
05/30 11:52:47午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 0.5204	Prec@(1,5) (84.7%, 99.1%)
05/30 11:52:49午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][350/391]	Step 14858	Loss 0.5190	Prec@(1,5) (84.6%, 99.1%)
05/30 11:52:51午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 0.5198	Prec@(1,5) (84.5%, 99.1%)
05/30 11:52:51午後 searchStage_trainer.py:260 [INFO] Valid: [ 37/49] Final Prec@1 84.5560%
05/30 11:52:51午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:52:51午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.6280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2518, 0.3081, 0.2613, 0.1788],
        [0.2629, 0.3128, 0.2533, 0.1710]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.3054, 0.2652, 0.1809],
        [0.2615, 0.2935, 0.2645, 0.1806],
        [0.2759, 0.3254, 0.2163, 0.1824]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2857, 0.2720, 0.1797],
        [0.2773, 0.3154, 0.2210, 0.1863],
        [0.2743, 0.3165, 0.2235, 0.1857]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2724, 0.3061, 0.2232, 0.1983],
        [0.2757, 0.3031, 0.2287, 0.1924],
        [0.2782, 0.3026, 0.2281, 0.1911]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2798, 0.3092, 0.2206, 0.1904],
        [0.2744, 0.3068, 0.2242, 0.1945],
        [0.2731, 0.2990, 0.2294, 0.1986]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2681, 0.3019, 0.2360, 0.1940],
        [0.2666, 0.2964, 0.2279, 0.2091],
        [0.2675, 0.2925, 0.2284, 0.2115]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2513],
        [0.2506, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2449],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2465],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2515, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2493],
        [0.2512, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2485, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2503, 0.2475, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2559, 0.2550, 0.2419],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2480, 0.2562, 0.2512],
        [0.2467, 0.2504, 0.2514, 0.2514],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2584, 0.2378],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2445, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:53:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][50/390]	Step 14908	lr 0.00425	Loss 0.1570 (0.1624)	Prec@(1,5) (94.0%, 100.0%)	
05/30 11:53:24午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.0421 (0.1568)	Prec@(1,5) (94.3%, 100.0%)	
05/30 11:53:40午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][150/390]	Step 15008	lr 0.00425	Loss 0.1283 (0.1547)	Prec@(1,5) (94.4%, 100.0%)	
05/30 11:53:56午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.0979 (0.1552)	Prec@(1,5) (94.5%, 100.0%)	
05/30 11:54:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][250/390]	Step 15108	lr 0.00425	Loss 0.1119 (0.1524)	Prec@(1,5) (94.7%, 100.0%)	
05/30 11:54:29午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.2725 (0.1523)	Prec@(1,5) (94.6%, 100.0%)	
05/30 11:54:45午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][350/390]	Step 15208	lr 0.00425	Loss 0.1658 (0.1516)	Prec@(1,5) (94.7%, 100.0%)	
05/30 11:54:58午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.2066 (0.1511)	Prec@(1,5) (94.7%, 100.0%)	
05/30 11:54:59午後 searchStage_trainer.py:221 [INFO] Train: [ 38/49] Final Prec@1 94.6840%
05/30 11:55:01午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][50/391]	Step 15249	Loss 0.4905	Prec@(1,5) (85.2%, 99.5%)
05/30 11:55:03午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 0.5137	Prec@(1,5) (84.9%, 99.4%)
05/30 11:55:05午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][150/391]	Step 15249	Loss 0.5332	Prec@(1,5) (84.5%, 99.3%)
05/30 11:55:08午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 0.5413	Prec@(1,5) (84.5%, 99.2%)
05/30 11:55:10午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][250/391]	Step 15249	Loss 0.5366	Prec@(1,5) (84.7%, 99.2%)
05/30 11:55:12午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 0.5359	Prec@(1,5) (84.8%, 99.2%)
05/30 11:55:14午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][350/391]	Step 15249	Loss 0.5292	Prec@(1,5) (84.9%, 99.2%)
05/30 11:55:16午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 0.5328	Prec@(1,5) (84.8%, 99.2%)
05/30 11:55:16午後 searchStage_trainer.py:260 [INFO] Valid: [ 38/49] Final Prec@1 84.8560%
05/30 11:55:16午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:55:17午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.8560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2517, 0.3080, 0.2614, 0.1789],
        [0.2628, 0.3127, 0.2533, 0.1712]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.3053, 0.2653, 0.1810],
        [0.2614, 0.2934, 0.2645, 0.1806],
        [0.2759, 0.3253, 0.2163, 0.1825]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2625, 0.2856, 0.2721, 0.1799],
        [0.2773, 0.3153, 0.2211, 0.1864],
        [0.2743, 0.3164, 0.2235, 0.1858]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2724, 0.3060, 0.2232, 0.1984],
        [0.2757, 0.3030, 0.2287, 0.1926],
        [0.2782, 0.3024, 0.2282, 0.1912]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2798, 0.3091, 0.2206, 0.1905],
        [0.2744, 0.3067, 0.2243, 0.1946],
        [0.2730, 0.2989, 0.2294, 0.1987]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2681, 0.3018, 0.2360, 0.1941],
        [0.2665, 0.2963, 0.2279, 0.2093],
        [0.2674, 0.2924, 0.2285, 0.2117]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2513],
        [0.2506, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2449],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2514, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2493],
        [0.2512, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2485, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2474]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2514, 0.2487, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2503, 0.2475, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2559, 0.2550, 0.2419],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2480, 0.2562, 0.2512],
        [0.2467, 0.2504, 0.2514, 0.2514],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2584, 0.2378],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2444, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:55:34午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][50/390]	Step 15299	lr 0.00375	Loss 0.0622 (0.1288)	Prec@(1,5) (95.5%, 99.9%)	
05/30 11:55:50午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.1594 (0.1293)	Prec@(1,5) (95.6%, 99.9%)	
05/30 11:56:07午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][150/390]	Step 15399	lr 0.00375	Loss 0.0970 (0.1315)	Prec@(1,5) (95.5%, 99.9%)	
05/30 11:56:23午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.1428 (0.1318)	Prec@(1,5) (95.5%, 99.9%)	
05/30 11:56:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][250/390]	Step 15499	lr 0.00375	Loss 0.1558 (0.1335)	Prec@(1,5) (95.3%, 99.9%)	
05/30 11:56:56午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.1187 (0.1335)	Prec@(1,5) (95.3%, 99.9%)	
05/30 11:57:12午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][350/390]	Step 15599	lr 0.00375	Loss 0.1313 (0.1353)	Prec@(1,5) (95.3%, 99.9%)	
05/30 11:57:25午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.1539 (0.1366)	Prec@(1,5) (95.2%, 99.9%)	
05/30 11:57:25午後 searchStage_trainer.py:221 [INFO] Train: [ 39/49] Final Prec@1 95.2280%
05/30 11:57:28午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][50/391]	Step 15640	Loss 0.5262	Prec@(1,5) (84.8%, 99.3%)
05/30 11:57:30午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 0.5435	Prec@(1,5) (84.2%, 99.1%)
05/30 11:57:32午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][150/391]	Step 15640	Loss 0.5319	Prec@(1,5) (84.6%, 99.2%)
05/30 11:57:34午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 0.5282	Prec@(1,5) (84.7%, 99.2%)
05/30 11:57:37午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][250/391]	Step 15640	Loss 0.5213	Prec@(1,5) (84.9%, 99.2%)
05/30 11:57:39午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 0.5204	Prec@(1,5) (85.0%, 99.2%)
05/30 11:57:41午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][350/391]	Step 15640	Loss 0.5252	Prec@(1,5) (84.9%, 99.2%)
05/30 11:57:43午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 0.5206	Prec@(1,5) (84.9%, 99.2%)
05/30 11:57:43午後 searchStage_trainer.py:260 [INFO] Valid: [ 39/49] Final Prec@1 84.9040%
05/30 11:57:43午後 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/30 11:57:44午後 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 84.9040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2517, 0.3079, 0.2614, 0.1790],
        [0.2628, 0.3126, 0.2534, 0.1713]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.3053, 0.2653, 0.1810],
        [0.2613, 0.2934, 0.2646, 0.1807],
        [0.2759, 0.3251, 0.2163, 0.1826]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2624, 0.2856, 0.2721, 0.1799],
        [0.2772, 0.3152, 0.2211, 0.1865],
        [0.2743, 0.3163, 0.2236, 0.1859]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.3059, 0.2232, 0.1986],
        [0.2757, 0.3029, 0.2287, 0.1926],
        [0.2781, 0.3023, 0.2282, 0.1913]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2797, 0.3091, 0.2206, 0.1906],
        [0.2744, 0.3066, 0.2243, 0.1947],
        [0.2730, 0.2987, 0.2294, 0.1988]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2681, 0.3017, 0.2360, 0.1942],
        [0.2665, 0.2962, 0.2279, 0.2094],
        [0.2674, 0.2923, 0.2285, 0.2118]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2514],
        [0.2506, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2450],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2541, 0.2474, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2514, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2494],
        [0.2512, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2485, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2515, 0.2487, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2504, 0.2475, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2560, 0.2550, 0.2419],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2480, 0.2562, 0.2512],
        [0.2467, 0.2504, 0.2514, 0.2514],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2584, 0.2378],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2444, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/30 11:58:01午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][50/390]	Step 15690	lr 0.00329	Loss 0.1409 (0.1270)	Prec@(1,5) (95.4%, 99.9%)	
05/30 11:58:17午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.0907 (0.1153)	Prec@(1,5) (96.0%, 100.0%)	
05/30 11:58:33午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][150/390]	Step 15790	lr 0.00329	Loss 0.0461 (0.1217)	Prec@(1,5) (95.8%, 100.0%)	
05/30 11:58:50午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.0818 (0.1229)	Prec@(1,5) (95.7%, 100.0%)	
05/30 11:59:06午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][250/390]	Step 15890	lr 0.00329	Loss 0.1254 (0.1198)	Prec@(1,5) (95.8%, 100.0%)	
05/30 11:59:22午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.0662 (0.1225)	Prec@(1,5) (95.7%, 100.0%)	
05/30 11:59:39午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][350/390]	Step 15990	lr 0.00329	Loss 0.1059 (0.1220)	Prec@(1,5) (95.7%, 100.0%)	
05/30 11:59:52午後 searchStage_trainer.py:211 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.1931 (0.1213)	Prec@(1,5) (95.7%, 100.0%)	
05/30 11:59:52午後 searchStage_trainer.py:221 [INFO] Train: [ 40/49] Final Prec@1 95.7400%
05/30 11:59:54午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][50/391]	Step 16031	Loss 0.5054	Prec@(1,5) (85.3%, 99.2%)
05/30 11:59:57午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 0.5317	Prec@(1,5) (84.7%, 99.2%)
05/30 11:59:59午後 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][150/391]	Step 16031	Loss 0.5335	Prec@(1,5) (84.8%, 99.2%)
05/31 12:00:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 0.5323	Prec@(1,5) (84.7%, 99.2%)
05/31 12:00:03午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][250/391]	Step 16031	Loss 0.5241	Prec@(1,5) (85.0%, 99.3%)
05/31 12:00:05午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 0.5191	Prec@(1,5) (85.2%, 99.2%)
05/31 12:00:08午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][350/391]	Step 16031	Loss 0.5162	Prec@(1,5) (85.2%, 99.3%)
05/31 12:00:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 0.5157	Prec@(1,5) (85.3%, 99.2%)
05/31 12:00:09午前 searchStage_trainer.py:260 [INFO] Valid: [ 40/49] Final Prec@1 85.2680%
05/31 12:00:09午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:00:10午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.2680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2517, 0.3077, 0.2615, 0.1791],
        [0.2627, 0.3125, 0.2535, 0.1713]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.3052, 0.2654, 0.1811],
        [0.2613, 0.2933, 0.2647, 0.1808],
        [0.2758, 0.3250, 0.2164, 0.1827]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2622, 0.2857, 0.2722, 0.1798],
        [0.2772, 0.3151, 0.2211, 0.1866],
        [0.2742, 0.3161, 0.2236, 0.1860]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.3059, 0.2232, 0.1986],
        [0.2757, 0.3028, 0.2288, 0.1927],
        [0.2781, 0.3022, 0.2282, 0.1915]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2798, 0.3090, 0.2205, 0.1907],
        [0.2743, 0.3065, 0.2243, 0.1948],
        [0.2730, 0.2986, 0.2295, 0.1989]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2681, 0.3016, 0.2360, 0.1943],
        [0.2664, 0.2961, 0.2280, 0.2095],
        [0.2673, 0.2921, 0.2286, 0.2120]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2514],
        [0.2506, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2450],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2540, 0.2474, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2514, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2518, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2494],
        [0.2512, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2486, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2515, 0.2487, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2504, 0.2474, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2560, 0.2550, 0.2418],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2480, 0.2562, 0.2512],
        [0.2467, 0.2505, 0.2514, 0.2514],
        [0.2438, 0.2520, 0.2562, 0.2480]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2584, 0.2378],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2444, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:00:27午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][50/390]	Step 16081	lr 0.00287	Loss 0.0859 (0.0974)	Prec@(1,5) (97.1%, 99.9%)	
05/31 12:00:43午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.1095 (0.1043)	Prec@(1,5) (96.7%, 100.0%)	
05/31 12:01:00午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][150/390]	Step 16181	lr 0.00287	Loss 0.1802 (0.1047)	Prec@(1,5) (96.6%, 100.0%)	
05/31 12:01:16午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.2150 (0.1061)	Prec@(1,5) (96.5%, 100.0%)	
05/31 12:01:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][250/390]	Step 16281	lr 0.00287	Loss 0.1436 (0.1064)	Prec@(1,5) (96.5%, 100.0%)	
05/31 12:01:48午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.1161 (0.1055)	Prec@(1,5) (96.5%, 100.0%)	
05/31 12:02:05午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][350/390]	Step 16381	lr 0.00287	Loss 0.0420 (0.1087)	Prec@(1,5) (96.4%, 100.0%)	
05/31 12:02:17午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.0804 (0.1084)	Prec@(1,5) (96.4%, 100.0%)	
05/31 12:02:18午前 searchStage_trainer.py:221 [INFO] Train: [ 41/49] Final Prec@1 96.3760%
05/31 12:02:20午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][50/391]	Step 16422	Loss 0.5104	Prec@(1,5) (85.6%, 99.5%)
05/31 12:02:22午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 0.5299	Prec@(1,5) (85.1%, 99.3%)
05/31 12:02:25午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][150/391]	Step 16422	Loss 0.5325	Prec@(1,5) (85.2%, 99.3%)
05/31 12:02:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 0.5355	Prec@(1,5) (85.2%, 99.3%)
05/31 12:02:29午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][250/391]	Step 16422	Loss 0.5427	Prec@(1,5) (84.9%, 99.3%)
05/31 12:02:31午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 0.5429	Prec@(1,5) (84.9%, 99.3%)
05/31 12:02:34午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][350/391]	Step 16422	Loss 0.5430	Prec@(1,5) (84.9%, 99.2%)
05/31 12:02:35午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 0.5390	Prec@(1,5) (85.0%, 99.3%)
05/31 12:02:35午前 searchStage_trainer.py:260 [INFO] Valid: [ 41/49] Final Prec@1 85.0440%
05/31 12:02:35午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:02:36午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.2680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2516, 0.3076, 0.2615, 0.1792],
        [0.2627, 0.3124, 0.2535, 0.1714]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.3051, 0.2655, 0.1812],
        [0.2612, 0.2932, 0.2647, 0.1809],
        [0.2758, 0.3249, 0.2164, 0.1829]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2622, 0.2857, 0.2723, 0.1798],
        [0.2772, 0.3150, 0.2211, 0.1867],
        [0.2742, 0.3160, 0.2237, 0.1861]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.3058, 0.2232, 0.1987],
        [0.2757, 0.3027, 0.2288, 0.1928],
        [0.2781, 0.3021, 0.2283, 0.1916]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2797, 0.3089, 0.2206, 0.1908],
        [0.2743, 0.3064, 0.2244, 0.1949],
        [0.2729, 0.2985, 0.2295, 0.1991]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2680, 0.3015, 0.2361, 0.1944],
        [0.2664, 0.2960, 0.2280, 0.2096],
        [0.2673, 0.2920, 0.2286, 0.2121]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2514],
        [0.2506, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2450],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2540, 0.2474, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2514, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2517, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2472]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2494],
        [0.2512, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2486, 0.2494],
        [0.2509, 0.2505, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2515, 0.2487, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2487, 0.2496, 0.2518],
        [0.2504, 0.2474, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2560, 0.2550, 0.2418],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2480, 0.2562, 0.2512],
        [0.2467, 0.2505, 0.2514, 0.2514],
        [0.2438, 0.2520, 0.2562, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2584, 0.2377],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2444, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:02:53午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][50/390]	Step 16472	lr 0.00248	Loss 0.0305 (0.0968)	Prec@(1,5) (97.0%, 100.0%)	
05/31 12:03:09午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.0815 (0.0951)	Prec@(1,5) (97.0%, 100.0%)	
05/31 12:03:25午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][150/390]	Step 16572	lr 0.00248	Loss 0.0387 (0.0980)	Prec@(1,5) (96.9%, 100.0%)	
05/31 12:03:42午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.1066 (0.0975)	Prec@(1,5) (96.9%, 100.0%)	
05/31 12:03:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][250/390]	Step 16672	lr 0.00248	Loss 0.0619 (0.0969)	Prec@(1,5) (96.8%, 100.0%)	
05/31 12:04:14午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.0717 (0.0968)	Prec@(1,5) (96.8%, 100.0%)	
05/31 12:04:30午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][350/390]	Step 16772	lr 0.00248	Loss 0.0741 (0.0975)	Prec@(1,5) (96.7%, 100.0%)	
05/31 12:04:44午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.0992 (0.0973)	Prec@(1,5) (96.7%, 100.0%)	
05/31 12:04:44午前 searchStage_trainer.py:221 [INFO] Train: [ 42/49] Final Prec@1 96.6840%
05/31 12:04:47午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][50/391]	Step 16813	Loss 0.4850	Prec@(1,5) (86.4%, 99.4%)
05/31 12:04:49午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 0.5136	Prec@(1,5) (86.2%, 99.3%)
05/31 12:04:51午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][150/391]	Step 16813	Loss 0.5334	Prec@(1,5) (85.7%, 99.2%)
05/31 12:04:53午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 0.5287	Prec@(1,5) (85.9%, 99.2%)
05/31 12:04:56午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][250/391]	Step 16813	Loss 0.5314	Prec@(1,5) (85.9%, 99.2%)
05/31 12:04:58午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 0.5404	Prec@(1,5) (85.6%, 99.2%)
05/31 12:05:00午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][350/391]	Step 16813	Loss 0.5374	Prec@(1,5) (85.6%, 99.2%)
05/31 12:05:02午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 0.5309	Prec@(1,5) (85.7%, 99.2%)
05/31 12:05:02午前 searchStage_trainer.py:260 [INFO] Valid: [ 42/49] Final Prec@1 85.6960%
05/31 12:05:02午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:05:02午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.6960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2516, 0.3075, 0.2616, 0.1793],
        [0.2627, 0.3123, 0.2536, 0.1715]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.3051, 0.2655, 0.1813],
        [0.2612, 0.2932, 0.2647, 0.1809],
        [0.2758, 0.3248, 0.2165, 0.1830]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2622, 0.2857, 0.2723, 0.1798],
        [0.2771, 0.3149, 0.2211, 0.1868],
        [0.2742, 0.3159, 0.2237, 0.1862]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.3057, 0.2232, 0.1988],
        [0.2756, 0.3026, 0.2288, 0.1929],
        [0.2780, 0.3020, 0.2283, 0.1917]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2797, 0.3088, 0.2206, 0.1908],
        [0.2743, 0.3063, 0.2244, 0.1950],
        [0.2729, 0.2984, 0.2296, 0.1992]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2680, 0.3014, 0.2361, 0.1944],
        [0.2664, 0.2959, 0.2280, 0.2097],
        [0.2672, 0.2919, 0.2286, 0.2122]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2514],
        [0.2506, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2450],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2540, 0.2474, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2465],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2514, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2517, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2519, 0.2492, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2494],
        [0.2512, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2486, 0.2494],
        [0.2509, 0.2504, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2515, 0.2487, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2487, 0.2496, 0.2518],
        [0.2504, 0.2474, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2560, 0.2550, 0.2418],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2548, 0.2465],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2480, 0.2562, 0.2512],
        [0.2467, 0.2505, 0.2514, 0.2514],
        [0.2438, 0.2521, 0.2562, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2584, 0.2377],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2444, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:05:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][50/390]	Step 16863	lr 0.00214	Loss 0.2229 (0.0809)	Prec@(1,5) (97.3%, 99.9%)	
05/31 12:05:36午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.0475 (0.0814)	Prec@(1,5) (97.4%, 100.0%)	
05/31 12:05:52午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][150/390]	Step 16963	lr 0.00214	Loss 0.1227 (0.0839)	Prec@(1,5) (97.3%, 100.0%)	
05/31 12:06:08午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.0910 (0.0833)	Prec@(1,5) (97.2%, 100.0%)	
05/31 12:06:25午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][250/390]	Step 17063	lr 0.00214	Loss 0.1350 (0.0837)	Prec@(1,5) (97.2%, 100.0%)	
05/31 12:06:41午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.0452 (0.0847)	Prec@(1,5) (97.2%, 100.0%)	
05/31 12:06:57午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][350/390]	Step 17163	lr 0.00214	Loss 0.0716 (0.0841)	Prec@(1,5) (97.2%, 100.0%)	
05/31 12:07:10午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.0349 (0.0835)	Prec@(1,5) (97.2%, 100.0%)	
05/31 12:07:10午前 searchStage_trainer.py:221 [INFO] Train: [ 43/49] Final Prec@1 97.1560%
05/31 12:07:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][50/391]	Step 17204	Loss 0.5470	Prec@(1,5) (85.2%, 99.2%)
05/31 12:07:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 0.5646	Prec@(1,5) (84.9%, 99.0%)
05/31 12:07:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][150/391]	Step 17204	Loss 0.5582	Prec@(1,5) (84.9%, 99.0%)
05/31 12:07:19午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 0.5579	Prec@(1,5) (84.9%, 99.0%)
05/31 12:07:21午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][250/391]	Step 17204	Loss 0.5539	Prec@(1,5) (85.0%, 99.0%)
05/31 12:07:24午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 0.5484	Prec@(1,5) (85.1%, 99.1%)
05/31 12:07:26午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][350/391]	Step 17204	Loss 0.5380	Prec@(1,5) (85.4%, 99.1%)
05/31 12:07:28午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 0.5323	Prec@(1,5) (85.5%, 99.1%)
05/31 12:07:28午前 searchStage_trainer.py:260 [INFO] Valid: [ 43/49] Final Prec@1 85.4920%
05/31 12:07:28午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:07:28午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.6960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2515, 0.3074, 0.2616, 0.1794],
        [0.2626, 0.3122, 0.2536, 0.1716]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.3050, 0.2655, 0.1814],
        [0.2612, 0.2931, 0.2648, 0.1810],
        [0.2757, 0.3247, 0.2165, 0.1831]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2621, 0.2856, 0.2724, 0.1798],
        [0.2771, 0.3148, 0.2212, 0.1869],
        [0.2741, 0.3158, 0.2237, 0.1863]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.3057, 0.2232, 0.1988],
        [0.2756, 0.3025, 0.2288, 0.1930],
        [0.2780, 0.3018, 0.2284, 0.1918]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2797, 0.3087, 0.2207, 0.1909],
        [0.2742, 0.3062, 0.2244, 0.1951],
        [0.2729, 0.2983, 0.2296, 0.1993]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2680, 0.3014, 0.2361, 0.1945],
        [0.2664, 0.2958, 0.2280, 0.2098],
        [0.2672, 0.2918, 0.2287, 0.2124]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2514],
        [0.2506, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2450],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2540, 0.2474, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2465],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2514, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2517, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2518, 0.2492, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2494],
        [0.2512, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2486, 0.2494],
        [0.2509, 0.2504, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2515, 0.2487, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2487, 0.2496, 0.2518],
        [0.2504, 0.2474, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2560, 0.2551, 0.2418],
        [0.2498, 0.2489, 0.2490, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2549, 0.2464],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2480, 0.2562, 0.2512],
        [0.2467, 0.2505, 0.2514, 0.2514],
        [0.2438, 0.2521, 0.2562, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2584, 0.2377],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2444, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:07:45午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][50/390]	Step 17254	lr 0.00184	Loss 0.0688 (0.0738)	Prec@(1,5) (97.6%, 100.0%)	
05/31 12:08:01午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.0401 (0.0794)	Prec@(1,5) (97.3%, 100.0%)	
05/31 12:08:18午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][150/390]	Step 17354	lr 0.00184	Loss 0.0670 (0.0753)	Prec@(1,5) (97.5%, 100.0%)	
05/31 12:08:34午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.0979 (0.0734)	Prec@(1,5) (97.5%, 100.0%)	
05/31 12:08:50午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][250/390]	Step 17454	lr 0.00184	Loss 0.0663 (0.0771)	Prec@(1,5) (97.4%, 100.0%)	
05/31 12:09:06午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.1203 (0.0768)	Prec@(1,5) (97.5%, 100.0%)	
05/31 12:09:23午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][350/390]	Step 17554	lr 0.00184	Loss 0.0772 (0.0749)	Prec@(1,5) (97.5%, 100.0%)	
05/31 12:09:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.1683 (0.0754)	Prec@(1,5) (97.5%, 100.0%)	
05/31 12:09:36午前 searchStage_trainer.py:221 [INFO] Train: [ 44/49] Final Prec@1 97.4720%
05/31 12:09:38午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][50/391]	Step 17595	Loss 0.5645	Prec@(1,5) (85.5%, 99.2%)
05/31 12:09:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 0.5288	Prec@(1,5) (86.2%, 99.2%)
05/31 12:09:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][150/391]	Step 17595	Loss 0.5298	Prec@(1,5) (85.9%, 99.3%)
05/31 12:09:45午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 0.5379	Prec@(1,5) (85.6%, 99.3%)
05/31 12:09:47午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][250/391]	Step 17595	Loss 0.5403	Prec@(1,5) (85.5%, 99.3%)
05/31 12:09:49午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 0.5378	Prec@(1,5) (85.6%, 99.3%)
05/31 12:09:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][350/391]	Step 17595	Loss 0.5417	Prec@(1,5) (85.6%, 99.3%)
05/31 12:09:53午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 0.5459	Prec@(1,5) (85.5%, 99.3%)
05/31 12:09:54午前 searchStage_trainer.py:260 [INFO] Valid: [ 44/49] Final Prec@1 85.5160%
05/31 12:09:54午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:09:54午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.6960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2515, 0.3073, 0.2617, 0.1795],
        [0.2626, 0.3121, 0.2537, 0.1716]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.3049, 0.2656, 0.1814],
        [0.2611, 0.2930, 0.2648, 0.1810],
        [0.2757, 0.3246, 0.2165, 0.1832]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2621, 0.2856, 0.2725, 0.1798],
        [0.2771, 0.3147, 0.2212, 0.1870],
        [0.2741, 0.3157, 0.2238, 0.1864]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.3056, 0.2233, 0.1988],
        [0.2756, 0.3025, 0.2289, 0.1931],
        [0.2780, 0.3018, 0.2284, 0.1919]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2797, 0.3087, 0.2207, 0.1909],
        [0.2742, 0.3061, 0.2244, 0.1952],
        [0.2728, 0.2982, 0.2296, 0.1994]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2679, 0.3013, 0.2361, 0.1946],
        [0.2663, 0.2957, 0.2281, 0.2099],
        [0.2672, 0.2917, 0.2287, 0.2125]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2514],
        [0.2506, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2450],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2540, 0.2474, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2514, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2517, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2518, 0.2492, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2494],
        [0.2512, 0.2524, 0.2488, 0.2476]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2486, 0.2494],
        [0.2509, 0.2504, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2515, 0.2487, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2487, 0.2496, 0.2518],
        [0.2504, 0.2474, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2560, 0.2551, 0.2417],
        [0.2498, 0.2489, 0.2489, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2549, 0.2464],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2480, 0.2563, 0.2512],
        [0.2467, 0.2505, 0.2514, 0.2514],
        [0.2438, 0.2521, 0.2562, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2584, 0.2377],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2444, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:10:11午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][50/390]	Step 17645	lr 0.00159	Loss 0.0429 (0.0647)	Prec@(1,5) (97.8%, 100.0%)	
05/31 12:10:27午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.0718 (0.0639)	Prec@(1,5) (97.8%, 100.0%)	
05/31 12:10:43午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][150/390]	Step 17745	lr 0.00159	Loss 0.0432 (0.0635)	Prec@(1,5) (97.8%, 100.0%)	
05/31 12:10:59午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.1333 (0.0652)	Prec@(1,5) (97.7%, 100.0%)	
05/31 12:11:16午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][250/390]	Step 17845	lr 0.00159	Loss 0.1989 (0.0661)	Prec@(1,5) (97.7%, 100.0%)	
05/31 12:11:32午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.0355 (0.0659)	Prec@(1,5) (97.8%, 100.0%)	
05/31 12:11:48午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][350/390]	Step 17945	lr 0.00159	Loss 0.0293 (0.0662)	Prec@(1,5) (97.8%, 100.0%)	
05/31 12:12:01午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.0702 (0.0664)	Prec@(1,5) (97.8%, 100.0%)	
05/31 12:12:01午前 searchStage_trainer.py:221 [INFO] Train: [ 45/49] Final Prec@1 97.7760%
05/31 12:12:04午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][50/391]	Step 17986	Loss 0.5673	Prec@(1,5) (85.4%, 99.0%)
05/31 12:12:06午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 0.5464	Prec@(1,5) (86.0%, 99.2%)
05/31 12:12:08午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][150/391]	Step 17986	Loss 0.5414	Prec@(1,5) (86.3%, 99.2%)
05/31 12:12:10午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 0.5402	Prec@(1,5) (86.2%, 99.1%)
05/31 12:12:13午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][250/391]	Step 17986	Loss 0.5502	Prec@(1,5) (86.1%, 99.2%)
05/31 12:12:15午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 0.5549	Prec@(1,5) (86.0%, 99.2%)
05/31 12:12:17午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][350/391]	Step 17986	Loss 0.5546	Prec@(1,5) (85.9%, 99.2%)
05/31 12:12:19午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 0.5576	Prec@(1,5) (85.9%, 99.2%)
05/31 12:12:19午前 searchStage_trainer.py:260 [INFO] Valid: [ 45/49] Final Prec@1 85.8640%
05/31 12:12:19午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:12:19午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.8640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2515, 0.3072, 0.2617, 0.1796],
        [0.2626, 0.3120, 0.2537, 0.1717]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.3049, 0.2656, 0.1815],
        [0.2611, 0.2929, 0.2648, 0.1811],
        [0.2757, 0.3245, 0.2166, 0.1833]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2620, 0.2856, 0.2725, 0.1799],
        [0.2771, 0.3147, 0.2212, 0.1871],
        [0.2741, 0.3156, 0.2238, 0.1865]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.3055, 0.2233, 0.1989],
        [0.2756, 0.3024, 0.2289, 0.1932],
        [0.2779, 0.3017, 0.2284, 0.1920]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2797, 0.3086, 0.2207, 0.1910],
        [0.2742, 0.3061, 0.2245, 0.1953],
        [0.2728, 0.2981, 0.2296, 0.1995]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2679, 0.3012, 0.2362, 0.1947],
        [0.2663, 0.2956, 0.2281, 0.2100],
        [0.2671, 0.2916, 0.2287, 0.2126]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2514],
        [0.2505, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2450],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2540, 0.2474, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2514, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2517, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2518, 0.2492, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2494],
        [0.2512, 0.2524, 0.2488, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2486, 0.2494],
        [0.2509, 0.2504, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2487, 0.2503, 0.2505],
        [0.2515, 0.2487, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2487, 0.2496, 0.2518],
        [0.2504, 0.2474, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2561, 0.2551, 0.2417],
        [0.2498, 0.2489, 0.2489, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2549, 0.2464],
        [0.2504, 0.2485, 0.2521, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2480, 0.2563, 0.2512],
        [0.2467, 0.2505, 0.2514, 0.2514],
        [0.2438, 0.2521, 0.2562, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2584, 0.2377],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2444, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:12:37午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][50/390]	Step 18036	lr 0.00138	Loss 0.0702 (0.0679)	Prec@(1,5) (97.6%, 100.0%)	
05/31 12:12:53午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.0798 (0.0645)	Prec@(1,5) (97.9%, 100.0%)	
05/31 12:13:09午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][150/390]	Step 18136	lr 0.00138	Loss 0.0576 (0.0648)	Prec@(1,5) (97.8%, 100.0%)	
05/31 12:13:25午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.0789 (0.0668)	Prec@(1,5) (97.7%, 100.0%)	
05/31 12:13:41午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][250/390]	Step 18236	lr 0.00138	Loss 0.0311 (0.0647)	Prec@(1,5) (97.8%, 100.0%)	
05/31 12:13:58午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.0216 (0.0641)	Prec@(1,5) (97.9%, 100.0%)	
05/31 12:14:14午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][350/390]	Step 18336	lr 0.00138	Loss 0.0194 (0.0634)	Prec@(1,5) (97.9%, 100.0%)	
05/31 12:14:27午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.0162 (0.0622)	Prec@(1,5) (97.9%, 100.0%)	
05/31 12:14:28午前 searchStage_trainer.py:221 [INFO] Train: [ 46/49] Final Prec@1 97.9280%
05/31 12:14:30午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][50/391]	Step 18377	Loss 0.5207	Prec@(1,5) (86.5%, 99.3%)
05/31 12:14:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 0.5285	Prec@(1,5) (86.3%, 99.2%)
05/31 12:14:34午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][150/391]	Step 18377	Loss 0.5377	Prec@(1,5) (86.0%, 99.2%)
05/31 12:14:37午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 0.5490	Prec@(1,5) (85.9%, 99.2%)
05/31 12:14:39午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][250/391]	Step 18377	Loss 0.5512	Prec@(1,5) (85.9%, 99.2%)
05/31 12:14:41午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 0.5486	Prec@(1,5) (85.9%, 99.3%)
05/31 12:14:43午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][350/391]	Step 18377	Loss 0.5442	Prec@(1,5) (85.9%, 99.3%)
05/31 12:14:45午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 0.5484	Prec@(1,5) (85.8%, 99.3%)
05/31 12:14:45午前 searchStage_trainer.py:260 [INFO] Valid: [ 46/49] Final Prec@1 85.8200%
05/31 12:14:45午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:14:45午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 85.8640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2515, 0.3071, 0.2618, 0.1796],
        [0.2625, 0.3119, 0.2538, 0.1718]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.3048, 0.2656, 0.1815],
        [0.2611, 0.2929, 0.2649, 0.1812],
        [0.2757, 0.3244, 0.2166, 0.1834]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2621, 0.2854, 0.2725, 0.1800],
        [0.2771, 0.3146, 0.2212, 0.1871],
        [0.2741, 0.3155, 0.2238, 0.1866]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.3055, 0.2233, 0.1989],
        [0.2755, 0.3023, 0.2289, 0.1933],
        [0.2779, 0.3015, 0.2284, 0.1921]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2796, 0.3085, 0.2208, 0.1911],
        [0.2742, 0.3060, 0.2245, 0.1954],
        [0.2728, 0.2980, 0.2297, 0.1996]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2679, 0.3012, 0.2362, 0.1948],
        [0.2663, 0.2955, 0.2281, 0.2101],
        [0.2671, 0.2915, 0.2288, 0.2127]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2514],
        [0.2505, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2450],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2540, 0.2474, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2514, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2517, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2518, 0.2492, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2494],
        [0.2512, 0.2524, 0.2488, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2486, 0.2494],
        [0.2509, 0.2504, 0.2494, 0.2493],
        [0.2513, 0.2517, 0.2496, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2487, 0.2503, 0.2505],
        [0.2515, 0.2487, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2487, 0.2496, 0.2518],
        [0.2504, 0.2474, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2561, 0.2551, 0.2417],
        [0.2498, 0.2489, 0.2489, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2549, 0.2464],
        [0.2504, 0.2485, 0.2520, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2480, 0.2563, 0.2512],
        [0.2467, 0.2505, 0.2514, 0.2514],
        [0.2438, 0.2521, 0.2562, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2585, 0.2377],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2444, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:15:02午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][50/390]	Step 18427	lr 0.00121	Loss 0.0547 (0.0515)	Prec@(1,5) (98.5%, 100.0%)	
05/31 12:15:19午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.0210 (0.0521)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:15:35午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][150/390]	Step 18527	lr 0.00121	Loss 0.0561 (0.0516)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:15:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.0438 (0.0526)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:16:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][250/390]	Step 18627	lr 0.00121	Loss 0.1037 (0.0526)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:16:24午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.0516 (0.0530)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:16:40午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][350/390]	Step 18727	lr 0.00121	Loss 0.0311 (0.0544)	Prec@(1,5) (98.2%, 100.0%)	
05/31 12:16:53午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.0521 (0.0544)	Prec@(1,5) (98.2%, 100.0%)	
05/31 12:16:53午前 searchStage_trainer.py:221 [INFO] Train: [ 47/49] Final Prec@1 98.2120%
05/31 12:16:56午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][50/391]	Step 18768	Loss 0.5397	Prec@(1,5) (86.3%, 99.4%)
05/31 12:16:58午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 0.5423	Prec@(1,5) (86.2%, 99.3%)
05/31 12:17:00午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][150/391]	Step 18768	Loss 0.5351	Prec@(1,5) (86.4%, 99.3%)
05/31 12:17:02午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 0.5420	Prec@(1,5) (86.2%, 99.3%)
05/31 12:17:05午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][250/391]	Step 18768	Loss 0.5459	Prec@(1,5) (86.1%, 99.2%)
05/31 12:17:07午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 0.5505	Prec@(1,5) (86.1%, 99.2%)
05/31 12:17:09午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][350/391]	Step 18768	Loss 0.5486	Prec@(1,5) (86.1%, 99.2%)
05/31 12:17:11午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 0.5541	Prec@(1,5) (86.1%, 99.2%)
05/31 12:17:11午前 searchStage_trainer.py:260 [INFO] Valid: [ 47/49] Final Prec@1 86.1080%
05/31 12:17:11午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:17:12午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.1080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2514, 0.3070, 0.2618, 0.1797],
        [0.2625, 0.3118, 0.2538, 0.1719]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.3048, 0.2656, 0.1815],
        [0.2611, 0.2928, 0.2649, 0.1812],
        [0.2756, 0.3243, 0.2166, 0.1834]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2620, 0.2854, 0.2725, 0.1801],
        [0.2770, 0.3145, 0.2212, 0.1872],
        [0.2740, 0.3154, 0.2239, 0.1867]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2722, 0.3054, 0.2233, 0.1990],
        [0.2755, 0.3022, 0.2289, 0.1933],
        [0.2779, 0.3014, 0.2285, 0.1922]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2796, 0.3085, 0.2208, 0.1911],
        [0.2741, 0.3059, 0.2245, 0.1955],
        [0.2727, 0.2979, 0.2297, 0.1997]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2679, 0.3011, 0.2362, 0.1948],
        [0.2663, 0.2954, 0.2281, 0.2102],
        [0.2671, 0.2913, 0.2288, 0.2128]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2514],
        [0.2505, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2450],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2540, 0.2474, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2464],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2514, 0.2524, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2517, 0.2518, 0.2490, 0.2474],
        [0.2517, 0.2518, 0.2492, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2494],
        [0.2512, 0.2523, 0.2488, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2486, 0.2494],
        [0.2509, 0.2504, 0.2494, 0.2494],
        [0.2513, 0.2517, 0.2496, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2487, 0.2503, 0.2505],
        [0.2515, 0.2487, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2487, 0.2496, 0.2518],
        [0.2504, 0.2474, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2561, 0.2551, 0.2417],
        [0.2498, 0.2489, 0.2489, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2549, 0.2464],
        [0.2504, 0.2485, 0.2520, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2480, 0.2563, 0.2512],
        [0.2467, 0.2505, 0.2514, 0.2514],
        [0.2438, 0.2521, 0.2562, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2540, 0.2585, 0.2377],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2444, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:17:29午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][50/390]	Step 18818	lr 0.00109	Loss 0.0110 (0.0488)	Prec@(1,5) (98.7%, 100.0%)	
05/31 12:17:45午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.0118 (0.0484)	Prec@(1,5) (98.6%, 100.0%)	
05/31 12:18:01午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][150/390]	Step 18918	lr 0.00109	Loss 0.0682 (0.0489)	Prec@(1,5) (98.5%, 100.0%)	
05/31 12:18:18午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.2204 (0.0515)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:18:34午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][250/390]	Step 19018	lr 0.00109	Loss 0.0680 (0.0518)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:18:51午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.0278 (0.0521)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:19:07午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][350/390]	Step 19118	lr 0.00109	Loss 0.0862 (0.0532)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:19:20午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.1308 (0.0533)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:19:20午前 searchStage_trainer.py:221 [INFO] Train: [ 48/49] Final Prec@1 98.2520%
05/31 12:19:23午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][50/391]	Step 19159	Loss 0.5931	Prec@(1,5) (85.1%, 99.0%)
05/31 12:19:25午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 0.5743	Prec@(1,5) (85.7%, 99.0%)
05/31 12:19:27午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][150/391]	Step 19159	Loss 0.5668	Prec@(1,5) (86.0%, 99.1%)
05/31 12:19:30午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 0.5612	Prec@(1,5) (86.1%, 99.2%)
05/31 12:19:32午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][250/391]	Step 19159	Loss 0.5592	Prec@(1,5) (86.1%, 99.3%)
05/31 12:19:34午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 0.5576	Prec@(1,5) (86.1%, 99.3%)
05/31 12:19:36午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][350/391]	Step 19159	Loss 0.5569	Prec@(1,5) (86.1%, 99.2%)
05/31 12:19:38午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 0.5567	Prec@(1,5) (86.1%, 99.2%)
05/31 12:19:38午前 searchStage_trainer.py:260 [INFO] Valid: [ 48/49] Final Prec@1 86.0600%
05/31 12:19:38午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:19:38午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.1080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2514, 0.3069, 0.2619, 0.1798],
        [0.2625, 0.3117, 0.2539, 0.1720]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.3047, 0.2657, 0.1816],
        [0.2611, 0.2927, 0.2649, 0.1813],
        [0.2756, 0.3242, 0.2166, 0.1835]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2620, 0.2854, 0.2725, 0.1801],
        [0.2770, 0.3145, 0.2213, 0.1873],
        [0.2740, 0.3153, 0.2239, 0.1868]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2722, 0.3054, 0.2234, 0.1990],
        [0.2755, 0.3021, 0.2290, 0.1934],
        [0.2779, 0.3014, 0.2285, 0.1923]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2796, 0.3084, 0.2208, 0.1912],
        [0.2741, 0.3058, 0.2245, 0.1955],
        [0.2727, 0.2978, 0.2297, 0.1998]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2679, 0.3010, 0.2362, 0.1949],
        [0.2663, 0.2953, 0.2281, 0.2103],
        [0.2670, 0.2913, 0.2288, 0.2129]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2488, 0.2529, 0.2514],
        [0.2505, 0.2522, 0.2521, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2516, 0.2526, 0.2450],
        [0.2511, 0.2497, 0.2522, 0.2470],
        [0.2530, 0.2540, 0.2474, 0.2456]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2519, 0.2511, 0.2465],
        [0.2512, 0.2526, 0.2476, 0.2486],
        [0.2514, 0.2523, 0.2503, 0.2459]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2509, 0.2476, 0.2499],
        [0.2517, 0.2518, 0.2490, 0.2475],
        [0.2517, 0.2518, 0.2492, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2523, 0.2492, 0.2467],
        [0.2510, 0.2516, 0.2480, 0.2494],
        [0.2512, 0.2523, 0.2488, 0.2477]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2504, 0.2486, 0.2494],
        [0.2509, 0.2504, 0.2494, 0.2494],
        [0.2513, 0.2517, 0.2496, 0.2475]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2487, 0.2503, 0.2505],
        [0.2515, 0.2487, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2487, 0.2496, 0.2518],
        [0.2504, 0.2474, 0.2491, 0.2531],
        [0.2503, 0.2506, 0.2518, 0.2473]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2561, 0.2551, 0.2417],
        [0.2498, 0.2489, 0.2489, 0.2523],
        [0.2486, 0.2496, 0.2499, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2523, 0.2549, 0.2464],
        [0.2505, 0.2485, 0.2520, 0.2490],
        [0.2504, 0.2486, 0.2476, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2480, 0.2563, 0.2512],
        [0.2467, 0.2505, 0.2514, 0.2514],
        [0.2438, 0.2521, 0.2562, 0.2479]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2541, 0.2585, 0.2377],
        [0.2470, 0.2511, 0.2562, 0.2457],
        [0.2488, 0.2444, 0.2439, 0.2629]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/31 12:19:56午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][50/390]	Step 19209	lr 0.00102	Loss 0.0137 (0.0516)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:20:12午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.0176 (0.0532)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:20:28午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][150/390]	Step 19309	lr 0.00102	Loss 0.0616 (0.0528)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:20:45午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.0450 (0.0512)	Prec@(1,5) (98.3%, 100.0%)	
05/31 12:21:01午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][250/390]	Step 19409	lr 0.00102	Loss 0.0453 (0.0515)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:21:17午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.0104 (0.0512)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:21:34午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][350/390]	Step 19509	lr 0.00102	Loss 0.0788 (0.0508)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:21:47午前 searchStage_trainer.py:211 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.0492 (0.0498)	Prec@(1,5) (98.4%, 100.0%)	
05/31 12:21:47午前 searchStage_trainer.py:221 [INFO] Train: [ 49/49] Final Prec@1 98.4000%
05/31 12:21:49午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][50/391]	Step 19550	Loss 0.5267	Prec@(1,5) (87.2%, 99.2%)
05/31 12:21:52午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 0.5501	Prec@(1,5) (86.5%, 99.3%)
05/31 12:21:54午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][150/391]	Step 19550	Loss 0.5713	Prec@(1,5) (86.1%, 99.2%)
05/31 12:21:56午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 0.5682	Prec@(1,5) (86.4%, 99.1%)
05/31 12:21:58午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][250/391]	Step 19550	Loss 0.5694	Prec@(1,5) (86.2%, 99.1%)
05/31 12:22:01午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 0.5749	Prec@(1,5) (86.1%, 99.2%)
05/31 12:22:03午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][350/391]	Step 19550	Loss 0.5710	Prec@(1,5) (86.1%, 99.2%)
05/31 12:22:05午前 searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 0.5726	Prec@(1,5) (86.1%, 99.1%)
05/31 12:22:05午前 searchStage_trainer.py:260 [INFO] Valid: [ 49/49] Final Prec@1 86.1080%
05/31 12:22:05午前 searchStage_main.py:59 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
05/31 12:22:05午前 searchStage_main.py:79 [INFO] Until now, best Prec@1 = 86.1080%
05/31 12:22:06午前 searchStage_main.py:84 [INFO] Final best Prec@1 = 86.1080%
05/31 12:22:06午前 searchStage_main.py:85 [INFO] Final Best Genotype = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('max_pool_3x3', 1), ('max_pool_3x3', 0)], [('skip_connect', 2), ('max_pool_3x3', 1)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('max_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
