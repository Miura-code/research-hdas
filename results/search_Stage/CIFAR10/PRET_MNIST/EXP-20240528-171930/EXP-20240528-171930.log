05/28 05:19:30PM parser.py:28 [INFO] 
05/28 05:19:30PM parser.py:29 [INFO] Parameters:
05/28 05:19:30PM parser.py:31 [INFO] DAG_PATH=results/search_Stage/CIFAR10/PRET_MNIST/EXP-20240528-171930/DAG
05/28 05:19:30PM parser.py:31 [INFO] ALPHA_LR=0.0003
05/28 05:19:30PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
05/28 05:19:30PM parser.py:31 [INFO] BATCH_SIZE=64
05/28 05:19:30PM parser.py:31 [INFO] CHECKPOINT_RESET=True
05/28 05:19:30PM parser.py:31 [INFO] DATA_PATH=../data/
05/28 05:19:30PM parser.py:31 [INFO] DATASET=CIFAR10
05/28 05:19:30PM parser.py:31 [INFO] EPOCHS=50
05/28 05:19:30PM parser.py:31 [INFO] EXP_NAME=EXP-20240528-171930
05/28 05:19:30PM parser.py:31 [INFO] GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('skip_connect', 2)]], normal_concat=[2, 3, 4, 5], reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce_concat=[2, 3, 4, 5])
05/28 05:19:30PM parser.py:31 [INFO] GPUS=[0]
05/28 05:19:30PM parser.py:31 [INFO] INIT_CHANNELS=16
05/28 05:19:30PM parser.py:31 [INFO] LAYERS=20
05/28 05:19:30PM parser.py:31 [INFO] LOCAL_RANK=0
05/28 05:19:30PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/28 05:19:30PM parser.py:31 [INFO] NAME=PRET_MNIST
05/28 05:19:30PM parser.py:31 [INFO] PATH=results/search_Stage/CIFAR10/PRET_MNIST/EXP-20240528-171930
05/28 05:19:30PM parser.py:31 [INFO] PRINT_FREQ=50
05/28 05:19:30PM parser.py:31 [INFO] RESUME_PATH=/home/miura/lab/research-hdas/results/search_Stage/mnist/SCRATCH/EXP-20240527-152425/best.pth.tar
05/28 05:19:30PM parser.py:31 [INFO] SAVE=EXP
05/28 05:19:30PM parser.py:31 [INFO] SEED=0
05/28 05:19:30PM parser.py:31 [INFO] SHARE_STAGE=True
05/28 05:19:30PM parser.py:31 [INFO] TRAIN_PORTION=0.5
05/28 05:19:30PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
05/28 05:19:30PM parser.py:31 [INFO] W_LR=0.025
05/28 05:19:30PM parser.py:31 [INFO] W_LR_MIN=0.001
05/28 05:19:30PM parser.py:31 [INFO] W_MOMENTUM=0.9
05/28 05:19:30PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
05/28 05:19:30PM parser.py:31 [INFO] WORKERS=4
05/28 05:19:30PM parser.py:32 [INFO] 
05/28 05:19:32PM searchStage_trainer.py:110 [INFO] --> Loaded checkpoint '/home/miura/lab/research-hdas/results/search_Stage/mnist/SCRATCH/EXP-20240527-152425/best.pth.tar'(Reseted epoch 0)
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2730, 0.2468, 0.2357],
        [0.2325, 0.3027, 0.2512, 0.2136]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2292, 0.2859, 0.2699, 0.2150],
        [0.2478, 0.2841, 0.2568, 0.2112],
        [0.2576, 0.2967, 0.2204, 0.2253]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2585, 0.2859, 0.2448, 0.2108],
        [0.2648, 0.2872, 0.2343, 0.2137],
        [0.2585, 0.2823, 0.2184, 0.2408]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2961, 0.2324, 0.2089],
        [0.2517, 0.2763, 0.2253, 0.2468],
        [0.2559, 0.2835, 0.2311, 0.2295]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2927, 0.2382, 0.2238],
        [0.2527, 0.2837, 0.2278, 0.2359],
        [0.2543, 0.2831, 0.2410, 0.2215]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2607, 0.2931, 0.2316, 0.2146],
        [0.2491, 0.2757, 0.2361, 0.2391],
        [0.2583, 0.2742, 0.2311, 0.2365]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:19:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 2.0386 (2.6399)	Prec@(1,5) (22.5%, 73.3%)	
05/28 05:20:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 1.7639 (2.2311)	Prec@(1,5) (26.6%, 79.2%)	
05/28 05:20:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 1.7986 (2.0666)	Prec@(1,5) (29.5%, 81.8%)	
05/28 05:20:47PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 1.7497 (1.9671)	Prec@(1,5) (31.6%, 83.6%)	
05/28 05:21:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 1.3994 (1.8922)	Prec@(1,5) (33.5%, 84.7%)	
05/28 05:21:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 1.5211 (1.8369)	Prec@(1,5) (35.2%, 85.6%)	
05/28 05:21:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 1.3966 (1.7921)	Prec@(1,5) (36.5%, 86.3%)	
05/28 05:21:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 1.4683 (1.7641)	Prec@(1,5) (37.4%, 86.8%)	
05/28 05:21:57PM searchShareStage_trainer.py:134 [INFO] Train: [  0/49] Final Prec@1 37.4400%
05/28 05:22:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 1.4938	Prec@(1,5) (43.9%, 91.6%)
05/28 05:22:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 1.4970	Prec@(1,5) (43.6%, 91.7%)
05/28 05:22:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 1.5045	Prec@(1,5) (43.4%, 91.4%)
05/28 05:22:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 1.4947	Prec@(1,5) (43.8%, 91.6%)
05/28 05:22:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 1.4923	Prec@(1,5) (43.8%, 91.6%)
05/28 05:22:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 1.4935	Prec@(1,5) (43.7%, 91.5%)
05/28 05:22:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 1.4904	Prec@(1,5) (43.9%, 91.5%)
05/28 05:22:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 1.4886	Prec@(1,5) (44.1%, 91.6%)
05/28 05:22:20PM searchStage_trainer.py:260 [INFO] Valid: [  0/49] Final Prec@1 44.1080%
05/28 05:22:20PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:22:21PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 44.1080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2730, 0.2469, 0.2356],
        [0.2326, 0.3026, 0.2511, 0.2137]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2859, 0.2698, 0.2149],
        [0.2479, 0.2841, 0.2569, 0.2112],
        [0.2576, 0.2968, 0.2203, 0.2253]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2858, 0.2447, 0.2108],
        [0.2647, 0.2873, 0.2344, 0.2136],
        [0.2584, 0.2823, 0.2183, 0.2409]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2960, 0.2324, 0.2090],
        [0.2518, 0.2762, 0.2252, 0.2468],
        [0.2559, 0.2836, 0.2311, 0.2295]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2928, 0.2383, 0.2237],
        [0.2526, 0.2837, 0.2279, 0.2358],
        [0.2543, 0.2831, 0.2410, 0.2217]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2930, 0.2315, 0.2147],
        [0.2491, 0.2757, 0.2362, 0.2390],
        [0.2582, 0.2741, 0.2310, 0.2366]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:22:40PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 1.3247 (1.4332)	Prec@(1,5) (46.8%, 92.9%)	
05/28 05:22:59PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 1.5532 (1.4319)	Prec@(1,5) (47.2%, 92.3%)	
05/28 05:23:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 1.4289 (1.4192)	Prec@(1,5) (47.8%, 92.3%)	
05/28 05:23:36PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 1.3324 (1.4016)	Prec@(1,5) (48.7%, 92.6%)	
05/28 05:23:55PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 1.3311 (1.3928)	Prec@(1,5) (49.1%, 92.7%)	
05/28 05:24:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 1.2352 (1.3798)	Prec@(1,5) (49.8%, 92.8%)	
05/28 05:24:32PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 1.0004 (1.3680)	Prec@(1,5) (50.3%, 93.0%)	
05/28 05:24:46PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 1.0213 (1.3554)	Prec@(1,5) (50.7%, 93.2%)	
05/28 05:24:47PM searchShareStage_trainer.py:134 [INFO] Train: [  1/49] Final Prec@1 50.7560%
05/28 05:24:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 1.5405	Prec@(1,5) (44.6%, 91.0%)
05/28 05:24:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 1.5153	Prec@(1,5) (45.5%, 91.5%)
05/28 05:24:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 1.5090	Prec@(1,5) (46.2%, 91.4%)
05/28 05:24:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 1.4985	Prec@(1,5) (46.6%, 91.6%)
05/28 05:25:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 1.5064	Prec@(1,5) (46.1%, 91.7%)
05/28 05:25:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 1.5061	Prec@(1,5) (46.0%, 91.7%)
05/28 05:25:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 1.5077	Prec@(1,5) (45.9%, 91.7%)
05/28 05:25:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 1.5041	Prec@(1,5) (45.9%, 91.8%)
05/28 05:25:10PM searchStage_trainer.py:260 [INFO] Valid: [  1/49] Final Prec@1 45.8880%
05/28 05:25:10PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:25:11PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 45.8880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2729, 0.2469, 0.2356],
        [0.2326, 0.3026, 0.2511, 0.2137]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2859, 0.2698, 0.2149],
        [0.2479, 0.2840, 0.2569, 0.2112],
        [0.2576, 0.2968, 0.2203, 0.2253]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2858, 0.2447, 0.2108],
        [0.2647, 0.2873, 0.2344, 0.2136],
        [0.2584, 0.2823, 0.2183, 0.2409]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2960, 0.2324, 0.2090],
        [0.2518, 0.2762, 0.2253, 0.2468],
        [0.2559, 0.2836, 0.2311, 0.2295]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2928, 0.2383, 0.2237],
        [0.2526, 0.2837, 0.2279, 0.2358],
        [0.2543, 0.2830, 0.2410, 0.2217]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2930, 0.2315, 0.2147],
        [0.2491, 0.2757, 0.2362, 0.2390],
        [0.2582, 0.2741, 0.2310, 0.2367]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:25:30PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 1.3567 (1.2426)	Prec@(1,5) (55.1%, 94.7%)	
05/28 05:25:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 1.2820 (1.2317)	Prec@(1,5) (55.4%, 95.0%)	
05/28 05:26:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 1.4463 (1.2358)	Prec@(1,5) (55.2%, 94.6%)	
05/28 05:26:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 1.2734 (1.2183)	Prec@(1,5) (56.0%, 94.7%)	
05/28 05:26:45PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 1.1640 (1.2114)	Prec@(1,5) (56.3%, 94.6%)	
05/28 05:27:03PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 1.2300 (1.2010)	Prec@(1,5) (57.1%, 94.7%)	
05/28 05:27:22PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 1.1384 (1.1854)	Prec@(1,5) (57.6%, 94.8%)	
05/28 05:27:36PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 1.3223 (1.1795)	Prec@(1,5) (57.9%, 94.9%)	
05/28 05:27:37PM searchShareStage_trainer.py:134 [INFO] Train: [  2/49] Final Prec@1 57.8600%
05/28 05:27:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 1.2604	Prec@(1,5) (53.6%, 94.7%)
05/28 05:27:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 1.2501	Prec@(1,5) (54.8%, 94.7%)
05/28 05:27:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 1.2394	Prec@(1,5) (55.4%, 94.5%)
05/28 05:27:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 1.2317	Prec@(1,5) (55.5%, 94.5%)
05/28 05:27:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 1.2313	Prec@(1,5) (55.5%, 94.6%)
05/28 05:27:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 1.2398	Prec@(1,5) (55.3%, 94.6%)
05/28 05:27:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 1.2428	Prec@(1,5) (55.2%, 94.6%)
05/28 05:27:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 1.2419	Prec@(1,5) (55.3%, 94.6%)
05/28 05:27:59PM searchStage_trainer.py:260 [INFO] Valid: [  2/49] Final Prec@1 55.3000%
05/28 05:27:59PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:28:00PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 55.3000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2729, 0.2469, 0.2356],
        [0.2326, 0.3026, 0.2511, 0.2137]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2858, 0.2699, 0.2150],
        [0.2479, 0.2840, 0.2569, 0.2112],
        [0.2576, 0.2967, 0.2203, 0.2253]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2858, 0.2448, 0.2109],
        [0.2647, 0.2873, 0.2344, 0.2137],
        [0.2584, 0.2823, 0.2183, 0.2409]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2960, 0.2324, 0.2090],
        [0.2518, 0.2762, 0.2253, 0.2468],
        [0.2559, 0.2835, 0.2311, 0.2295]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2928, 0.2383, 0.2237],
        [0.2526, 0.2836, 0.2279, 0.2359],
        [0.2543, 0.2830, 0.2410, 0.2217]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2930, 0.2315, 0.2147],
        [0.2491, 0.2757, 0.2362, 0.2390],
        [0.2582, 0.2741, 0.2310, 0.2367]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:28:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 0.9571 (1.1084)	Prec@(1,5) (61.3%, 95.4%)	
05/28 05:28:38PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 1.1410 (1.0986)	Prec@(1,5) (61.2%, 95.7%)	
05/28 05:28:57PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 0.9841 (1.0896)	Prec@(1,5) (61.5%, 95.9%)	
05/28 05:29:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 0.9812 (1.0831)	Prec@(1,5) (61.8%, 95.9%)	
05/28 05:29:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 0.9384 (1.0739)	Prec@(1,5) (62.1%, 95.9%)	
05/28 05:29:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 1.0124 (1.0712)	Prec@(1,5) (62.3%, 95.9%)	
05/28 05:30:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 0.9108 (1.0638)	Prec@(1,5) (62.6%, 95.9%)	
05/28 05:30:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 0.9067 (1.0553)	Prec@(1,5) (62.8%, 96.0%)	
05/28 05:30:27PM searchShareStage_trainer.py:134 [INFO] Train: [  3/49] Final Prec@1 62.8440%
05/28 05:30:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 1.1476	Prec@(1,5) (59.6%, 95.6%)
05/28 05:30:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 1.1550	Prec@(1,5) (59.6%, 95.5%)
05/28 05:30:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 1.1519	Prec@(1,5) (59.4%, 95.4%)
05/28 05:30:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 1.1402	Prec@(1,5) (59.7%, 95.6%)
05/28 05:30:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 1.1367	Prec@(1,5) (59.7%, 95.7%)
05/28 05:30:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 1.1365	Prec@(1,5) (59.7%, 95.6%)
05/28 05:30:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 1.1398	Prec@(1,5) (59.7%, 95.6%)
05/28 05:30:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 1.1432	Prec@(1,5) (59.5%, 95.6%)
05/28 05:30:50PM searchStage_trainer.py:260 [INFO] Valid: [  3/49] Final Prec@1 59.5560%
05/28 05:30:50PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:30:50PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 59.5560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2729, 0.2469, 0.2356],
        [0.2326, 0.3026, 0.2511, 0.2137]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2858, 0.2699, 0.2150],
        [0.2479, 0.2840, 0.2569, 0.2112],
        [0.2576, 0.2967, 0.2203, 0.2254]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2858, 0.2448, 0.2108],
        [0.2647, 0.2872, 0.2344, 0.2137],
        [0.2584, 0.2823, 0.2183, 0.2410]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2960, 0.2324, 0.2090],
        [0.2518, 0.2762, 0.2253, 0.2468],
        [0.2559, 0.2835, 0.2311, 0.2295]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2927, 0.2383, 0.2237],
        [0.2526, 0.2836, 0.2279, 0.2359],
        [0.2543, 0.2830, 0.2410, 0.2217]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2929, 0.2316, 0.2147],
        [0.2491, 0.2757, 0.2362, 0.2390],
        [0.2582, 0.2740, 0.2311, 0.2367]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:31:09PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 1.1269 (1.0374)	Prec@(1,5) (62.5%, 96.1%)	
05/28 05:31:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 0.5092 (0.9909)	Prec@(1,5) (64.7%, 96.6%)	
05/28 05:31:46PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 1.1979 (0.9941)	Prec@(1,5) (64.6%, 96.7%)	
05/28 05:32:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 1.1608 (0.9818)	Prec@(1,5) (65.1%, 96.8%)	
05/28 05:32:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 0.8117 (0.9718)	Prec@(1,5) (65.5%, 96.9%)	
05/28 05:32:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 0.9699 (0.9651)	Prec@(1,5) (65.8%, 97.0%)	
05/28 05:33:01PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 1.1221 (0.9610)	Prec@(1,5) (66.0%, 97.0%)	
05/28 05:33:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 0.9343 (0.9590)	Prec@(1,5) (66.0%, 97.0%)	
05/28 05:33:16PM searchShareStage_trainer.py:134 [INFO] Train: [  4/49] Final Prec@1 66.0000%
05/28 05:33:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 1.0010	Prec@(1,5) (65.2%, 96.3%)
05/28 05:33:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 0.9933	Prec@(1,5) (65.3%, 96.6%)
05/28 05:33:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 1.0029	Prec@(1,5) (64.9%, 96.6%)
05/28 05:33:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 1.0101	Prec@(1,5) (64.9%, 96.5%)
05/28 05:33:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 1.0207	Prec@(1,5) (64.6%, 96.6%)
05/28 05:33:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 1.0197	Prec@(1,5) (64.4%, 96.6%)
05/28 05:33:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 1.0167	Prec@(1,5) (64.7%, 96.7%)
05/28 05:33:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 1.0120	Prec@(1,5) (64.8%, 96.7%)
05/28 05:33:40PM searchStage_trainer.py:260 [INFO] Valid: [  4/49] Final Prec@1 64.8160%
05/28 05:33:40PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:33:41PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 64.8160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2729, 0.2469, 0.2357],
        [0.2326, 0.3025, 0.2512, 0.2137]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2858, 0.2699, 0.2150],
        [0.2479, 0.2840, 0.2569, 0.2112],
        [0.2576, 0.2967, 0.2203, 0.2254]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2858, 0.2448, 0.2108],
        [0.2647, 0.2872, 0.2344, 0.2137],
        [0.2584, 0.2822, 0.2184, 0.2410]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2960, 0.2324, 0.2090],
        [0.2518, 0.2761, 0.2253, 0.2468],
        [0.2558, 0.2835, 0.2311, 0.2295]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2927, 0.2384, 0.2237],
        [0.2526, 0.2836, 0.2279, 0.2359],
        [0.2543, 0.2830, 0.2410, 0.2217]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2929, 0.2316, 0.2147],
        [0.2491, 0.2756, 0.2362, 0.2391],
        [0.2582, 0.2740, 0.2311, 0.2367]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:34:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 0.9712 (0.8417)	Prec@(1,5) (69.6%, 97.8%)	
05/28 05:34:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 0.8333 (0.8700)	Prec@(1,5) (68.9%, 97.5%)	
05/28 05:34:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 0.7393 (0.8730)	Prec@(1,5) (69.1%, 97.4%)	
05/28 05:34:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 0.6704 (0.8659)	Prec@(1,5) (69.6%, 97.5%)	
05/28 05:35:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 0.7638 (0.8758)	Prec@(1,5) (69.1%, 97.5%)	
05/28 05:35:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 0.8366 (0.8677)	Prec@(1,5) (69.5%, 97.5%)	
05/28 05:35:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 0.9405 (0.8652)	Prec@(1,5) (69.6%, 97.5%)	
05/28 05:36:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 0.6923 (0.8645)	Prec@(1,5) (69.7%, 97.5%)	
05/28 05:36:07PM searchShareStage_trainer.py:134 [INFO] Train: [  5/49] Final Prec@1 69.6680%
05/28 05:36:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 0.9059	Prec@(1,5) (68.5%, 96.8%)
05/28 05:36:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 0.9257	Prec@(1,5) (67.9%, 96.8%)
05/28 05:36:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 0.9361	Prec@(1,5) (67.5%, 96.9%)
05/28 05:36:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 0.9379	Prec@(1,5) (67.5%, 97.0%)
05/28 05:36:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 0.9372	Prec@(1,5) (67.5%, 96.9%)
05/28 05:36:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 0.9320	Prec@(1,5) (67.6%, 97.0%)
05/28 05:36:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 0.9285	Prec@(1,5) (67.8%, 97.0%)
05/28 05:36:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 0.9301	Prec@(1,5) (67.7%, 97.1%)
05/28 05:36:31PM searchStage_trainer.py:260 [INFO] Valid: [  5/49] Final Prec@1 67.6760%
05/28 05:36:31PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:36:31PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 67.6760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2728, 0.2469, 0.2357],
        [0.2326, 0.3025, 0.2512, 0.2137]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2858, 0.2699, 0.2150],
        [0.2479, 0.2840, 0.2570, 0.2112],
        [0.2576, 0.2966, 0.2204, 0.2254]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2857, 0.2448, 0.2109],
        [0.2647, 0.2872, 0.2344, 0.2137],
        [0.2584, 0.2822, 0.2184, 0.2410]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2960, 0.2324, 0.2090],
        [0.2518, 0.2761, 0.2253, 0.2469],
        [0.2558, 0.2834, 0.2312, 0.2296]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2927, 0.2384, 0.2237],
        [0.2526, 0.2835, 0.2279, 0.2359],
        [0.2543, 0.2829, 0.2411, 0.2217]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2929, 0.2316, 0.2147],
        [0.2491, 0.2756, 0.2362, 0.2391],
        [0.2582, 0.2739, 0.2311, 0.2367]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:36:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 1.0231 (0.8129)	Prec@(1,5) (71.6%, 97.9%)	
05/28 05:37:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 0.4972 (0.7913)	Prec@(1,5) (72.3%, 98.2%)	
05/28 05:37:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 0.6986 (0.8017)	Prec@(1,5) (71.8%, 98.1%)	
05/28 05:37:47PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 0.8458 (0.8020)	Prec@(1,5) (71.8%, 98.1%)	
05/28 05:38:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 0.6113 (0.7978)	Prec@(1,5) (72.1%, 98.1%)	
05/28 05:38:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 1.0665 (0.8002)	Prec@(1,5) (72.1%, 98.0%)	
05/28 05:38:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 0.8557 (0.7968)	Prec@(1,5) (72.3%, 98.0%)	
05/28 05:38:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 0.7349 (0.7974)	Prec@(1,5) (72.2%, 98.0%)	
05/28 05:38:58PM searchShareStage_trainer.py:134 [INFO] Train: [  6/49] Final Prec@1 72.1680%
05/28 05:39:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 1.0291	Prec@(1,5) (66.3%, 96.2%)
05/28 05:39:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 1.0301	Prec@(1,5) (65.7%, 96.3%)
05/28 05:39:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 1.0105	Prec@(1,5) (66.2%, 96.5%)
05/28 05:39:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 1.0088	Prec@(1,5) (66.1%, 96.5%)
05/28 05:39:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 1.0064	Prec@(1,5) (66.3%, 96.4%)
05/28 05:39:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 1.0098	Prec@(1,5) (66.2%, 96.4%)
05/28 05:39:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 1.0107	Prec@(1,5) (66.2%, 96.4%)
05/28 05:39:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 1.0102	Prec@(1,5) (66.3%, 96.4%)
05/28 05:39:22PM searchStage_trainer.py:260 [INFO] Valid: [  6/49] Final Prec@1 66.3440%
05/28 05:39:22PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:39:22PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 67.6760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2728, 0.2469, 0.2357],
        [0.2326, 0.3025, 0.2512, 0.2137]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2857, 0.2699, 0.2150],
        [0.2479, 0.2839, 0.2570, 0.2112],
        [0.2576, 0.2966, 0.2204, 0.2254]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2857, 0.2448, 0.2109],
        [0.2647, 0.2872, 0.2344, 0.2137],
        [0.2585, 0.2821, 0.2184, 0.2410]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2960, 0.2324, 0.2090],
        [0.2518, 0.2761, 0.2253, 0.2469],
        [0.2559, 0.2834, 0.2312, 0.2296]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2927, 0.2384, 0.2237],
        [0.2526, 0.2835, 0.2279, 0.2359],
        [0.2543, 0.2829, 0.2411, 0.2218]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2928, 0.2316, 0.2148],
        [0.2491, 0.2755, 0.2362, 0.2391],
        [0.2582, 0.2739, 0.2311, 0.2367]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:39:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 0.7405 (0.7259)	Prec@(1,5) (74.8%, 98.4%)	
05/28 05:40:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 0.8653 (0.7350)	Prec@(1,5) (74.4%, 98.2%)	
05/28 05:40:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 0.7434 (0.7503)	Prec@(1,5) (73.5%, 98.2%)	
05/28 05:40:38PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 0.7983 (0.7460)	Prec@(1,5) (73.7%, 98.2%)	
05/28 05:40:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 0.6771 (0.7471)	Prec@(1,5) (73.7%, 98.1%)	
05/28 05:41:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 0.8505 (0.7384)	Prec@(1,5) (74.2%, 98.2%)	
05/28 05:41:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 0.7287 (0.7408)	Prec@(1,5) (74.2%, 98.2%)	
05/28 05:41:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 0.7209 (0.7400)	Prec@(1,5) (74.3%, 98.2%)	
05/28 05:41:49PM searchShareStage_trainer.py:134 [INFO] Train: [  7/49] Final Prec@1 74.2680%
05/28 05:41:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 0.8549	Prec@(1,5) (69.1%, 98.1%)
05/28 05:41:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 0.8793	Prec@(1,5) (68.5%, 97.9%)
05/28 05:41:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 0.8701	Prec@(1,5) (69.0%, 97.9%)
05/28 05:42:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 0.8725	Prec@(1,5) (68.9%, 97.8%)
05/28 05:42:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 0.8727	Prec@(1,5) (69.1%, 97.8%)
05/28 05:42:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 0.8771	Prec@(1,5) (69.1%, 97.8%)
05/28 05:42:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 0.8761	Prec@(1,5) (69.2%, 97.8%)
05/28 05:42:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 0.8761	Prec@(1,5) (69.2%, 97.8%)
05/28 05:42:13PM searchStage_trainer.py:260 [INFO] Valid: [  7/49] Final Prec@1 69.1920%
05/28 05:42:13PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:42:13PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 69.1920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2728, 0.2469, 0.2357],
        [0.2326, 0.3025, 0.2512, 0.2137]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2857, 0.2699, 0.2150],
        [0.2479, 0.2839, 0.2570, 0.2112],
        [0.2576, 0.2965, 0.2204, 0.2255]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2857, 0.2448, 0.2109],
        [0.2647, 0.2871, 0.2345, 0.2138],
        [0.2585, 0.2820, 0.2184, 0.2411]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2959, 0.2324, 0.2090],
        [0.2518, 0.2760, 0.2253, 0.2469],
        [0.2559, 0.2834, 0.2312, 0.2296]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2926, 0.2384, 0.2238],
        [0.2526, 0.2835, 0.2280, 0.2360],
        [0.2543, 0.2829, 0.2411, 0.2218]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2928, 0.2316, 0.2148],
        [0.2491, 0.2755, 0.2363, 0.2391],
        [0.2582, 0.2739, 0.2311, 0.2367]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:42:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 0.9510 (0.6996)	Prec@(1,5) (76.4%, 98.3%)	
05/28 05:42:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 0.6944 (0.6961)	Prec@(1,5) (76.0%, 98.4%)	
05/28 05:43:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 1.0801 (0.7014)	Prec@(1,5) (75.9%, 98.2%)	
05/28 05:43:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 0.3387 (0.6977)	Prec@(1,5) (76.1%, 98.3%)	
05/28 05:43:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 0.7633 (0.6988)	Prec@(1,5) (75.7%, 98.4%)	
05/28 05:44:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 0.6961 (0.6969)	Prec@(1,5) (75.8%, 98.4%)	
05/28 05:44:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 0.7609 (0.6929)	Prec@(1,5) (76.1%, 98.4%)	
05/28 05:44:40PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 0.6899 (0.6924)	Prec@(1,5) (76.0%, 98.4%)	
05/28 05:44:41PM searchShareStage_trainer.py:134 [INFO] Train: [  8/49] Final Prec@1 76.0120%
05/28 05:44:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 0.8682	Prec@(1,5) (71.7%, 97.8%)
05/28 05:44:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 0.8344	Prec@(1,5) (72.4%, 98.1%)
05/28 05:44:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 0.8412	Prec@(1,5) (72.0%, 98.0%)
05/28 05:44:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 0.8407	Prec@(1,5) (72.0%, 98.0%)
05/28 05:44:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 0.8365	Prec@(1,5) (72.1%, 97.9%)
05/28 05:44:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 0.8322	Prec@(1,5) (72.2%, 98.0%)
05/28 05:45:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 0.8366	Prec@(1,5) (72.1%, 98.1%)
05/28 05:45:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 0.8360	Prec@(1,5) (72.1%, 98.0%)
05/28 05:45:04PM searchStage_trainer.py:260 [INFO] Valid: [  8/49] Final Prec@1 72.1120%
05/28 05:45:04PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:45:04PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 72.1120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2728, 0.2469, 0.2357],
        [0.2326, 0.3025, 0.2512, 0.2137]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2857, 0.2699, 0.2150],
        [0.2479, 0.2839, 0.2570, 0.2112],
        [0.2576, 0.2965, 0.2204, 0.2255]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2857, 0.2448, 0.2109],
        [0.2647, 0.2870, 0.2345, 0.2138],
        [0.2585, 0.2820, 0.2184, 0.2411]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2959, 0.2325, 0.2090],
        [0.2517, 0.2760, 0.2253, 0.2469],
        [0.2559, 0.2833, 0.2312, 0.2296]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2926, 0.2385, 0.2238],
        [0.2526, 0.2834, 0.2280, 0.2360],
        [0.2543, 0.2828, 0.2411, 0.2218]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2928, 0.2316, 0.2148],
        [0.2491, 0.2754, 0.2363, 0.2392],
        [0.2582, 0.2739, 0.2311, 0.2368]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:45:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 0.6279 (0.6771)	Prec@(1,5) (76.0%, 98.3%)	
05/28 05:45:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 0.5949 (0.6447)	Prec@(1,5) (77.5%, 98.6%)	
05/28 05:46:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 0.5336 (0.6495)	Prec@(1,5) (77.5%, 98.6%)	
05/28 05:46:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 0.7190 (0.6503)	Prec@(1,5) (77.3%, 98.6%)	
05/28 05:46:39PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 0.6520 (0.6476)	Prec@(1,5) (77.6%, 98.6%)	
05/28 05:46:57PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 0.6061 (0.6506)	Prec@(1,5) (77.6%, 98.6%)	
05/28 05:47:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 0.8302 (0.6505)	Prec@(1,5) (77.6%, 98.6%)	
05/28 05:47:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 0.6629 (0.6519)	Prec@(1,5) (77.7%, 98.6%)	
05/28 05:47:31PM searchShareStage_trainer.py:134 [INFO] Train: [  9/49] Final Prec@1 77.6920%
05/28 05:47:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 0.7035	Prec@(1,5) (75.5%, 98.4%)
05/28 05:47:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 0.6941	Prec@(1,5) (75.8%, 98.4%)
05/28 05:47:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 0.6988	Prec@(1,5) (75.6%, 98.3%)
05/28 05:47:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 0.7020	Prec@(1,5) (75.6%, 98.3%)
05/28 05:47:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 0.6963	Prec@(1,5) (75.8%, 98.4%)
05/28 05:47:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 0.6899	Prec@(1,5) (76.0%, 98.4%)
05/28 05:47:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 0.6907	Prec@(1,5) (76.0%, 98.4%)
05/28 05:47:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 0.6888	Prec@(1,5) (76.0%, 98.4%)
05/28 05:47:55PM searchStage_trainer.py:260 [INFO] Valid: [  9/49] Final Prec@1 76.0560%
05/28 05:47:55PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:47:56PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 76.0560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2728, 0.2469, 0.2357],
        [0.2326, 0.3025, 0.2512, 0.2137]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2857, 0.2700, 0.2150],
        [0.2479, 0.2839, 0.2570, 0.2112],
        [0.2576, 0.2964, 0.2205, 0.2255]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2857, 0.2448, 0.2109],
        [0.2647, 0.2870, 0.2345, 0.2138],
        [0.2585, 0.2819, 0.2185, 0.2411]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2959, 0.2325, 0.2090],
        [0.2518, 0.2759, 0.2254, 0.2470],
        [0.2559, 0.2832, 0.2312, 0.2296]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2925, 0.2385, 0.2238],
        [0.2526, 0.2834, 0.2280, 0.2360],
        [0.2543, 0.2828, 0.2411, 0.2218]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2927, 0.2317, 0.2148],
        [0.2491, 0.2754, 0.2363, 0.2392],
        [0.2582, 0.2739, 0.2311, 0.2368]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:48:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 0.6843 (0.6274)	Prec@(1,5) (79.0%, 98.7%)	
05/28 05:48:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 0.8500 (0.6293)	Prec@(1,5) (78.7%, 98.9%)	
05/28 05:48:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 0.5934 (0.6338)	Prec@(1,5) (78.0%, 98.9%)	
05/28 05:49:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 0.4970 (0.6282)	Prec@(1,5) (78.3%, 98.8%)	
05/28 05:49:30PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 0.6177 (0.6283)	Prec@(1,5) (78.4%, 98.8%)	
05/28 05:49:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 0.8331 (0.6284)	Prec@(1,5) (78.4%, 98.7%)	
05/28 05:50:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 0.5167 (0.6255)	Prec@(1,5) (78.4%, 98.8%)	
05/28 05:50:22PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 0.5613 (0.6274)	Prec@(1,5) (78.3%, 98.7%)	
05/28 05:50:23PM searchShareStage_trainer.py:134 [INFO] Train: [ 10/49] Final Prec@1 78.3360%
05/28 05:50:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 0.7307	Prec@(1,5) (74.6%, 98.1%)
05/28 05:50:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 0.7170	Prec@(1,5) (74.9%, 98.2%)
05/28 05:50:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 0.7108	Prec@(1,5) (74.9%, 98.2%)
05/28 05:50:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 0.7144	Prec@(1,5) (75.0%, 98.2%)
05/28 05:50:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 0.7101	Prec@(1,5) (75.2%, 98.3%)
05/28 05:50:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 0.7167	Prec@(1,5) (75.0%, 98.2%)
05/28 05:50:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 0.7231	Prec@(1,5) (74.7%, 98.2%)
05/28 05:50:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 0.7209	Prec@(1,5) (74.8%, 98.2%)
05/28 05:50:46PM searchStage_trainer.py:260 [INFO] Valid: [ 10/49] Final Prec@1 74.8560%
05/28 05:50:46PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:50:46PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 76.0560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2728, 0.2469, 0.2357],
        [0.2326, 0.3024, 0.2512, 0.2138]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2856, 0.2700, 0.2150],
        [0.2479, 0.2839, 0.2571, 0.2112],
        [0.2576, 0.2964, 0.2205, 0.2255]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2857, 0.2448, 0.2109],
        [0.2647, 0.2870, 0.2345, 0.2138],
        [0.2585, 0.2819, 0.2185, 0.2412]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2959, 0.2324, 0.2090],
        [0.2517, 0.2759, 0.2254, 0.2470],
        [0.2559, 0.2832, 0.2313, 0.2297]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2925, 0.2385, 0.2238],
        [0.2526, 0.2833, 0.2280, 0.2360],
        [0.2543, 0.2827, 0.2412, 0.2218]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2927, 0.2317, 0.2148],
        [0.2491, 0.2753, 0.2363, 0.2392],
        [0.2582, 0.2738, 0.2312, 0.2368]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:51:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 0.5704 (0.5996)	Prec@(1,5) (79.4%, 98.8%)	
05/28 05:51:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 0.3933 (0.5854)	Prec@(1,5) (79.7%, 98.8%)	
05/28 05:51:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 0.5380 (0.5908)	Prec@(1,5) (79.4%, 98.8%)	
05/28 05:52:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 0.4465 (0.5848)	Prec@(1,5) (79.7%, 98.8%)	
05/28 05:52:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 0.5049 (0.5898)	Prec@(1,5) (79.6%, 98.8%)	
05/28 05:52:39PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 0.4116 (0.5917)	Prec@(1,5) (79.5%, 98.8%)	
05/28 05:52:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 0.6400 (0.5942)	Prec@(1,5) (79.5%, 98.8%)	
05/28 05:53:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 0.7886 (0.5929)	Prec@(1,5) (79.5%, 98.9%)	
05/28 05:53:13PM searchShareStage_trainer.py:134 [INFO] Train: [ 11/49] Final Prec@1 79.4840%
05/28 05:53:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 0.6399	Prec@(1,5) (77.9%, 98.5%)
05/28 05:53:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 0.6346	Prec@(1,5) (78.2%, 98.5%)
05/28 05:53:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 0.6394	Prec@(1,5) (78.1%, 98.5%)
05/28 05:53:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 0.6492	Prec@(1,5) (77.7%, 98.5%)
05/28 05:53:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 0.6512	Prec@(1,5) (77.6%, 98.5%)
05/28 05:53:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 0.6500	Prec@(1,5) (77.7%, 98.6%)
05/28 05:53:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 0.6482	Prec@(1,5) (77.8%, 98.6%)
05/28 05:53:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 0.6499	Prec@(1,5) (77.8%, 98.6%)
05/28 05:53:37PM searchStage_trainer.py:260 [INFO] Valid: [ 11/49] Final Prec@1 77.7440%
05/28 05:53:37PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:53:37PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 77.7440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2727, 0.2470, 0.2357],
        [0.2326, 0.3024, 0.2512, 0.2138]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2856, 0.2700, 0.2150],
        [0.2479, 0.2839, 0.2571, 0.2112],
        [0.2576, 0.2964, 0.2205, 0.2255]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2856, 0.2449, 0.2109],
        [0.2647, 0.2870, 0.2345, 0.2138],
        [0.2585, 0.2819, 0.2185, 0.2412]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2959, 0.2325, 0.2090],
        [0.2517, 0.2759, 0.2254, 0.2470],
        [0.2559, 0.2832, 0.2313, 0.2297]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2925, 0.2385, 0.2238],
        [0.2526, 0.2833, 0.2280, 0.2360],
        [0.2543, 0.2827, 0.2412, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2927, 0.2317, 0.2148],
        [0.2491, 0.2753, 0.2363, 0.2392],
        [0.2582, 0.2738, 0.2312, 0.2369]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:53:57PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 0.5592 (0.5402)	Prec@(1,5) (81.6%, 99.0%)	
05/28 05:54:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 0.5284 (0.5548)	Prec@(1,5) (80.8%, 99.0%)	
05/28 05:54:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 0.7063 (0.5601)	Prec@(1,5) (80.6%, 99.0%)	
05/28 05:54:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 0.7303 (0.5591)	Prec@(1,5) (80.6%, 99.0%)	
05/28 05:55:12PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 0.5158 (0.5601)	Prec@(1,5) (80.5%, 99.0%)	
05/28 05:55:30PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 0.3582 (0.5643)	Prec@(1,5) (80.3%, 99.0%)	
05/28 05:55:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 0.5195 (0.5602)	Prec@(1,5) (80.4%, 99.0%)	
05/28 05:56:03PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 0.8771 (0.5612)	Prec@(1,5) (80.4%, 99.0%)	
05/28 05:56:04PM searchShareStage_trainer.py:134 [INFO] Train: [ 12/49] Final Prec@1 80.4080%
05/28 05:56:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 0.7547	Prec@(1,5) (74.0%, 98.4%)
05/28 05:56:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 0.7653	Prec@(1,5) (73.7%, 98.3%)
05/28 05:56:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 0.7535	Prec@(1,5) (74.1%, 98.3%)
05/28 05:56:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 0.7462	Prec@(1,5) (74.5%, 98.3%)
05/28 05:56:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 0.7444	Prec@(1,5) (74.8%, 98.3%)
05/28 05:56:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 0.7448	Prec@(1,5) (74.6%, 98.3%)
05/28 05:56:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 0.7412	Prec@(1,5) (74.7%, 98.3%)
05/28 05:56:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 0.7401	Prec@(1,5) (74.7%, 98.3%)
05/28 05:56:27PM searchStage_trainer.py:260 [INFO] Valid: [ 12/49] Final Prec@1 74.7560%
05/28 05:56:27PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:56:27PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 77.7440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2727, 0.2470, 0.2357],
        [0.2326, 0.3024, 0.2512, 0.2138]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2856, 0.2700, 0.2150],
        [0.2479, 0.2838, 0.2571, 0.2112],
        [0.2576, 0.2964, 0.2205, 0.2255]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2856, 0.2449, 0.2109],
        [0.2647, 0.2870, 0.2345, 0.2138],
        [0.2585, 0.2819, 0.2185, 0.2412]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2959, 0.2325, 0.2090],
        [0.2517, 0.2759, 0.2254, 0.2470],
        [0.2559, 0.2832, 0.2313, 0.2297]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2925, 0.2385, 0.2238],
        [0.2526, 0.2833, 0.2280, 0.2360],
        [0.2543, 0.2827, 0.2412, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2927, 0.2317, 0.2148],
        [0.2491, 0.2753, 0.2363, 0.2392],
        [0.2582, 0.2738, 0.2312, 0.2369]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:56:47PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 0.4808 (0.5021)	Prec@(1,5) (81.5%, 99.4%)	
05/28 05:57:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 0.4048 (0.5151)	Prec@(1,5) (81.5%, 99.3%)	
05/28 05:57:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 0.4693 (0.5190)	Prec@(1,5) (81.4%, 99.3%)	
05/28 05:57:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 0.4444 (0.5246)	Prec@(1,5) (81.3%, 99.3%)	
05/28 05:58:01PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 0.6997 (0.5356)	Prec@(1,5) (81.0%, 99.2%)	
05/28 05:58:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 0.5883 (0.5372)	Prec@(1,5) (81.0%, 99.2%)	
05/28 05:58:39PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 0.7114 (0.5425)	Prec@(1,5) (80.9%, 99.1%)	
05/28 05:58:54PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 0.3099 (0.5420)	Prec@(1,5) (81.0%, 99.1%)	
05/28 05:58:55PM searchShareStage_trainer.py:134 [INFO] Train: [ 13/49] Final Prec@1 80.9840%
05/28 05:58:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 0.6740	Prec@(1,5) (76.5%, 98.7%)
05/28 05:59:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 0.6793	Prec@(1,5) (76.7%, 98.6%)
05/28 05:59:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 0.6732	Prec@(1,5) (76.8%, 98.6%)
05/28 05:59:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 0.6620	Prec@(1,5) (77.2%, 98.6%)
05/28 05:59:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 0.6586	Prec@(1,5) (77.5%, 98.6%)
05/28 05:59:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 0.6592	Prec@(1,5) (77.4%, 98.6%)
05/28 05:59:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 0.6531	Prec@(1,5) (77.6%, 98.6%)
05/28 05:59:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 0.6510	Prec@(1,5) (77.7%, 98.5%)
05/28 05:59:17PM searchStage_trainer.py:260 [INFO] Valid: [ 13/49] Final Prec@1 77.7240%
05/28 05:59:17PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 05:59:18PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 77.7440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2727, 0.2470, 0.2357],
        [0.2326, 0.3023, 0.2513, 0.2138]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2856, 0.2700, 0.2150],
        [0.2479, 0.2838, 0.2571, 0.2112],
        [0.2576, 0.2964, 0.2205, 0.2256]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2856, 0.2449, 0.2109],
        [0.2647, 0.2870, 0.2345, 0.2138],
        [0.2585, 0.2818, 0.2185, 0.2412]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2959, 0.2325, 0.2090],
        [0.2517, 0.2759, 0.2254, 0.2470],
        [0.2559, 0.2832, 0.2313, 0.2297]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2925, 0.2385, 0.2238],
        [0.2526, 0.2833, 0.2280, 0.2360],
        [0.2543, 0.2827, 0.2412, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2927, 0.2317, 0.2148],
        [0.2491, 0.2753, 0.2363, 0.2392],
        [0.2582, 0.2738, 0.2312, 0.2369]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 05:59:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 0.4873 (0.5078)	Prec@(1,5) (82.7%, 99.2%)	
05/28 05:59:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 0.5224 (0.5080)	Prec@(1,5) (82.5%, 99.2%)	
05/28 06:00:14PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 0.7023 (0.5140)	Prec@(1,5) (82.1%, 99.2%)	
05/28 06:00:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 0.4501 (0.5113)	Prec@(1,5) (82.2%, 99.1%)	
05/28 06:00:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 0.4798 (0.5099)	Prec@(1,5) (82.2%, 99.1%)	
05/28 06:01:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 0.3837 (0.5073)	Prec@(1,5) (82.3%, 99.2%)	
05/28 06:01:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 0.5374 (0.5122)	Prec@(1,5) (82.2%, 99.2%)	
05/28 06:01:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 0.6971 (0.5124)	Prec@(1,5) (82.2%, 99.1%)	
05/28 06:01:45PM searchShareStage_trainer.py:134 [INFO] Train: [ 14/49] Final Prec@1 82.2040%
05/28 06:01:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 0.6344	Prec@(1,5) (78.2%, 98.9%)
05/28 06:01:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 0.6541	Prec@(1,5) (77.7%, 98.8%)
05/28 06:01:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 0.6813	Prec@(1,5) (76.9%, 98.7%)
05/28 06:01:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 0.6786	Prec@(1,5) (77.0%, 98.7%)
05/28 06:02:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 0.6716	Prec@(1,5) (77.3%, 98.7%)
05/28 06:02:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 0.6781	Prec@(1,5) (77.0%, 98.6%)
05/28 06:02:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 0.6775	Prec@(1,5) (77.1%, 98.6%)
05/28 06:02:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 0.6765	Prec@(1,5) (77.2%, 98.6%)
05/28 06:02:08PM searchStage_trainer.py:260 [INFO] Valid: [ 14/49] Final Prec@1 77.1440%
05/28 06:02:08PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:02:09PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 77.7440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2727, 0.2470, 0.2358],
        [0.2326, 0.3023, 0.2513, 0.2138]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2856, 0.2700, 0.2150],
        [0.2479, 0.2838, 0.2571, 0.2112],
        [0.2576, 0.2964, 0.2205, 0.2256]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2856, 0.2449, 0.2109],
        [0.2647, 0.2870, 0.2345, 0.2139],
        [0.2585, 0.2818, 0.2185, 0.2412]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2959, 0.2325, 0.2091],
        [0.2517, 0.2758, 0.2254, 0.2470],
        [0.2559, 0.2831, 0.2313, 0.2297]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2925, 0.2385, 0.2238],
        [0.2526, 0.2833, 0.2280, 0.2361],
        [0.2543, 0.2826, 0.2412, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2927, 0.2317, 0.2148],
        [0.2491, 0.2753, 0.2363, 0.2392],
        [0.2582, 0.2737, 0.2312, 0.2369]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:02:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 0.5045 (0.4621)	Prec@(1,5) (83.9%, 99.5%)	
05/28 06:02:47PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 0.5398 (0.4787)	Prec@(1,5) (83.5%, 99.4%)	
05/28 06:03:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 0.4162 (0.4817)	Prec@(1,5) (83.2%, 99.4%)	
05/28 06:03:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 0.6273 (0.4850)	Prec@(1,5) (83.1%, 99.3%)	
05/28 06:03:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 0.5907 (0.4881)	Prec@(1,5) (82.8%, 99.3%)	
05/28 06:04:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 0.4513 (0.4951)	Prec@(1,5) (82.7%, 99.2%)	
05/28 06:04:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 0.5595 (0.4906)	Prec@(1,5) (82.8%, 99.2%)	
05/28 06:04:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 0.5383 (0.4892)	Prec@(1,5) (82.9%, 99.2%)	
05/28 06:04:36PM searchShareStage_trainer.py:134 [INFO] Train: [ 15/49] Final Prec@1 82.9320%
05/28 06:04:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 0.5961	Prec@(1,5) (79.7%, 98.7%)
05/28 06:04:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 0.5989	Prec@(1,5) (79.5%, 98.7%)
05/28 06:04:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 0.6049	Prec@(1,5) (79.5%, 98.7%)
05/28 06:04:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 0.6065	Prec@(1,5) (79.2%, 98.7%)
05/28 06:04:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 0.6091	Prec@(1,5) (79.1%, 98.8%)
05/28 06:04:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 0.6036	Prec@(1,5) (79.3%, 98.8%)
05/28 06:04:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 0.6045	Prec@(1,5) (79.3%, 98.8%)
05/28 06:05:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 0.6021	Prec@(1,5) (79.4%, 98.8%)
05/28 06:05:00PM searchStage_trainer.py:260 [INFO] Valid: [ 15/49] Final Prec@1 79.4160%
05/28 06:05:00PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:05:00PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 79.4160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2726, 0.2471, 0.2358],
        [0.2326, 0.3022, 0.2513, 0.2139]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2855, 0.2701, 0.2151],
        [0.2479, 0.2838, 0.2571, 0.2112],
        [0.2576, 0.2963, 0.2205, 0.2256]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2856, 0.2449, 0.2109],
        [0.2647, 0.2870, 0.2345, 0.2139],
        [0.2585, 0.2818, 0.2185, 0.2412]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2958, 0.2325, 0.2091],
        [0.2517, 0.2758, 0.2254, 0.2471],
        [0.2559, 0.2831, 0.2313, 0.2297]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2925, 0.2385, 0.2238],
        [0.2526, 0.2832, 0.2281, 0.2361],
        [0.2543, 0.2826, 0.2412, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2926, 0.2317, 0.2148],
        [0.2491, 0.2753, 0.2364, 0.2393],
        [0.2582, 0.2737, 0.2312, 0.2369]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:05:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 0.5307 (0.4691)	Prec@(1,5) (84.1%, 99.2%)	
05/28 06:05:39PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 0.5659 (0.4699)	Prec@(1,5) (84.0%, 99.2%)	
05/28 06:05:57PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 0.5952 (0.4729)	Prec@(1,5) (83.9%, 99.2%)	
05/28 06:06:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 0.6562 (0.4780)	Prec@(1,5) (83.6%, 99.2%)	
05/28 06:06:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 0.2619 (0.4758)	Prec@(1,5) (83.8%, 99.2%)	
05/28 06:06:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 0.5248 (0.4763)	Prec@(1,5) (83.8%, 99.2%)	
05/28 06:07:12PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 0.4716 (0.4749)	Prec@(1,5) (83.7%, 99.3%)	
05/28 06:07:27PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 0.5060 (0.4725)	Prec@(1,5) (83.8%, 99.3%)	
05/28 06:07:27PM searchShareStage_trainer.py:134 [INFO] Train: [ 16/49] Final Prec@1 83.7640%
05/28 06:07:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 0.5750	Prec@(1,5) (79.9%, 98.9%)
05/28 06:07:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 0.5899	Prec@(1,5) (79.6%, 98.8%)
05/28 06:07:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 0.5882	Prec@(1,5) (79.9%, 98.8%)
05/28 06:07:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 0.5859	Prec@(1,5) (80.1%, 98.8%)
05/28 06:07:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 0.5903	Prec@(1,5) (79.9%, 98.8%)
05/28 06:07:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 0.5898	Prec@(1,5) (79.9%, 98.8%)
05/28 06:07:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 0.5869	Prec@(1,5) (80.0%, 98.8%)
05/28 06:07:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 0.5842	Prec@(1,5) (80.1%, 98.8%)
05/28 06:07:51PM searchStage_trainer.py:260 [INFO] Valid: [ 16/49] Final Prec@1 80.1080%
05/28 06:07:51PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:07:52PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.1080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2726, 0.2471, 0.2358],
        [0.2326, 0.3022, 0.2513, 0.2139]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2855, 0.2701, 0.2151],
        [0.2479, 0.2837, 0.2571, 0.2113],
        [0.2576, 0.2963, 0.2205, 0.2256]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2856, 0.2449, 0.2109],
        [0.2647, 0.2869, 0.2345, 0.2139],
        [0.2585, 0.2817, 0.2185, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2958, 0.2325, 0.2091],
        [0.2517, 0.2758, 0.2254, 0.2471],
        [0.2559, 0.2831, 0.2313, 0.2297]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2924, 0.2386, 0.2238],
        [0.2526, 0.2832, 0.2281, 0.2361],
        [0.2543, 0.2826, 0.2412, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2926, 0.2317, 0.2148],
        [0.2491, 0.2753, 0.2364, 0.2393],
        [0.2582, 0.2737, 0.2312, 0.2369]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:08:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 0.3442 (0.4231)	Prec@(1,5) (85.7%, 99.3%)	
05/28 06:08:30PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 0.2588 (0.4300)	Prec@(1,5) (85.0%, 99.4%)	
05/28 06:08:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 0.4568 (0.4295)	Prec@(1,5) (85.0%, 99.4%)	
05/28 06:09:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 0.6049 (0.4384)	Prec@(1,5) (84.7%, 99.4%)	
05/28 06:09:27PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 0.4923 (0.4426)	Prec@(1,5) (84.5%, 99.4%)	
05/28 06:09:46PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 0.7120 (0.4489)	Prec@(1,5) (84.3%, 99.4%)	
05/28 06:10:04PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 0.3114 (0.4510)	Prec@(1,5) (84.2%, 99.4%)	
05/28 06:10:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 0.5491 (0.4531)	Prec@(1,5) (84.2%, 99.4%)	
05/28 06:10:19PM searchShareStage_trainer.py:134 [INFO] Train: [ 17/49] Final Prec@1 84.1800%
05/28 06:10:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 0.6467	Prec@(1,5) (79.0%, 98.9%)
05/28 06:10:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 0.6162	Prec@(1,5) (79.4%, 98.8%)
05/28 06:10:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 0.6137	Prec@(1,5) (79.2%, 98.8%)
05/28 06:10:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 0.6199	Prec@(1,5) (78.9%, 98.8%)
05/28 06:10:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 0.6185	Prec@(1,5) (79.0%, 98.7%)
05/28 06:10:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 0.6243	Prec@(1,5) (79.0%, 98.7%)
05/28 06:10:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 0.6282	Prec@(1,5) (78.9%, 98.7%)
05/28 06:10:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 0.6238	Prec@(1,5) (79.0%, 98.6%)
05/28 06:10:43PM searchStage_trainer.py:260 [INFO] Valid: [ 17/49] Final Prec@1 78.9800%
05/28 06:10:43PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:10:43PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.1080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2726, 0.2471, 0.2358],
        [0.2326, 0.3022, 0.2513, 0.2139]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2855, 0.2701, 0.2151],
        [0.2479, 0.2837, 0.2571, 0.2113],
        [0.2576, 0.2962, 0.2206, 0.2256]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2856, 0.2449, 0.2109],
        [0.2647, 0.2869, 0.2346, 0.2139],
        [0.2585, 0.2817, 0.2185, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2958, 0.2325, 0.2091],
        [0.2517, 0.2758, 0.2254, 0.2471],
        [0.2559, 0.2830, 0.2313, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2924, 0.2386, 0.2238],
        [0.2526, 0.2832, 0.2281, 0.2361],
        [0.2543, 0.2825, 0.2412, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2926, 0.2317, 0.2149],
        [0.2491, 0.2752, 0.2364, 0.2393],
        [0.2582, 0.2736, 0.2312, 0.2369]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:11:03PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 0.4226 (0.4120)	Prec@(1,5) (85.1%, 99.7%)	
05/28 06:11:22PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 0.2955 (0.3975)	Prec@(1,5) (86.1%, 99.6%)	
05/28 06:11:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 0.3709 (0.4128)	Prec@(1,5) (85.6%, 99.5%)	
05/28 06:12:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 0.4050 (0.4253)	Prec@(1,5) (85.2%, 99.4%)	
05/28 06:12:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 0.4729 (0.4298)	Prec@(1,5) (85.2%, 99.4%)	
05/28 06:12:38PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 0.3458 (0.4329)	Prec@(1,5) (85.0%, 99.4%)	
05/28 06:12:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 0.7212 (0.4344)	Prec@(1,5) (84.9%, 99.4%)	
05/28 06:13:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 0.4770 (0.4373)	Prec@(1,5) (84.9%, 99.4%)	
05/28 06:13:11PM searchShareStage_trainer.py:134 [INFO] Train: [ 18/49] Final Prec@1 84.8520%
05/28 06:13:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 0.6096	Prec@(1,5) (79.5%, 98.6%)
05/28 06:13:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 0.6115	Prec@(1,5) (79.0%, 98.6%)
05/28 06:13:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 0.6079	Prec@(1,5) (79.4%, 98.6%)
05/28 06:13:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 0.6111	Prec@(1,5) (79.4%, 98.5%)
05/28 06:13:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 0.6067	Prec@(1,5) (79.5%, 98.7%)
05/28 06:13:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 0.6060	Prec@(1,5) (79.5%, 98.7%)
05/28 06:13:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 0.6091	Prec@(1,5) (79.3%, 98.7%)
05/28 06:13:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 0.6081	Prec@(1,5) (79.2%, 98.7%)
05/28 06:13:36PM searchStage_trainer.py:260 [INFO] Valid: [ 18/49] Final Prec@1 79.2440%
05/28 06:13:36PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:13:36PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.1080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2726, 0.2471, 0.2358],
        [0.2326, 0.3022, 0.2513, 0.2139]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2855, 0.2701, 0.2151],
        [0.2479, 0.2837, 0.2571, 0.2113],
        [0.2576, 0.2962, 0.2206, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2856, 0.2449, 0.2109],
        [0.2647, 0.2868, 0.2346, 0.2139],
        [0.2585, 0.2816, 0.2186, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2958, 0.2325, 0.2091],
        [0.2517, 0.2758, 0.2254, 0.2471],
        [0.2559, 0.2830, 0.2314, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2924, 0.2386, 0.2238],
        [0.2526, 0.2832, 0.2281, 0.2361],
        [0.2543, 0.2825, 0.2413, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2926, 0.2317, 0.2149],
        [0.2491, 0.2752, 0.2364, 0.2393],
        [0.2582, 0.2736, 0.2312, 0.2370]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:13:55PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 0.3202 (0.3934)	Prec@(1,5) (85.9%, 99.7%)	
05/28 06:14:14PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 0.4174 (0.3811)	Prec@(1,5) (86.7%, 99.6%)	
05/28 06:14:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 0.4089 (0.3942)	Prec@(1,5) (86.1%, 99.5%)	
05/28 06:14:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 0.4293 (0.3984)	Prec@(1,5) (86.2%, 99.6%)	
05/28 06:15:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 0.5257 (0.3978)	Prec@(1,5) (86.3%, 99.6%)	
05/28 06:15:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 0.4157 (0.4017)	Prec@(1,5) (86.2%, 99.6%)	
05/28 06:15:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 0.3029 (0.4029)	Prec@(1,5) (86.2%, 99.5%)	
05/28 06:16:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 0.4055 (0.4070)	Prec@(1,5) (86.1%, 99.5%)	
05/28 06:16:03PM searchShareStage_trainer.py:134 [INFO] Train: [ 19/49] Final Prec@1 86.0480%
05/28 06:16:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 0.6317	Prec@(1,5) (78.6%, 98.6%)
05/28 06:16:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 0.6325	Prec@(1,5) (78.6%, 98.6%)
05/28 06:16:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 0.6274	Prec@(1,5) (79.0%, 98.6%)
05/28 06:16:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 0.6286	Prec@(1,5) (79.2%, 98.5%)
05/28 06:16:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 0.6245	Prec@(1,5) (79.3%, 98.5%)
05/28 06:16:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 0.6222	Prec@(1,5) (79.3%, 98.5%)
05/28 06:16:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 0.6232	Prec@(1,5) (79.2%, 98.5%)
05/28 06:16:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 0.6261	Prec@(1,5) (79.1%, 98.5%)
05/28 06:16:27PM searchStage_trainer.py:260 [INFO] Valid: [ 19/49] Final Prec@1 79.1360%
05/28 06:16:27PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:16:27PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.1080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2726, 0.2471, 0.2358],
        [0.2326, 0.3022, 0.2514, 0.2139]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2855, 0.2701, 0.2151],
        [0.2479, 0.2837, 0.2571, 0.2113],
        [0.2576, 0.2962, 0.2206, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2855, 0.2449, 0.2109],
        [0.2647, 0.2868, 0.2346, 0.2139],
        [0.2585, 0.2816, 0.2186, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2091],
        [0.2517, 0.2757, 0.2254, 0.2471],
        [0.2558, 0.2830, 0.2314, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2924, 0.2386, 0.2238],
        [0.2526, 0.2831, 0.2281, 0.2361],
        [0.2543, 0.2825, 0.2413, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2926, 0.2318, 0.2149],
        [0.2491, 0.2752, 0.2364, 0.2393],
        [0.2582, 0.2735, 0.2313, 0.2370]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:16:47PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 0.2026 (0.3943)	Prec@(1,5) (86.6%, 99.7%)	
05/28 06:17:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 0.2783 (0.3824)	Prec@(1,5) (86.8%, 99.7%)	
05/28 06:17:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 0.4182 (0.3801)	Prec@(1,5) (86.9%, 99.6%)	
05/28 06:17:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 0.3789 (0.3900)	Prec@(1,5) (86.5%, 99.6%)	
05/28 06:18:01PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 0.4263 (0.3902)	Prec@(1,5) (86.4%, 99.6%)	
05/28 06:18:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 0.3630 (0.3956)	Prec@(1,5) (86.2%, 99.6%)	
05/28 06:18:39PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 0.4817 (0.3969)	Prec@(1,5) (86.2%, 99.6%)	
05/28 06:18:54PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 0.5757 (0.4001)	Prec@(1,5) (86.1%, 99.6%)	
05/28 06:18:54PM searchShareStage_trainer.py:134 [INFO] Train: [ 20/49] Final Prec@1 86.1520%
05/28 06:18:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 0.6406	Prec@(1,5) (79.4%, 98.7%)
05/28 06:19:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 0.6560	Prec@(1,5) (78.8%, 98.6%)
05/28 06:19:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 0.6581	Prec@(1,5) (78.6%, 98.6%)
05/28 06:19:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 0.6513	Prec@(1,5) (78.7%, 98.6%)
05/28 06:19:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 0.6522	Prec@(1,5) (78.6%, 98.5%)
05/28 06:19:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 0.6466	Prec@(1,5) (78.7%, 98.6%)
05/28 06:19:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 0.6450	Prec@(1,5) (78.8%, 98.6%)
05/28 06:19:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 0.6453	Prec@(1,5) (78.9%, 98.6%)
05/28 06:19:18PM searchStage_trainer.py:260 [INFO] Valid: [ 20/49] Final Prec@1 78.8640%
05/28 06:19:18PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:19:18PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.1080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2725, 0.2471, 0.2358],
        [0.2326, 0.3022, 0.2514, 0.2139]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2855, 0.2701, 0.2151],
        [0.2479, 0.2837, 0.2572, 0.2113],
        [0.2576, 0.2962, 0.2206, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2855, 0.2449, 0.2109],
        [0.2647, 0.2868, 0.2346, 0.2139],
        [0.2585, 0.2816, 0.2186, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2757, 0.2254, 0.2472],
        [0.2558, 0.2830, 0.2314, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2923, 0.2386, 0.2238],
        [0.2526, 0.2831, 0.2281, 0.2362],
        [0.2543, 0.2825, 0.2413, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2926, 0.2318, 0.2149],
        [0.2491, 0.2751, 0.2364, 0.2394],
        [0.2582, 0.2735, 0.2313, 0.2370]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:19:38PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 0.4582 (0.3958)	Prec@(1,5) (85.8%, 99.4%)	
05/28 06:19:57PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 0.3826 (0.3789)	Prec@(1,5) (86.9%, 99.5%)	
05/28 06:20:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 0.5713 (0.3730)	Prec@(1,5) (86.9%, 99.5%)	
05/28 06:20:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 0.3322 (0.3755)	Prec@(1,5) (86.7%, 99.5%)	
05/28 06:20:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 0.2716 (0.3811)	Prec@(1,5) (86.7%, 99.6%)	
05/28 06:21:12PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 0.3954 (0.3835)	Prec@(1,5) (86.7%, 99.6%)	
05/28 06:21:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 0.4120 (0.3803)	Prec@(1,5) (86.8%, 99.6%)	
05/28 06:21:46PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 0.4083 (0.3799)	Prec@(1,5) (86.9%, 99.6%)	
05/28 06:21:46PM searchShareStage_trainer.py:134 [INFO] Train: [ 21/49] Final Prec@1 86.8800%
05/28 06:21:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 0.6111	Prec@(1,5) (80.5%, 99.2%)
05/28 06:21:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 0.6080	Prec@(1,5) (80.3%, 99.2%)
05/28 06:21:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 0.6065	Prec@(1,5) (80.2%, 99.0%)
05/28 06:21:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 0.6059	Prec@(1,5) (80.2%, 99.0%)
05/28 06:22:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 0.6025	Prec@(1,5) (80.1%, 99.0%)
05/28 06:22:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 0.5935	Prec@(1,5) (80.5%, 99.0%)
05/28 06:22:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 0.5948	Prec@(1,5) (80.5%, 99.0%)
05/28 06:22:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 0.5932	Prec@(1,5) (80.5%, 99.0%)
05/28 06:22:09PM searchStage_trainer.py:260 [INFO] Valid: [ 21/49] Final Prec@1 80.5480%
05/28 06:22:09PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:22:10PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 80.5480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2725, 0.2471, 0.2358],
        [0.2326, 0.3022, 0.2514, 0.2139]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2854, 0.2701, 0.2151],
        [0.2479, 0.2837, 0.2572, 0.2113],
        [0.2576, 0.2962, 0.2206, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2855, 0.2450, 0.2109],
        [0.2647, 0.2868, 0.2346, 0.2139],
        [0.2585, 0.2816, 0.2186, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2757, 0.2254, 0.2472],
        [0.2558, 0.2830, 0.2314, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2923, 0.2386, 0.2239],
        [0.2526, 0.2831, 0.2281, 0.2362],
        [0.2543, 0.2825, 0.2413, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2925, 0.2318, 0.2149],
        [0.2491, 0.2751, 0.2364, 0.2394],
        [0.2582, 0.2735, 0.2313, 0.2370]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:22:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 0.4586 (0.3404)	Prec@(1,5) (88.8%, 99.7%)	
05/28 06:22:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 0.3636 (0.3445)	Prec@(1,5) (88.1%, 99.6%)	
05/28 06:23:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 0.2030 (0.3475)	Prec@(1,5) (87.8%, 99.7%)	
05/28 06:23:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 0.5322 (0.3511)	Prec@(1,5) (87.7%, 99.7%)	
05/28 06:23:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 0.4273 (0.3534)	Prec@(1,5) (87.6%, 99.7%)	
05/28 06:24:03PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 0.2129 (0.3587)	Prec@(1,5) (87.4%, 99.6%)	
05/28 06:24:21PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 0.3426 (0.3633)	Prec@(1,5) (87.2%, 99.6%)	
05/28 06:24:36PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 0.2886 (0.3647)	Prec@(1,5) (87.2%, 99.6%)	
05/28 06:24:36PM searchShareStage_trainer.py:134 [INFO] Train: [ 22/49] Final Prec@1 87.1520%
05/28 06:24:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 0.5354	Prec@(1,5) (82.9%, 98.9%)
05/28 06:24:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 0.5384	Prec@(1,5) (82.5%, 98.9%)
05/28 06:24:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 0.5349	Prec@(1,5) (82.4%, 98.8%)
05/28 06:24:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 0.5382	Prec@(1,5) (82.2%, 98.8%)
05/28 06:24:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 0.5320	Prec@(1,5) (82.4%, 98.8%)
05/28 06:24:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 0.5319	Prec@(1,5) (82.3%, 98.8%)
05/28 06:24:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 0.5350	Prec@(1,5) (82.2%, 98.8%)
05/28 06:25:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 0.5357	Prec@(1,5) (82.1%, 98.8%)
05/28 06:25:00PM searchStage_trainer.py:260 [INFO] Valid: [ 22/49] Final Prec@1 82.1280%
05/28 06:25:00PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:25:01PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.1280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2725, 0.2471, 0.2358],
        [0.2326, 0.3021, 0.2514, 0.2139]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2854, 0.2701, 0.2151],
        [0.2478, 0.2837, 0.2572, 0.2113],
        [0.2576, 0.2961, 0.2206, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2855, 0.2450, 0.2109],
        [0.2647, 0.2868, 0.2346, 0.2139],
        [0.2585, 0.2816, 0.2186, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2757, 0.2254, 0.2472],
        [0.2559, 0.2829, 0.2314, 0.2298]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2923, 0.2387, 0.2239],
        [0.2526, 0.2831, 0.2281, 0.2362],
        [0.2543, 0.2824, 0.2413, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2925, 0.2318, 0.2149],
        [0.2491, 0.2751, 0.2364, 0.2394],
        [0.2582, 0.2735, 0.2313, 0.2370]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:25:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 0.5131 (0.3249)	Prec@(1,5) (88.9%, 99.7%)	
05/28 06:25:39PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 0.2949 (0.3137)	Prec@(1,5) (89.3%, 99.6%)	
05/28 06:25:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 0.1707 (0.3249)	Prec@(1,5) (89.0%, 99.7%)	
05/28 06:26:17PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 0.3893 (0.3261)	Prec@(1,5) (88.7%, 99.7%)	
05/28 06:26:36PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 0.4926 (0.3278)	Prec@(1,5) (88.7%, 99.7%)	
05/28 06:26:54PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 0.4616 (0.3317)	Prec@(1,5) (88.6%, 99.7%)	
05/28 06:27:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 0.3042 (0.3375)	Prec@(1,5) (88.4%, 99.7%)	
05/28 06:27:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 0.4302 (0.3416)	Prec@(1,5) (88.2%, 99.7%)	
05/28 06:27:28PM searchShareStage_trainer.py:134 [INFO] Train: [ 23/49] Final Prec@1 88.1960%
05/28 06:27:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 0.5094	Prec@(1,5) (83.3%, 99.2%)
05/28 06:27:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 0.5247	Prec@(1,5) (82.4%, 99.0%)
05/28 06:27:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 0.5199	Prec@(1,5) (82.6%, 99.1%)
05/28 06:27:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 0.5216	Prec@(1,5) (82.7%, 99.1%)
05/28 06:27:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 0.5246	Prec@(1,5) (82.5%, 99.1%)
05/28 06:27:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 0.5238	Prec@(1,5) (82.6%, 99.1%)
05/28 06:27:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 0.5261	Prec@(1,5) (82.4%, 99.1%)
05/28 06:27:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 0.5287	Prec@(1,5) (82.3%, 99.1%)
05/28 06:27:52PM searchStage_trainer.py:260 [INFO] Valid: [ 23/49] Final Prec@1 82.3160%
05/28 06:27:52PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:27:52PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 82.3160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2725, 0.2472, 0.2359],
        [0.2325, 0.3021, 0.2514, 0.2139]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2854, 0.2701, 0.2151],
        [0.2479, 0.2837, 0.2572, 0.2113],
        [0.2576, 0.2961, 0.2206, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2855, 0.2450, 0.2109],
        [0.2647, 0.2868, 0.2346, 0.2139],
        [0.2585, 0.2816, 0.2186, 0.2414]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2757, 0.2254, 0.2472],
        [0.2558, 0.2829, 0.2314, 0.2299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2923, 0.2387, 0.2239],
        [0.2526, 0.2831, 0.2281, 0.2362],
        [0.2543, 0.2824, 0.2413, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2925, 0.2318, 0.2149],
        [0.2491, 0.2751, 0.2364, 0.2394],
        [0.2582, 0.2735, 0.2313, 0.2370]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:28:12PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 0.4254 (0.3055)	Prec@(1,5) (89.5%, 99.6%)	
05/28 06:28:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 0.2680 (0.3144)	Prec@(1,5) (89.0%, 99.7%)	
05/28 06:28:50PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 0.3874 (0.3159)	Prec@(1,5) (89.0%, 99.7%)	
05/28 06:29:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 0.2453 (0.3212)	Prec@(1,5) (88.9%, 99.7%)	
05/28 06:29:27PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 0.6758 (0.3268)	Prec@(1,5) (88.6%, 99.7%)	
05/28 06:29:45PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 0.2671 (0.3299)	Prec@(1,5) (88.4%, 99.7%)	
05/28 06:30:04PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 0.4110 (0.3318)	Prec@(1,5) (88.4%, 99.7%)	
05/28 06:30:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 0.2102 (0.3323)	Prec@(1,5) (88.3%, 99.7%)	
05/28 06:30:20PM searchShareStage_trainer.py:134 [INFO] Train: [ 24/49] Final Prec@1 88.3560%
05/28 06:30:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 0.4960	Prec@(1,5) (83.7%, 99.3%)
05/28 06:30:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 0.5032	Prec@(1,5) (83.4%, 99.1%)
05/28 06:30:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 0.4951	Prec@(1,5) (83.4%, 99.2%)
05/28 06:30:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 0.5048	Prec@(1,5) (83.0%, 99.2%)
05/28 06:30:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 0.5016	Prec@(1,5) (83.1%, 99.2%)
05/28 06:30:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 0.5012	Prec@(1,5) (83.0%, 99.2%)
05/28 06:30:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 0.4986	Prec@(1,5) (83.1%, 99.2%)
05/28 06:30:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 0.4980	Prec@(1,5) (83.2%, 99.2%)
05/28 06:30:43PM searchStage_trainer.py:260 [INFO] Valid: [ 24/49] Final Prec@1 83.2080%
05/28 06:30:43PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:30:43PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.2080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2724, 0.2472, 0.2359],
        [0.2326, 0.3020, 0.2514, 0.2140]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2854, 0.2701, 0.2151],
        [0.2478, 0.2837, 0.2572, 0.2113],
        [0.2576, 0.2961, 0.2206, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2855, 0.2450, 0.2109],
        [0.2647, 0.2868, 0.2346, 0.2140],
        [0.2585, 0.2816, 0.2186, 0.2414]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2757, 0.2254, 0.2472],
        [0.2558, 0.2829, 0.2314, 0.2299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2923, 0.2387, 0.2239],
        [0.2526, 0.2831, 0.2281, 0.2362],
        [0.2543, 0.2824, 0.2413, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2924, 0.2318, 0.2149],
        [0.2491, 0.2751, 0.2364, 0.2394],
        [0.2582, 0.2734, 0.2313, 0.2371]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:31:03PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 0.4852 (0.2911)	Prec@(1,5) (89.9%, 99.8%)	
05/28 06:31:22PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 0.3559 (0.2926)	Prec@(1,5) (90.0%, 99.7%)	
05/28 06:31:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 0.3047 (0.2985)	Prec@(1,5) (89.7%, 99.8%)	
05/28 06:31:59PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 0.4335 (0.3033)	Prec@(1,5) (89.6%, 99.7%)	
05/28 06:32:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 0.2279 (0.3049)	Prec@(1,5) (89.6%, 99.7%)	
05/28 06:32:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 0.1581 (0.3074)	Prec@(1,5) (89.5%, 99.7%)	
05/28 06:32:57PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 0.2495 (0.3094)	Prec@(1,5) (89.4%, 99.7%)	
05/28 06:33:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 0.2980 (0.3106)	Prec@(1,5) (89.4%, 99.7%)	
05/28 06:33:12PM searchShareStage_trainer.py:134 [INFO] Train: [ 25/49] Final Prec@1 89.3680%
05/28 06:33:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 0.5093	Prec@(1,5) (83.4%, 99.2%)
05/28 06:33:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 0.5081	Prec@(1,5) (83.4%, 99.2%)
05/28 06:33:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 0.5047	Prec@(1,5) (83.6%, 99.1%)
05/28 06:33:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 0.5104	Prec@(1,5) (83.4%, 99.0%)
05/28 06:33:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 0.5094	Prec@(1,5) (83.4%, 99.1%)
05/28 06:33:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 0.5015	Prec@(1,5) (83.6%, 99.1%)
05/28 06:33:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 0.5023	Prec@(1,5) (83.5%, 99.1%)
05/28 06:33:35PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 0.5003	Prec@(1,5) (83.4%, 99.1%)
05/28 06:33:36PM searchStage_trainer.py:260 [INFO] Valid: [ 25/49] Final Prec@1 83.4560%
05/28 06:33:36PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:33:36PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.4560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2724, 0.2472, 0.2359],
        [0.2325, 0.3020, 0.2515, 0.2140]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2854, 0.2702, 0.2151],
        [0.2478, 0.2836, 0.2572, 0.2113],
        [0.2576, 0.2961, 0.2206, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2855, 0.2450, 0.2109],
        [0.2647, 0.2868, 0.2346, 0.2140],
        [0.2585, 0.2815, 0.2186, 0.2414]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2756, 0.2255, 0.2472],
        [0.2558, 0.2829, 0.2314, 0.2299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2923, 0.2387, 0.2239],
        [0.2526, 0.2831, 0.2281, 0.2362],
        [0.2543, 0.2824, 0.2413, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2924, 0.2318, 0.2149],
        [0.2491, 0.2751, 0.2364, 0.2394],
        [0.2582, 0.2734, 0.2313, 0.2371]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:33:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 0.2098 (0.2723)	Prec@(1,5) (90.8%, 99.9%)	
05/28 06:34:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 0.2675 (0.2738)	Prec@(1,5) (90.4%, 99.8%)	
05/28 06:34:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 0.2377 (0.2745)	Prec@(1,5) (90.5%, 99.8%)	
05/28 06:34:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 0.5340 (0.2809)	Prec@(1,5) (90.3%, 99.8%)	
05/28 06:35:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 0.3196 (0.2861)	Prec@(1,5) (90.0%, 99.8%)	
05/28 06:35:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 0.3620 (0.2901)	Prec@(1,5) (89.9%, 99.8%)	
05/28 06:35:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 0.1820 (0.2903)	Prec@(1,5) (89.8%, 99.8%)	
05/28 06:36:04PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 0.4588 (0.2948)	Prec@(1,5) (89.7%, 99.8%)	
05/28 06:36:04PM searchShareStage_trainer.py:134 [INFO] Train: [ 26/49] Final Prec@1 89.6720%
05/28 06:36:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 0.5115	Prec@(1,5) (83.7%, 98.9%)
05/28 06:36:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 0.5151	Prec@(1,5) (83.7%, 98.9%)
05/28 06:36:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 0.5062	Prec@(1,5) (83.8%, 99.0%)
05/28 06:36:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 0.5092	Prec@(1,5) (83.6%, 99.0%)
05/28 06:36:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 0.5087	Prec@(1,5) (83.5%, 99.0%)
05/28 06:36:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 0.5122	Prec@(1,5) (83.3%, 99.1%)
05/28 06:36:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 0.5109	Prec@(1,5) (83.3%, 99.0%)
05/28 06:36:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 0.5114	Prec@(1,5) (83.2%, 99.0%)
05/28 06:36:29PM searchStage_trainer.py:260 [INFO] Valid: [ 26/49] Final Prec@1 83.2400%
05/28 06:36:29PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:36:29PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.4560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2724, 0.2472, 0.2359],
        [0.2325, 0.3020, 0.2515, 0.2140]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2853, 0.2702, 0.2152],
        [0.2478, 0.2836, 0.2572, 0.2113],
        [0.2576, 0.2961, 0.2206, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2855, 0.2450, 0.2109],
        [0.2647, 0.2868, 0.2346, 0.2140],
        [0.2585, 0.2815, 0.2186, 0.2414]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2756, 0.2255, 0.2472],
        [0.2559, 0.2828, 0.2314, 0.2299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2923, 0.2387, 0.2239],
        [0.2526, 0.2830, 0.2282, 0.2362],
        [0.2543, 0.2823, 0.2413, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2924, 0.2318, 0.2149],
        [0.2491, 0.2750, 0.2365, 0.2394],
        [0.2582, 0.2734, 0.2313, 0.2371]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:36:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 0.2671 (0.2727)	Prec@(1,5) (90.7%, 99.9%)	
05/28 06:37:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 0.2257 (0.2609)	Prec@(1,5) (91.0%, 99.9%)	
05/28 06:37:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 0.2349 (0.2667)	Prec@(1,5) (90.7%, 99.9%)	
05/28 06:37:45PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 0.2890 (0.2675)	Prec@(1,5) (90.7%, 99.9%)	
05/28 06:38:04PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 0.1374 (0.2764)	Prec@(1,5) (90.3%, 99.9%)	
05/28 06:38:22PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 0.3173 (0.2785)	Prec@(1,5) (90.3%, 99.8%)	
05/28 06:38:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 0.2152 (0.2780)	Prec@(1,5) (90.3%, 99.8%)	
05/28 06:38:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 0.2455 (0.2810)	Prec@(1,5) (90.2%, 99.8%)	
05/28 06:38:57PM searchShareStage_trainer.py:134 [INFO] Train: [ 27/49] Final Prec@1 90.1720%
05/28 06:39:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 0.5733	Prec@(1,5) (81.5%, 99.0%)
05/28 06:39:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 0.5813	Prec@(1,5) (81.3%, 99.0%)
05/28 06:39:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 0.5952	Prec@(1,5) (81.3%, 99.1%)
05/28 06:39:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 0.5872	Prec@(1,5) (81.4%, 99.0%)
05/28 06:39:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 0.5817	Prec@(1,5) (81.5%, 99.1%)
05/28 06:39:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 0.5772	Prec@(1,5) (81.8%, 99.0%)
05/28 06:39:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 0.5676	Prec@(1,5) (82.0%, 99.0%)
05/28 06:39:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 0.5720	Prec@(1,5) (81.9%, 99.0%)
05/28 06:39:21PM searchStage_trainer.py:260 [INFO] Valid: [ 27/49] Final Prec@1 81.9240%
05/28 06:39:21PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:39:21PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.4560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2724, 0.2472, 0.2359],
        [0.2325, 0.3020, 0.2515, 0.2140]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2853, 0.2702, 0.2152],
        [0.2478, 0.2836, 0.2573, 0.2113],
        [0.2576, 0.2960, 0.2207, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2854, 0.2450, 0.2109],
        [0.2647, 0.2867, 0.2346, 0.2140],
        [0.2585, 0.2815, 0.2186, 0.2414]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2756, 0.2255, 0.2472],
        [0.2558, 0.2828, 0.2314, 0.2299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2923, 0.2387, 0.2239],
        [0.2526, 0.2830, 0.2282, 0.2362],
        [0.2543, 0.2823, 0.2414, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2925, 0.2318, 0.2149],
        [0.2491, 0.2750, 0.2365, 0.2394],
        [0.2582, 0.2734, 0.2313, 0.2371]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:39:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 0.1721 (0.2343)	Prec@(1,5) (91.8%, 99.8%)	
05/28 06:40:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 0.2871 (0.2342)	Prec@(1,5) (91.9%, 99.9%)	
05/28 06:40:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 0.3186 (0.2348)	Prec@(1,5) (91.9%, 99.9%)	
05/28 06:40:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.2056 (0.2406)	Prec@(1,5) (91.6%, 99.9%)	
05/28 06:40:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 0.1429 (0.2438)	Prec@(1,5) (91.5%, 99.8%)	
05/28 06:41:14PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 0.4554 (0.2516)	Prec@(1,5) (91.3%, 99.8%)	
05/28 06:41:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][350/390]	Step 11298	lr 0.01075	Loss 0.1842 (0.2584)	Prec@(1,5) (91.0%, 99.9%)	
05/28 06:41:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 0.2089 (0.2595)	Prec@(1,5) (91.0%, 99.8%)	
05/28 06:41:48PM searchShareStage_trainer.py:134 [INFO] Train: [ 28/49] Final Prec@1 90.9800%
05/28 06:41:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][50/391]	Step 11339	Loss 0.4888	Prec@(1,5) (84.3%, 99.2%)
05/28 06:41:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 0.4962	Prec@(1,5) (84.0%, 99.2%)
05/28 06:41:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][150/391]	Step 11339	Loss 0.5077	Prec@(1,5) (83.7%, 99.2%)
05/28 06:42:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 0.5121	Prec@(1,5) (83.5%, 99.1%)
05/28 06:42:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][250/391]	Step 11339	Loss 0.5146	Prec@(1,5) (83.5%, 99.1%)
05/28 06:42:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 0.5129	Prec@(1,5) (83.7%, 99.1%)
05/28 06:42:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][350/391]	Step 11339	Loss 0.5086	Prec@(1,5) (83.8%, 99.0%)
05/28 06:42:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 0.5115	Prec@(1,5) (83.7%, 99.1%)
05/28 06:42:12PM searchStage_trainer.py:260 [INFO] Valid: [ 28/49] Final Prec@1 83.6920%
05/28 06:42:12PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:42:12PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.6920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2724, 0.2472, 0.2359],
        [0.2325, 0.3020, 0.2515, 0.2140]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2853, 0.2702, 0.2152],
        [0.2478, 0.2835, 0.2573, 0.2113],
        [0.2576, 0.2960, 0.2207, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2854, 0.2450, 0.2109],
        [0.2647, 0.2867, 0.2346, 0.2140],
        [0.2585, 0.2815, 0.2186, 0.2414]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2756, 0.2255, 0.2472],
        [0.2558, 0.2828, 0.2314, 0.2299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2923, 0.2387, 0.2239],
        [0.2526, 0.2830, 0.2282, 0.2362],
        [0.2543, 0.2823, 0.2414, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2925, 0.2318, 0.2149],
        [0.2491, 0.2750, 0.2365, 0.2394],
        [0.2582, 0.2734, 0.2313, 0.2371]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:42:32PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][50/390]	Step 11389	lr 0.01002	Loss 0.2587 (0.2372)	Prec@(1,5) (91.7%, 100.0%)	
05/28 06:42:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 0.1547 (0.2317)	Prec@(1,5) (92.0%, 99.9%)	
05/28 06:43:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][150/390]	Step 11489	lr 0.01002	Loss 0.1656 (0.2347)	Prec@(1,5) (91.8%, 99.8%)	
05/28 06:43:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.3202 (0.2336)	Prec@(1,5) (92.0%, 99.9%)	
05/28 06:43:47PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][250/390]	Step 11589	lr 0.01002	Loss 0.2951 (0.2370)	Prec@(1,5) (91.8%, 99.8%)	
05/28 06:44:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 0.3006 (0.2405)	Prec@(1,5) (91.7%, 99.8%)	
05/28 06:44:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][350/390]	Step 11689	lr 0.01002	Loss 0.2636 (0.2410)	Prec@(1,5) (91.7%, 99.8%)	
05/28 06:44:40PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 0.3539 (0.2415)	Prec@(1,5) (91.7%, 99.9%)	
05/28 06:44:40PM searchShareStage_trainer.py:134 [INFO] Train: [ 29/49] Final Prec@1 91.7280%
05/28 06:44:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][50/391]	Step 11730	Loss 0.5024	Prec@(1,5) (84.0%, 99.1%)
05/28 06:44:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 0.4924	Prec@(1,5) (84.3%, 99.1%)
05/28 06:44:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][150/391]	Step 11730	Loss 0.4956	Prec@(1,5) (84.4%, 99.2%)
05/28 06:44:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 0.4968	Prec@(1,5) (84.3%, 99.2%)
05/28 06:44:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][250/391]	Step 11730	Loss 0.4957	Prec@(1,5) (84.2%, 99.2%)
05/28 06:44:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 0.5008	Prec@(1,5) (84.1%, 99.2%)
05/28 06:45:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][350/391]	Step 11730	Loss 0.5057	Prec@(1,5) (84.0%, 99.2%)
05/28 06:45:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 0.5063	Prec@(1,5) (84.0%, 99.2%)
05/28 06:45:04PM searchStage_trainer.py:260 [INFO] Valid: [ 29/49] Final Prec@1 83.9520%
05/28 06:45:04PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:45:04PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 83.9520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2724, 0.2472, 0.2359],
        [0.2325, 0.3020, 0.2515, 0.2140]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2853, 0.2702, 0.2152],
        [0.2478, 0.2835, 0.2573, 0.2113],
        [0.2576, 0.2961, 0.2206, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2854, 0.2450, 0.2109],
        [0.2647, 0.2867, 0.2346, 0.2140],
        [0.2585, 0.2815, 0.2186, 0.2415]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2756, 0.2255, 0.2472],
        [0.2558, 0.2828, 0.2314, 0.2299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2922, 0.2387, 0.2239],
        [0.2526, 0.2830, 0.2282, 0.2362],
        [0.2543, 0.2823, 0.2414, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2924, 0.2318, 0.2149],
        [0.2491, 0.2750, 0.2365, 0.2394],
        [0.2582, 0.2734, 0.2313, 0.2371]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:45:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][50/390]	Step 11780	lr 0.00929	Loss 0.2937 (0.2140)	Prec@(1,5) (92.1%, 99.9%)	
05/28 06:45:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 0.2338 (0.2181)	Prec@(1,5) (92.3%, 99.9%)	
05/28 06:46:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][150/390]	Step 11880	lr 0.00929	Loss 0.1826 (0.2216)	Prec@(1,5) (92.1%, 99.9%)	
05/28 06:46:21PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 0.1395 (0.2226)	Prec@(1,5) (92.1%, 99.9%)	
05/28 06:46:40PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][250/390]	Step 11980	lr 0.00929	Loss 0.1525 (0.2245)	Prec@(1,5) (92.1%, 99.9%)	
05/28 06:46:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 0.1819 (0.2284)	Prec@(1,5) (91.9%, 99.9%)	
05/28 06:47:17PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][350/390]	Step 12080	lr 0.00929	Loss 0.1490 (0.2315)	Prec@(1,5) (91.8%, 99.9%)	
05/28 06:47:32PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 0.2972 (0.2343)	Prec@(1,5) (91.8%, 99.9%)	
05/28 06:47:32PM searchShareStage_trainer.py:134 [INFO] Train: [ 30/49] Final Prec@1 91.7520%
05/28 06:47:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][50/391]	Step 12121	Loss 0.4389	Prec@(1,5) (86.2%, 99.2%)
05/28 06:47:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 0.4534	Prec@(1,5) (85.6%, 99.2%)
05/28 06:47:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][150/391]	Step 12121	Loss 0.4652	Prec@(1,5) (85.4%, 99.2%)
05/28 06:47:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 0.4697	Prec@(1,5) (85.2%, 99.2%)
05/28 06:47:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][250/391]	Step 12121	Loss 0.4727	Prec@(1,5) (85.1%, 99.3%)
05/28 06:47:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 0.4750	Prec@(1,5) (85.1%, 99.3%)
05/28 06:47:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][350/391]	Step 12121	Loss 0.4680	Prec@(1,5) (85.3%, 99.3%)
05/28 06:47:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 0.4707	Prec@(1,5) (85.2%, 99.3%)
05/28 06:47:56PM searchStage_trainer.py:260 [INFO] Valid: [ 30/49] Final Prec@1 85.1640%
05/28 06:47:56PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:47:56PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.1640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2723, 0.2472, 0.2359],
        [0.2326, 0.3019, 0.2515, 0.2140]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2853, 0.2702, 0.2152],
        [0.2478, 0.2835, 0.2573, 0.2113],
        [0.2576, 0.2961, 0.2206, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2854, 0.2450, 0.2109],
        [0.2647, 0.2867, 0.2346, 0.2140],
        [0.2585, 0.2815, 0.2186, 0.2415]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2755, 0.2255, 0.2472],
        [0.2558, 0.2828, 0.2314, 0.2299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2922, 0.2387, 0.2239],
        [0.2526, 0.2830, 0.2282, 0.2363],
        [0.2543, 0.2823, 0.2414, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2924, 0.2319, 0.2149],
        [0.2491, 0.2750, 0.2365, 0.2395],
        [0.2582, 0.2734, 0.2313, 0.2371]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:48:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][50/390]	Step 12171	lr 0.00858	Loss 0.2161 (0.1827)	Prec@(1,5) (93.9%, 99.8%)	
05/28 06:48:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.1805 (0.1839)	Prec@(1,5) (93.8%, 99.9%)	
05/28 06:48:54PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][150/390]	Step 12271	lr 0.00858	Loss 0.2514 (0.1861)	Prec@(1,5) (93.8%, 99.9%)	
05/28 06:49:12PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.2154 (0.1910)	Prec@(1,5) (93.4%, 99.9%)	
05/28 06:49:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][250/390]	Step 12371	lr 0.00858	Loss 0.3462 (0.1931)	Prec@(1,5) (93.4%, 99.9%)	
05/28 06:49:50PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 0.1684 (0.1990)	Prec@(1,5) (93.1%, 99.9%)	
05/28 06:50:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][350/390]	Step 12471	lr 0.00858	Loss 0.2117 (0.2011)	Prec@(1,5) (93.1%, 99.9%)	
05/28 06:50:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 0.1154 (0.2024)	Prec@(1,5) (93.0%, 99.9%)	
05/28 06:50:24PM searchShareStage_trainer.py:134 [INFO] Train: [ 31/49] Final Prec@1 93.0440%
05/28 06:50:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][50/391]	Step 12512	Loss 0.4948	Prec@(1,5) (84.9%, 98.9%)
05/28 06:50:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 0.4676	Prec@(1,5) (85.7%, 99.1%)
05/28 06:50:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][150/391]	Step 12512	Loss 0.4810	Prec@(1,5) (85.2%, 99.1%)
05/28 06:50:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 0.4866	Prec@(1,5) (85.0%, 99.1%)
05/28 06:50:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][250/391]	Step 12512	Loss 0.4990	Prec@(1,5) (84.7%, 99.1%)
05/28 06:50:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 0.4924	Prec@(1,5) (84.9%, 99.0%)
05/28 06:50:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][350/391]	Step 12512	Loss 0.4938	Prec@(1,5) (84.8%, 99.1%)
05/28 06:50:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 0.4944	Prec@(1,5) (84.8%, 99.1%)
05/28 06:50:47PM searchStage_trainer.py:260 [INFO] Valid: [ 31/49] Final Prec@1 84.8000%
05/28 06:50:47PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:50:47PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.1640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2723, 0.2472, 0.2360],
        [0.2325, 0.3019, 0.2515, 0.2140]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2853, 0.2702, 0.2152],
        [0.2478, 0.2835, 0.2573, 0.2113],
        [0.2576, 0.2960, 0.2207, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2854, 0.2450, 0.2109],
        [0.2647, 0.2867, 0.2346, 0.2140],
        [0.2585, 0.2814, 0.2186, 0.2415]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2755, 0.2255, 0.2472],
        [0.2559, 0.2828, 0.2315, 0.2299]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2922, 0.2387, 0.2239],
        [0.2526, 0.2829, 0.2282, 0.2363],
        [0.2543, 0.2823, 0.2414, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2924, 0.2318, 0.2149],
        [0.2491, 0.2750, 0.2365, 0.2395],
        [0.2582, 0.2734, 0.2313, 0.2371]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:51:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][50/390]	Step 12562	lr 0.00789	Loss 0.1082 (0.1691)	Prec@(1,5) (93.7%, 99.9%)	
05/28 06:51:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.1387 (0.1681)	Prec@(1,5) (93.9%, 99.9%)	
05/28 06:51:45PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][150/390]	Step 12662	lr 0.00789	Loss 0.1250 (0.1720)	Prec@(1,5) (93.7%, 99.9%)	
05/28 06:52:04PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.1095 (0.1767)	Prec@(1,5) (93.5%, 99.9%)	
05/28 06:52:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][250/390]	Step 12762	lr 0.00789	Loss 0.1855 (0.1809)	Prec@(1,5) (93.4%, 99.9%)	
05/28 06:52:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.1110 (0.1863)	Prec@(1,5) (93.3%, 99.9%)	
05/28 06:53:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][350/390]	Step 12862	lr 0.00789	Loss 0.2347 (0.1881)	Prec@(1,5) (93.3%, 99.9%)	
05/28 06:53:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 0.1759 (0.1894)	Prec@(1,5) (93.2%, 99.9%)	
05/28 06:53:16PM searchShareStage_trainer.py:134 [INFO] Train: [ 32/49] Final Prec@1 93.2240%
05/28 06:53:19PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][50/391]	Step 12903	Loss 0.5186	Prec@(1,5) (84.0%, 99.0%)
05/28 06:53:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 0.4997	Prec@(1,5) (84.5%, 99.2%)
05/28 06:53:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][150/391]	Step 12903	Loss 0.5060	Prec@(1,5) (84.4%, 99.2%)
05/28 06:53:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 0.5050	Prec@(1,5) (84.5%, 99.2%)
05/28 06:53:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][250/391]	Step 12903	Loss 0.5063	Prec@(1,5) (84.4%, 99.1%)
05/28 06:53:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 0.5084	Prec@(1,5) (84.5%, 99.1%)
05/28 06:53:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][350/391]	Step 12903	Loss 0.5107	Prec@(1,5) (84.4%, 99.1%)
05/28 06:53:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 0.5072	Prec@(1,5) (84.5%, 99.1%)
05/28 06:53:40PM searchStage_trainer.py:260 [INFO] Valid: [ 32/49] Final Prec@1 84.4560%
05/28 06:53:40PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:53:40PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.1640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2723, 0.2472, 0.2360],
        [0.2325, 0.3019, 0.2515, 0.2140]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2852, 0.2702, 0.2152],
        [0.2478, 0.2835, 0.2573, 0.2113],
        [0.2576, 0.2960, 0.2207, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2854, 0.2450, 0.2109],
        [0.2647, 0.2867, 0.2346, 0.2140],
        [0.2585, 0.2813, 0.2187, 0.2415]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2956, 0.2325, 0.2092],
        [0.2517, 0.2755, 0.2255, 0.2472],
        [0.2558, 0.2827, 0.2315, 0.2300]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2923, 0.2387, 0.2238],
        [0.2526, 0.2829, 0.2282, 0.2363],
        [0.2543, 0.2822, 0.2414, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2925, 0.2318, 0.2149],
        [0.2491, 0.2750, 0.2365, 0.2395],
        [0.2582, 0.2732, 0.2314, 0.2372]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:54:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][50/390]	Step 12953	lr 0.00722	Loss 0.1802 (0.1844)	Prec@(1,5) (93.1%, 100.0%)	
05/28 06:54:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.0926 (0.1779)	Prec@(1,5) (93.5%, 99.9%)	
05/28 06:54:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][150/390]	Step 13053	lr 0.00722	Loss 0.1282 (0.1825)	Prec@(1,5) (93.5%, 99.9%)	
05/28 06:54:57PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.3068 (0.1811)	Prec@(1,5) (93.6%, 99.9%)	
05/28 06:55:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][250/390]	Step 13153	lr 0.00722	Loss 0.1551 (0.1809)	Prec@(1,5) (93.6%, 99.9%)	
05/28 06:55:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.3463 (0.1825)	Prec@(1,5) (93.6%, 99.9%)	
05/28 06:55:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][350/390]	Step 13253	lr 0.00722	Loss 0.0874 (0.1824)	Prec@(1,5) (93.6%, 99.9%)	
05/28 06:56:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 0.2065 (0.1850)	Prec@(1,5) (93.4%, 99.9%)	
05/28 06:56:08PM searchShareStage_trainer.py:134 [INFO] Train: [ 33/49] Final Prec@1 93.4400%
05/28 06:56:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][50/391]	Step 13294	Loss 0.5369	Prec@(1,5) (84.3%, 98.8%)
05/28 06:56:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 0.5072	Prec@(1,5) (85.1%, 99.1%)
05/28 06:56:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][150/391]	Step 13294	Loss 0.4953	Prec@(1,5) (85.3%, 99.2%)
05/28 06:56:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 0.5071	Prec@(1,5) (85.0%, 99.2%)
05/28 06:56:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][250/391]	Step 13294	Loss 0.5114	Prec@(1,5) (85.0%, 99.2%)
05/28 06:56:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 0.5116	Prec@(1,5) (85.0%, 99.1%)
05/28 06:56:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][350/391]	Step 13294	Loss 0.5115	Prec@(1,5) (84.9%, 99.1%)
05/28 06:56:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 0.5058	Prec@(1,5) (85.1%, 99.2%)
05/28 06:56:31PM searchStage_trainer.py:260 [INFO] Valid: [ 33/49] Final Prec@1 85.0920%
05/28 06:56:31PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:56:31PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.1640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2326, 0.3018, 0.2515, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2852, 0.2702, 0.2152],
        [0.2478, 0.2835, 0.2573, 0.2114],
        [0.2576, 0.2960, 0.2207, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2854, 0.2450, 0.2110],
        [0.2647, 0.2867, 0.2346, 0.2140],
        [0.2585, 0.2813, 0.2187, 0.2415]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2956, 0.2326, 0.2092],
        [0.2517, 0.2755, 0.2255, 0.2472],
        [0.2558, 0.2827, 0.2315, 0.2300]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2923, 0.2387, 0.2238],
        [0.2526, 0.2829, 0.2282, 0.2363],
        [0.2543, 0.2822, 0.2414, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2925, 0.2318, 0.2148],
        [0.2491, 0.2750, 0.2365, 0.2395],
        [0.2582, 0.2732, 0.2314, 0.2372]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:56:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][50/390]	Step 13344	lr 0.00657	Loss 0.1820 (0.1553)	Prec@(1,5) (94.0%, 99.9%)	
05/28 06:57:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.2350 (0.1560)	Prec@(1,5) (94.3%, 100.0%)	
05/28 06:57:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][150/390]	Step 13444	lr 0.00657	Loss 0.1502 (0.1627)	Prec@(1,5) (94.2%, 100.0%)	
05/28 06:57:47PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.0855 (0.1608)	Prec@(1,5) (94.2%, 100.0%)	
05/28 06:58:06PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][250/390]	Step 13544	lr 0.00657	Loss 0.2523 (0.1621)	Prec@(1,5) (94.2%, 99.9%)	
05/28 06:58:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.2250 (0.1612)	Prec@(1,5) (94.3%, 99.9%)	
05/28 06:58:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][350/390]	Step 13644	lr 0.00657	Loss 0.2084 (0.1631)	Prec@(1,5) (94.3%, 99.9%)	
05/28 06:58:59PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 0.0986 (0.1658)	Prec@(1,5) (94.1%, 99.9%)	
05/28 06:58:59PM searchShareStage_trainer.py:134 [INFO] Train: [ 34/49] Final Prec@1 94.1120%
05/28 06:59:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][50/391]	Step 13685	Loss 0.4866	Prec@(1,5) (85.4%, 99.0%)
05/28 06:59:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 0.4919	Prec@(1,5) (85.5%, 99.0%)
05/28 06:59:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][150/391]	Step 13685	Loss 0.4829	Prec@(1,5) (85.7%, 99.1%)
05/28 06:59:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 0.4880	Prec@(1,5) (85.6%, 99.1%)
05/28 06:59:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][250/391]	Step 13685	Loss 0.4909	Prec@(1,5) (85.5%, 99.1%)
05/28 06:59:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 0.4867	Prec@(1,5) (85.6%, 99.1%)
05/28 06:59:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][350/391]	Step 13685	Loss 0.4857	Prec@(1,5) (85.5%, 99.1%)
05/28 06:59:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 0.4850	Prec@(1,5) (85.5%, 99.2%)
05/28 06:59:23PM searchStage_trainer.py:260 [INFO] Valid: [ 34/49] Final Prec@1 85.4920%
05/28 06:59:23PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 06:59:23PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.4920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2852, 0.2703, 0.2153],
        [0.2478, 0.2835, 0.2573, 0.2114],
        [0.2576, 0.2960, 0.2207, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2853, 0.2450, 0.2110],
        [0.2647, 0.2867, 0.2346, 0.2140],
        [0.2585, 0.2813, 0.2187, 0.2415]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2956, 0.2326, 0.2092],
        [0.2517, 0.2755, 0.2255, 0.2472],
        [0.2558, 0.2827, 0.2315, 0.2300]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2922, 0.2387, 0.2238],
        [0.2526, 0.2829, 0.2282, 0.2363],
        [0.2543, 0.2822, 0.2414, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2924, 0.2319, 0.2149],
        [0.2491, 0.2750, 0.2365, 0.2395],
        [0.2582, 0.2732, 0.2314, 0.2372]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 06:59:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][50/390]	Step 13735	lr 0.00595	Loss 0.0677 (0.1338)	Prec@(1,5) (95.4%, 99.9%)	
05/28 07:00:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.1000 (0.1328)	Prec@(1,5) (95.6%, 100.0%)	
05/28 07:00:21PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][150/390]	Step 13835	lr 0.00595	Loss 0.2394 (0.1348)	Prec@(1,5) (95.3%, 99.9%)	
05/28 07:00:40PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 0.1514 (0.1332)	Prec@(1,5) (95.4%, 99.9%)	
05/28 07:00:59PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][250/390]	Step 13935	lr 0.00595	Loss 0.1390 (0.1376)	Prec@(1,5) (95.2%, 99.9%)	
05/28 07:01:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 0.2117 (0.1408)	Prec@(1,5) (95.1%, 99.9%)	
05/28 07:01:36PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][350/390]	Step 14035	lr 0.00595	Loss 0.1583 (0.1427)	Prec@(1,5) (95.1%, 99.9%)	
05/28 07:01:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 0.1697 (0.1429)	Prec@(1,5) (95.0%, 99.9%)	
05/28 07:01:52PM searchShareStage_trainer.py:134 [INFO] Train: [ 35/49] Final Prec@1 95.0080%
05/28 07:01:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][50/391]	Step 14076	Loss 0.4741	Prec@(1,5) (86.0%, 99.3%)
05/28 07:01:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 0.4795	Prec@(1,5) (85.7%, 99.3%)
05/28 07:02:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][150/391]	Step 14076	Loss 0.4693	Prec@(1,5) (85.9%, 99.2%)
05/28 07:02:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 0.4811	Prec@(1,5) (85.7%, 99.2%)
05/28 07:02:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][250/391]	Step 14076	Loss 0.4860	Prec@(1,5) (85.6%, 99.2%)
05/28 07:02:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 0.4878	Prec@(1,5) (85.5%, 99.3%)
05/28 07:02:13PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][350/391]	Step 14076	Loss 0.4874	Prec@(1,5) (85.5%, 99.3%)
05/28 07:02:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 0.4903	Prec@(1,5) (85.5%, 99.2%)
05/28 07:02:16PM searchStage_trainer.py:260 [INFO] Valid: [ 35/49] Final Prec@1 85.4920%
05/28 07:02:16PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:02:16PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 85.4920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2852, 0.2703, 0.2153],
        [0.2478, 0.2835, 0.2573, 0.2114],
        [0.2576, 0.2959, 0.2207, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2853, 0.2451, 0.2110],
        [0.2647, 0.2866, 0.2347, 0.2140],
        [0.2585, 0.2813, 0.2187, 0.2415]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2956, 0.2326, 0.2092],
        [0.2517, 0.2755, 0.2255, 0.2472],
        [0.2558, 0.2827, 0.2315, 0.2300]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2922, 0.2388, 0.2239],
        [0.2526, 0.2829, 0.2282, 0.2363],
        [0.2543, 0.2822, 0.2414, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2924, 0.2319, 0.2149],
        [0.2491, 0.2750, 0.2365, 0.2395],
        [0.2582, 0.2732, 0.2314, 0.2372]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:02:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][50/390]	Step 14126	lr 0.00535	Loss 0.1410 (0.1176)	Prec@(1,5) (96.0%, 99.9%)	
05/28 07:02:54PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.1683 (0.1184)	Prec@(1,5) (96.1%, 100.0%)	
05/28 07:03:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][150/390]	Step 14226	lr 0.00535	Loss 0.1882 (0.1219)	Prec@(1,5) (95.8%, 100.0%)	
05/28 07:03:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.1911 (0.1232)	Prec@(1,5) (95.8%, 100.0%)	
05/28 07:03:50PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][250/390]	Step 14326	lr 0.00535	Loss 0.0599 (0.1250)	Prec@(1,5) (95.7%, 100.0%)	
05/28 07:04:09PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.1033 (0.1265)	Prec@(1,5) (95.6%, 100.0%)	
05/28 07:04:27PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][350/390]	Step 14426	lr 0.00535	Loss 0.1095 (0.1263)	Prec@(1,5) (95.6%, 100.0%)	
05/28 07:04:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.1710 (0.1285)	Prec@(1,5) (95.5%, 100.0%)	
05/28 07:04:42PM searchShareStage_trainer.py:134 [INFO] Train: [ 36/49] Final Prec@1 95.5280%
05/28 07:04:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][50/391]	Step 14467	Loss 0.4494	Prec@(1,5) (86.8%, 99.1%)
05/28 07:04:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 0.4719	Prec@(1,5) (86.4%, 99.1%)
05/28 07:04:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][150/391]	Step 14467	Loss 0.4710	Prec@(1,5) (86.2%, 99.2%)
05/28 07:04:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 0.4712	Prec@(1,5) (86.2%, 99.3%)
05/28 07:04:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][250/391]	Step 14467	Loss 0.4696	Prec@(1,5) (86.1%, 99.3%)
05/28 07:05:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 0.4677	Prec@(1,5) (86.1%, 99.3%)
05/28 07:05:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][350/391]	Step 14467	Loss 0.4706	Prec@(1,5) (86.1%, 99.3%)
05/28 07:05:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 0.4733	Prec@(1,5) (86.1%, 99.3%)
05/28 07:05:05PM searchStage_trainer.py:260 [INFO] Valid: [ 36/49] Final Prec@1 86.0840%
05/28 07:05:05PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:05:06PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.0840%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2852, 0.2703, 0.2153],
        [0.2478, 0.2835, 0.2573, 0.2114],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2853, 0.2451, 0.2110],
        [0.2647, 0.2866, 0.2347, 0.2141],
        [0.2585, 0.2813, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2956, 0.2325, 0.2092],
        [0.2518, 0.2755, 0.2255, 0.2473],
        [0.2558, 0.2826, 0.2315, 0.2300]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2922, 0.2388, 0.2239],
        [0.2526, 0.2829, 0.2282, 0.2363],
        [0.2543, 0.2821, 0.2414, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2924, 0.2319, 0.2149],
        [0.2491, 0.2750, 0.2365, 0.2395],
        [0.2582, 0.2732, 0.2314, 0.2372]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:05:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][50/390]	Step 14517	lr 0.00479	Loss 0.0341 (0.1127)	Prec@(1,5) (96.2%, 100.0%)	
05/28 07:05:45PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.0846 (0.1105)	Prec@(1,5) (96.3%, 100.0%)	
05/28 07:06:04PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][150/390]	Step 14617	lr 0.00479	Loss 0.0948 (0.1150)	Prec@(1,5) (96.1%, 100.0%)	
05/28 07:06:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.1243 (0.1107)	Prec@(1,5) (96.2%, 100.0%)	
05/28 07:06:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][250/390]	Step 14717	lr 0.00479	Loss 0.0998 (0.1082)	Prec@(1,5) (96.3%, 100.0%)	
05/28 07:06:59PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.0648 (0.1073)	Prec@(1,5) (96.3%, 100.0%)	
05/28 07:07:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][350/390]	Step 14817	lr 0.00479	Loss 0.2662 (0.1085)	Prec@(1,5) (96.3%, 100.0%)	
05/28 07:07:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.1289 (0.1102)	Prec@(1,5) (96.3%, 100.0%)	
05/28 07:07:34PM searchShareStage_trainer.py:134 [INFO] Train: [ 37/49] Final Prec@1 96.2560%
05/28 07:07:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][50/391]	Step 14858	Loss 0.4748	Prec@(1,5) (86.4%, 99.4%)
05/28 07:07:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 0.4758	Prec@(1,5) (86.6%, 99.4%)
05/28 07:07:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][150/391]	Step 14858	Loss 0.4861	Prec@(1,5) (86.6%, 99.3%)
05/28 07:07:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 0.4786	Prec@(1,5) (86.6%, 99.3%)
05/28 07:07:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][250/391]	Step 14858	Loss 0.4802	Prec@(1,5) (86.3%, 99.3%)
05/28 07:07:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 0.4848	Prec@(1,5) (86.2%, 99.3%)
05/28 07:07:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][350/391]	Step 14858	Loss 0.4841	Prec@(1,5) (86.3%, 99.3%)
05/28 07:07:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 0.4812	Prec@(1,5) (86.3%, 99.3%)
05/28 07:07:58PM searchStage_trainer.py:260 [INFO] Valid: [ 37/49] Final Prec@1 86.3400%
05/28 07:07:58PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:07:58PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.3400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2851, 0.2703, 0.2153],
        [0.2478, 0.2835, 0.2573, 0.2114],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2853, 0.2451, 0.2110],
        [0.2647, 0.2865, 0.2347, 0.2141],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2956, 0.2325, 0.2092],
        [0.2518, 0.2754, 0.2255, 0.2473],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2922, 0.2388, 0.2239],
        [0.2526, 0.2828, 0.2283, 0.2363],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2608, 0.2924, 0.2319, 0.2149],
        [0.2491, 0.2749, 0.2365, 0.2395],
        [0.2582, 0.2731, 0.2314, 0.2372]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:08:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][50/390]	Step 14908	lr 0.00425	Loss 0.0887 (0.0987)	Prec@(1,5) (96.7%, 100.0%)	
05/28 07:08:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.0664 (0.0908)	Prec@(1,5) (96.8%, 100.0%)	
05/28 07:08:55PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][150/390]	Step 15008	lr 0.00425	Loss 0.0946 (0.0894)	Prec@(1,5) (96.9%, 100.0%)	
05/28 07:09:14PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.1035 (0.0914)	Prec@(1,5) (96.8%, 100.0%)	
05/28 07:09:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][250/390]	Step 15108	lr 0.00425	Loss 0.0416 (0.0908)	Prec@(1,5) (96.9%, 100.0%)	
05/28 07:09:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.0887 (0.0929)	Prec@(1,5) (96.8%, 100.0%)	
05/28 07:10:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][350/390]	Step 15208	lr 0.00425	Loss 0.0739 (0.0950)	Prec@(1,5) (96.7%, 100.0%)	
05/28 07:10:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.0728 (0.0955)	Prec@(1,5) (96.7%, 100.0%)	
05/28 07:10:25PM searchShareStage_trainer.py:134 [INFO] Train: [ 38/49] Final Prec@1 96.6880%
05/28 07:10:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][50/391]	Step 15249	Loss 0.4713	Prec@(1,5) (86.2%, 99.4%)
05/28 07:10:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 0.4617	Prec@(1,5) (86.4%, 99.3%)
05/28 07:10:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][150/391]	Step 15249	Loss 0.4806	Prec@(1,5) (86.1%, 99.3%)
05/28 07:10:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 0.4807	Prec@(1,5) (86.3%, 99.3%)
05/28 07:10:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][250/391]	Step 15249	Loss 0.4850	Prec@(1,5) (86.3%, 99.3%)
05/28 07:10:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 0.4822	Prec@(1,5) (86.4%, 99.3%)
05/28 07:10:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][350/391]	Step 15249	Loss 0.4838	Prec@(1,5) (86.3%, 99.3%)
05/28 07:10:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 0.4827	Prec@(1,5) (86.3%, 99.3%)
05/28 07:10:49PM searchStage_trainer.py:260 [INFO] Valid: [ 38/49] Final Prec@1 86.2920%
05/28 07:10:49PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:10:49PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.3400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2851, 0.2703, 0.2153],
        [0.2478, 0.2834, 0.2573, 0.2114],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2853, 0.2451, 0.2110],
        [0.2647, 0.2865, 0.2347, 0.2141],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2754, 0.2255, 0.2473],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2922, 0.2387, 0.2238],
        [0.2526, 0.2828, 0.2283, 0.2363],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2924, 0.2319, 0.2149],
        [0.2491, 0.2748, 0.2366, 0.2395],
        [0.2582, 0.2731, 0.2315, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:11:09PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][50/390]	Step 15299	lr 0.00375	Loss 0.0999 (0.0831)	Prec@(1,5) (97.1%, 100.0%)	
05/28 07:11:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.0693 (0.0828)	Prec@(1,5) (97.1%, 100.0%)	
05/28 07:11:46PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][150/390]	Step 15399	lr 0.00375	Loss 0.0303 (0.0826)	Prec@(1,5) (97.1%, 100.0%)	
05/28 07:12:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.0476 (0.0814)	Prec@(1,5) (97.1%, 100.0%)	
05/28 07:12:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][250/390]	Step 15499	lr 0.00375	Loss 0.0527 (0.0839)	Prec@(1,5) (97.1%, 100.0%)	
05/28 07:12:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.0920 (0.0839)	Prec@(1,5) (97.1%, 100.0%)	
05/28 07:13:01PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][350/390]	Step 15599	lr 0.00375	Loss 0.0925 (0.0839)	Prec@(1,5) (97.1%, 100.0%)	
05/28 07:13:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.1891 (0.0836)	Prec@(1,5) (97.1%, 100.0%)	
05/28 07:13:17PM searchShareStage_trainer.py:134 [INFO] Train: [ 39/49] Final Prec@1 97.1280%
05/28 07:13:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][50/391]	Step 15640	Loss 0.4461	Prec@(1,5) (87.8%, 99.4%)
05/28 07:13:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 0.4776	Prec@(1,5) (87.2%, 99.4%)
05/28 07:13:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][150/391]	Step 15640	Loss 0.4759	Prec@(1,5) (87.0%, 99.4%)
05/28 07:13:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 0.4818	Prec@(1,5) (86.9%, 99.4%)
05/28 07:13:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][250/391]	Step 15640	Loss 0.4751	Prec@(1,5) (86.9%, 99.4%)
05/28 07:13:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 0.4798	Prec@(1,5) (86.8%, 99.4%)
05/28 07:13:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][350/391]	Step 15640	Loss 0.4805	Prec@(1,5) (86.7%, 99.4%)
05/28 07:13:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 0.4833	Prec@(1,5) (86.8%, 99.4%)
05/28 07:13:40PM searchStage_trainer.py:260 [INFO] Valid: [ 39/49] Final Prec@1 86.7600%
05/28 07:13:40PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:13:40PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.7600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2851, 0.2703, 0.2153],
        [0.2478, 0.2834, 0.2574, 0.2114],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2853, 0.2451, 0.2110],
        [0.2647, 0.2865, 0.2347, 0.2141],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2754, 0.2255, 0.2473],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2922, 0.2387, 0.2238],
        [0.2526, 0.2828, 0.2283, 0.2363],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2924, 0.2319, 0.2149],
        [0.2491, 0.2748, 0.2366, 0.2396],
        [0.2582, 0.2731, 0.2315, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:14:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][50/390]	Step 15690	lr 0.00329	Loss 0.0707 (0.0644)	Prec@(1,5) (97.9%, 100.0%)	
05/28 07:14:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.0617 (0.0662)	Prec@(1,5) (97.9%, 100.0%)	
05/28 07:14:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][150/390]	Step 15790	lr 0.00329	Loss 0.1694 (0.0671)	Prec@(1,5) (97.9%, 100.0%)	
05/28 07:14:56PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.0352 (0.0684)	Prec@(1,5) (97.8%, 100.0%)	
05/28 07:15:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][250/390]	Step 15890	lr 0.00329	Loss 0.0658 (0.0699)	Prec@(1,5) (97.7%, 100.0%)	
05/28 07:15:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.0276 (0.0714)	Prec@(1,5) (97.7%, 100.0%)	
05/28 07:15:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][350/390]	Step 15990	lr 0.00329	Loss 0.1368 (0.0726)	Prec@(1,5) (97.6%, 100.0%)	
05/28 07:16:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.0512 (0.0745)	Prec@(1,5) (97.6%, 100.0%)	
05/28 07:16:08PM searchShareStage_trainer.py:134 [INFO] Train: [ 40/49] Final Prec@1 97.5560%
05/28 07:16:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][50/391]	Step 16031	Loss 0.4985	Prec@(1,5) (86.3%, 99.3%)
05/28 07:16:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 0.4900	Prec@(1,5) (86.3%, 99.3%)
05/28 07:16:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][150/391]	Step 16031	Loss 0.4878	Prec@(1,5) (86.3%, 99.3%)
05/28 07:16:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 0.4878	Prec@(1,5) (86.4%, 99.3%)
05/28 07:16:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][250/391]	Step 16031	Loss 0.4940	Prec@(1,5) (86.3%, 99.3%)
05/28 07:16:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 0.4954	Prec@(1,5) (86.2%, 99.3%)
05/28 07:16:29PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][350/391]	Step 16031	Loss 0.4891	Prec@(1,5) (86.4%, 99.3%)
05/28 07:16:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 0.4896	Prec@(1,5) (86.4%, 99.3%)
05/28 07:16:32PM searchStage_trainer.py:260 [INFO] Valid: [ 40/49] Final Prec@1 86.3680%
05/28 07:16:32PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:16:32PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 86.7600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2850, 0.2704, 0.2153],
        [0.2478, 0.2833, 0.2574, 0.2115],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2853, 0.2451, 0.2110],
        [0.2647, 0.2865, 0.2347, 0.2141],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2754, 0.2255, 0.2473],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2922, 0.2387, 0.2238],
        [0.2526, 0.2828, 0.2283, 0.2363],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2924, 0.2319, 0.2149],
        [0.2491, 0.2748, 0.2366, 0.2396],
        [0.2582, 0.2731, 0.2315, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:16:51PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][50/390]	Step 16081	lr 0.00287	Loss 0.0808 (0.0625)	Prec@(1,5) (98.0%, 100.0%)	
05/28 07:17:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.1036 (0.0653)	Prec@(1,5) (97.8%, 100.0%)	
05/28 07:17:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][150/390]	Step 16181	lr 0.00287	Loss 0.0686 (0.0635)	Prec@(1,5) (97.9%, 100.0%)	
05/28 07:17:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.0538 (0.0618)	Prec@(1,5) (98.0%, 100.0%)	
05/28 07:18:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][250/390]	Step 16281	lr 0.00287	Loss 0.0543 (0.0637)	Prec@(1,5) (97.9%, 100.0%)	
05/28 07:18:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.0382 (0.0623)	Prec@(1,5) (97.9%, 100.0%)	
05/28 07:18:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][350/390]	Step 16381	lr 0.00287	Loss 0.0504 (0.0629)	Prec@(1,5) (97.9%, 100.0%)	
05/28 07:18:59PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.1039 (0.0634)	Prec@(1,5) (97.9%, 100.0%)	
05/28 07:18:59PM searchShareStage_trainer.py:134 [INFO] Train: [ 41/49] Final Prec@1 97.8800%
05/28 07:19:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][50/391]	Step 16422	Loss 0.4507	Prec@(1,5) (87.9%, 99.3%)
05/28 07:19:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 0.4700	Prec@(1,5) (87.5%, 99.4%)
05/28 07:19:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][150/391]	Step 16422	Loss 0.4741	Prec@(1,5) (87.5%, 99.3%)
05/28 07:19:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 0.4715	Prec@(1,5) (87.5%, 99.3%)
05/28 07:19:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][250/391]	Step 16422	Loss 0.4825	Prec@(1,5) (87.2%, 99.3%)
05/28 07:19:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 0.4790	Prec@(1,5) (87.2%, 99.4%)
05/28 07:19:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][350/391]	Step 16422	Loss 0.4808	Prec@(1,5) (87.2%, 99.4%)
05/28 07:19:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 0.4781	Prec@(1,5) (87.2%, 99.4%)
05/28 07:19:22PM searchStage_trainer.py:260 [INFO] Valid: [ 41/49] Final Prec@1 87.1600%
05/28 07:19:22PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:19:23PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.1600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2850, 0.2704, 0.2153],
        [0.2478, 0.2833, 0.2574, 0.2115],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2853, 0.2451, 0.2111],
        [0.2647, 0.2865, 0.2347, 0.2141],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2754, 0.2255, 0.2473],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2922, 0.2387, 0.2238],
        [0.2526, 0.2828, 0.2283, 0.2363],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2924, 0.2319, 0.2149],
        [0.2491, 0.2748, 0.2366, 0.2396],
        [0.2582, 0.2731, 0.2315, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:19:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][50/390]	Step 16472	lr 0.00248	Loss 0.1197 (0.0535)	Prec@(1,5) (98.3%, 100.0%)	
05/28 07:20:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.0705 (0.0514)	Prec@(1,5) (98.3%, 100.0%)	
05/28 07:20:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][150/390]	Step 16572	lr 0.00248	Loss 0.0624 (0.0535)	Prec@(1,5) (98.2%, 100.0%)	
05/28 07:20:39PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.1052 (0.0544)	Prec@(1,5) (98.3%, 100.0%)	
05/28 07:20:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][250/390]	Step 16672	lr 0.00248	Loss 0.1429 (0.0537)	Prec@(1,5) (98.3%, 100.0%)	
05/28 07:21:17PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.1163 (0.0527)	Prec@(1,5) (98.3%, 100.0%)	
05/28 07:21:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][350/390]	Step 16772	lr 0.00248	Loss 0.0454 (0.0525)	Prec@(1,5) (98.3%, 100.0%)	
05/28 07:21:50PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.0311 (0.0520)	Prec@(1,5) (98.3%, 100.0%)	
05/28 07:21:51PM searchShareStage_trainer.py:134 [INFO] Train: [ 42/49] Final Prec@1 98.3280%
05/28 07:21:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][50/391]	Step 16813	Loss 0.4951	Prec@(1,5) (86.7%, 99.2%)
05/28 07:21:57PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 0.4912	Prec@(1,5) (87.1%, 99.3%)
05/28 07:22:00PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][150/391]	Step 16813	Loss 0.4804	Prec@(1,5) (87.3%, 99.3%)
05/28 07:22:03PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 0.4793	Prec@(1,5) (87.2%, 99.3%)
05/28 07:22:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][250/391]	Step 16813	Loss 0.4767	Prec@(1,5) (87.2%, 99.3%)
05/28 07:22:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 0.4740	Prec@(1,5) (87.3%, 99.3%)
05/28 07:22:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][350/391]	Step 16813	Loss 0.4733	Prec@(1,5) (87.3%, 99.3%)
05/28 07:22:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 0.4769	Prec@(1,5) (87.3%, 99.3%)
05/28 07:22:14PM searchStage_trainer.py:260 [INFO] Valid: [ 42/49] Final Prec@1 87.3240%
05/28 07:22:14PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:22:15PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.3240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2850, 0.2704, 0.2153],
        [0.2478, 0.2833, 0.2574, 0.2115],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2853, 0.2451, 0.2111],
        [0.2647, 0.2865, 0.2347, 0.2141],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2957, 0.2325, 0.2092],
        [0.2517, 0.2754, 0.2255, 0.2473],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2922, 0.2387, 0.2238],
        [0.2526, 0.2828, 0.2283, 0.2363],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2924, 0.2319, 0.2148],
        [0.2491, 0.2748, 0.2366, 0.2396],
        [0.2582, 0.2731, 0.2315, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:22:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][50/390]	Step 16863	lr 0.00214	Loss 0.0382 (0.0504)	Prec@(1,5) (98.5%, 100.0%)	
05/28 07:22:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.0400 (0.0507)	Prec@(1,5) (98.5%, 100.0%)	
05/28 07:23:12PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][150/390]	Step 16963	lr 0.00214	Loss 0.0674 (0.0522)	Prec@(1,5) (98.3%, 100.0%)	
05/28 07:23:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.0228 (0.0512)	Prec@(1,5) (98.4%, 100.0%)	
05/28 07:23:50PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][250/390]	Step 17063	lr 0.00214	Loss 0.0356 (0.0509)	Prec@(1,5) (98.4%, 100.0%)	
05/28 07:24:09PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.0355 (0.0515)	Prec@(1,5) (98.4%, 100.0%)	
05/28 07:24:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][350/390]	Step 17163	lr 0.00214	Loss 0.0421 (0.0513)	Prec@(1,5) (98.4%, 100.0%)	
05/28 07:24:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.0827 (0.0514)	Prec@(1,5) (98.4%, 100.0%)	
05/28 07:24:43PM searchShareStage_trainer.py:134 [INFO] Train: [ 43/49] Final Prec@1 98.4400%
05/28 07:24:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][50/391]	Step 17204	Loss 0.4950	Prec@(1,5) (87.3%, 99.4%)
05/28 07:24:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 0.5004	Prec@(1,5) (87.2%, 99.2%)
05/28 07:24:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][150/391]	Step 17204	Loss 0.4795	Prec@(1,5) (87.5%, 99.3%)
05/28 07:24:55PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 0.4863	Prec@(1,5) (87.4%, 99.2%)
05/28 07:24:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][250/391]	Step 17204	Loss 0.4898	Prec@(1,5) (87.2%, 99.3%)
05/28 07:25:01PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 0.4982	Prec@(1,5) (87.1%, 99.2%)
05/28 07:25:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][350/391]	Step 17204	Loss 0.4928	Prec@(1,5) (87.2%, 99.3%)
05/28 07:25:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 0.4932	Prec@(1,5) (87.2%, 99.3%)
05/28 07:25:06PM searchStage_trainer.py:260 [INFO] Valid: [ 43/49] Final Prec@1 87.1840%
05/28 07:25:06PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:25:07PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.3240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2850, 0.2704, 0.2153],
        [0.2478, 0.2833, 0.2574, 0.2115],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2852, 0.2451, 0.2111],
        [0.2647, 0.2864, 0.2348, 0.2142],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2956, 0.2325, 0.2092],
        [0.2517, 0.2754, 0.2255, 0.2473],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2922, 0.2387, 0.2238],
        [0.2526, 0.2828, 0.2283, 0.2363],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2924, 0.2318, 0.2148],
        [0.2491, 0.2748, 0.2366, 0.2396],
        [0.2582, 0.2731, 0.2315, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:25:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][50/390]	Step 17254	lr 0.00184	Loss 0.0181 (0.0436)	Prec@(1,5) (98.5%, 100.0%)	
05/28 07:25:45PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.0541 (0.0431)	Prec@(1,5) (98.6%, 100.0%)	
05/28 07:26:04PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][150/390]	Step 17354	lr 0.00184	Loss 0.0210 (0.0419)	Prec@(1,5) (98.8%, 100.0%)	
05/28 07:26:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.0792 (0.0415)	Prec@(1,5) (98.8%, 100.0%)	
05/28 07:26:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][250/390]	Step 17454	lr 0.00184	Loss 0.0100 (0.0398)	Prec@(1,5) (98.9%, 100.0%)	
05/28 07:27:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.1050 (0.0398)	Prec@(1,5) (98.9%, 100.0%)	
05/28 07:27:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][350/390]	Step 17554	lr 0.00184	Loss 0.0234 (0.0398)	Prec@(1,5) (98.9%, 100.0%)	
05/28 07:27:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.0069 (0.0406)	Prec@(1,5) (98.8%, 100.0%)	
05/28 07:27:34PM searchShareStage_trainer.py:134 [INFO] Train: [ 44/49] Final Prec@1 98.8480%
05/28 07:27:38PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][50/391]	Step 17595	Loss 0.4603	Prec@(1,5) (87.7%, 99.5%)
05/28 07:27:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 0.4825	Prec@(1,5) (87.6%, 99.2%)
05/28 07:27:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][150/391]	Step 17595	Loss 0.4874	Prec@(1,5) (87.5%, 99.2%)
05/28 07:27:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 0.4950	Prec@(1,5) (87.3%, 99.3%)
05/28 07:27:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][250/391]	Step 17595	Loss 0.5056	Prec@(1,5) (87.0%, 99.3%)
05/28 07:27:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 0.5076	Prec@(1,5) (87.0%, 99.3%)
05/28 07:27:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][350/391]	Step 17595	Loss 0.5015	Prec@(1,5) (87.1%, 99.3%)
05/28 07:27:58PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 0.5031	Prec@(1,5) (87.1%, 99.3%)
05/28 07:27:58PM searchStage_trainer.py:260 [INFO] Valid: [ 44/49] Final Prec@1 87.1200%
05/28 07:27:58PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:27:58PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.3240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2850, 0.2704, 0.2153],
        [0.2478, 0.2833, 0.2574, 0.2115],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2852, 0.2450, 0.2111],
        [0.2647, 0.2864, 0.2348, 0.2142],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2956, 0.2325, 0.2092],
        [0.2518, 0.2753, 0.2256, 0.2474],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2922, 0.2387, 0.2238],
        [0.2526, 0.2827, 0.2283, 0.2364],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2924, 0.2318, 0.2148],
        [0.2491, 0.2748, 0.2366, 0.2396],
        [0.2582, 0.2731, 0.2315, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:28:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][50/390]	Step 17645	lr 0.00159	Loss 0.0149 (0.0480)	Prec@(1,5) (98.4%, 100.0%)	
05/28 07:28:36PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.0094 (0.0419)	Prec@(1,5) (98.6%, 100.0%)	
05/28 07:28:55PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][150/390]	Step 17745	lr 0.00159	Loss 0.0278 (0.0401)	Prec@(1,5) (98.7%, 100.0%)	
05/28 07:29:14PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.0177 (0.0393)	Prec@(1,5) (98.8%, 100.0%)	
05/28 07:29:33PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][250/390]	Step 17845	lr 0.00159	Loss 0.0319 (0.0390)	Prec@(1,5) (98.8%, 100.0%)	
05/28 07:29:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.0337 (0.0388)	Prec@(1,5) (98.8%, 100.0%)	
05/28 07:30:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][350/390]	Step 17945	lr 0.00159	Loss 0.0199 (0.0385)	Prec@(1,5) (98.8%, 100.0%)	
05/28 07:30:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.0441 (0.0389)	Prec@(1,5) (98.8%, 100.0%)	
05/28 07:30:26PM searchShareStage_trainer.py:134 [INFO] Train: [ 45/49] Final Prec@1 98.8080%
05/28 07:30:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][50/391]	Step 17986	Loss 0.5086	Prec@(1,5) (87.1%, 99.3%)
05/28 07:30:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 0.5000	Prec@(1,5) (87.5%, 99.4%)
05/28 07:30:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][150/391]	Step 17986	Loss 0.4914	Prec@(1,5) (87.5%, 99.4%)
05/28 07:30:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 0.5044	Prec@(1,5) (87.3%, 99.4%)
05/28 07:30:42PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][250/391]	Step 17986	Loss 0.4896	Prec@(1,5) (87.6%, 99.4%)
05/28 07:30:45PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 0.4941	Prec@(1,5) (87.4%, 99.4%)
05/28 07:30:48PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][350/391]	Step 17986	Loss 0.4924	Prec@(1,5) (87.3%, 99.4%)
05/28 07:30:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 0.4937	Prec@(1,5) (87.3%, 99.4%)
05/28 07:30:50PM searchStage_trainer.py:260 [INFO] Valid: [ 45/49] Final Prec@1 87.3280%
05/28 07:30:50PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:30:51PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.3280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2850, 0.2704, 0.2153],
        [0.2478, 0.2833, 0.2574, 0.2115],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2852, 0.2450, 0.2111],
        [0.2647, 0.2864, 0.2348, 0.2142],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2956, 0.2326, 0.2092],
        [0.2517, 0.2752, 0.2256, 0.2474],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2922, 0.2388, 0.2238],
        [0.2526, 0.2826, 0.2283, 0.2364],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2924, 0.2319, 0.2148],
        [0.2491, 0.2747, 0.2366, 0.2396],
        [0.2582, 0.2731, 0.2315, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:31:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][50/390]	Step 18036	lr 0.00138	Loss 0.0297 (0.0292)	Prec@(1,5) (99.1%, 100.0%)	
05/28 07:31:29PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.0262 (0.0324)	Prec@(1,5) (98.9%, 100.0%)	
05/28 07:31:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][150/390]	Step 18136	lr 0.00138	Loss 0.0235 (0.0320)	Prec@(1,5) (98.9%, 100.0%)	
05/28 07:32:07PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.0278 (0.0318)	Prec@(1,5) (99.0%, 100.0%)	
05/28 07:32:25PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][250/390]	Step 18236	lr 0.00138	Loss 0.0077 (0.0314)	Prec@(1,5) (99.0%, 100.0%)	
05/28 07:32:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.0432 (0.0318)	Prec@(1,5) (99.0%, 100.0%)	
05/28 07:33:03PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][350/390]	Step 18336	lr 0.00138	Loss 0.0163 (0.0322)	Prec@(1,5) (99.0%, 100.0%)	
05/28 07:33:17PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.0195 (0.0317)	Prec@(1,5) (99.0%, 100.0%)	
05/28 07:33:18PM searchShareStage_trainer.py:134 [INFO] Train: [ 46/49] Final Prec@1 99.0360%
05/28 07:33:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][50/391]	Step 18377	Loss 0.4592	Prec@(1,5) (87.8%, 99.5%)
05/28 07:33:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 0.4927	Prec@(1,5) (87.7%, 99.4%)
05/28 07:33:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][150/391]	Step 18377	Loss 0.4831	Prec@(1,5) (87.7%, 99.4%)
05/28 07:33:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 0.4916	Prec@(1,5) (87.6%, 99.4%)
05/28 07:33:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][250/391]	Step 18377	Loss 0.4952	Prec@(1,5) (87.6%, 99.4%)
05/28 07:33:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 0.4984	Prec@(1,5) (87.6%, 99.4%)
05/28 07:33:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][350/391]	Step 18377	Loss 0.5045	Prec@(1,5) (87.5%, 99.3%)
05/28 07:33:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 0.5032	Prec@(1,5) (87.4%, 99.4%)
05/28 07:33:41PM searchStage_trainer.py:260 [INFO] Valid: [ 46/49] Final Prec@1 87.4320%
05/28 07:33:41PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:33:41PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.4320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2850, 0.2704, 0.2153],
        [0.2478, 0.2833, 0.2574, 0.2115],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2852, 0.2450, 0.2111],
        [0.2647, 0.2864, 0.2348, 0.2142],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2955, 0.2326, 0.2092],
        [0.2517, 0.2752, 0.2256, 0.2474],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2921, 0.2388, 0.2238],
        [0.2526, 0.2826, 0.2283, 0.2364],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2924, 0.2319, 0.2149],
        [0.2491, 0.2747, 0.2366, 0.2396],
        [0.2582, 0.2731, 0.2315, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:34:01PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][50/390]	Step 18427	lr 0.00121	Loss 0.0177 (0.0331)	Prec@(1,5) (99.1%, 100.0%)	
05/28 07:34:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.0594 (0.0328)	Prec@(1,5) (99.1%, 100.0%)	
05/28 07:34:39PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][150/390]	Step 18527	lr 0.00121	Loss 0.0785 (0.0321)	Prec@(1,5) (99.1%, 100.0%)	
05/28 07:34:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.0516 (0.0324)	Prec@(1,5) (99.1%, 100.0%)	
05/28 07:35:16PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][250/390]	Step 18627	lr 0.00121	Loss 0.0499 (0.0312)	Prec@(1,5) (99.1%, 100.0%)	
05/28 07:35:35PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.0447 (0.0313)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:35:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][350/390]	Step 18727	lr 0.00121	Loss 0.0893 (0.0306)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:36:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.0204 (0.0311)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:36:09PM searchShareStage_trainer.py:134 [INFO] Train: [ 47/49] Final Prec@1 99.1760%
05/28 07:36:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][50/391]	Step 18768	Loss 0.4745	Prec@(1,5) (87.9%, 99.6%)
05/28 07:36:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 0.4910	Prec@(1,5) (87.6%, 99.5%)
05/28 07:36:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][150/391]	Step 18768	Loss 0.4933	Prec@(1,5) (87.7%, 99.4%)
05/28 07:36:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 0.5079	Prec@(1,5) (87.5%, 99.4%)
05/28 07:36:24PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][250/391]	Step 18768	Loss 0.5047	Prec@(1,5) (87.4%, 99.4%)
05/28 07:36:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 0.5071	Prec@(1,5) (87.3%, 99.4%)
05/28 07:36:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][350/391]	Step 18768	Loss 0.5049	Prec@(1,5) (87.5%, 99.4%)
05/28 07:36:32PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 0.5075	Prec@(1,5) (87.5%, 99.4%)
05/28 07:36:32PM searchStage_trainer.py:260 [INFO] Valid: [ 47/49] Final Prec@1 87.4600%
05/28 07:36:33PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:36:33PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.4600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2722, 0.2473, 0.2360],
        [0.2325, 0.3018, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2850, 0.2704, 0.2153],
        [0.2478, 0.2833, 0.2574, 0.2115],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2852, 0.2450, 0.2111],
        [0.2647, 0.2864, 0.2348, 0.2142],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2955, 0.2326, 0.2092],
        [0.2517, 0.2752, 0.2256, 0.2474],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2921, 0.2388, 0.2238],
        [0.2526, 0.2826, 0.2283, 0.2364],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2923, 0.2319, 0.2149],
        [0.2491, 0.2747, 0.2366, 0.2396],
        [0.2582, 0.2731, 0.2315, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:36:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][50/390]	Step 18818	lr 0.00109	Loss 0.0261 (0.0292)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:37:12PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.0241 (0.0270)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:37:30PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][150/390]	Step 18918	lr 0.00109	Loss 0.0145 (0.0269)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:37:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.1069 (0.0280)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:38:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][250/390]	Step 19018	lr 0.00109	Loss 0.0149 (0.0277)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:38:27PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.0447 (0.0274)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:38:45PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][350/390]	Step 19118	lr 0.00109	Loss 0.0364 (0.0270)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:39:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.0505 (0.0275)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:39:01PM searchShareStage_trainer.py:134 [INFO] Train: [ 48/49] Final Prec@1 99.1920%
05/28 07:39:04PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][50/391]	Step 19159	Loss 0.5140	Prec@(1,5) (87.0%, 99.3%)
05/28 07:39:07PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 0.5062	Prec@(1,5) (87.3%, 99.3%)
05/28 07:39:10PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][150/391]	Step 19159	Loss 0.5158	Prec@(1,5) (87.1%, 99.3%)
05/28 07:39:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 0.5150	Prec@(1,5) (87.2%, 99.3%)
05/28 07:39:15PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][250/391]	Step 19159	Loss 0.5228	Prec@(1,5) (87.2%, 99.2%)
05/28 07:39:18PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 0.5110	Prec@(1,5) (87.3%, 99.3%)
05/28 07:39:21PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][350/391]	Step 19159	Loss 0.5071	Prec@(1,5) (87.4%, 99.3%)
05/28 07:39:23PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 0.4997	Prec@(1,5) (87.5%, 99.3%)
05/28 07:39:23PM searchStage_trainer.py:260 [INFO] Valid: [ 48/49] Final Prec@1 87.4760%
05/28 07:39:23PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:39:24PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.4760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2721, 0.2474, 0.2361],
        [0.2325, 0.3017, 0.2516, 0.2141]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2850, 0.2704, 0.2153],
        [0.2478, 0.2833, 0.2574, 0.2115],
        [0.2576, 0.2958, 0.2207, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2586, 0.2852, 0.2451, 0.2111],
        [0.2647, 0.2864, 0.2348, 0.2142],
        [0.2585, 0.2812, 0.2187, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2626, 0.2955, 0.2326, 0.2093],
        [0.2517, 0.2752, 0.2256, 0.2474],
        [0.2558, 0.2825, 0.2316, 0.2301]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2920, 0.2388, 0.2239],
        [0.2526, 0.2826, 0.2283, 0.2364],
        [0.2543, 0.2820, 0.2415, 0.2222]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2609, 0.2923, 0.2319, 0.2149],
        [0.2491, 0.2747, 0.2366, 0.2396],
        [0.2582, 0.2731, 0.2315, 0.2373]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 07:39:44PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][50/390]	Step 19209	lr 0.00102	Loss 0.0158 (0.0315)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:40:03PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.0168 (0.0291)	Prec@(1,5) (99.3%, 100.0%)	
05/28 07:40:22PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][150/390]	Step 19309	lr 0.00102	Loss 0.0056 (0.0281)	Prec@(1,5) (99.3%, 100.0%)	
05/28 07:40:41PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.0626 (0.0271)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:41:00PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][250/390]	Step 19409	lr 0.00102	Loss 0.0227 (0.0271)	Prec@(1,5) (99.3%, 100.0%)	
05/28 07:41:19PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.0496 (0.0266)	Prec@(1,5) (99.3%, 100.0%)	
05/28 07:41:38PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][350/390]	Step 19509	lr 0.00102	Loss 0.0662 (0.0273)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:41:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.0126 (0.0277)	Prec@(1,5) (99.2%, 100.0%)	
05/28 07:41:53PM searchShareStage_trainer.py:134 [INFO] Train: [ 49/49] Final Prec@1 99.2000%
05/28 07:41:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][50/391]	Step 19550	Loss 0.4708	Prec@(1,5) (88.6%, 99.2%)
05/28 07:41:59PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 0.5115	Prec@(1,5) (87.6%, 99.1%)
05/28 07:42:02PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][150/391]	Step 19550	Loss 0.4997	Prec@(1,5) (87.6%, 99.3%)
05/28 07:42:05PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 0.5006	Prec@(1,5) (87.5%, 99.3%)
05/28 07:42:08PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][250/391]	Step 19550	Loss 0.4961	Prec@(1,5) (87.7%, 99.3%)
05/28 07:42:11PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 0.5038	Prec@(1,5) (87.5%, 99.3%)
05/28 07:42:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][350/391]	Step 19550	Loss 0.5084	Prec@(1,5) (87.4%, 99.3%)
05/28 07:42:16PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 0.5107	Prec@(1,5) (87.4%, 99.3%)
05/28 07:42:17PM searchStage_trainer.py:260 [INFO] Valid: [ 49/49] Final Prec@1 87.3880%
05/28 07:42:17PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 07:42:17PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 87.4760%
05/28 07:42:17PM searchStage_main.py:73 [INFO] Final best Prec@1 = 87.4760%
05/28 07:42:17PM searchStage_main.py:74 [INFO] Final Best Genotype = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
