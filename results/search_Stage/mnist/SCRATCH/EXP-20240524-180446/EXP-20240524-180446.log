05/24 06:04:46PM parser.py:28 [INFO] 
05/24 06:04:46PM parser.py:29 [INFO] Parameters:
05/24 06:04:46PM parser.py:31 [INFO] DAG_PATH=results/search_Stage/mnist/SCRATCH/EXP-20240524-180446/DAG
05/24 06:04:46PM parser.py:31 [INFO] ALPHA_LR=0.0003
05/24 06:04:46PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
05/24 06:04:46PM parser.py:31 [INFO] BATCH_SIZE=64
05/24 06:04:46PM parser.py:31 [INFO] DATA_PATH=../data/
05/24 06:04:46PM parser.py:31 [INFO] DATASET=mnist
05/24 06:04:46PM parser.py:31 [INFO] EPOCHS=50
05/24 06:04:46PM parser.py:31 [INFO] EXP_NAME=EXP-20240524-180446
05/24 06:04:46PM parser.py:31 [INFO] GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('skip_connect', 2)]], normal_concat=[2, 3, 4, 5], reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce_concat=[2, 3, 4, 5])
05/24 06:04:46PM parser.py:31 [INFO] GPUS=[0]
05/24 06:04:46PM parser.py:31 [INFO] INIT_CHANNELS=16
05/24 06:04:46PM parser.py:31 [INFO] LAYERS=20
05/24 06:04:46PM parser.py:31 [INFO] LOCAL_RANK=0
05/24 06:04:46PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/24 06:04:46PM parser.py:31 [INFO] NAME=SCRATCH
05/24 06:04:46PM parser.py:31 [INFO] PATH=results/search_Stage/mnist/SCRATCH/EXP-20240524-180446
05/24 06:04:46PM parser.py:31 [INFO] PRINT_FREQ=50
05/24 06:04:46PM parser.py:31 [INFO] RESUME_PATH=None
05/24 06:04:46PM parser.py:31 [INFO] SAVE=EXP
05/24 06:04:46PM parser.py:31 [INFO] SEED=2
05/24 06:04:46PM parser.py:31 [INFO] SHARE_STAGE=True
05/24 06:04:46PM parser.py:31 [INFO] TRAIN_PORTION=0.5
05/24 06:04:46PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
05/24 06:04:46PM parser.py:31 [INFO] W_LR=0.025
05/24 06:04:46PM parser.py:31 [INFO] W_LR_MIN=0.001
05/24 06:04:46PM parser.py:31 [INFO] W_MOMENTUM=0.9
05/24 06:04:46PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
05/24 06:04:46PM parser.py:31 [INFO] WORKERS=4
05/24 06:04:46PM parser.py:32 [INFO] 
05/24 06:04:48PM searchShareStage_trainer.py:102 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2502, 0.2499, 0.2497],
        [0.2500, 0.2502, 0.2498, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2501, 0.2498, 0.2500],
        [0.2504, 0.2497, 0.2500, 0.2499],
        [0.2499, 0.2501, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2498, 0.2498, 0.2501],
        [0.2502, 0.2500, 0.2499, 0.2499],
        [0.2505, 0.2499, 0.2499, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2496, 0.2506],
        [0.2497, 0.2503, 0.2498, 0.2502],
        [0.2499, 0.2504, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2502, 0.2500, 0.2500],
        [0.2498, 0.2498, 0.2503, 0.2501],
        [0.2504, 0.2500, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2500, 0.2497, 0.2501],
        [0.2500, 0.2500, 0.2501, 0.2499],
        [0.2499, 0.2498, 0.2499, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:05:11PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [0][50/468]	Step 50	lr 0.025	Loss 0.5083 (1.4726)	Prec@(1,5) (47.7%, 82.7%)	
05/24 06:05:33PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [0][100/468]	Step 100	lr 0.025	Loss 0.1257 (0.9711)	Prec@(1,5) (67.4%, 90.9%)	
05/24 06:05:56PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [0][150/468]	Step 150	lr 0.025	Loss 0.2012 (0.7427)	Prec@(1,5) (75.4%, 93.7%)	
05/24 06:06:18PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [0][200/468]	Step 200	lr 0.025	Loss 0.3767 (0.6193)	Prec@(1,5) (79.7%, 95.2%)	
05/24 06:06:41PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [0][250/468]	Step 250	lr 0.025	Loss 0.1368 (0.5314)	Prec@(1,5) (82.8%, 96.1%)	
05/24 06:07:03PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [0][300/468]	Step 300	lr 0.025	Loss 0.0562 (0.4727)	Prec@(1,5) (84.8%, 96.7%)	
05/24 06:07:26PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [0][350/468]	Step 350	lr 0.025	Loss 0.2346 (0.4224)	Prec@(1,5) (86.5%, 97.2%)	
05/24 06:07:48PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [0][400/468]	Step 400	lr 0.025	Loss 0.1108 (0.3868)	Prec@(1,5) (87.7%, 97.5%)	
05/24 06:08:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [0][450/468]	Step 450	lr 0.025	Loss 0.0228 (0.3591)	Prec@(1,5) (88.6%, 97.8%)	
05/24 06:08:18PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [0][468/468]	Step 468	lr 0.025	Loss 0.1173 (0.3497)	Prec@(1,5) (88.9%, 97.9%)	
05/24 06:08:19PM searchShareStage_trainer.py:212 [INFO] Train: [  0/49] Final Prec@1 88.9167%
05/24 06:08:22PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [0][50/469]	Step 469	Loss 0.1381	Prec@(1,5) (95.4%, 99.9%)
05/24 06:08:25PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [0][100/469]	Step 469	Loss 0.1514	Prec@(1,5) (95.4%, 99.8%)
05/24 06:08:29PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [0][150/469]	Step 469	Loss 0.1525	Prec@(1,5) (95.4%, 99.7%)
05/24 06:08:33PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [0][200/469]	Step 469	Loss 0.1585	Prec@(1,5) (95.3%, 99.7%)
05/24 06:08:36PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [0][250/469]	Step 469	Loss 0.1559	Prec@(1,5) (95.5%, 99.7%)
05/24 06:08:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [0][300/469]	Step 469	Loss 0.1569	Prec@(1,5) (95.5%, 99.7%)
05/24 06:08:43PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [0][350/469]	Step 469	Loss 0.1568	Prec@(1,5) (95.5%, 99.7%)
05/24 06:08:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [0][400/469]	Step 469	Loss 0.1556	Prec@(1,5) (95.5%, 99.7%)
05/24 06:08:50PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [0][450/469]	Step 469	Loss 0.1563	Prec@(1,5) (95.5%, 99.7%)
05/24 06:08:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [0][468/469]	Step 469	Loss 0.1563	Prec@(1,5) (95.5%, 99.7%)
05/24 06:08:52PM searchShareStage_trainer.py:251 [INFO] Valid: [  0/49] Final Prec@1 95.5000%
05/24 06:08:52PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 4)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 4)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 4)]], DAG3_concat=range(6, 8))
05/24 06:08:52PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 95.5000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2468, 0.2546, 0.2526, 0.2460],
        [0.2454, 0.2501, 0.2498, 0.2547]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2556, 0.2531, 0.2448],
        [0.2413, 0.2591, 0.2564, 0.2432],
        [0.2500, 0.2632, 0.2370, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2530, 0.2473, 0.2513],
        [0.2514, 0.2627, 0.2456, 0.2402],
        [0.2539, 0.2598, 0.2430, 0.2433]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2583, 0.2459, 0.2459],
        [0.2507, 0.2578, 0.2424, 0.2490],
        [0.2499, 0.2600, 0.2479, 0.2422]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2576, 0.2464, 0.2459],
        [0.2516, 0.2616, 0.2460, 0.2408],
        [0.2501, 0.2581, 0.2431, 0.2486]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2579, 0.2466, 0.2460],
        [0.2508, 0.2569, 0.2449, 0.2474],
        [0.2493, 0.2587, 0.2488, 0.2432]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:09:15PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [1][50/468]	Step 519	lr 0.02498	Loss 0.1292 (0.1124)	Prec@(1,5) (96.5%, 99.9%)	
05/24 06:09:38PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [1][100/468]	Step 569	lr 0.02498	Loss 0.0226 (0.1089)	Prec@(1,5) (96.6%, 99.9%)	
05/24 06:10:00PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [1][150/468]	Step 619	lr 0.02498	Loss 0.0324 (0.0969)	Prec@(1,5) (97.0%, 99.9%)	
05/24 06:10:23PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [1][200/468]	Step 669	lr 0.02498	Loss 0.0465 (0.0978)	Prec@(1,5) (97.1%, 99.9%)	
05/24 06:10:45PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [1][250/468]	Step 719	lr 0.02498	Loss 0.1355 (0.0948)	Prec@(1,5) (97.1%, 99.9%)	
05/24 06:11:08PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [1][300/468]	Step 769	lr 0.02498	Loss 0.1892 (0.0940)	Prec@(1,5) (97.2%, 99.9%)	
05/24 06:11:30PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [1][350/468]	Step 819	lr 0.02498	Loss 0.1711 (0.0910)	Prec@(1,5) (97.3%, 99.9%)	
05/24 06:11:52PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [1][400/468]	Step 869	lr 0.02498	Loss 0.0231 (0.0928)	Prec@(1,5) (97.3%, 99.9%)	
05/24 06:12:15PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [1][450/468]	Step 919	lr 0.02498	Loss 0.0716 (0.0931)	Prec@(1,5) (97.2%, 99.9%)	
05/24 06:12:23PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [1][468/468]	Step 937	lr 0.02498	Loss 0.0489 (0.0924)	Prec@(1,5) (97.3%, 99.9%)	
05/24 06:12:23PM searchShareStage_trainer.py:212 [INFO] Train: [  1/49] Final Prec@1 97.2600%
05/24 06:12:26PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [1][50/469]	Step 938	Loss 0.0856	Prec@(1,5) (97.5%, 100.0%)
05/24 06:12:30PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [1][100/469]	Step 938	Loss 0.0819	Prec@(1,5) (97.5%, 100.0%)
05/24 06:12:33PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [1][150/469]	Step 938	Loss 0.0768	Prec@(1,5) (97.6%, 100.0%)
05/24 06:12:37PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [1][200/469]	Step 938	Loss 0.0753	Prec@(1,5) (97.7%, 100.0%)
05/24 06:12:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [1][250/469]	Step 938	Loss 0.0760	Prec@(1,5) (97.7%, 100.0%)
05/24 06:12:44PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [1][300/469]	Step 938	Loss 0.0766	Prec@(1,5) (97.7%, 100.0%)
05/24 06:12:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [1][350/469]	Step 938	Loss 0.0788	Prec@(1,5) (97.7%, 100.0%)
05/24 06:12:51PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [1][400/469]	Step 938	Loss 0.0801	Prec@(1,5) (97.6%, 100.0%)
05/24 06:12:55PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [1][450/469]	Step 938	Loss 0.0795	Prec@(1,5) (97.6%, 100.0%)
05/24 06:12:56PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [1][468/469]	Step 938	Loss 0.0780	Prec@(1,5) (97.7%, 100.0%)
05/24 06:12:56PM searchShareStage_trainer.py:251 [INFO] Valid: [  1/49] Final Prec@1 97.6633%
05/24 06:12:56PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:12:56PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 97.6633%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2407, 0.2605, 0.2568, 0.2419],
        [0.2409, 0.2521, 0.2467, 0.2602]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2408, 0.2632, 0.2571, 0.2389],
        [0.2340, 0.2649, 0.2593, 0.2419],
        [0.2483, 0.2776, 0.2297, 0.2444]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2585, 0.2458, 0.2482],
        [0.2508, 0.2761, 0.2415, 0.2316],
        [0.2559, 0.2708, 0.2342, 0.2390]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2667, 0.2408, 0.2422],
        [0.2511, 0.2670, 0.2373, 0.2446],
        [0.2496, 0.2714, 0.2432, 0.2358]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2679, 0.2432, 0.2394],
        [0.2504, 0.2722, 0.2454, 0.2321],
        [0.2507, 0.2671, 0.2341, 0.2481]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2649, 0.2419, 0.2420],
        [0.2521, 0.2646, 0.2395, 0.2438],
        [0.2501, 0.2682, 0.2455, 0.2363]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:13:20PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [2][50/468]	Step 988	lr 0.02491	Loss 0.0926 (0.0751)	Prec@(1,5) (97.8%, 100.0%)	
05/24 06:13:42PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [2][100/468]	Step 1038	lr 0.02491	Loss 0.0630 (0.0742)	Prec@(1,5) (97.9%, 100.0%)	
05/24 06:14:04PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [2][150/468]	Step 1088	lr 0.02491	Loss 0.1147 (0.0688)	Prec@(1,5) (98.0%, 100.0%)	
05/24 06:14:27PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [2][200/468]	Step 1138	lr 0.02491	Loss 0.0466 (0.0716)	Prec@(1,5) (97.8%, 100.0%)	
05/24 06:14:49PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [2][250/468]	Step 1188	lr 0.02491	Loss 0.0833 (0.0748)	Prec@(1,5) (97.7%, 100.0%)	
05/24 06:15:11PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [2][300/468]	Step 1238	lr 0.02491	Loss 0.1079 (0.0740)	Prec@(1,5) (97.8%, 100.0%)	
05/24 06:15:34PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [2][350/468]	Step 1288	lr 0.02491	Loss 0.0852 (0.0717)	Prec@(1,5) (97.8%, 100.0%)	
05/24 06:15:56PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [2][400/468]	Step 1338	lr 0.02491	Loss 0.0365 (0.0721)	Prec@(1,5) (97.8%, 100.0%)	
05/24 06:16:19PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [2][450/468]	Step 1388	lr 0.02491	Loss 0.0764 (0.0722)	Prec@(1,5) (97.8%, 100.0%)	
05/24 06:16:27PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [2][468/468]	Step 1406	lr 0.02491	Loss 0.0134 (0.0719)	Prec@(1,5) (97.8%, 100.0%)	
05/24 06:16:27PM searchShareStage_trainer.py:212 [INFO] Train: [  2/49] Final Prec@1 97.7700%
05/24 06:16:30PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [2][50/469]	Step 1407	Loss 0.0855	Prec@(1,5) (97.2%, 99.9%)
05/24 06:16:34PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [2][100/469]	Step 1407	Loss 0.0832	Prec@(1,5) (97.3%, 100.0%)
05/24 06:16:37PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [2][150/469]	Step 1407	Loss 0.0808	Prec@(1,5) (97.4%, 99.9%)
05/24 06:16:41PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [2][200/469]	Step 1407	Loss 0.0825	Prec@(1,5) (97.5%, 99.9%)
05/24 06:16:44PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [2][250/469]	Step 1407	Loss 0.0793	Prec@(1,5) (97.6%, 99.9%)
05/24 06:16:48PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [2][300/469]	Step 1407	Loss 0.0773	Prec@(1,5) (97.6%, 99.9%)
05/24 06:16:51PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [2][350/469]	Step 1407	Loss 0.0770	Prec@(1,5) (97.6%, 99.9%)
05/24 06:16:55PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [2][400/469]	Step 1407	Loss 0.0762	Prec@(1,5) (97.7%, 99.9%)
05/24 06:16:58PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [2][450/469]	Step 1407	Loss 0.0758	Prec@(1,5) (97.7%, 99.9%)
05/24 06:17:00PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [2][468/469]	Step 1407	Loss 0.0764	Prec@(1,5) (97.7%, 99.9%)
05/24 06:17:00PM searchShareStage_trainer.py:251 [INFO] Valid: [  2/49] Final Prec@1 97.7167%
05/24 06:17:00PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:17:00PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 97.7167%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2356, 0.2691, 0.2567, 0.2385],
        [0.2377, 0.2573, 0.2430, 0.2620]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2347, 0.2689, 0.2605, 0.2360],
        [0.2259, 0.2726, 0.2639, 0.2376],
        [0.2468, 0.2931, 0.2219, 0.2383]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2608, 0.2445, 0.2471],
        [0.2512, 0.2874, 0.2359, 0.2255],
        [0.2572, 0.2827, 0.2295, 0.2305]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2761, 0.2361, 0.2382],
        [0.2504, 0.2769, 0.2334, 0.2393],
        [0.2496, 0.2806, 0.2384, 0.2314]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2769, 0.2375, 0.2351],
        [0.2523, 0.2824, 0.2408, 0.2245],
        [0.2518, 0.2758, 0.2284, 0.2441]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2735, 0.2398, 0.2360],
        [0.2527, 0.2713, 0.2348, 0.2412],
        [0.2503, 0.2769, 0.2413, 0.2316]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:17:23PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [3][50/468]	Step 1457	lr 0.02479	Loss 0.1151 (0.0651)	Prec@(1,5) (97.8%, 100.0%)	
05/24 06:17:46PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [3][100/468]	Step 1507	lr 0.02479	Loss 0.1856 (0.0662)	Prec@(1,5) (98.0%, 100.0%)	
05/24 06:18:08PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [3][150/468]	Step 1557	lr 0.02479	Loss 0.1191 (0.0652)	Prec@(1,5) (98.0%, 100.0%)	
05/24 06:18:31PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [3][200/468]	Step 1607	lr 0.02479	Loss 0.0751 (0.0609)	Prec@(1,5) (98.1%, 100.0%)	
05/24 06:18:53PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [3][250/468]	Step 1657	lr 0.02479	Loss 0.0389 (0.0605)	Prec@(1,5) (98.2%, 100.0%)	
05/24 06:19:15PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [3][300/468]	Step 1707	lr 0.02479	Loss 0.0853 (0.0614)	Prec@(1,5) (98.1%, 100.0%)	
05/24 06:19:38PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [3][350/468]	Step 1757	lr 0.02479	Loss 0.1092 (0.0604)	Prec@(1,5) (98.1%, 100.0%)	
05/24 06:20:00PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [3][400/468]	Step 1807	lr 0.02479	Loss 0.0361 (0.0595)	Prec@(1,5) (98.2%, 100.0%)	
05/24 06:20:23PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [3][450/468]	Step 1857	lr 0.02479	Loss 0.0479 (0.0596)	Prec@(1,5) (98.2%, 100.0%)	
05/24 06:20:31PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [3][468/468]	Step 1875	lr 0.02479	Loss 0.0328 (0.0592)	Prec@(1,5) (98.2%, 100.0%)	
05/24 06:20:31PM searchShareStage_trainer.py:212 [INFO] Train: [  3/49] Final Prec@1 98.1767%
05/24 06:20:34PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [3][50/469]	Step 1876	Loss 0.0567	Prec@(1,5) (98.1%, 100.0%)
05/24 06:20:38PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [3][100/469]	Step 1876	Loss 0.0608	Prec@(1,5) (98.0%, 100.0%)
05/24 06:20:41PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [3][150/469]	Step 1876	Loss 0.0606	Prec@(1,5) (98.1%, 100.0%)
05/24 06:20:45PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [3][200/469]	Step 1876	Loss 0.0618	Prec@(1,5) (98.0%, 100.0%)
05/24 06:20:48PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [3][250/469]	Step 1876	Loss 0.0632	Prec@(1,5) (98.1%, 99.9%)
05/24 06:20:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [3][300/469]	Step 1876	Loss 0.0624	Prec@(1,5) (98.1%, 100.0%)
05/24 06:20:55PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [3][350/469]	Step 1876	Loss 0.0625	Prec@(1,5) (98.1%, 100.0%)
05/24 06:20:59PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [3][400/469]	Step 1876	Loss 0.0619	Prec@(1,5) (98.1%, 100.0%)
05/24 06:21:03PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [3][450/469]	Step 1876	Loss 0.0623	Prec@(1,5) (98.1%, 100.0%)
05/24 06:21:04PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [3][468/469]	Step 1876	Loss 0.0624	Prec@(1,5) (98.1%, 100.0%)
05/24 06:21:04PM searchShareStage_trainer.py:251 [INFO] Valid: [  3/49] Final Prec@1 98.1300%
05/24 06:21:04PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:21:04PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.1300%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2310, 0.2805, 0.2554, 0.2331],
        [0.2374, 0.2599, 0.2383, 0.2644]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2303, 0.2780, 0.2614, 0.2303],
        [0.2186, 0.2814, 0.2663, 0.2336],
        [0.2445, 0.3072, 0.2151, 0.2332]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2621, 0.2463, 0.2438],
        [0.2533, 0.2979, 0.2282, 0.2207],
        [0.2602, 0.2922, 0.2239, 0.2237]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2843, 0.2321, 0.2350],
        [0.2495, 0.2848, 0.2300, 0.2357],
        [0.2489, 0.2883, 0.2361, 0.2267]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2856, 0.2323, 0.2317],
        [0.2550, 0.2913, 0.2364, 0.2173],
        [0.2520, 0.2832, 0.2232, 0.2416]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2812, 0.2373, 0.2311],
        [0.2524, 0.2783, 0.2323, 0.2370],
        [0.2507, 0.2846, 0.2356, 0.2291]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:21:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [4][50/468]	Step 1926	lr 0.02462	Loss 0.0312 (0.0459)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:21:50PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [4][100/468]	Step 1976	lr 0.02462	Loss 0.0033 (0.0458)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:22:12PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [4][150/468]	Step 2026	lr 0.02462	Loss 0.0026 (0.0480)	Prec@(1,5) (98.6%, 100.0%)	
05/24 06:22:35PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [4][200/468]	Step 2076	lr 0.02462	Loss 0.0810 (0.0494)	Prec@(1,5) (98.6%, 100.0%)	
05/24 06:22:57PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [4][250/468]	Step 2126	lr 0.02462	Loss 0.0287 (0.0513)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:23:20PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [4][300/468]	Step 2176	lr 0.02462	Loss 0.0331 (0.0510)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:23:42PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [4][350/468]	Step 2226	lr 0.02462	Loss 0.3119 (0.0519)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:24:04PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [4][400/468]	Step 2276	lr 0.02462	Loss 0.0313 (0.0537)	Prec@(1,5) (98.4%, 100.0%)	
05/24 06:24:27PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [4][450/468]	Step 2326	lr 0.02462	Loss 0.0087 (0.0532)	Prec@(1,5) (98.4%, 100.0%)	
05/24 06:24:35PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [4][468/468]	Step 2344	lr 0.02462	Loss 0.0022 (0.0523)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:24:35PM searchShareStage_trainer.py:212 [INFO] Train: [  4/49] Final Prec@1 98.4600%
05/24 06:24:38PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [4][50/469]	Step 2345	Loss 0.0452	Prec@(1,5) (98.6%, 100.0%)
05/24 06:24:42PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [4][100/469]	Step 2345	Loss 0.0561	Prec@(1,5) (98.4%, 100.0%)
05/24 06:24:45PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [4][150/469]	Step 2345	Loss 0.0568	Prec@(1,5) (98.4%, 100.0%)
05/24 06:24:49PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [4][200/469]	Step 2345	Loss 0.0561	Prec@(1,5) (98.4%, 100.0%)
05/24 06:24:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [4][250/469]	Step 2345	Loss 0.0559	Prec@(1,5) (98.4%, 100.0%)
05/24 06:24:56PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [4][300/469]	Step 2345	Loss 0.0582	Prec@(1,5) (98.3%, 100.0%)
05/24 06:24:59PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [4][350/469]	Step 2345	Loss 0.0584	Prec@(1,5) (98.3%, 100.0%)
05/24 06:25:03PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [4][400/469]	Step 2345	Loss 0.0583	Prec@(1,5) (98.3%, 100.0%)
05/24 06:25:06PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [4][450/469]	Step 2345	Loss 0.0573	Prec@(1,5) (98.3%, 100.0%)
05/24 06:25:08PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [4][468/469]	Step 2345	Loss 0.0571	Prec@(1,5) (98.3%, 100.0%)
05/24 06:25:08PM searchShareStage_trainer.py:251 [INFO] Valid: [  4/49] Final Prec@1 98.3167%
05/24 06:25:08PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:25:08PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.3167%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2267, 0.2903, 0.2545, 0.2285],
        [0.2332, 0.2658, 0.2369, 0.2641]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2294, 0.2827, 0.2606, 0.2273],
        [0.2151, 0.2867, 0.2678, 0.2304],
        [0.2448, 0.3172, 0.2104, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2686, 0.2469, 0.2382],
        [0.2550, 0.3051, 0.2233, 0.2165],
        [0.2616, 0.2981, 0.2194, 0.2209]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2898, 0.2306, 0.2319],
        [0.2496, 0.2899, 0.2268, 0.2336],
        [0.2496, 0.2940, 0.2329, 0.2235]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2939, 0.2303, 0.2253],
        [0.2561, 0.2981, 0.2323, 0.2134],
        [0.2521, 0.2885, 0.2190, 0.2405]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2868, 0.2337, 0.2285],
        [0.2524, 0.2830, 0.2307, 0.2340],
        [0.2508, 0.2910, 0.2326, 0.2255]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:25:31PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [5][50/468]	Step 2395	lr 0.02441	Loss 0.0483 (0.0382)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:25:54PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [5][100/468]	Step 2445	lr 0.02441	Loss 0.0647 (0.0517)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:26:16PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [5][150/468]	Step 2495	lr 0.02441	Loss 0.0481 (0.0495)	Prec@(1,5) (98.6%, 100.0%)	
05/24 06:26:39PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [5][200/468]	Step 2545	lr 0.02441	Loss 0.0046 (0.0502)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:27:01PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [5][250/468]	Step 2595	lr 0.02441	Loss 0.0115 (0.0491)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:27:23PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [5][300/468]	Step 2645	lr 0.02441	Loss 0.0137 (0.0508)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:27:46PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [5][350/468]	Step 2695	lr 0.02441	Loss 0.0064 (0.0495)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:28:08PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [5][400/468]	Step 2745	lr 0.02441	Loss 0.0815 (0.0501)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:28:31PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [5][450/468]	Step 2795	lr 0.02441	Loss 0.0427 (0.0500)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:28:39PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [5][468/468]	Step 2813	lr 0.02441	Loss 0.0503 (0.0497)	Prec@(1,5) (98.5%, 100.0%)	
05/24 06:28:39PM searchShareStage_trainer.py:212 [INFO] Train: [  5/49] Final Prec@1 98.4833%
05/24 06:28:42PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [5][50/469]	Step 2814	Loss 0.0798	Prec@(1,5) (97.4%, 99.9%)
05/24 06:28:46PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [5][100/469]	Step 2814	Loss 0.0836	Prec@(1,5) (97.3%, 99.9%)
05/24 06:28:49PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [5][150/469]	Step 2814	Loss 0.0864	Prec@(1,5) (97.3%, 99.9%)
05/24 06:28:53PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [5][200/469]	Step 2814	Loss 0.0893	Prec@(1,5) (97.4%, 99.9%)
05/24 06:28:56PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [5][250/469]	Step 2814	Loss 0.0877	Prec@(1,5) (97.4%, 99.9%)
05/24 06:29:00PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [5][300/469]	Step 2814	Loss 0.0876	Prec@(1,5) (97.4%, 99.9%)
05/24 06:29:03PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [5][350/469]	Step 2814	Loss 0.0896	Prec@(1,5) (97.4%, 99.9%)
05/24 06:29:07PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [5][400/469]	Step 2814	Loss 0.0888	Prec@(1,5) (97.4%, 99.9%)
05/24 06:29:11PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [5][450/469]	Step 2814	Loss 0.0882	Prec@(1,5) (97.4%, 99.9%)
05/24 06:29:12PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [5][468/469]	Step 2814	Loss 0.0871	Prec@(1,5) (97.4%, 99.9%)
05/24 06:29:12PM searchShareStage_trainer.py:251 [INFO] Valid: [  5/49] Final Prec@1 97.4200%
05/24 06:29:12PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:29:12PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.3167%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2237, 0.2982, 0.2553, 0.2228],
        [0.2331, 0.2687, 0.2309, 0.2672]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2243, 0.2921, 0.2630, 0.2206],
        [0.2111, 0.2924, 0.2677, 0.2287],
        [0.2438, 0.3243, 0.2060, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2717, 0.2476, 0.2352],
        [0.2555, 0.3111, 0.2199, 0.2135],
        [0.2618, 0.3023, 0.2169, 0.2190]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2945, 0.2277, 0.2295],
        [0.2497, 0.2928, 0.2242, 0.2333],
        [0.2494, 0.2975, 0.2321, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2997, 0.2272, 0.2223],
        [0.2556, 0.3036, 0.2314, 0.2094],
        [0.2509, 0.2920, 0.2171, 0.2400]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2915, 0.2310, 0.2258],
        [0.2525, 0.2864, 0.2294, 0.2317],
        [0.2515, 0.2947, 0.2301, 0.2237]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:29:35PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [6][50/468]	Step 2864	lr 0.02416	Loss 0.0020 (0.0415)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:29:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [6][100/468]	Step 2914	lr 0.02416	Loss 0.0057 (0.0421)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:30:20PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [6][150/468]	Step 2964	lr 0.02416	Loss 0.0397 (0.0437)	Prec@(1,5) (98.6%, 100.0%)	
05/24 06:30:43PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [6][200/468]	Step 3014	lr 0.02416	Loss 0.0111 (0.0436)	Prec@(1,5) (98.6%, 100.0%)	
05/24 06:31:05PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [6][250/468]	Step 3064	lr 0.02416	Loss 0.0149 (0.0435)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:31:27PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [6][300/468]	Step 3114	lr 0.02416	Loss 0.0472 (0.0426)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:31:50PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [6][350/468]	Step 3164	lr 0.02416	Loss 0.0464 (0.0425)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:32:12PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [6][400/468]	Step 3214	lr 0.02416	Loss 0.0707 (0.0427)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:32:35PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [6][450/468]	Step 3264	lr 0.02416	Loss 0.0982 (0.0443)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:32:43PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [6][468/468]	Step 3282	lr 0.02416	Loss 0.1202 (0.0442)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:32:43PM searchShareStage_trainer.py:212 [INFO] Train: [  6/49] Final Prec@1 98.6667%
05/24 06:32:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [6][50/469]	Step 3283	Loss 0.0382	Prec@(1,5) (98.5%, 100.0%)
05/24 06:32:50PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [6][100/469]	Step 3283	Loss 0.0428	Prec@(1,5) (98.4%, 100.0%)
05/24 06:32:53PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [6][150/469]	Step 3283	Loss 0.0432	Prec@(1,5) (98.5%, 100.0%)
05/24 06:32:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [6][200/469]	Step 3283	Loss 0.0456	Prec@(1,5) (98.5%, 100.0%)
05/24 06:33:00PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [6][250/469]	Step 3283	Loss 0.0438	Prec@(1,5) (98.6%, 100.0%)
05/24 06:33:04PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [6][300/469]	Step 3283	Loss 0.0432	Prec@(1,5) (98.6%, 100.0%)
05/24 06:33:07PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [6][350/469]	Step 3283	Loss 0.0417	Prec@(1,5) (98.6%, 100.0%)
05/24 06:33:11PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [6][400/469]	Step 3283	Loss 0.0431	Prec@(1,5) (98.6%, 100.0%)
05/24 06:33:15PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [6][450/469]	Step 3283	Loss 0.0429	Prec@(1,5) (98.6%, 100.0%)
05/24 06:33:16PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [6][468/469]	Step 3283	Loss 0.0434	Prec@(1,5) (98.6%, 100.0%)
05/24 06:33:16PM searchShareStage_trainer.py:251 [INFO] Valid: [  6/49] Final Prec@1 98.6300%
05/24 06:33:16PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:33:17PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.6300%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2209, 0.3047, 0.2548, 0.2195],
        [0.2302, 0.2714, 0.2296, 0.2689]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2235, 0.2966, 0.2611, 0.2188],
        [0.2090, 0.2970, 0.2695, 0.2245],
        [0.2434, 0.3277, 0.2038, 0.2251]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2736, 0.2480, 0.2339],
        [0.2548, 0.3145, 0.2187, 0.2121],
        [0.2619, 0.3047, 0.2159, 0.2176]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2976, 0.2264, 0.2281],
        [0.2492, 0.2949, 0.2233, 0.2326],
        [0.2484, 0.2995, 0.2321, 0.2199]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3037, 0.2253, 0.2203],
        [0.2560, 0.3065, 0.2305, 0.2070],
        [0.2495, 0.2941, 0.2166, 0.2398]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2944, 0.2296, 0.2239],
        [0.2524, 0.2884, 0.2290, 0.2301],
        [0.2509, 0.2969, 0.2292, 0.2230]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:33:39PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [7][50/468]	Step 3333	lr 0.02386	Loss 0.0329 (0.0372)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:34:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [7][100/468]	Step 3383	lr 0.02386	Loss 0.0423 (0.0390)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:34:24PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [7][150/468]	Step 3433	lr 0.02386	Loss 0.0381 (0.0376)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:34:47PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [7][200/468]	Step 3483	lr 0.02386	Loss 0.0660 (0.0403)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:35:09PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [7][250/468]	Step 3533	lr 0.02386	Loss 0.0113 (0.0411)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:35:32PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [7][300/468]	Step 3583	lr 0.02386	Loss 0.1230 (0.0406)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:35:54PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [7][350/468]	Step 3633	lr 0.02386	Loss 0.0065 (0.0429)	Prec@(1,5) (98.6%, 100.0%)	
05/24 06:36:17PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [7][400/468]	Step 3683	lr 0.02386	Loss 0.1080 (0.0435)	Prec@(1,5) (98.6%, 100.0%)	
05/24 06:36:39PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [7][450/468]	Step 3733	lr 0.02386	Loss 0.0194 (0.0424)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:36:47PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [7][468/468]	Step 3751	lr 0.02386	Loss 0.0553 (0.0425)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:36:47PM searchShareStage_trainer.py:212 [INFO] Train: [  7/49] Final Prec@1 98.6633%
05/24 06:36:51PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [7][50/469]	Step 3752	Loss 0.0331	Prec@(1,5) (99.0%, 100.0%)
05/24 06:36:54PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [7][100/469]	Step 3752	Loss 0.0380	Prec@(1,5) (98.8%, 100.0%)
05/24 06:36:58PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [7][150/469]	Step 3752	Loss 0.0442	Prec@(1,5) (98.7%, 100.0%)
05/24 06:37:01PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [7][200/469]	Step 3752	Loss 0.0478	Prec@(1,5) (98.6%, 100.0%)
05/24 06:37:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [7][250/469]	Step 3752	Loss 0.0472	Prec@(1,5) (98.6%, 100.0%)
05/24 06:37:08PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [7][300/469]	Step 3752	Loss 0.0481	Prec@(1,5) (98.6%, 100.0%)
05/24 06:37:12PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [7][350/469]	Step 3752	Loss 0.0499	Prec@(1,5) (98.5%, 100.0%)
05/24 06:37:16PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [7][400/469]	Step 3752	Loss 0.0511	Prec@(1,5) (98.5%, 100.0%)
05/24 06:37:19PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [7][450/469]	Step 3752	Loss 0.0504	Prec@(1,5) (98.5%, 100.0%)
05/24 06:37:20PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [7][468/469]	Step 3752	Loss 0.0505	Prec@(1,5) (98.5%, 100.0%)
05/24 06:37:20PM searchShareStage_trainer.py:251 [INFO] Valid: [  7/49] Final Prec@1 98.5433%
05/24 06:37:20PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:37:21PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.6300%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2197, 0.3090, 0.2540, 0.2173],
        [0.2297, 0.2724, 0.2277, 0.2702]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2236, 0.2978, 0.2598, 0.2188],
        [0.2063, 0.2994, 0.2713, 0.2229],
        [0.2434, 0.3297, 0.2034, 0.2235]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2743, 0.2471, 0.2335],
        [0.2545, 0.3162, 0.2178, 0.2115],
        [0.2616, 0.3056, 0.2162, 0.2166]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.3001, 0.2262, 0.2264],
        [0.2494, 0.2957, 0.2215, 0.2334],
        [0.2480, 0.3002, 0.2324, 0.2193]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.3053, 0.2243, 0.2198],
        [0.2556, 0.3079, 0.2310, 0.2055],
        [0.2490, 0.2951, 0.2162, 0.2397]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2965, 0.2279, 0.2231],
        [0.2520, 0.2895, 0.2285, 0.2300],
        [0.2506, 0.2978, 0.2295, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:37:44PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [8][50/468]	Step 3802	lr 0.02352	Loss 0.0093 (0.0404)	Prec@(1,5) (98.6%, 100.0%)	
05/24 06:38:06PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [8][100/468]	Step 3852	lr 0.02352	Loss 0.0121 (0.0391)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:38:29PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [8][150/468]	Step 3902	lr 0.02352	Loss 0.0747 (0.0408)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:38:51PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [8][200/468]	Step 3952	lr 0.02352	Loss 0.0238 (0.0378)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:39:13PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [8][250/468]	Step 4002	lr 0.02352	Loss 0.0118 (0.0368)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:39:36PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [8][300/468]	Step 4052	lr 0.02352	Loss 0.0744 (0.0415)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:39:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [8][350/468]	Step 4102	lr 0.02352	Loss 0.0042 (0.0417)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:40:21PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [8][400/468]	Step 4152	lr 0.02352	Loss 0.0327 (0.0412)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:40:43PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [8][450/468]	Step 4202	lr 0.02352	Loss 0.0190 (0.0424)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:40:51PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [8][468/468]	Step 4220	lr 0.02352	Loss 0.0640 (0.0422)	Prec@(1,5) (98.7%, 100.0%)	
05/24 06:40:51PM searchShareStage_trainer.py:212 [INFO] Train: [  8/49] Final Prec@1 98.6967%
05/24 06:40:55PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [8][50/469]	Step 4221	Loss 0.0543	Prec@(1,5) (98.4%, 100.0%)
05/24 06:40:58PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [8][100/469]	Step 4221	Loss 0.0504	Prec@(1,5) (98.5%, 100.0%)
05/24 06:41:01PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [8][150/469]	Step 4221	Loss 0.0511	Prec@(1,5) (98.6%, 100.0%)
05/24 06:41:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [8][200/469]	Step 4221	Loss 0.0523	Prec@(1,5) (98.5%, 100.0%)
05/24 06:41:08PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [8][250/469]	Step 4221	Loss 0.0522	Prec@(1,5) (98.5%, 100.0%)
05/24 06:41:12PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [8][300/469]	Step 4221	Loss 0.0511	Prec@(1,5) (98.5%, 100.0%)
05/24 06:41:15PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [8][350/469]	Step 4221	Loss 0.0511	Prec@(1,5) (98.5%, 100.0%)
05/24 06:41:19PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [8][400/469]	Step 4221	Loss 0.0510	Prec@(1,5) (98.5%, 100.0%)
05/24 06:41:23PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [8][450/469]	Step 4221	Loss 0.0498	Prec@(1,5) (98.5%, 100.0%)
05/24 06:41:24PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [8][468/469]	Step 4221	Loss 0.0493	Prec@(1,5) (98.5%, 100.0%)
05/24 06:41:24PM searchShareStage_trainer.py:251 [INFO] Valid: [  8/49] Final Prec@1 98.5433%
05/24 06:41:24PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:41:24PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.6300%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2192, 0.3108, 0.2529, 0.2171],
        [0.2299, 0.2727, 0.2273, 0.2701]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2232, 0.3000, 0.2600, 0.2168],
        [0.2031, 0.3015, 0.2732, 0.2222],
        [0.2431, 0.3303, 0.2031, 0.2236]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2749, 0.2462, 0.2338],
        [0.2551, 0.3169, 0.2174, 0.2106],
        [0.2622, 0.3056, 0.2156, 0.2166]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.3014, 0.2264, 0.2254],
        [0.2490, 0.2960, 0.2213, 0.2337],
        [0.2478, 0.3004, 0.2324, 0.2193]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.3065, 0.2233, 0.2198],
        [0.2553, 0.3092, 0.2314, 0.2040],
        [0.2489, 0.2954, 0.2156, 0.2401]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2979, 0.2276, 0.2223],
        [0.2516, 0.2898, 0.2285, 0.2301],
        [0.2505, 0.2979, 0.2297, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:41:47PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [9][50/468]	Step 4271	lr 0.02313	Loss 0.0507 (0.0333)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:42:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [9][100/468]	Step 4321	lr 0.02313	Loss 0.0122 (0.0389)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:42:32PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [9][150/468]	Step 4371	lr 0.02313	Loss 0.1334 (0.0370)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:42:54PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [9][200/468]	Step 4421	lr 0.02313	Loss 0.0388 (0.0393)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:43:17PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [9][250/468]	Step 4471	lr 0.02313	Loss 0.0466 (0.0382)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:43:39PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [9][300/468]	Step 4521	lr 0.02313	Loss 0.0880 (0.0379)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:44:01PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [9][350/468]	Step 4571	lr 0.02313	Loss 0.0296 (0.0377)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:44:24PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [9][400/468]	Step 4621	lr 0.02313	Loss 0.0215 (0.0379)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:44:46PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [9][450/468]	Step 4671	lr 0.02313	Loss 0.0084 (0.0370)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:44:54PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [9][468/468]	Step 4689	lr 0.02313	Loss 0.0174 (0.0370)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:44:55PM searchShareStage_trainer.py:212 [INFO] Train: [  9/49] Final Prec@1 98.8867%
05/24 06:44:59PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [9][50/469]	Step 4690	Loss 0.0352	Prec@(1,5) (98.8%, 100.0%)
05/24 06:45:01PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [9][100/469]	Step 4690	Loss 0.0424	Prec@(1,5) (98.7%, 100.0%)
05/24 06:45:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [9][150/469]	Step 4690	Loss 0.0473	Prec@(1,5) (98.6%, 100.0%)
05/24 06:45:08PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [9][200/469]	Step 4690	Loss 0.0486	Prec@(1,5) (98.6%, 100.0%)
05/24 06:45:12PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [9][250/469]	Step 4690	Loss 0.0476	Prec@(1,5) (98.6%, 100.0%)
05/24 06:45:16PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [9][300/469]	Step 4690	Loss 0.0469	Prec@(1,5) (98.7%, 100.0%)
05/24 06:45:19PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [9][350/469]	Step 4690	Loss 0.0484	Prec@(1,5) (98.6%, 100.0%)
05/24 06:45:23PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [9][400/469]	Step 4690	Loss 0.0509	Prec@(1,5) (98.6%, 100.0%)
05/24 06:45:26PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [9][450/469]	Step 4690	Loss 0.0505	Prec@(1,5) (98.6%, 100.0%)
05/24 06:45:27PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [9][468/469]	Step 4690	Loss 0.0515	Prec@(1,5) (98.6%, 100.0%)
05/24 06:45:28PM searchShareStage_trainer.py:251 [INFO] Valid: [  9/49] Final Prec@1 98.5733%
05/24 06:45:28PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:45:28PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.6300%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2184, 0.3115, 0.2539, 0.2162],
        [0.2301, 0.2720, 0.2265, 0.2714]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2224, 0.3010, 0.2604, 0.2161],
        [0.2017, 0.3024, 0.2736, 0.2223],
        [0.2430, 0.3303, 0.2032, 0.2236]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2757, 0.2450, 0.2330],
        [0.2551, 0.3172, 0.2173, 0.2104],
        [0.2624, 0.3053, 0.2156, 0.2167]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.3020, 0.2265, 0.2247],
        [0.2486, 0.2956, 0.2213, 0.2345],
        [0.2477, 0.3003, 0.2327, 0.2193]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.3068, 0.2228, 0.2200],
        [0.2554, 0.3096, 0.2317, 0.2033],
        [0.2487, 0.2951, 0.2157, 0.2405]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2988, 0.2278, 0.2215],
        [0.2514, 0.2897, 0.2286, 0.2303],
        [0.2504, 0.2975, 0.2300, 0.2221]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:45:51PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [10][50/468]	Step 4740	lr 0.02271	Loss 0.0185 (0.0342)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:46:13PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [10][100/468]	Step 4790	lr 0.02271	Loss 0.0090 (0.0343)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:46:36PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [10][150/468]	Step 4840	lr 0.02271	Loss 0.0278 (0.0358)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:46:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [10][200/468]	Step 4890	lr 0.02271	Loss 0.0040 (0.0346)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:47:21PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [10][250/468]	Step 4940	lr 0.02271	Loss 0.0288 (0.0348)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:47:43PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [10][300/468]	Step 4990	lr 0.02271	Loss 0.0197 (0.0349)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:48:05PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [10][350/468]	Step 5040	lr 0.02271	Loss 0.0432 (0.0355)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:48:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [10][400/468]	Step 5090	lr 0.02271	Loss 0.0408 (0.0364)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:48:50PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [10][450/468]	Step 5140	lr 0.02271	Loss 0.0440 (0.0366)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:48:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [10][468/468]	Step 5158	lr 0.02271	Loss 0.0361 (0.0363)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:48:59PM searchShareStage_trainer.py:212 [INFO] Train: [ 10/49] Final Prec@1 98.8700%
05/24 06:49:02PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [10][50/469]	Step 5159	Loss 0.0404	Prec@(1,5) (98.8%, 100.0%)
05/24 06:49:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [10][100/469]	Step 5159	Loss 0.0439	Prec@(1,5) (98.7%, 100.0%)
05/24 06:49:09PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [10][150/469]	Step 5159	Loss 0.0393	Prec@(1,5) (98.7%, 100.0%)
05/24 06:49:12PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [10][200/469]	Step 5159	Loss 0.0393	Prec@(1,5) (98.7%, 100.0%)
05/24 06:49:16PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [10][250/469]	Step 5159	Loss 0.0385	Prec@(1,5) (98.7%, 100.0%)
05/24 06:49:19PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [10][300/469]	Step 5159	Loss 0.0397	Prec@(1,5) (98.7%, 100.0%)
05/24 06:49:23PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [10][350/469]	Step 5159	Loss 0.0396	Prec@(1,5) (98.8%, 100.0%)
05/24 06:49:26PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [10][400/469]	Step 5159	Loss 0.0405	Prec@(1,5) (98.8%, 100.0%)
05/24 06:49:30PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [10][450/469]	Step 5159	Loss 0.0412	Prec@(1,5) (98.8%, 100.0%)
05/24 06:49:31PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [10][468/469]	Step 5159	Loss 0.0409	Prec@(1,5) (98.8%, 100.0%)
05/24 06:49:31PM searchShareStage_trainer.py:251 [INFO] Valid: [ 10/49] Final Prec@1 98.7633%
05/24 06:49:31PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:49:32PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.7633%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2182, 0.3121, 0.2544, 0.2154],
        [0.2304, 0.2712, 0.2257, 0.2727]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2224, 0.3010, 0.2602, 0.2164],
        [0.2004, 0.3030, 0.2749, 0.2217],
        [0.2431, 0.3300, 0.2033, 0.2236]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2764, 0.2457, 0.2325],
        [0.2550, 0.3172, 0.2176, 0.2103],
        [0.2627, 0.3049, 0.2153, 0.2170]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.3023, 0.2270, 0.2242],
        [0.2481, 0.2954, 0.2212, 0.2352],
        [0.2475, 0.3000, 0.2332, 0.2193]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.3068, 0.2231, 0.2197],
        [0.2553, 0.3095, 0.2321, 0.2031],
        [0.2486, 0.2947, 0.2158, 0.2409]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2992, 0.2282, 0.2209],
        [0.2514, 0.2896, 0.2288, 0.2303],
        [0.2503, 0.2971, 0.2300, 0.2226]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:49:55PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [11][50/468]	Step 5209	lr 0.02225	Loss 0.0081 (0.0268)	Prec@(1,5) (99.2%, 100.0%)	
05/24 06:50:17PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [11][100/468]	Step 5259	lr 0.02225	Loss 0.0263 (0.0334)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:50:40PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [11][150/468]	Step 5309	lr 0.02225	Loss 0.1401 (0.0348)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:51:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [11][200/468]	Step 5359	lr 0.02225	Loss 0.1223 (0.0332)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:51:25PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [11][250/468]	Step 5409	lr 0.02225	Loss 0.1023 (0.0340)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:51:47PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [11][300/468]	Step 5459	lr 0.02225	Loss 0.0364 (0.0349)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:52:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [11][350/468]	Step 5509	lr 0.02225	Loss 0.0107 (0.0358)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:52:32PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [11][400/468]	Step 5559	lr 0.02225	Loss 0.0122 (0.0357)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:52:54PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [11][450/468]	Step 5609	lr 0.02225	Loss 0.0993 (0.0352)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:53:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [11][468/468]	Step 5627	lr 0.02225	Loss 0.0272 (0.0353)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:53:03PM searchShareStage_trainer.py:212 [INFO] Train: [ 11/49] Final Prec@1 98.9100%
05/24 06:53:07PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [11][50/469]	Step 5628	Loss 0.0430	Prec@(1,5) (98.9%, 100.0%)
05/24 06:53:09PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [11][100/469]	Step 5628	Loss 0.0415	Prec@(1,5) (98.8%, 100.0%)
05/24 06:53:13PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [11][150/469]	Step 5628	Loss 0.0410	Prec@(1,5) (98.8%, 100.0%)
05/24 06:53:17PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [11][200/469]	Step 5628	Loss 0.0429	Prec@(1,5) (98.7%, 100.0%)
05/24 06:53:20PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [11][250/469]	Step 5628	Loss 0.0424	Prec@(1,5) (98.8%, 100.0%)
05/24 06:53:24PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [11][300/469]	Step 5628	Loss 0.0407	Prec@(1,5) (98.8%, 100.0%)
05/24 06:53:27PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [11][350/469]	Step 5628	Loss 0.0412	Prec@(1,5) (98.8%, 100.0%)
05/24 06:53:31PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [11][400/469]	Step 5628	Loss 0.0417	Prec@(1,5) (98.8%, 100.0%)
05/24 06:53:34PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [11][450/469]	Step 5628	Loss 0.0433	Prec@(1,5) (98.8%, 100.0%)
05/24 06:53:35PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [11][468/469]	Step 5628	Loss 0.0435	Prec@(1,5) (98.8%, 100.0%)
05/24 06:53:36PM searchShareStage_trainer.py:251 [INFO] Valid: [ 11/49] Final Prec@1 98.7567%
05/24 06:53:36PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:53:36PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.7633%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2179, 0.3123, 0.2549, 0.2149],
        [0.2302, 0.2706, 0.2256, 0.2736]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2219, 0.3015, 0.2606, 0.2160],
        [0.1993, 0.3032, 0.2758, 0.2216],
        [0.2432, 0.3297, 0.2033, 0.2239]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2766, 0.2456, 0.2323],
        [0.2553, 0.3170, 0.2175, 0.2102],
        [0.2629, 0.3045, 0.2152, 0.2173]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.3023, 0.2270, 0.2239],
        [0.2481, 0.2951, 0.2211, 0.2357],
        [0.2475, 0.2997, 0.2334, 0.2194]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.3069, 0.2231, 0.2194],
        [0.2553, 0.3092, 0.2324, 0.2031],
        [0.2486, 0.2942, 0.2159, 0.2413]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2997, 0.2283, 0.2207],
        [0.2513, 0.2893, 0.2292, 0.2302],
        [0.2502, 0.2966, 0.2302, 0.2230]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:53:59PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [12][50/468]	Step 5678	lr 0.02175	Loss 0.0491 (0.0307)	Prec@(1,5) (99.1%, 100.0%)	
05/24 06:54:21PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [12][100/468]	Step 5728	lr 0.02175	Loss 0.0040 (0.0312)	Prec@(1,5) (99.1%, 100.0%)	
05/24 06:54:44PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [12][150/468]	Step 5778	lr 0.02175	Loss 0.0128 (0.0352)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:55:06PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [12][200/468]	Step 5828	lr 0.02175	Loss 0.0297 (0.0379)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:55:29PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [12][250/468]	Step 5878	lr 0.02175	Loss 0.1208 (0.0391)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:55:51PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [12][300/468]	Step 5928	lr 0.02175	Loss 0.0294 (0.0378)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:56:13PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [12][350/468]	Step 5978	lr 0.02175	Loss 0.0062 (0.0375)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:56:36PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [12][400/468]	Step 6028	lr 0.02175	Loss 0.0367 (0.0363)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:56:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [12][450/468]	Step 6078	lr 0.02175	Loss 0.1045 (0.0364)	Prec@(1,5) (98.9%, 100.0%)	
05/24 06:57:06PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [12][468/468]	Step 6096	lr 0.02175	Loss 0.0046 (0.0364)	Prec@(1,5) (98.8%, 100.0%)	
05/24 06:57:07PM searchShareStage_trainer.py:212 [INFO] Train: [ 12/49] Final Prec@1 98.8433%
05/24 06:57:10PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [12][50/469]	Step 6097	Loss 0.0574	Prec@(1,5) (98.1%, 100.0%)
05/24 06:57:13PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [12][100/469]	Step 6097	Loss 0.0569	Prec@(1,5) (98.3%, 100.0%)
05/24 06:57:17PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [12][150/469]	Step 6097	Loss 0.0557	Prec@(1,5) (98.4%, 100.0%)
05/24 06:57:20PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [12][200/469]	Step 6097	Loss 0.0554	Prec@(1,5) (98.4%, 100.0%)
05/24 06:57:24PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [12][250/469]	Step 6097	Loss 0.0525	Prec@(1,5) (98.5%, 100.0%)
05/24 06:57:27PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [12][300/469]	Step 6097	Loss 0.0542	Prec@(1,5) (98.4%, 100.0%)
05/24 06:57:31PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [12][350/469]	Step 6097	Loss 0.0552	Prec@(1,5) (98.4%, 100.0%)
05/24 06:57:34PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [12][400/469]	Step 6097	Loss 0.0546	Prec@(1,5) (98.5%, 100.0%)
05/24 06:57:38PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [12][450/469]	Step 6097	Loss 0.0539	Prec@(1,5) (98.5%, 100.0%)
05/24 06:57:39PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [12][468/469]	Step 6097	Loss 0.0538	Prec@(1,5) (98.5%, 100.0%)
05/24 06:57:39PM searchShareStage_trainer.py:251 [INFO] Valid: [ 12/49] Final Prec@1 98.4733%
05/24 06:57:39PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 06:57:40PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.7633%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2177, 0.3123, 0.2554, 0.2146],
        [0.2302, 0.2699, 0.2254, 0.2745]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2225, 0.3016, 0.2604, 0.2155],
        [0.1986, 0.3039, 0.2768, 0.2208],
        [0.2432, 0.3292, 0.2032, 0.2244]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2768, 0.2455, 0.2324],
        [0.2554, 0.3169, 0.2176, 0.2102],
        [0.2631, 0.3040, 0.2154, 0.2176]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.3022, 0.2269, 0.2241],
        [0.2482, 0.2948, 0.2212, 0.2358],
        [0.2475, 0.2993, 0.2337, 0.2196]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3069, 0.2227, 0.2195],
        [0.2555, 0.3089, 0.2328, 0.2028],
        [0.2484, 0.2937, 0.2161, 0.2418]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2999, 0.2288, 0.2202],
        [0.2512, 0.2890, 0.2295, 0.2303],
        [0.2501, 0.2961, 0.2303, 0.2235]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 06:58:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [13][50/468]	Step 6147	lr 0.02121	Loss 0.0621 (0.0318)	Prec@(1,5) (99.1%, 100.0%)	
05/24 06:58:25PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [13][100/468]	Step 6197	lr 0.02121	Loss 0.0111 (0.0319)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:58:47PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [13][150/468]	Step 6247	lr 0.02121	Loss 0.0278 (0.0313)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:59:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [13][200/468]	Step 6297	lr 0.02121	Loss 0.0150 (0.0326)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:59:32PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [13][250/468]	Step 6347	lr 0.02121	Loss 0.0686 (0.0317)	Prec@(1,5) (99.0%, 100.0%)	
05/24 06:59:55PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [13][300/468]	Step 6397	lr 0.02121	Loss 0.0082 (0.0307)	Prec@(1,5) (99.0%, 100.0%)	
05/24 07:00:17PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [13][350/468]	Step 6447	lr 0.02121	Loss 0.0647 (0.0310)	Prec@(1,5) (99.0%, 100.0%)	
05/24 07:00:40PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [13][400/468]	Step 6497	lr 0.02121	Loss 0.0049 (0.0310)	Prec@(1,5) (99.0%, 100.0%)	
05/24 07:01:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [13][450/468]	Step 6547	lr 0.02121	Loss 0.0023 (0.0324)	Prec@(1,5) (99.0%, 100.0%)	
05/24 07:01:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [13][468/468]	Step 6565	lr 0.02121	Loss 0.0197 (0.0319)	Prec@(1,5) (99.0%, 100.0%)	
05/24 07:01:11PM searchShareStage_trainer.py:212 [INFO] Train: [ 13/49] Final Prec@1 98.9967%
05/24 07:01:14PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [13][50/469]	Step 6566	Loss 0.0382	Prec@(1,5) (98.8%, 99.9%)
05/24 07:01:17PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [13][100/469]	Step 6566	Loss 0.0419	Prec@(1,5) (98.7%, 99.9%)
05/24 07:01:21PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [13][150/469]	Step 6566	Loss 0.0415	Prec@(1,5) (98.7%, 100.0%)
05/24 07:01:24PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [13][200/469]	Step 6566	Loss 0.0395	Prec@(1,5) (98.7%, 100.0%)
05/24 07:01:28PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [13][250/469]	Step 6566	Loss 0.0398	Prec@(1,5) (98.7%, 100.0%)
05/24 07:01:31PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [13][300/469]	Step 6566	Loss 0.0404	Prec@(1,5) (98.7%, 100.0%)
05/24 07:01:35PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [13][350/469]	Step 6566	Loss 0.0399	Prec@(1,5) (98.7%, 100.0%)
05/24 07:01:38PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [13][400/469]	Step 6566	Loss 0.0405	Prec@(1,5) (98.7%, 100.0%)
05/24 07:01:42PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [13][450/469]	Step 6566	Loss 0.0393	Prec@(1,5) (98.8%, 100.0%)
05/24 07:01:43PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [13][468/469]	Step 6566	Loss 0.0392	Prec@(1,5) (98.8%, 100.0%)
05/24 07:01:43PM searchShareStage_trainer.py:251 [INFO] Valid: [ 13/49] Final Prec@1 98.7667%
05/24 07:01:43PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:01:44PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.7667%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2174, 0.3120, 0.2561, 0.2145],
        [0.2301, 0.2695, 0.2254, 0.2751]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.3023, 0.2609, 0.2148],
        [0.1978, 0.3042, 0.2774, 0.2207],
        [0.2432, 0.3288, 0.2032, 0.2249]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2768, 0.2452, 0.2324],
        [0.2554, 0.3166, 0.2177, 0.2103],
        [0.2632, 0.3036, 0.2153, 0.2178]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.3022, 0.2269, 0.2240],
        [0.2482, 0.2944, 0.2213, 0.2361],
        [0.2474, 0.2989, 0.2340, 0.2198]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.3065, 0.2223, 0.2200],
        [0.2555, 0.3087, 0.2332, 0.2026],
        [0.2483, 0.2932, 0.2164, 0.2421]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.3000, 0.2292, 0.2201],
        [0.2512, 0.2885, 0.2295, 0.2308],
        [0.2500, 0.2956, 0.2307, 0.2236]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:02:07PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [14][50/468]	Step 6616	lr 0.02065	Loss 0.0683 (0.0294)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:02:29PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [14][100/468]	Step 6666	lr 0.02065	Loss 0.0825 (0.0306)	Prec@(1,5) (99.0%, 100.0%)	
05/24 07:02:52PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [14][150/468]	Step 6716	lr 0.02065	Loss 0.0054 (0.0334)	Prec@(1,5) (99.0%, 100.0%)	
05/24 07:03:14PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [14][200/468]	Step 6766	lr 0.02065	Loss 0.0028 (0.0343)	Prec@(1,5) (99.0%, 100.0%)	
05/24 07:03:36PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [14][250/468]	Step 6816	lr 0.02065	Loss 0.0082 (0.0324)	Prec@(1,5) (99.0%, 100.0%)	
05/24 07:03:59PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [14][300/468]	Step 6866	lr 0.02065	Loss 0.0028 (0.0311)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:04:21PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [14][350/468]	Step 6916	lr 0.02065	Loss 0.0226 (0.0318)	Prec@(1,5) (99.0%, 100.0%)	
05/24 07:04:44PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [14][400/468]	Step 6966	lr 0.02065	Loss 0.0076 (0.0313)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:05:06PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [14][450/468]	Step 7016	lr 0.02065	Loss 0.0149 (0.0313)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:05:14PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [14][468/468]	Step 7034	lr 0.02065	Loss 0.1023 (0.0315)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:05:15PM searchShareStage_trainer.py:212 [INFO] Train: [ 14/49] Final Prec@1 99.0600%
05/24 07:05:18PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [14][50/469]	Step 7035	Loss 0.0409	Prec@(1,5) (98.8%, 100.0%)
05/24 07:05:22PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [14][100/469]	Step 7035	Loss 0.0462	Prec@(1,5) (98.7%, 100.0%)
05/24 07:05:25PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [14][150/469]	Step 7035	Loss 0.0476	Prec@(1,5) (98.7%, 100.0%)
05/24 07:05:28PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [14][200/469]	Step 7035	Loss 0.0445	Prec@(1,5) (98.8%, 100.0%)
05/24 07:05:32PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [14][250/469]	Step 7035	Loss 0.0441	Prec@(1,5) (98.8%, 100.0%)
05/24 07:05:36PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [14][300/469]	Step 7035	Loss 0.0452	Prec@(1,5) (98.8%, 100.0%)
05/24 07:05:39PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [14][350/469]	Step 7035	Loss 0.0450	Prec@(1,5) (98.7%, 100.0%)
05/24 07:05:42PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [14][400/469]	Step 7035	Loss 0.0442	Prec@(1,5) (98.8%, 100.0%)
05/24 07:05:46PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [14][450/469]	Step 7035	Loss 0.0440	Prec@(1,5) (98.8%, 100.0%)
05/24 07:05:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [14][468/469]	Step 7035	Loss 0.0437	Prec@(1,5) (98.8%, 100.0%)
05/24 07:05:47PM searchShareStage_trainer.py:251 [INFO] Valid: [ 14/49] Final Prec@1 98.7767%
05/24 07:05:47PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:05:48PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.7767%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2171, 0.3118, 0.2566, 0.2145],
        [0.2300, 0.2690, 0.2255, 0.2756]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2225, 0.3021, 0.2608, 0.2146],
        [0.1973, 0.3043, 0.2778, 0.2207],
        [0.2433, 0.3283, 0.2033, 0.2251]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2768, 0.2453, 0.2326],
        [0.2555, 0.3163, 0.2177, 0.2104],
        [0.2633, 0.3032, 0.2155, 0.2180]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.3021, 0.2272, 0.2238],
        [0.2483, 0.2940, 0.2212, 0.2365],
        [0.2474, 0.2985, 0.2342, 0.2199]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.3065, 0.2222, 0.2202],
        [0.2555, 0.3084, 0.2334, 0.2026],
        [0.2483, 0.2928, 0.2165, 0.2423]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2999, 0.2294, 0.2200],
        [0.2512, 0.2881, 0.2296, 0.2311],
        [0.2500, 0.2952, 0.2310, 0.2238]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:06:11PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [15][50/468]	Step 7085	lr 0.02005	Loss 0.0105 (0.0280)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:06:34PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [15][100/468]	Step 7135	lr 0.02005	Loss 0.0381 (0.0269)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:06:56PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [15][150/468]	Step 7185	lr 0.02005	Loss 0.0653 (0.0293)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:07:18PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [15][200/468]	Step 7235	lr 0.02005	Loss 0.0123 (0.0326)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:07:41PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [15][250/468]	Step 7285	lr 0.02005	Loss 0.0027 (0.0308)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:08:03PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [15][300/468]	Step 7335	lr 0.02005	Loss 0.0297 (0.0295)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:08:26PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [15][350/468]	Step 7385	lr 0.02005	Loss 0.0337 (0.0301)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:08:48PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [15][400/468]	Step 7435	lr 0.02005	Loss 0.0282 (0.0303)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:09:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [15][450/468]	Step 7485	lr 0.02005	Loss 0.0131 (0.0300)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:09:18PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [15][468/468]	Step 7503	lr 0.02005	Loss 0.0905 (0.0299)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:09:19PM searchShareStage_trainer.py:212 [INFO] Train: [ 15/49] Final Prec@1 99.1100%
05/24 07:09:23PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [15][50/469]	Step 7504	Loss 0.0430	Prec@(1,5) (98.6%, 100.0%)
05/24 07:09:26PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [15][100/469]	Step 7504	Loss 0.0475	Prec@(1,5) (98.4%, 100.0%)
05/24 07:09:29PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [15][150/469]	Step 7504	Loss 0.0496	Prec@(1,5) (98.5%, 100.0%)
05/24 07:09:33PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [15][200/469]	Step 7504	Loss 0.0460	Prec@(1,5) (98.6%, 100.0%)
05/24 07:09:36PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [15][250/469]	Step 7504	Loss 0.0450	Prec@(1,5) (98.6%, 100.0%)
05/24 07:09:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [15][300/469]	Step 7504	Loss 0.0457	Prec@(1,5) (98.6%, 100.0%)
05/24 07:09:43PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [15][350/469]	Step 7504	Loss 0.0456	Prec@(1,5) (98.6%, 100.0%)
05/24 07:09:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [15][400/469]	Step 7504	Loss 0.0454	Prec@(1,5) (98.7%, 100.0%)
05/24 07:09:50PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [15][450/469]	Step 7504	Loss 0.0451	Prec@(1,5) (98.7%, 100.0%)
05/24 07:09:51PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [15][468/469]	Step 7504	Loss 0.0450	Prec@(1,5) (98.7%, 100.0%)
05/24 07:09:52PM searchShareStage_trainer.py:251 [INFO] Valid: [ 15/49] Final Prec@1 98.6633%
05/24 07:09:52PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:09:52PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.7767%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2169, 0.3117, 0.2568, 0.2145],
        [0.2301, 0.2686, 0.2255, 0.2759]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2224, 0.3023, 0.2609, 0.2144],
        [0.1971, 0.3042, 0.2780, 0.2207],
        [0.2434, 0.3279, 0.2034, 0.2254]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2769, 0.2451, 0.2329],
        [0.2555, 0.3162, 0.2180, 0.2104],
        [0.2635, 0.3028, 0.2154, 0.2182]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.3021, 0.2274, 0.2236],
        [0.2482, 0.2936, 0.2213, 0.2368],
        [0.2475, 0.2982, 0.2343, 0.2200]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.3064, 0.2221, 0.2204],
        [0.2555, 0.3083, 0.2338, 0.2024],
        [0.2483, 0.2924, 0.2166, 0.2427]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2998, 0.2298, 0.2198],
        [0.2512, 0.2878, 0.2298, 0.2313],
        [0.2500, 0.2947, 0.2312, 0.2241]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:10:15PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [16][50/468]	Step 7554	lr 0.01943	Loss 0.0033 (0.0277)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:10:37PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [16][100/468]	Step 7604	lr 0.01943	Loss 0.0175 (0.0259)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:11:00PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [16][150/468]	Step 7654	lr 0.01943	Loss 0.0014 (0.0261)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:11:22PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [16][200/468]	Step 7704	lr 0.01943	Loss 0.0071 (0.0284)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:11:45PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [16][250/468]	Step 7754	lr 0.01943	Loss 0.0853 (0.0287)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:12:07PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [16][300/468]	Step 7804	lr 0.01943	Loss 0.0028 (0.0283)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:12:30PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [16][350/468]	Step 7854	lr 0.01943	Loss 0.0986 (0.0279)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:12:52PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [16][400/468]	Step 7904	lr 0.01943	Loss 0.0120 (0.0281)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:13:14PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [16][450/468]	Step 7954	lr 0.01943	Loss 0.0046 (0.0279)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:13:22PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [16][468/468]	Step 7972	lr 0.01943	Loss 0.0251 (0.0277)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:13:23PM searchShareStage_trainer.py:212 [INFO] Train: [ 16/49] Final Prec@1 99.1933%
05/24 07:13:27PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [16][50/469]	Step 7973	Loss 0.0408	Prec@(1,5) (98.9%, 100.0%)
05/24 07:13:30PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [16][100/469]	Step 7973	Loss 0.0386	Prec@(1,5) (98.9%, 100.0%)
05/24 07:13:32PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [16][150/469]	Step 7973	Loss 0.0424	Prec@(1,5) (98.8%, 100.0%)
05/24 07:13:35PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [16][200/469]	Step 7973	Loss 0.0415	Prec@(1,5) (98.8%, 100.0%)
05/24 07:13:37PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [16][250/469]	Step 7973	Loss 0.0412	Prec@(1,5) (98.8%, 100.0%)
05/24 07:13:41PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [16][300/469]	Step 7973	Loss 0.0422	Prec@(1,5) (98.8%, 100.0%)
05/24 07:13:44PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [16][350/469]	Step 7973	Loss 0.0420	Prec@(1,5) (98.8%, 100.0%)
05/24 07:13:48PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [16][400/469]	Step 7973	Loss 0.0409	Prec@(1,5) (98.8%, 100.0%)
05/24 07:13:51PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [16][450/469]	Step 7973	Loss 0.0408	Prec@(1,5) (98.8%, 100.0%)
05/24 07:13:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [16][468/469]	Step 7973	Loss 0.0411	Prec@(1,5) (98.8%, 100.0%)
05/24 07:13:52PM searchShareStage_trainer.py:251 [INFO] Valid: [ 16/49] Final Prec@1 98.8100%
05/24 07:13:52PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:13:53PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.8100%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2169, 0.3115, 0.2570, 0.2146],
        [0.2300, 0.2682, 0.2255, 0.2763]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2222, 0.3022, 0.2610, 0.2145],
        [0.1965, 0.3045, 0.2785, 0.2205],
        [0.2433, 0.3276, 0.2036, 0.2256]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2761, 0.2448, 0.2337],
        [0.2556, 0.3160, 0.2181, 0.2104],
        [0.2637, 0.3025, 0.2155, 0.2184]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.3021, 0.2273, 0.2236],
        [0.2483, 0.2933, 0.2213, 0.2370],
        [0.2474, 0.2978, 0.2346, 0.2202]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.3064, 0.2221, 0.2203],
        [0.2555, 0.3079, 0.2340, 0.2025],
        [0.2484, 0.2920, 0.2167, 0.2429]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2999, 0.2301, 0.2196],
        [0.2511, 0.2875, 0.2300, 0.2314],
        [0.2500, 0.2943, 0.2313, 0.2244]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:14:16PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [17][50/468]	Step 8023	lr 0.01878	Loss 0.0074 (0.0222)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:14:39PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [17][100/468]	Step 8073	lr 0.01878	Loss 0.0037 (0.0210)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:15:01PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [17][150/468]	Step 8123	lr 0.01878	Loss 0.0259 (0.0236)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:15:24PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [17][200/468]	Step 8173	lr 0.01878	Loss 0.0031 (0.0252)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:15:46PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [17][250/468]	Step 8223	lr 0.01878	Loss 0.0101 (0.0245)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:16:09PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [17][300/468]	Step 8273	lr 0.01878	Loss 0.0053 (0.0260)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:16:31PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [17][350/468]	Step 8323	lr 0.01878	Loss 0.0487 (0.0274)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:16:54PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [17][400/468]	Step 8373	lr 0.01878	Loss 0.0049 (0.0274)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:17:16PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [17][450/468]	Step 8423	lr 0.01878	Loss 0.0497 (0.0274)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:17:24PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [17][468/468]	Step 8441	lr 0.01878	Loss 0.0896 (0.0280)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:17:24PM searchShareStage_trainer.py:212 [INFO] Train: [ 17/49] Final Prec@1 99.1233%
05/24 07:17:28PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [17][50/469]	Step 8442	Loss 0.0542	Prec@(1,5) (98.5%, 100.0%)
05/24 07:17:32PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [17][100/469]	Step 8442	Loss 0.0494	Prec@(1,5) (98.5%, 100.0%)
05/24 07:17:35PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [17][150/469]	Step 8442	Loss 0.0494	Prec@(1,5) (98.5%, 100.0%)
05/24 07:17:39PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [17][200/469]	Step 8442	Loss 0.0502	Prec@(1,5) (98.5%, 100.0%)
05/24 07:17:41PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [17][250/469]	Step 8442	Loss 0.0470	Prec@(1,5) (98.6%, 100.0%)
05/24 07:17:45PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [17][300/469]	Step 8442	Loss 0.0481	Prec@(1,5) (98.6%, 100.0%)
05/24 07:17:49PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [17][350/469]	Step 8442	Loss 0.0465	Prec@(1,5) (98.6%, 100.0%)
05/24 07:17:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [17][400/469]	Step 8442	Loss 0.0480	Prec@(1,5) (98.6%, 100.0%)
05/24 07:17:56PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [17][450/469]	Step 8442	Loss 0.0480	Prec@(1,5) (98.6%, 100.0%)
05/24 07:17:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [17][468/469]	Step 8442	Loss 0.0475	Prec@(1,5) (98.6%, 100.0%)
05/24 07:17:57PM searchShareStage_trainer.py:251 [INFO] Valid: [ 17/49] Final Prec@1 98.6033%
05/24 07:17:57PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:17:57PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.8100%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2167, 0.3114, 0.2574, 0.2146],
        [0.2299, 0.2679, 0.2255, 0.2766]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.3021, 0.2614, 0.2145],
        [0.1962, 0.3046, 0.2789, 0.2203],
        [0.2433, 0.3273, 0.2037, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2764, 0.2451, 0.2333],
        [0.2556, 0.3157, 0.2180, 0.2106],
        [0.2637, 0.3021, 0.2156, 0.2185]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.3020, 0.2273, 0.2237],
        [0.2483, 0.2930, 0.2214, 0.2372],
        [0.2474, 0.2975, 0.2348, 0.2203]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.3064, 0.2224, 0.2204],
        [0.2555, 0.3076, 0.2343, 0.2026],
        [0.2484, 0.2917, 0.2168, 0.2431]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2998, 0.2303, 0.2196],
        [0.2511, 0.2872, 0.2302, 0.2315],
        [0.2500, 0.2939, 0.2315, 0.2246]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:18:20PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [18][50/468]	Step 8492	lr 0.01811	Loss 0.0076 (0.0258)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:18:43PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [18][100/468]	Step 8542	lr 0.01811	Loss 0.0207 (0.0274)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:19:05PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [18][150/468]	Step 8592	lr 0.01811	Loss 0.0132 (0.0287)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:19:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [18][200/468]	Step 8642	lr 0.01811	Loss 0.0934 (0.0269)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:19:50PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [18][250/468]	Step 8692	lr 0.01811	Loss 0.0025 (0.0275)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:20:12PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [18][300/468]	Step 8742	lr 0.01811	Loss 0.0153 (0.0259)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:20:35PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [18][350/468]	Step 8792	lr 0.01811	Loss 0.0129 (0.0254)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:20:57PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [18][400/468]	Step 8842	lr 0.01811	Loss 0.0035 (0.0248)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:21:20PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [18][450/468]	Step 8892	lr 0.01811	Loss 0.0008 (0.0248)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:21:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [18][468/468]	Step 8910	lr 0.01811	Loss 0.0023 (0.0246)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:21:28PM searchShareStage_trainer.py:212 [INFO] Train: [ 18/49] Final Prec@1 99.2400%
05/24 07:21:32PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [18][50/469]	Step 8911	Loss 0.0364	Prec@(1,5) (98.9%, 100.0%)
05/24 07:21:36PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [18][100/469]	Step 8911	Loss 0.0426	Prec@(1,5) (98.8%, 100.0%)
05/24 07:21:39PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [18][150/469]	Step 8911	Loss 0.0430	Prec@(1,5) (98.7%, 100.0%)
05/24 07:21:43PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [18][200/469]	Step 8911	Loss 0.0446	Prec@(1,5) (98.7%, 100.0%)
05/24 07:21:46PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [18][250/469]	Step 8911	Loss 0.0421	Prec@(1,5) (98.7%, 100.0%)
05/24 07:21:49PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [18][300/469]	Step 8911	Loss 0.0412	Prec@(1,5) (98.8%, 100.0%)
05/24 07:21:53PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [18][350/469]	Step 8911	Loss 0.0441	Prec@(1,5) (98.7%, 100.0%)
05/24 07:21:56PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [18][400/469]	Step 8911	Loss 0.0437	Prec@(1,5) (98.7%, 100.0%)
05/24 07:22:00PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [18][450/469]	Step 8911	Loss 0.0430	Prec@(1,5) (98.8%, 100.0%)
05/24 07:22:01PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [18][468/469]	Step 8911	Loss 0.0437	Prec@(1,5) (98.7%, 100.0%)
05/24 07:22:01PM searchShareStage_trainer.py:251 [INFO] Valid: [ 18/49] Final Prec@1 98.7400%
05/24 07:22:01PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:22:01PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.8100%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2167, 0.3110, 0.2575, 0.2148],
        [0.2300, 0.2676, 0.2257, 0.2767]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2223, 0.3020, 0.2613, 0.2144],
        [0.1960, 0.3045, 0.2792, 0.2204],
        [0.2433, 0.3269, 0.2038, 0.2260]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2761, 0.2447, 0.2338],
        [0.2557, 0.3155, 0.2181, 0.2107],
        [0.2639, 0.3018, 0.2157, 0.2186]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.3019, 0.2274, 0.2237],
        [0.2483, 0.2928, 0.2215, 0.2374],
        [0.2474, 0.2973, 0.2349, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3062, 0.2226, 0.2204],
        [0.2555, 0.3074, 0.2344, 0.2027],
        [0.2484, 0.2914, 0.2169, 0.2433]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2996, 0.2304, 0.2197],
        [0.2511, 0.2869, 0.2302, 0.2317],
        [0.2500, 0.2936, 0.2317, 0.2247]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:22:25PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [19][50/468]	Step 8961	lr 0.01742	Loss 0.0016 (0.0213)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:22:47PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [19][100/468]	Step 9011	lr 0.01742	Loss 0.1066 (0.0229)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:23:09PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [19][150/468]	Step 9061	lr 0.01742	Loss 0.0278 (0.0223)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:23:32PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [19][200/468]	Step 9111	lr 0.01742	Loss 0.0111 (0.0230)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:23:54PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [19][250/468]	Step 9161	lr 0.01742	Loss 0.0256 (0.0241)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:24:17PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [19][300/468]	Step 9211	lr 0.01742	Loss 0.0048 (0.0250)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:24:39PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [19][350/468]	Step 9261	lr 0.01742	Loss 0.0020 (0.0249)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:25:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [19][400/468]	Step 9311	lr 0.01742	Loss 0.0368 (0.0255)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:25:24PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [19][450/468]	Step 9361	lr 0.01742	Loss 0.0031 (0.0261)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:25:32PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [19][468/468]	Step 9379	lr 0.01742	Loss 0.0100 (0.0261)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:25:32PM searchShareStage_trainer.py:212 [INFO] Train: [ 19/49] Final Prec@1 99.2300%
05/24 07:25:36PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [19][50/469]	Step 9380	Loss 0.0413	Prec@(1,5) (98.7%, 100.0%)
05/24 07:25:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [19][100/469]	Step 9380	Loss 0.0409	Prec@(1,5) (98.7%, 100.0%)
05/24 07:25:43PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [19][150/469]	Step 9380	Loss 0.0449	Prec@(1,5) (98.7%, 100.0%)
05/24 07:25:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [19][200/469]	Step 9380	Loss 0.0459	Prec@(1,5) (98.7%, 100.0%)
05/24 07:25:50PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [19][250/469]	Step 9380	Loss 0.0450	Prec@(1,5) (98.7%, 100.0%)
05/24 07:25:53PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [19][300/469]	Step 9380	Loss 0.0444	Prec@(1,5) (98.7%, 100.0%)
05/24 07:25:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [19][350/469]	Step 9380	Loss 0.0444	Prec@(1,5) (98.7%, 100.0%)
05/24 07:26:00PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [19][400/469]	Step 9380	Loss 0.0437	Prec@(1,5) (98.7%, 100.0%)
05/24 07:26:04PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [19][450/469]	Step 9380	Loss 0.0441	Prec@(1,5) (98.7%, 100.0%)
05/24 07:26:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [19][468/469]	Step 9380	Loss 0.0450	Prec@(1,5) (98.7%, 100.0%)
05/24 07:26:05PM searchShareStage_trainer.py:251 [INFO] Valid: [ 19/49] Final Prec@1 98.7067%
05/24 07:26:05PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:26:06PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.8100%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2166, 0.3108, 0.2577, 0.2149],
        [0.2301, 0.2673, 0.2258, 0.2769]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3022, 0.2615, 0.2143],
        [0.1956, 0.3045, 0.2795, 0.2204],
        [0.2433, 0.3267, 0.2039, 0.2261]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2757, 0.2447, 0.2343],
        [0.2557, 0.3154, 0.2182, 0.2107],
        [0.2639, 0.3015, 0.2158, 0.2188]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.3019, 0.2276, 0.2237],
        [0.2483, 0.2925, 0.2216, 0.2376],
        [0.2475, 0.2970, 0.2351, 0.2205]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3061, 0.2227, 0.2204],
        [0.2555, 0.3071, 0.2346, 0.2028],
        [0.2484, 0.2911, 0.2170, 0.2435]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2995, 0.2304, 0.2198],
        [0.2511, 0.2867, 0.2304, 0.2318],
        [0.2501, 0.2933, 0.2318, 0.2249]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:26:29PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [20][50/468]	Step 9430	lr 0.01671	Loss 0.0157 (0.0301)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:26:51PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [20][100/468]	Step 9480	lr 0.01671	Loss 0.0054 (0.0268)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:27:14PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [20][150/468]	Step 9530	lr 0.01671	Loss 0.0005 (0.0269)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:27:36PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [20][200/468]	Step 9580	lr 0.01671	Loss 0.0967 (0.0274)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:27:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [20][250/468]	Step 9630	lr 0.01671	Loss 0.0403 (0.0282)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:28:21PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [20][300/468]	Step 9680	lr 0.01671	Loss 0.0263 (0.0275)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:28:43PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [20][350/468]	Step 9730	lr 0.01671	Loss 0.0039 (0.0272)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:29:06PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [20][400/468]	Step 9780	lr 0.01671	Loss 0.0066 (0.0273)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:29:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [20][450/468]	Step 9830	lr 0.01671	Loss 0.0013 (0.0268)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:29:36PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [20][468/468]	Step 9848	lr 0.01671	Loss 0.0027 (0.0262)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:29:37PM searchShareStage_trainer.py:212 [INFO] Train: [ 20/49] Final Prec@1 99.1567%
05/24 07:29:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [20][50/469]	Step 9849	Loss 0.0312	Prec@(1,5) (99.1%, 100.0%)
05/24 07:29:44PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [20][100/469]	Step 9849	Loss 0.0352	Prec@(1,5) (99.0%, 100.0%)
05/24 07:29:48PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [20][150/469]	Step 9849	Loss 0.0370	Prec@(1,5) (99.0%, 100.0%)
05/24 07:29:51PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [20][200/469]	Step 9849	Loss 0.0362	Prec@(1,5) (99.0%, 100.0%)
05/24 07:29:54PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [20][250/469]	Step 9849	Loss 0.0353	Prec@(1,5) (99.0%, 100.0%)
05/24 07:29:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [20][300/469]	Step 9849	Loss 0.0358	Prec@(1,5) (99.0%, 100.0%)
05/24 07:30:01PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [20][350/469]	Step 9849	Loss 0.0366	Prec@(1,5) (98.9%, 100.0%)
05/24 07:30:04PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [20][400/469]	Step 9849	Loss 0.0360	Prec@(1,5) (98.9%, 100.0%)
05/24 07:30:08PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [20][450/469]	Step 9849	Loss 0.0372	Prec@(1,5) (98.9%, 100.0%)
05/24 07:30:09PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [20][468/469]	Step 9849	Loss 0.0372	Prec@(1,5) (98.9%, 100.0%)
05/24 07:30:09PM searchShareStage_trainer.py:251 [INFO] Valid: [ 20/49] Final Prec@1 98.9133%
05/24 07:30:09PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:30:10PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 98.9133%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2166, 0.3105, 0.2579, 0.2150],
        [0.2300, 0.2670, 0.2259, 0.2771]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3023, 0.2616, 0.2141],
        [0.1955, 0.3044, 0.2796, 0.2205],
        [0.2433, 0.3264, 0.2040, 0.2263]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2753, 0.2446, 0.2346],
        [0.2558, 0.3152, 0.2183, 0.2108],
        [0.2640, 0.3012, 0.2159, 0.2189]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.3018, 0.2277, 0.2236],
        [0.2483, 0.2923, 0.2217, 0.2377],
        [0.2475, 0.2967, 0.2352, 0.2206]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3060, 0.2226, 0.2205],
        [0.2555, 0.3068, 0.2349, 0.2028],
        [0.2485, 0.2908, 0.2171, 0.2436]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2995, 0.2306, 0.2197],
        [0.2511, 0.2864, 0.2305, 0.2319],
        [0.2501, 0.2930, 0.2319, 0.2250]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:30:33PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [21][50/468]	Step 9899	lr 0.01598	Loss 0.0073 (0.0193)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:30:56PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [21][100/468]	Step 9949	lr 0.01598	Loss 0.0021 (0.0193)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:31:18PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [21][150/468]	Step 9999	lr 0.01598	Loss 0.0034 (0.0187)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:31:41PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [21][200/468]	Step 10049	lr 0.01598	Loss 0.0082 (0.0172)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:32:03PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [21][250/468]	Step 10099	lr 0.01598	Loss 0.0021 (0.0181)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:32:25PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [21][300/468]	Step 10149	lr 0.01598	Loss 0.0115 (0.0190)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:32:48PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [21][350/468]	Step 10199	lr 0.01598	Loss 0.0084 (0.0188)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:33:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [21][400/468]	Step 10249	lr 0.01598	Loss 0.0636 (0.0194)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:33:32PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [21][450/468]	Step 10299	lr 0.01598	Loss 0.0007 (0.0195)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:33:40PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [21][468/468]	Step 10317	lr 0.01598	Loss 0.0091 (0.0195)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:33:41PM searchShareStage_trainer.py:212 [INFO] Train: [ 21/49] Final Prec@1 99.3867%
05/24 07:33:45PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [21][50/469]	Step 10318	Loss 0.0462	Prec@(1,5) (98.8%, 100.0%)
05/24 07:33:48PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [21][100/469]	Step 10318	Loss 0.0374	Prec@(1,5) (99.0%, 100.0%)
05/24 07:33:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [21][150/469]	Step 10318	Loss 0.0352	Prec@(1,5) (99.0%, 100.0%)
05/24 07:33:55PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [21][200/469]	Step 10318	Loss 0.0337	Prec@(1,5) (99.1%, 100.0%)
05/24 07:33:58PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [21][250/469]	Step 10318	Loss 0.0328	Prec@(1,5) (99.1%, 100.0%)
05/24 07:34:02PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [21][300/469]	Step 10318	Loss 0.0332	Prec@(1,5) (99.1%, 100.0%)
05/24 07:34:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [21][350/469]	Step 10318	Loss 0.0341	Prec@(1,5) (99.1%, 100.0%)
05/24 07:34:09PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [21][400/469]	Step 10318	Loss 0.0360	Prec@(1,5) (99.0%, 100.0%)
05/24 07:34:12PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [21][450/469]	Step 10318	Loss 0.0360	Prec@(1,5) (99.0%, 100.0%)
05/24 07:34:14PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [21][468/469]	Step 10318	Loss 0.0355	Prec@(1,5) (99.0%, 100.0%)
05/24 07:34:14PM searchShareStage_trainer.py:251 [INFO] Valid: [ 21/49] Final Prec@1 99.0433%
05/24 07:34:14PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:34:15PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.0433%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2165, 0.3104, 0.2581, 0.2150],
        [0.2300, 0.2668, 0.2260, 0.2773]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3022, 0.2616, 0.2141],
        [0.1953, 0.3043, 0.2798, 0.2206],
        [0.2433, 0.3262, 0.2041, 0.2264]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2454, 0.2754, 0.2448, 0.2344],
        [0.2559, 0.3150, 0.2182, 0.2109],
        [0.2641, 0.3010, 0.2159, 0.2190]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3018, 0.2280, 0.2236],
        [0.2483, 0.2921, 0.2217, 0.2378],
        [0.2475, 0.2965, 0.2353, 0.2207]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.3059, 0.2227, 0.2205],
        [0.2555, 0.3066, 0.2350, 0.2029],
        [0.2485, 0.2905, 0.2172, 0.2438]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2994, 0.2307, 0.2198],
        [0.2511, 0.2862, 0.2306, 0.2320],
        [0.2501, 0.2927, 0.2321, 0.2251]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:34:37PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [22][50/468]	Step 10368	lr 0.01525	Loss 0.0032 (0.0291)	Prec@(1,5) (99.0%, 100.0%)	
05/24 07:35:00PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [22][100/468]	Step 10418	lr 0.01525	Loss 0.0268 (0.0286)	Prec@(1,5) (99.1%, 100.0%)	
05/24 07:35:22PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [22][150/468]	Step 10468	lr 0.01525	Loss 0.0118 (0.0256)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:35:45PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [22][200/468]	Step 10518	lr 0.01525	Loss 0.0020 (0.0256)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:36:07PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [22][250/468]	Step 10568	lr 0.01525	Loss 0.0036 (0.0270)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:36:30PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [22][300/468]	Step 10618	lr 0.01525	Loss 0.0140 (0.0254)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:36:52PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [22][350/468]	Step 10668	lr 0.01525	Loss 0.0257 (0.0258)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:37:15PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [22][400/468]	Step 10718	lr 0.01525	Loss 0.0060 (0.0260)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:37:37PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [22][450/468]	Step 10768	lr 0.01525	Loss 0.0028 (0.0258)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:37:45PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [22][468/468]	Step 10786	lr 0.01525	Loss 0.0021 (0.0255)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:37:46PM searchShareStage_trainer.py:212 [INFO] Train: [ 22/49] Final Prec@1 99.2267%
05/24 07:37:49PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [22][50/469]	Step 10787	Loss 0.0389	Prec@(1,5) (98.9%, 100.0%)
05/24 07:37:53PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [22][100/469]	Step 10787	Loss 0.0433	Prec@(1,5) (98.8%, 100.0%)
05/24 07:37:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [22][150/469]	Step 10787	Loss 0.0424	Prec@(1,5) (98.8%, 100.0%)
05/24 07:38:00PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [22][200/469]	Step 10787	Loss 0.0422	Prec@(1,5) (98.8%, 100.0%)
05/24 07:38:03PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [22][250/469]	Step 10787	Loss 0.0398	Prec@(1,5) (98.9%, 100.0%)
05/24 07:38:06PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [22][300/469]	Step 10787	Loss 0.0400	Prec@(1,5) (98.9%, 100.0%)
05/24 07:38:10PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [22][350/469]	Step 10787	Loss 0.0396	Prec@(1,5) (98.9%, 100.0%)
05/24 07:38:13PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [22][400/469]	Step 10787	Loss 0.0387	Prec@(1,5) (98.9%, 100.0%)
05/24 07:38:17PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [22][450/469]	Step 10787	Loss 0.0375	Prec@(1,5) (98.9%, 100.0%)
05/24 07:38:18PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [22][468/469]	Step 10787	Loss 0.0376	Prec@(1,5) (98.9%, 100.0%)
05/24 07:38:18PM searchShareStage_trainer.py:251 [INFO] Valid: [ 22/49] Final Prec@1 98.9100%
05/24 07:38:18PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:38:19PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.0433%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2164, 0.3102, 0.2582, 0.2152],
        [0.2299, 0.2666, 0.2261, 0.2774]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3022, 0.2617, 0.2141],
        [0.1950, 0.3042, 0.2800, 0.2207],
        [0.2434, 0.3260, 0.2042, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2752, 0.2446, 0.2347],
        [0.2559, 0.3148, 0.2183, 0.2109],
        [0.2641, 0.3007, 0.2160, 0.2191]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3017, 0.2281, 0.2236],
        [0.2484, 0.2920, 0.2218, 0.2379],
        [0.2475, 0.2962, 0.2355, 0.2208]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3058, 0.2228, 0.2205],
        [0.2555, 0.3065, 0.2351, 0.2029],
        [0.2485, 0.2903, 0.2172, 0.2439]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2993, 0.2308, 0.2198],
        [0.2511, 0.2860, 0.2307, 0.2321],
        [0.2501, 0.2924, 0.2322, 0.2253]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:38:42PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [23][50/468]	Step 10837	lr 0.0145	Loss 0.0641 (0.0228)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:39:04PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [23][100/468]	Step 10887	lr 0.0145	Loss 0.0545 (0.0216)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:39:27PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [23][150/468]	Step 10937	lr 0.0145	Loss 0.0061 (0.0246)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:39:49PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [23][200/468]	Step 10987	lr 0.0145	Loss 0.0018 (0.0255)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:40:12PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [23][250/468]	Step 11037	lr 0.0145	Loss 0.0063 (0.0246)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:40:34PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [23][300/468]	Step 11087	lr 0.0145	Loss 0.0027 (0.0235)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:40:57PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [23][350/468]	Step 11137	lr 0.0145	Loss 0.0660 (0.0238)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:41:19PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [23][400/468]	Step 11187	lr 0.0145	Loss 0.0286 (0.0233)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:41:41PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [23][450/468]	Step 11237	lr 0.0145	Loss 0.0593 (0.0237)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:41:49PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [23][468/468]	Step 11255	lr 0.0145	Loss 0.0066 (0.0236)	Prec@(1,5) (99.2%, 100.0%)	
05/24 07:41:50PM searchShareStage_trainer.py:212 [INFO] Train: [ 23/49] Final Prec@1 99.2267%
05/24 07:41:54PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [23][50/469]	Step 11256	Loss 0.0426	Prec@(1,5) (98.7%, 100.0%)
05/24 07:41:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [23][100/469]	Step 11256	Loss 0.0383	Prec@(1,5) (98.9%, 100.0%)
05/24 07:42:01PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [23][150/469]	Step 11256	Loss 0.0366	Prec@(1,5) (98.9%, 100.0%)
05/24 07:42:04PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [23][200/469]	Step 11256	Loss 0.0368	Prec@(1,5) (98.9%, 100.0%)
05/24 07:42:07PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [23][250/469]	Step 11256	Loss 0.0352	Prec@(1,5) (98.9%, 100.0%)
05/24 07:42:11PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [23][300/469]	Step 11256	Loss 0.0341	Prec@(1,5) (98.9%, 100.0%)
05/24 07:42:14PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [23][350/469]	Step 11256	Loss 0.0328	Prec@(1,5) (99.0%, 100.0%)
05/24 07:42:18PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [23][400/469]	Step 11256	Loss 0.0342	Prec@(1,5) (99.0%, 100.0%)
05/24 07:42:21PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [23][450/469]	Step 11256	Loss 0.0346	Prec@(1,5) (98.9%, 100.0%)
05/24 07:42:23PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [23][468/469]	Step 11256	Loss 0.0341	Prec@(1,5) (99.0%, 100.0%)
05/24 07:42:23PM searchShareStage_trainer.py:251 [INFO] Valid: [ 23/49] Final Prec@1 98.9533%
05/24 07:42:23PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:42:23PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.0433%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2164, 0.3101, 0.2584, 0.2152],
        [0.2299, 0.2664, 0.2262, 0.2775]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.3021, 0.2619, 0.2139],
        [0.1948, 0.3042, 0.2802, 0.2208],
        [0.2434, 0.3257, 0.2043, 0.2266]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2751, 0.2447, 0.2345],
        [0.2560, 0.3146, 0.2184, 0.2110],
        [0.2642, 0.3005, 0.2160, 0.2192]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3016, 0.2281, 0.2236],
        [0.2484, 0.2918, 0.2219, 0.2380],
        [0.2475, 0.2960, 0.2356, 0.2209]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3057, 0.2229, 0.2206],
        [0.2555, 0.3063, 0.2353, 0.2029],
        [0.2486, 0.2901, 0.2173, 0.2441]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2992, 0.2310, 0.2198],
        [0.2511, 0.2858, 0.2308, 0.2322],
        [0.2501, 0.2922, 0.2323, 0.2254]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:42:46PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [24][50/468]	Step 11306	lr 0.01375	Loss 0.0010 (0.0170)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:43:08PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [24][100/468]	Step 11356	lr 0.01375	Loss 0.0046 (0.0164)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:43:31PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [24][150/468]	Step 11406	lr 0.01375	Loss 0.0566 (0.0192)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:43:53PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [24][200/468]	Step 11456	lr 0.01375	Loss 0.0749 (0.0191)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:44:16PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [24][250/468]	Step 11506	lr 0.01375	Loss 0.0150 (0.0191)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:44:38PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [24][300/468]	Step 11556	lr 0.01375	Loss 0.0004 (0.0182)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:45:01PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [24][350/468]	Step 11606	lr 0.01375	Loss 0.0081 (0.0197)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:45:23PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [24][400/468]	Step 11656	lr 0.01375	Loss 0.0420 (0.0201)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:45:46PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [24][450/468]	Step 11706	lr 0.01375	Loss 0.0266 (0.0200)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:45:54PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [24][468/468]	Step 11724	lr 0.01375	Loss 0.0264 (0.0203)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:45:54PM searchShareStage_trainer.py:212 [INFO] Train: [ 24/49] Final Prec@1 99.3600%
05/24 07:45:58PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [24][50/469]	Step 11725	Loss 0.0405	Prec@(1,5) (98.8%, 100.0%)
05/24 07:46:02PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [24][100/469]	Step 11725	Loss 0.0413	Prec@(1,5) (98.7%, 100.0%)
05/24 07:46:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [24][150/469]	Step 11725	Loss 0.0419	Prec@(1,5) (98.7%, 100.0%)
05/24 07:46:08PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [24][200/469]	Step 11725	Loss 0.0401	Prec@(1,5) (98.8%, 100.0%)
05/24 07:46:12PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [24][250/469]	Step 11725	Loss 0.0395	Prec@(1,5) (98.8%, 100.0%)
05/24 07:46:15PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [24][300/469]	Step 11725	Loss 0.0404	Prec@(1,5) (98.8%, 100.0%)
05/24 07:46:19PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [24][350/469]	Step 11725	Loss 0.0394	Prec@(1,5) (98.8%, 100.0%)
05/24 07:46:22PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [24][400/469]	Step 11725	Loss 0.0386	Prec@(1,5) (98.9%, 100.0%)
05/24 07:46:26PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [24][450/469]	Step 11725	Loss 0.0387	Prec@(1,5) (98.9%, 100.0%)
05/24 07:46:27PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [24][468/469]	Step 11725	Loss 0.0387	Prec@(1,5) (98.9%, 100.0%)
05/24 07:46:27PM searchShareStage_trainer.py:251 [INFO] Valid: [ 24/49] Final Prec@1 98.8700%
05/24 07:46:27PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:46:27PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.0433%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2163, 0.3099, 0.2585, 0.2153],
        [0.2298, 0.2661, 0.2263, 0.2777]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2222, 0.3021, 0.2619, 0.2138],
        [0.1947, 0.3041, 0.2803, 0.2209],
        [0.2434, 0.3255, 0.2044, 0.2267]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2748, 0.2447, 0.2348],
        [0.2560, 0.3145, 0.2184, 0.2111],
        [0.2643, 0.3003, 0.2161, 0.2193]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3015, 0.2281, 0.2237],
        [0.2484, 0.2916, 0.2220, 0.2380],
        [0.2475, 0.2958, 0.2357, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3056, 0.2231, 0.2206],
        [0.2556, 0.3061, 0.2354, 0.2030],
        [0.2486, 0.2899, 0.2174, 0.2442]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2991, 0.2310, 0.2199],
        [0.2512, 0.2857, 0.2309, 0.2323],
        [0.2502, 0.2920, 0.2324, 0.2255]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:46:51PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [25][50/468]	Step 11775	lr 0.013	Loss 0.0069 (0.0160)	Prec@(1,5) (99.5%, 100.0%)	
05/24 07:47:13PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [25][100/468]	Step 11825	lr 0.013	Loss 0.0085 (0.0166)	Prec@(1,5) (99.5%, 100.0%)	
05/24 07:47:35PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [25][150/468]	Step 11875	lr 0.013	Loss 0.0008 (0.0152)	Prec@(1,5) (99.5%, 100.0%)	
05/24 07:47:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [25][200/468]	Step 11925	lr 0.013	Loss 0.0028 (0.0171)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:48:20PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [25][250/468]	Step 11975	lr 0.013	Loss 0.0028 (0.0169)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:48:43PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [25][300/468]	Step 12025	lr 0.013	Loss 0.0136 (0.0162)	Prec@(1,5) (99.5%, 100.0%)	
05/24 07:49:05PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [25][350/468]	Step 12075	lr 0.013	Loss 0.0363 (0.0161)	Prec@(1,5) (99.5%, 100.0%)	
05/24 07:49:27PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [25][400/468]	Step 12125	lr 0.013	Loss 0.0072 (0.0164)	Prec@(1,5) (99.5%, 100.0%)	
05/24 07:49:50PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [25][450/468]	Step 12175	lr 0.013	Loss 0.0054 (0.0172)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:49:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [25][468/468]	Step 12193	lr 0.013	Loss 0.0961 (0.0177)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:49:58PM searchShareStage_trainer.py:212 [INFO] Train: [ 25/49] Final Prec@1 99.4200%
05/24 07:50:02PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [25][50/469]	Step 12194	Loss 0.0567	Prec@(1,5) (98.4%, 100.0%)
05/24 07:50:06PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [25][100/469]	Step 12194	Loss 0.0526	Prec@(1,5) (98.4%, 100.0%)
05/24 07:50:09PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [25][150/469]	Step 12194	Loss 0.0496	Prec@(1,5) (98.5%, 100.0%)
05/24 07:50:12PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [25][200/469]	Step 12194	Loss 0.0475	Prec@(1,5) (98.5%, 100.0%)
05/24 07:50:15PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [25][250/469]	Step 12194	Loss 0.0478	Prec@(1,5) (98.5%, 100.0%)
05/24 07:50:19PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [25][300/469]	Step 12194	Loss 0.0480	Prec@(1,5) (98.5%, 100.0%)
05/24 07:50:23PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [25][350/469]	Step 12194	Loss 0.0503	Prec@(1,5) (98.5%, 100.0%)
05/24 07:50:26PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [25][400/469]	Step 12194	Loss 0.0504	Prec@(1,5) (98.5%, 100.0%)
05/24 07:50:30PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [25][450/469]	Step 12194	Loss 0.0500	Prec@(1,5) (98.5%, 100.0%)
05/24 07:50:31PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [25][468/469]	Step 12194	Loss 0.0502	Prec@(1,5) (98.5%, 100.0%)
05/24 07:50:31PM searchShareStage_trainer.py:251 [INFO] Valid: [ 25/49] Final Prec@1 98.4967%
05/24 07:50:31PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:50:32PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.0433%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2164, 0.3098, 0.2585, 0.2153],
        [0.2299, 0.2659, 0.2263, 0.2779]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.3020, 0.2621, 0.2139],
        [0.1945, 0.3041, 0.2805, 0.2209],
        [0.2434, 0.3254, 0.2044, 0.2268]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2749, 0.2448, 0.2347],
        [0.2560, 0.3144, 0.2185, 0.2112],
        [0.2644, 0.3001, 0.2162, 0.2194]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3014, 0.2281, 0.2238],
        [0.2484, 0.2914, 0.2220, 0.2381],
        [0.2475, 0.2957, 0.2358, 0.2210]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3055, 0.2232, 0.2205],
        [0.2556, 0.3059, 0.2354, 0.2031],
        [0.2486, 0.2897, 0.2174, 0.2443]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2991, 0.2310, 0.2199],
        [0.2512, 0.2855, 0.2309, 0.2324],
        [0.2502, 0.2917, 0.2325, 0.2256]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:50:54PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [26][50/468]	Step 12244	lr 0.01225	Loss 0.0028 (0.0194)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:51:17PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [26][100/468]	Step 12294	lr 0.01225	Loss 0.0059 (0.0191)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:51:39PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [26][150/468]	Step 12344	lr 0.01225	Loss 0.0068 (0.0208)	Prec@(1,5) (99.3%, 100.0%)	
05/24 07:52:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [26][200/468]	Step 12394	lr 0.01225	Loss 0.0252 (0.0194)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:52:24PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [26][250/468]	Step 12444	lr 0.01225	Loss 0.0004 (0.0189)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:52:47PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [26][300/468]	Step 12494	lr 0.01225	Loss 0.0012 (0.0174)	Prec@(1,5) (99.5%, 100.0%)	
05/24 07:53:09PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [26][350/468]	Step 12544	lr 0.01225	Loss 0.0209 (0.0174)	Prec@(1,5) (99.5%, 100.0%)	
05/24 07:53:31PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [26][400/468]	Step 12594	lr 0.01225	Loss 0.0027 (0.0170)	Prec@(1,5) (99.5%, 100.0%)	
05/24 07:53:54PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [26][450/468]	Step 12644	lr 0.01225	Loss 0.0020 (0.0169)	Prec@(1,5) (99.5%, 100.0%)	
05/24 07:54:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [26][468/468]	Step 12662	lr 0.01225	Loss 0.0017 (0.0167)	Prec@(1,5) (99.5%, 100.0%)	
05/24 07:54:02PM searchShareStage_trainer.py:212 [INFO] Train: [ 26/49] Final Prec@1 99.4667%
05/24 07:54:06PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [26][50/469]	Step 12663	Loss 0.0258	Prec@(1,5) (99.4%, 100.0%)
05/24 07:54:10PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [26][100/469]	Step 12663	Loss 0.0280	Prec@(1,5) (99.2%, 100.0%)
05/24 07:54:13PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [26][150/469]	Step 12663	Loss 0.0283	Prec@(1,5) (99.2%, 100.0%)
05/24 07:54:16PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [26][200/469]	Step 12663	Loss 0.0292	Prec@(1,5) (99.1%, 100.0%)
05/24 07:54:19PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [26][250/469]	Step 12663	Loss 0.0294	Prec@(1,5) (99.1%, 100.0%)
05/24 07:54:23PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [26][300/469]	Step 12663	Loss 0.0317	Prec@(1,5) (99.0%, 100.0%)
05/24 07:54:27PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [26][350/469]	Step 12663	Loss 0.0319	Prec@(1,5) (99.1%, 100.0%)
05/24 07:54:30PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [26][400/469]	Step 12663	Loss 0.0312	Prec@(1,5) (99.1%, 100.0%)
05/24 07:54:34PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [26][450/469]	Step 12663	Loss 0.0308	Prec@(1,5) (99.1%, 100.0%)
05/24 07:54:35PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [26][468/469]	Step 12663	Loss 0.0309	Prec@(1,5) (99.1%, 100.0%)
05/24 07:54:35PM searchShareStage_trainer.py:251 [INFO] Valid: [ 26/49] Final Prec@1 99.0900%
05/24 07:54:35PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:54:36PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.0900%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2164, 0.3096, 0.2587, 0.2154],
        [0.2299, 0.2657, 0.2264, 0.2780]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3019, 0.2621, 0.2140],
        [0.1944, 0.3041, 0.2806, 0.2210],
        [0.2434, 0.3252, 0.2045, 0.2269]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2748, 0.2447, 0.2350],
        [0.2560, 0.3142, 0.2185, 0.2112],
        [0.2644, 0.2999, 0.2162, 0.2195]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3014, 0.2282, 0.2237],
        [0.2484, 0.2913, 0.2221, 0.2382],
        [0.2475, 0.2955, 0.2359, 0.2211]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.3054, 0.2232, 0.2206],
        [0.2556, 0.3057, 0.2355, 0.2031],
        [0.2486, 0.2895, 0.2175, 0.2444]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2990, 0.2311, 0.2200],
        [0.2512, 0.2854, 0.2310, 0.2324],
        [0.2502, 0.2916, 0.2326, 0.2257]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:54:59PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [27][50/468]	Step 12713	lr 0.0115	Loss 0.0014 (0.0104)	Prec@(1,5) (99.6%, 100.0%)	
05/24 07:55:21PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [27][100/468]	Step 12763	lr 0.0115	Loss 0.0392 (0.0173)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:55:44PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [27][150/468]	Step 12813	lr 0.0115	Loss 0.0005 (0.0169)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:56:06PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [27][200/468]	Step 12863	lr 0.0115	Loss 0.0151 (0.0175)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:56:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [27][250/468]	Step 12913	lr 0.0115	Loss 0.0141 (0.0175)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:56:51PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [27][300/468]	Step 12963	lr 0.0115	Loss 0.0030 (0.0166)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:57:13PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [27][350/468]	Step 13013	lr 0.0115	Loss 0.0231 (0.0170)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:57:36PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [27][400/468]	Step 13063	lr 0.0115	Loss 0.0152 (0.0172)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:57:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [27][450/468]	Step 13113	lr 0.0115	Loss 0.0259 (0.0171)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:58:06PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [27][468/468]	Step 13131	lr 0.0115	Loss 0.0026 (0.0170)	Prec@(1,5) (99.4%, 100.0%)	
05/24 07:58:06PM searchShareStage_trainer.py:212 [INFO] Train: [ 27/49] Final Prec@1 99.4200%
05/24 07:58:10PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [27][50/469]	Step 13132	Loss 0.0311	Prec@(1,5) (99.1%, 100.0%)
05/24 07:58:14PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [27][100/469]	Step 13132	Loss 0.0288	Prec@(1,5) (99.2%, 100.0%)
05/24 07:58:17PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [27][150/469]	Step 13132	Loss 0.0308	Prec@(1,5) (99.1%, 100.0%)
05/24 07:58:20PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [27][200/469]	Step 13132	Loss 0.0319	Prec@(1,5) (99.1%, 100.0%)
05/24 07:58:24PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [27][250/469]	Step 13132	Loss 0.0318	Prec@(1,5) (99.1%, 100.0%)
05/24 07:58:27PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [27][300/469]	Step 13132	Loss 0.0333	Prec@(1,5) (99.0%, 100.0%)
05/24 07:58:31PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [27][350/469]	Step 13132	Loss 0.0331	Prec@(1,5) (99.0%, 100.0%)
05/24 07:58:34PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [27][400/469]	Step 13132	Loss 0.0328	Prec@(1,5) (99.1%, 100.0%)
05/24 07:58:38PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [27][450/469]	Step 13132	Loss 0.0322	Prec@(1,5) (99.1%, 100.0%)
05/24 07:58:39PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [27][468/469]	Step 13132	Loss 0.0321	Prec@(1,5) (99.1%, 100.0%)
05/24 07:58:39PM searchShareStage_trainer.py:251 [INFO] Valid: [ 27/49] Final Prec@1 99.0867%
05/24 07:58:39PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 07:58:40PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.0900%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2163, 0.3095, 0.2588, 0.2154],
        [0.2299, 0.2655, 0.2264, 0.2781]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3018, 0.2622, 0.2140],
        [0.1943, 0.3040, 0.2807, 0.2209],
        [0.2434, 0.3250, 0.2046, 0.2270]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2747, 0.2447, 0.2350],
        [0.2560, 0.3141, 0.2186, 0.2113],
        [0.2644, 0.2997, 0.2163, 0.2196]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3013, 0.2282, 0.2238],
        [0.2484, 0.2912, 0.2221, 0.2383],
        [0.2475, 0.2953, 0.2360, 0.2212]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.3053, 0.2233, 0.2206],
        [0.2556, 0.3056, 0.2356, 0.2032],
        [0.2487, 0.2893, 0.2175, 0.2445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2989, 0.2311, 0.2200],
        [0.2512, 0.2852, 0.2311, 0.2325],
        [0.2502, 0.2914, 0.2327, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 07:59:03PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [28][50/468]	Step 13182	lr 0.01075	Loss 0.0055 (0.0140)	Prec@(1,5) (99.6%, 100.0%)	
05/24 07:59:25PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [28][100/468]	Step 13232	lr 0.01075	Loss 0.0057 (0.0135)	Prec@(1,5) (99.6%, 100.0%)	
05/24 07:59:48PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [28][150/468]	Step 13282	lr 0.01075	Loss 0.0067 (0.0146)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:00:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [28][200/468]	Step 13332	lr 0.01075	Loss 0.0466 (0.0157)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:00:32PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [28][250/468]	Step 13382	lr 0.01075	Loss 0.0451 (0.0155)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:00:55PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [28][300/468]	Step 13432	lr 0.01075	Loss 0.0091 (0.0151)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:01:17PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [28][350/468]	Step 13482	lr 0.01075	Loss 0.0197 (0.0158)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:01:40PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [28][400/468]	Step 13532	lr 0.01075	Loss 0.0007 (0.0154)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:02:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [28][450/468]	Step 13582	lr 0.01075	Loss 0.0571 (0.0148)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:02:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [28][468/468]	Step 13600	lr 0.01075	Loss 0.0182 (0.0154)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:02:11PM searchShareStage_trainer.py:212 [INFO] Train: [ 28/49] Final Prec@1 99.5033%
05/24 08:02:14PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [28][50/469]	Step 13601	Loss 0.0313	Prec@(1,5) (99.1%, 100.0%)
05/24 08:02:18PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [28][100/469]	Step 13601	Loss 0.0273	Prec@(1,5) (99.2%, 100.0%)
05/24 08:02:22PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [28][150/469]	Step 13601	Loss 0.0336	Prec@(1,5) (99.0%, 100.0%)
05/24 08:02:24PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [28][200/469]	Step 13601	Loss 0.0336	Prec@(1,5) (99.0%, 100.0%)
05/24 08:02:28PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [28][250/469]	Step 13601	Loss 0.0312	Prec@(1,5) (99.1%, 100.0%)
05/24 08:02:32PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [28][300/469]	Step 13601	Loss 0.0311	Prec@(1,5) (99.1%, 100.0%)
05/24 08:02:35PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [28][350/469]	Step 13601	Loss 0.0289	Prec@(1,5) (99.2%, 100.0%)
05/24 08:02:39PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [28][400/469]	Step 13601	Loss 0.0295	Prec@(1,5) (99.1%, 100.0%)
05/24 08:02:42PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [28][450/469]	Step 13601	Loss 0.0292	Prec@(1,5) (99.1%, 100.0%)
05/24 08:02:43PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [28][468/469]	Step 13601	Loss 0.0293	Prec@(1,5) (99.2%, 100.0%)
05/24 08:02:44PM searchShareStage_trainer.py:251 [INFO] Valid: [ 28/49] Final Prec@1 99.1533%
05/24 08:02:44PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:02:44PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.1533%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2163, 0.3093, 0.2589, 0.2155],
        [0.2299, 0.2654, 0.2265, 0.2782]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.3017, 0.2622, 0.2140],
        [0.1943, 0.3040, 0.2808, 0.2210],
        [0.2435, 0.3249, 0.2046, 0.2270]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2745, 0.2447, 0.2353],
        [0.2561, 0.3140, 0.2186, 0.2113],
        [0.2645, 0.2996, 0.2163, 0.2196]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3012, 0.2281, 0.2239],
        [0.2485, 0.2910, 0.2222, 0.2383],
        [0.2476, 0.2952, 0.2360, 0.2212]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3052, 0.2233, 0.2206],
        [0.2556, 0.3054, 0.2357, 0.2032],
        [0.2487, 0.2892, 0.2176, 0.2445]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2988, 0.2312, 0.2201],
        [0.2512, 0.2851, 0.2312, 0.2325],
        [0.2502, 0.2912, 0.2327, 0.2258]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:03:07PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [29][50/468]	Step 13651	lr 0.01002	Loss 0.0027 (0.0163)	Prec@(1,5) (99.4%, 100.0%)	
05/24 08:03:30PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [29][100/468]	Step 13701	lr 0.01002	Loss 0.0057 (0.0142)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:03:52PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [29][150/468]	Step 13751	lr 0.01002	Loss 0.0003 (0.0134)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:04:15PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [29][200/468]	Step 13801	lr 0.01002	Loss 0.1219 (0.0149)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:04:37PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [29][250/468]	Step 13851	lr 0.01002	Loss 0.0028 (0.0150)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:04:59PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [29][300/468]	Step 13901	lr 0.01002	Loss 0.0612 (0.0146)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:05:22PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [29][350/468]	Step 13951	lr 0.01002	Loss 0.0218 (0.0142)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:05:44PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [29][400/468]	Step 14001	lr 0.01002	Loss 0.0019 (0.0137)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:06:07PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [29][450/468]	Step 14051	lr 0.01002	Loss 0.0028 (0.0141)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:06:15PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [29][468/468]	Step 14069	lr 0.01002	Loss 0.0006 (0.0141)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:06:15PM searchShareStage_trainer.py:212 [INFO] Train: [ 29/49] Final Prec@1 99.5667%
05/24 08:06:19PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [29][50/469]	Step 14070	Loss 0.0294	Prec@(1,5) (99.2%, 100.0%)
05/24 08:06:22PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [29][100/469]	Step 14070	Loss 0.0262	Prec@(1,5) (99.2%, 100.0%)
05/24 08:06:26PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [29][150/469]	Step 14070	Loss 0.0283	Prec@(1,5) (99.2%, 100.0%)
05/24 08:06:29PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [29][200/469]	Step 14070	Loss 0.0274	Prec@(1,5) (99.2%, 100.0%)
05/24 08:06:32PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [29][250/469]	Step 14070	Loss 0.0276	Prec@(1,5) (99.2%, 100.0%)
05/24 08:06:36PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [29][300/469]	Step 14070	Loss 0.0281	Prec@(1,5) (99.2%, 100.0%)
05/24 08:06:39PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [29][350/469]	Step 14070	Loss 0.0286	Prec@(1,5) (99.2%, 100.0%)
05/24 08:06:43PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [29][400/469]	Step 14070	Loss 0.0285	Prec@(1,5) (99.2%, 100.0%)
05/24 08:06:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [29][450/469]	Step 14070	Loss 0.0282	Prec@(1,5) (99.2%, 100.0%)
05/24 08:06:48PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [29][468/469]	Step 14070	Loss 0.0286	Prec@(1,5) (99.2%, 100.0%)
05/24 08:06:48PM searchShareStage_trainer.py:251 [INFO] Valid: [ 29/49] Final Prec@1 99.1900%
05/24 08:06:48PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:06:49PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.1900%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2163, 0.3092, 0.2590, 0.2155],
        [0.2299, 0.2652, 0.2265, 0.2783]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.3016, 0.2623, 0.2140],
        [0.1942, 0.3039, 0.2809, 0.2210],
        [0.2435, 0.3247, 0.2047, 0.2271]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2742, 0.2446, 0.2356],
        [0.2561, 0.3139, 0.2187, 0.2114],
        [0.2645, 0.2994, 0.2164, 0.2197]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3012, 0.2282, 0.2239],
        [0.2485, 0.2909, 0.2223, 0.2384],
        [0.2476, 0.2950, 0.2361, 0.2213]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3051, 0.2234, 0.2207],
        [0.2556, 0.3053, 0.2357, 0.2033],
        [0.2487, 0.2890, 0.2177, 0.2446]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2987, 0.2313, 0.2201],
        [0.2512, 0.2849, 0.2313, 0.2326],
        [0.2502, 0.2910, 0.2328, 0.2259]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:07:12PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [30][50/468]	Step 14120	lr 0.00929	Loss 0.0023 (0.0142)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:07:34PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [30][100/468]	Step 14170	lr 0.00929	Loss 0.0011 (0.0150)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:07:57PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [30][150/468]	Step 14220	lr 0.00929	Loss 0.0698 (0.0150)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:08:19PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [30][200/468]	Step 14270	lr 0.00929	Loss 0.0003 (0.0140)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:08:42PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [30][250/468]	Step 14320	lr 0.00929	Loss 0.0073 (0.0137)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:09:04PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [30][300/468]	Step 14370	lr 0.00929	Loss 0.0019 (0.0137)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:09:26PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [30][350/468]	Step 14420	lr 0.00929	Loss 0.0103 (0.0135)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:09:49PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [30][400/468]	Step 14470	lr 0.00929	Loss 0.0100 (0.0138)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:10:11PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [30][450/468]	Step 14520	lr 0.00929	Loss 0.0020 (0.0139)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:10:19PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [30][468/468]	Step 14538	lr 0.00929	Loss 0.0015 (0.0136)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:10:20PM searchShareStage_trainer.py:212 [INFO] Train: [ 30/49] Final Prec@1 99.5700%
05/24 08:10:24PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [30][50/469]	Step 14539	Loss 0.0266	Prec@(1,5) (99.2%, 100.0%)
05/24 08:10:27PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [30][100/469]	Step 14539	Loss 0.0248	Prec@(1,5) (99.3%, 100.0%)
05/24 08:10:30PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [30][150/469]	Step 14539	Loss 0.0233	Prec@(1,5) (99.3%, 100.0%)
05/24 08:10:33PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [30][200/469]	Step 14539	Loss 0.0235	Prec@(1,5) (99.3%, 100.0%)
05/24 08:10:37PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [30][250/469]	Step 14539	Loss 0.0252	Prec@(1,5) (99.3%, 100.0%)
05/24 08:10:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [30][300/469]	Step 14539	Loss 0.0249	Prec@(1,5) (99.3%, 100.0%)
05/24 08:10:44PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [30][350/469]	Step 14539	Loss 0.0260	Prec@(1,5) (99.2%, 100.0%)
05/24 08:10:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [30][400/469]	Step 14539	Loss 0.0258	Prec@(1,5) (99.2%, 100.0%)
05/24 08:10:51PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [30][450/469]	Step 14539	Loss 0.0258	Prec@(1,5) (99.2%, 100.0%)
05/24 08:10:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [30][468/469]	Step 14539	Loss 0.0257	Prec@(1,5) (99.2%, 100.0%)
05/24 08:10:52PM searchShareStage_trainer.py:251 [INFO] Valid: [ 30/49] Final Prec@1 99.2367%
05/24 08:10:52PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:10:53PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.2367%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2163, 0.3091, 0.2591, 0.2155],
        [0.2299, 0.2651, 0.2266, 0.2785]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.3015, 0.2624, 0.2140],
        [0.1941, 0.3038, 0.2810, 0.2210],
        [0.2435, 0.3246, 0.2047, 0.2272]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2742, 0.2447, 0.2356],
        [0.2561, 0.3138, 0.2187, 0.2114],
        [0.2646, 0.2992, 0.2164, 0.2198]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3012, 0.2282, 0.2239],
        [0.2485, 0.2908, 0.2223, 0.2384],
        [0.2476, 0.2949, 0.2362, 0.2214]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.3050, 0.2235, 0.2207],
        [0.2556, 0.3052, 0.2358, 0.2034],
        [0.2487, 0.2889, 0.2177, 0.2447]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2986, 0.2313, 0.2201],
        [0.2512, 0.2848, 0.2313, 0.2326],
        [0.2503, 0.2909, 0.2329, 0.2260]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:11:16PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [31][50/468]	Step 14589	lr 0.00858	Loss 0.0012 (0.0103)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:11:39PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [31][100/468]	Step 14639	lr 0.00858	Loss 0.0067 (0.0098)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:12:01PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [31][150/468]	Step 14689	lr 0.00858	Loss 0.0114 (0.0093)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:12:24PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [31][200/468]	Step 14739	lr 0.00858	Loss 0.0036 (0.0099)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:12:46PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [31][250/468]	Step 14789	lr 0.00858	Loss 0.0113 (0.0094)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:13:08PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [31][300/468]	Step 14839	lr 0.00858	Loss 0.0014 (0.0097)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:13:31PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [31][350/468]	Step 14889	lr 0.00858	Loss 0.0970 (0.0107)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:13:53PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [31][400/468]	Step 14939	lr 0.00858	Loss 0.0010 (0.0106)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:14:16PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [31][450/468]	Step 14989	lr 0.00858	Loss 0.0034 (0.0109)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:14:24PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [31][468/468]	Step 15007	lr 0.00858	Loss 0.0581 (0.0109)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:14:24PM searchShareStage_trainer.py:212 [INFO] Train: [ 31/49] Final Prec@1 99.6700%
05/24 08:14:28PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [31][50/469]	Step 15008	Loss 0.0290	Prec@(1,5) (98.9%, 100.0%)
05/24 08:14:31PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [31][100/469]	Step 15008	Loss 0.0275	Prec@(1,5) (99.0%, 100.0%)
05/24 08:14:34PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [31][150/469]	Step 15008	Loss 0.0273	Prec@(1,5) (99.0%, 100.0%)
05/24 08:14:38PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [31][200/469]	Step 15008	Loss 0.0291	Prec@(1,5) (99.0%, 100.0%)
05/24 08:14:41PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [31][250/469]	Step 15008	Loss 0.0297	Prec@(1,5) (99.1%, 100.0%)
05/24 08:14:45PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [31][300/469]	Step 15008	Loss 0.0308	Prec@(1,5) (99.0%, 100.0%)
05/24 08:14:48PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [31][350/469]	Step 15008	Loss 0.0305	Prec@(1,5) (99.0%, 100.0%)
05/24 08:14:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [31][400/469]	Step 15008	Loss 0.0306	Prec@(1,5) (99.1%, 100.0%)
05/24 08:14:56PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [31][450/469]	Step 15008	Loss 0.0307	Prec@(1,5) (99.0%, 100.0%)
05/24 08:14:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [31][468/469]	Step 15008	Loss 0.0316	Prec@(1,5) (99.0%, 100.0%)
05/24 08:14:57PM searchShareStage_trainer.py:251 [INFO] Valid: [ 31/49] Final Prec@1 99.0167%
05/24 08:14:57PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:14:57PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.2367%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2163, 0.3090, 0.2592, 0.2156],
        [0.2299, 0.2649, 0.2266, 0.2786]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3015, 0.2625, 0.2140],
        [0.1941, 0.3037, 0.2811, 0.2211],
        [0.2435, 0.3245, 0.2048, 0.2272]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2740, 0.2446, 0.2357],
        [0.2561, 0.3137, 0.2187, 0.2114],
        [0.2646, 0.2991, 0.2165, 0.2198]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3011, 0.2283, 0.2239],
        [0.2485, 0.2907, 0.2223, 0.2385],
        [0.2476, 0.2947, 0.2363, 0.2214]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.3049, 0.2235, 0.2207],
        [0.2556, 0.3051, 0.2359, 0.2034],
        [0.2487, 0.2887, 0.2178, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2986, 0.2314, 0.2201],
        [0.2513, 0.2847, 0.2314, 0.2327],
        [0.2503, 0.2907, 0.2330, 0.2260]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:15:20PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [32][50/468]	Step 15058	lr 0.00789	Loss 0.0008 (0.0156)	Prec@(1,5) (99.4%, 100.0%)	
05/24 08:15:43PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [32][100/468]	Step 15108	lr 0.00789	Loss 0.0078 (0.0140)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:16:05PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [32][150/468]	Step 15158	lr 0.00789	Loss 0.0070 (0.0131)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:16:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [32][200/468]	Step 15208	lr 0.00789	Loss 0.0003 (0.0121)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:16:50PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [32][250/468]	Step 15258	lr 0.00789	Loss 0.0011 (0.0123)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:17:13PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [32][300/468]	Step 15308	lr 0.00789	Loss 0.0060 (0.0119)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:17:35PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [32][350/468]	Step 15358	lr 0.00789	Loss 0.0028 (0.0118)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:17:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [32][400/468]	Step 15408	lr 0.00789	Loss 0.0049 (0.0119)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:18:20PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [32][450/468]	Step 15458	lr 0.00789	Loss 0.0043 (0.0119)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:18:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [32][468/468]	Step 15476	lr 0.00789	Loss 0.0065 (0.0117)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:18:29PM searchShareStage_trainer.py:212 [INFO] Train: [ 32/49] Final Prec@1 99.5933%
05/24 08:18:32PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [32][50/469]	Step 15477	Loss 0.0258	Prec@(1,5) (99.2%, 100.0%)
05/24 08:18:36PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [32][100/469]	Step 15477	Loss 0.0274	Prec@(1,5) (99.2%, 100.0%)
05/24 08:18:39PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [32][150/469]	Step 15477	Loss 0.0255	Prec@(1,5) (99.2%, 100.0%)
05/24 08:18:42PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [32][200/469]	Step 15477	Loss 0.0260	Prec@(1,5) (99.2%, 100.0%)
05/24 08:18:46PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [32][250/469]	Step 15477	Loss 0.0251	Prec@(1,5) (99.3%, 100.0%)
05/24 08:18:49PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [32][300/469]	Step 15477	Loss 0.0259	Prec@(1,5) (99.2%, 100.0%)
05/24 08:18:53PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [32][350/469]	Step 15477	Loss 0.0265	Prec@(1,5) (99.2%, 100.0%)
05/24 08:18:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [32][400/469]	Step 15477	Loss 0.0273	Prec@(1,5) (99.2%, 100.0%)
05/24 08:19:00PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [32][450/469]	Step 15477	Loss 0.0287	Prec@(1,5) (99.2%, 100.0%)
05/24 08:19:01PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [32][468/469]	Step 15477	Loss 0.0282	Prec@(1,5) (99.2%, 100.0%)
05/24 08:19:01PM searchShareStage_trainer.py:251 [INFO] Valid: [ 32/49] Final Prec@1 99.1833%
05/24 08:19:01PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:19:02PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.2367%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2163, 0.3088, 0.2593, 0.2156],
        [0.2299, 0.2648, 0.2267, 0.2786]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.3014, 0.2625, 0.2140],
        [0.1940, 0.3037, 0.2812, 0.2211],
        [0.2435, 0.3243, 0.2048, 0.2273]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2739, 0.2447, 0.2358],
        [0.2562, 0.3136, 0.2187, 0.2115],
        [0.2646, 0.2990, 0.2165, 0.2199]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3011, 0.2283, 0.2239],
        [0.2485, 0.2906, 0.2224, 0.2385],
        [0.2476, 0.2946, 0.2363, 0.2215]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.3048, 0.2235, 0.2207],
        [0.2557, 0.3049, 0.2359, 0.2035],
        [0.2488, 0.2886, 0.2178, 0.2448]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2985, 0.2314, 0.2202],
        [0.2513, 0.2846, 0.2314, 0.2328],
        [0.2503, 0.2906, 0.2330, 0.2261]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:19:25PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [33][50/468]	Step 15527	lr 0.00722	Loss 0.0043 (0.0103)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:19:47PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [33][100/468]	Step 15577	lr 0.00722	Loss 0.0011 (0.0094)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:20:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [33][150/468]	Step 15627	lr 0.00722	Loss 0.0035 (0.0100)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:20:32PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [33][200/468]	Step 15677	lr 0.00722	Loss 0.0007 (0.0103)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:20:55PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [33][250/468]	Step 15727	lr 0.00722	Loss 0.0272 (0.0102)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:21:17PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [33][300/468]	Step 15777	lr 0.00722	Loss 0.0006 (0.0104)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:21:39PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [33][350/468]	Step 15827	lr 0.00722	Loss 0.0111 (0.0109)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:22:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [33][400/468]	Step 15877	lr 0.00722	Loss 0.0028 (0.0108)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:22:24PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [33][450/468]	Step 15927	lr 0.00722	Loss 0.0234 (0.0107)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:22:32PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [33][468/468]	Step 15945	lr 0.00722	Loss 0.0043 (0.0109)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:22:33PM searchShareStage_trainer.py:212 [INFO] Train: [ 33/49] Final Prec@1 99.6900%
05/24 08:22:36PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [33][50/469]	Step 15946	Loss 0.0233	Prec@(1,5) (99.4%, 100.0%)
05/24 08:22:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [33][100/469]	Step 15946	Loss 0.0302	Prec@(1,5) (99.2%, 100.0%)
05/24 08:22:43PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [33][150/469]	Step 15946	Loss 0.0273	Prec@(1,5) (99.3%, 100.0%)
05/24 08:22:46PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [33][200/469]	Step 15946	Loss 0.0267	Prec@(1,5) (99.3%, 100.0%)
05/24 08:22:50PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [33][250/469]	Step 15946	Loss 0.0261	Prec@(1,5) (99.3%, 100.0%)
05/24 08:22:54PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [33][300/469]	Step 15946	Loss 0.0269	Prec@(1,5) (99.3%, 100.0%)
05/24 08:22:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [33][350/469]	Step 15946	Loss 0.0260	Prec@(1,5) (99.3%, 100.0%)
05/24 08:23:00PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [33][400/469]	Step 15946	Loss 0.0259	Prec@(1,5) (99.3%, 100.0%)
05/24 08:23:04PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [33][450/469]	Step 15946	Loss 0.0267	Prec@(1,5) (99.3%, 100.0%)
05/24 08:23:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [33][468/469]	Step 15946	Loss 0.0262	Prec@(1,5) (99.3%, 100.0%)
05/24 08:23:05PM searchShareStage_trainer.py:251 [INFO] Valid: [ 33/49] Final Prec@1 99.3133%
05/24 08:23:05PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:23:06PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3133%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2163, 0.3087, 0.2593, 0.2157],
        [0.2299, 0.2647, 0.2267, 0.2787]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.3013, 0.2625, 0.2141],
        [0.1940, 0.3036, 0.2812, 0.2211],
        [0.2435, 0.3242, 0.2049, 0.2273]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2738, 0.2447, 0.2358],
        [0.2562, 0.3135, 0.2187, 0.2115],
        [0.2647, 0.2989, 0.2166, 0.2199]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3010, 0.2284, 0.2239],
        [0.2485, 0.2905, 0.2224, 0.2386],
        [0.2476, 0.2945, 0.2364, 0.2215]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.3048, 0.2236, 0.2207],
        [0.2557, 0.3048, 0.2360, 0.2035],
        [0.2488, 0.2885, 0.2179, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2984, 0.2315, 0.2202],
        [0.2513, 0.2845, 0.2314, 0.2328],
        [0.2503, 0.2904, 0.2331, 0.2262]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:23:29PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [34][50/468]	Step 15996	lr 0.00657	Loss 0.0012 (0.0095)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:23:51PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [34][100/468]	Step 16046	lr 0.00657	Loss 0.0004 (0.0142)	Prec@(1,5) (99.5%, 100.0%)	
05/24 08:24:14PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [34][150/468]	Step 16096	lr 0.00657	Loss 0.0044 (0.0131)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:24:36PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [34][200/468]	Step 16146	lr 0.00657	Loss 0.1277 (0.0130)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:24:59PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [34][250/468]	Step 16196	lr 0.00657	Loss 0.0015 (0.0126)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:25:21PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [34][300/468]	Step 16246	lr 0.00657	Loss 0.0018 (0.0117)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:25:44PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [34][350/468]	Step 16296	lr 0.00657	Loss 0.0011 (0.0110)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:26:06PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [34][400/468]	Step 16346	lr 0.00657	Loss 0.0008 (0.0112)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:26:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [34][450/468]	Step 16396	lr 0.00657	Loss 0.0573 (0.0111)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:26:37PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [34][468/468]	Step 16414	lr 0.00657	Loss 0.0052 (0.0110)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:26:37PM searchShareStage_trainer.py:212 [INFO] Train: [ 34/49] Final Prec@1 99.6533%
05/24 08:26:41PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [34][50/469]	Step 16415	Loss 0.0242	Prec@(1,5) (99.3%, 100.0%)
05/24 08:26:44PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [34][100/469]	Step 16415	Loss 0.0285	Prec@(1,5) (99.2%, 100.0%)
05/24 08:26:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [34][150/469]	Step 16415	Loss 0.0293	Prec@(1,5) (99.1%, 100.0%)
05/24 08:26:51PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [34][200/469]	Step 16415	Loss 0.0300	Prec@(1,5) (99.1%, 100.0%)
05/24 08:26:54PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [34][250/469]	Step 16415	Loss 0.0293	Prec@(1,5) (99.1%, 100.0%)
05/24 08:26:58PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [34][300/469]	Step 16415	Loss 0.0291	Prec@(1,5) (99.1%, 100.0%)
05/24 08:27:01PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [34][350/469]	Step 16415	Loss 0.0287	Prec@(1,5) (99.1%, 100.0%)
05/24 08:27:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [34][400/469]	Step 16415	Loss 0.0295	Prec@(1,5) (99.1%, 100.0%)
05/24 08:27:08PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [34][450/469]	Step 16415	Loss 0.0296	Prec@(1,5) (99.1%, 100.0%)
05/24 08:27:10PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [34][468/469]	Step 16415	Loss 0.0296	Prec@(1,5) (99.1%, 100.0%)
05/24 08:27:10PM searchShareStage_trainer.py:251 [INFO] Valid: [ 34/49] Final Prec@1 99.1200%
05/24 08:27:10PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:27:10PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3133%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3086, 0.2594, 0.2157],
        [0.2298, 0.2646, 0.2268, 0.2788]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.3013, 0.2626, 0.2141],
        [0.1940, 0.3036, 0.2813, 0.2211],
        [0.2436, 0.3241, 0.2049, 0.2274]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2738, 0.2447, 0.2359],
        [0.2562, 0.3135, 0.2188, 0.2116],
        [0.2647, 0.2987, 0.2166, 0.2200]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3010, 0.2284, 0.2239],
        [0.2485, 0.2904, 0.2225, 0.2386],
        [0.2476, 0.2944, 0.2364, 0.2216]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.3047, 0.2237, 0.2207],
        [0.2557, 0.3047, 0.2361, 0.2035],
        [0.2488, 0.2884, 0.2179, 0.2449]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2983, 0.2315, 0.2203],
        [0.2513, 0.2844, 0.2315, 0.2328],
        [0.2503, 0.2903, 0.2331, 0.2262]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:27:33PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [35][50/468]	Step 16465	lr 0.00595	Loss 0.0003 (0.0099)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:27:56PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [35][100/468]	Step 16515	lr 0.00595	Loss 0.0008 (0.0077)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:28:18PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [35][150/468]	Step 16565	lr 0.00595	Loss 0.0018 (0.0073)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:28:41PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [35][200/468]	Step 16615	lr 0.00595	Loss 0.0007 (0.0074)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:29:03PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [35][250/468]	Step 16665	lr 0.00595	Loss 0.0488 (0.0074)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:29:26PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [35][300/468]	Step 16715	lr 0.00595	Loss 0.0116 (0.0073)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:29:48PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [35][350/468]	Step 16765	lr 0.00595	Loss 0.0018 (0.0075)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:30:11PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [35][400/468]	Step 16815	lr 0.00595	Loss 0.0501 (0.0082)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:30:33PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [35][450/468]	Step 16865	lr 0.00595	Loss 0.0007 (0.0083)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:30:41PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [35][468/468]	Step 16883	lr 0.00595	Loss 0.0026 (0.0081)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:30:41PM searchShareStage_trainer.py:212 [INFO] Train: [ 35/49] Final Prec@1 99.7433%
05/24 08:30:45PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [35][50/469]	Step 16884	Loss 0.0284	Prec@(1,5) (99.3%, 99.9%)
05/24 08:30:48PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [35][100/469]	Step 16884	Loss 0.0289	Prec@(1,5) (99.2%, 100.0%)
05/24 08:30:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [35][150/469]	Step 16884	Loss 0.0285	Prec@(1,5) (99.2%, 100.0%)
05/24 08:30:55PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [35][200/469]	Step 16884	Loss 0.0269	Prec@(1,5) (99.2%, 100.0%)
05/24 08:30:59PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [35][250/469]	Step 16884	Loss 0.0253	Prec@(1,5) (99.2%, 100.0%)
05/24 08:31:02PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [35][300/469]	Step 16884	Loss 0.0261	Prec@(1,5) (99.2%, 100.0%)
05/24 08:31:06PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [35][350/469]	Step 16884	Loss 0.0273	Prec@(1,5) (99.2%, 100.0%)
05/24 08:31:09PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [35][400/469]	Step 16884	Loss 0.0276	Prec@(1,5) (99.2%, 100.0%)
05/24 08:31:13PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [35][450/469]	Step 16884	Loss 0.0263	Prec@(1,5) (99.2%, 100.0%)
05/24 08:31:14PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [35][468/469]	Step 16884	Loss 0.0271	Prec@(1,5) (99.2%, 100.0%)
05/24 08:31:14PM searchShareStage_trainer.py:251 [INFO] Valid: [ 35/49] Final Prec@1 99.2233%
05/24 08:31:14PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:31:14PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3133%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3085, 0.2595, 0.2158],
        [0.2298, 0.2645, 0.2269, 0.2788]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.3012, 0.2626, 0.2141],
        [0.1939, 0.3035, 0.2814, 0.2211],
        [0.2436, 0.3240, 0.2050, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2737, 0.2447, 0.2360],
        [0.2562, 0.3134, 0.2188, 0.2116],
        [0.2647, 0.2986, 0.2167, 0.2200]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3010, 0.2284, 0.2239],
        [0.2486, 0.2903, 0.2225, 0.2386],
        [0.2477, 0.2943, 0.2365, 0.2216]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.3046, 0.2237, 0.2208],
        [0.2557, 0.3046, 0.2361, 0.2035],
        [0.2488, 0.2883, 0.2179, 0.2450]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2983, 0.2316, 0.2203],
        [0.2513, 0.2843, 0.2315, 0.2329],
        [0.2503, 0.2902, 0.2332, 0.2263]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:31:38PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [36][50/468]	Step 16934	lr 0.00535	Loss 0.0167 (0.0082)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:32:00PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [36][100/468]	Step 16984	lr 0.00535	Loss 0.0324 (0.0084)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:32:22PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [36][150/468]	Step 17034	lr 0.00535	Loss 0.0025 (0.0087)	Prec@(1,5) (99.6%, 100.0%)	
05/24 08:32:45PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [36][200/468]	Step 17084	lr 0.00535	Loss 0.0013 (0.0086)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:33:07PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [36][250/468]	Step 17134	lr 0.00535	Loss 0.0020 (0.0091)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:33:30PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [36][300/468]	Step 17184	lr 0.00535	Loss 0.0667 (0.0087)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:33:52PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [36][350/468]	Step 17234	lr 0.00535	Loss 0.0040 (0.0086)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:34:15PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [36][400/468]	Step 17284	lr 0.00535	Loss 0.0014 (0.0085)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:34:37PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [36][450/468]	Step 17334	lr 0.00535	Loss 0.0310 (0.0086)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:34:45PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [36][468/468]	Step 17352	lr 0.00535	Loss 0.0076 (0.0085)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:34:45PM searchShareStage_trainer.py:212 [INFO] Train: [ 36/49] Final Prec@1 99.7200%
05/24 08:34:49PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [36][50/469]	Step 17353	Loss 0.0233	Prec@(1,5) (99.4%, 100.0%)
05/24 08:34:53PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [36][100/469]	Step 17353	Loss 0.0284	Prec@(1,5) (99.3%, 100.0%)
05/24 08:34:56PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [36][150/469]	Step 17353	Loss 0.0299	Prec@(1,5) (99.2%, 100.0%)
05/24 08:34:59PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [36][200/469]	Step 17353	Loss 0.0269	Prec@(1,5) (99.2%, 100.0%)
05/24 08:35:03PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [36][250/469]	Step 17353	Loss 0.0278	Prec@(1,5) (99.2%, 100.0%)
05/24 08:35:06PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [36][300/469]	Step 17353	Loss 0.0286	Prec@(1,5) (99.2%, 100.0%)
05/24 08:35:10PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [36][350/469]	Step 17353	Loss 0.0284	Prec@(1,5) (99.2%, 100.0%)
05/24 08:35:13PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [36][400/469]	Step 17353	Loss 0.0284	Prec@(1,5) (99.2%, 100.0%)
05/24 08:35:17PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [36][450/469]	Step 17353	Loss 0.0283	Prec@(1,5) (99.2%, 100.0%)
05/24 08:35:18PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [36][468/469]	Step 17353	Loss 0.0275	Prec@(1,5) (99.2%, 100.0%)
05/24 08:35:18PM searchShareStage_trainer.py:251 [INFO] Valid: [ 36/49] Final Prec@1 99.2233%
05/24 08:35:18PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:35:18PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3133%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3084, 0.2596, 0.2158],
        [0.2298, 0.2644, 0.2269, 0.2789]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3012, 0.2626, 0.2142],
        [0.1939, 0.3035, 0.2815, 0.2211],
        [0.2436, 0.3239, 0.2050, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2737, 0.2445, 0.2361],
        [0.2562, 0.3133, 0.2188, 0.2116],
        [0.2647, 0.2985, 0.2167, 0.2201]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3009, 0.2285, 0.2239],
        [0.2486, 0.2902, 0.2226, 0.2387],
        [0.2476, 0.2942, 0.2365, 0.2216]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.3046, 0.2238, 0.2207],
        [0.2557, 0.3045, 0.2362, 0.2036],
        [0.2488, 0.2881, 0.2180, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2982, 0.2317, 0.2203],
        [0.2513, 0.2842, 0.2316, 0.2329],
        [0.2504, 0.2901, 0.2333, 0.2263]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:35:42PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [37][50/468]	Step 17403	lr 0.00479	Loss 0.0024 (0.0068)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:36:04PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [37][100/468]	Step 17453	lr 0.00479	Loss 0.0015 (0.0067)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:36:27PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [37][150/468]	Step 17503	lr 0.00479	Loss 0.0054 (0.0069)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:36:49PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [37][200/468]	Step 17553	lr 0.00479	Loss 0.0017 (0.0072)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:37:11PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [37][250/468]	Step 17603	lr 0.00479	Loss 0.0040 (0.0072)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:37:34PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [37][300/468]	Step 17653	lr 0.00479	Loss 0.0024 (0.0071)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:37:56PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [37][350/468]	Step 17703	lr 0.00479	Loss 0.0018 (0.0067)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:38:19PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [37][400/468]	Step 17753	lr 0.00479	Loss 0.0011 (0.0068)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:38:41PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [37][450/468]	Step 17803	lr 0.00479	Loss 0.0910 (0.0072)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:38:49PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [37][468/468]	Step 17821	lr 0.00479	Loss 0.0012 (0.0073)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:38:50PM searchShareStage_trainer.py:212 [INFO] Train: [ 37/49] Final Prec@1 99.7767%
05/24 08:38:53PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [37][50/469]	Step 17822	Loss 0.0239	Prec@(1,5) (99.5%, 100.0%)
05/24 08:38:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [37][100/469]	Step 17822	Loss 0.0261	Prec@(1,5) (99.3%, 100.0%)
05/24 08:39:00PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [37][150/469]	Step 17822	Loss 0.0271	Prec@(1,5) (99.3%, 100.0%)
05/24 08:39:03PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [37][200/469]	Step 17822	Loss 0.0311	Prec@(1,5) (99.2%, 100.0%)
05/24 08:39:07PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [37][250/469]	Step 17822	Loss 0.0307	Prec@(1,5) (99.2%, 100.0%)
05/24 08:39:10PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [37][300/469]	Step 17822	Loss 0.0287	Prec@(1,5) (99.2%, 100.0%)
05/24 08:39:14PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [37][350/469]	Step 17822	Loss 0.0268	Prec@(1,5) (99.3%, 100.0%)
05/24 08:39:17PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [37][400/469]	Step 17822	Loss 0.0265	Prec@(1,5) (99.3%, 100.0%)
05/24 08:39:21PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [37][450/469]	Step 17822	Loss 0.0267	Prec@(1,5) (99.2%, 100.0%)
05/24 08:39:22PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [37][468/469]	Step 17822	Loss 0.0271	Prec@(1,5) (99.2%, 100.0%)
05/24 08:39:22PM searchShareStage_trainer.py:251 [INFO] Valid: [ 37/49] Final Prec@1 99.2367%
05/24 08:39:22PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:39:23PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3133%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3083, 0.2596, 0.2158],
        [0.2298, 0.2643, 0.2269, 0.2790]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3011, 0.2627, 0.2142],
        [0.1939, 0.3035, 0.2815, 0.2212],
        [0.2436, 0.3238, 0.2051, 0.2275]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2736, 0.2445, 0.2362],
        [0.2562, 0.3132, 0.2189, 0.2117],
        [0.2648, 0.2984, 0.2167, 0.2201]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3009, 0.2285, 0.2239],
        [0.2486, 0.2901, 0.2226, 0.2387],
        [0.2477, 0.2941, 0.2366, 0.2217]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3045, 0.2238, 0.2208],
        [0.2557, 0.3045, 0.2362, 0.2036],
        [0.2488, 0.2881, 0.2180, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2981, 0.2317, 0.2203],
        [0.2513, 0.2841, 0.2316, 0.2330],
        [0.2504, 0.2900, 0.2333, 0.2263]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:39:46PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [38][50/468]	Step 17872	lr 0.00425	Loss 0.0003 (0.0071)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:40:08PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [38][100/468]	Step 17922	lr 0.00425	Loss 0.0003 (0.0082)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:40:31PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [38][150/468]	Step 17972	lr 0.00425	Loss 0.0080 (0.0071)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:40:53PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [38][200/468]	Step 18022	lr 0.00425	Loss 0.0008 (0.0073)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:41:16PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [38][250/468]	Step 18072	lr 0.00425	Loss 0.0003 (0.0069)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:41:38PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [38][300/468]	Step 18122	lr 0.00425	Loss 0.0006 (0.0067)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:42:00PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [38][350/468]	Step 18172	lr 0.00425	Loss 0.0283 (0.0071)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:42:23PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [38][400/468]	Step 18222	lr 0.00425	Loss 0.0008 (0.0075)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:42:45PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [38][450/468]	Step 18272	lr 0.00425	Loss 0.0015 (0.0077)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:42:53PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [38][468/468]	Step 18290	lr 0.00425	Loss 0.0026 (0.0076)	Prec@(1,5) (99.7%, 100.0%)	
05/24 08:42:54PM searchShareStage_trainer.py:212 [INFO] Train: [ 38/49] Final Prec@1 99.7500%
05/24 08:42:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [38][50/469]	Step 18291	Loss 0.0271	Prec@(1,5) (99.3%, 100.0%)
05/24 08:43:01PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [38][100/469]	Step 18291	Loss 0.0287	Prec@(1,5) (99.2%, 100.0%)
05/24 08:43:04PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [38][150/469]	Step 18291	Loss 0.0286	Prec@(1,5) (99.3%, 100.0%)
05/24 08:43:08PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [38][200/469]	Step 18291	Loss 0.0259	Prec@(1,5) (99.3%, 100.0%)
05/24 08:43:11PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [38][250/469]	Step 18291	Loss 0.0259	Prec@(1,5) (99.3%, 100.0%)
05/24 08:43:15PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [38][300/469]	Step 18291	Loss 0.0273	Prec@(1,5) (99.3%, 100.0%)
05/24 08:43:18PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [38][350/469]	Step 18291	Loss 0.0267	Prec@(1,5) (99.3%, 100.0%)
05/24 08:43:22PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [38][400/469]	Step 18291	Loss 0.0261	Prec@(1,5) (99.3%, 100.0%)
05/24 08:43:25PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [38][450/469]	Step 18291	Loss 0.0251	Prec@(1,5) (99.3%, 100.0%)
05/24 08:43:27PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [38][468/469]	Step 18291	Loss 0.0255	Prec@(1,5) (99.3%, 100.0%)
05/24 08:43:27PM searchShareStage_trainer.py:251 [INFO] Valid: [ 38/49] Final Prec@1 99.3300%
05/24 08:43:27PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:43:27PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3300%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3082, 0.2597, 0.2159],
        [0.2298, 0.2642, 0.2270, 0.2790]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3011, 0.2628, 0.2142],
        [0.1938, 0.3034, 0.2816, 0.2212],
        [0.2436, 0.3237, 0.2051, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2735, 0.2445, 0.2362],
        [0.2562, 0.3132, 0.2189, 0.2117],
        [0.2648, 0.2983, 0.2168, 0.2201]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3009, 0.2285, 0.2239],
        [0.2486, 0.2901, 0.2226, 0.2387],
        [0.2477, 0.2940, 0.2366, 0.2217]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3045, 0.2238, 0.2208],
        [0.2557, 0.3044, 0.2363, 0.2037],
        [0.2489, 0.2879, 0.2180, 0.2451]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2981, 0.2318, 0.2204],
        [0.2514, 0.2840, 0.2316, 0.2330],
        [0.2504, 0.2899, 0.2333, 0.2264]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:43:50PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [39][50/468]	Step 18341	lr 0.00375	Loss 0.0050 (0.0059)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:44:13PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [39][100/468]	Step 18391	lr 0.00375	Loss 0.0078 (0.0064)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:44:35PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [39][150/468]	Step 18441	lr 0.00375	Loss 0.0005 (0.0062)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:44:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [39][200/468]	Step 18491	lr 0.00375	Loss 0.0051 (0.0064)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:45:20PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [39][250/468]	Step 18541	lr 0.00375	Loss 0.0012 (0.0063)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:45:43PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [39][300/468]	Step 18591	lr 0.00375	Loss 0.0003 (0.0062)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:46:05PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [39][350/468]	Step 18641	lr 0.00375	Loss 0.0256 (0.0058)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:46:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [39][400/468]	Step 18691	lr 0.00375	Loss 0.0093 (0.0059)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:46:50PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [39][450/468]	Step 18741	lr 0.00375	Loss 0.0020 (0.0058)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:46:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [39][468/468]	Step 18759	lr 0.00375	Loss 0.0047 (0.0057)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:46:59PM searchShareStage_trainer.py:212 [INFO] Train: [ 39/49] Final Prec@1 99.8400%
05/24 08:47:02PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [39][50/469]	Step 18760	Loss 0.0195	Prec@(1,5) (99.4%, 100.0%)
05/24 08:47:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [39][100/469]	Step 18760	Loss 0.0220	Prec@(1,5) (99.3%, 100.0%)
05/24 08:47:09PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [39][150/469]	Step 18760	Loss 0.0240	Prec@(1,5) (99.3%, 100.0%)
05/24 08:47:12PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [39][200/469]	Step 18760	Loss 0.0244	Prec@(1,5) (99.3%, 100.0%)
05/24 08:47:16PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [39][250/469]	Step 18760	Loss 0.0235	Prec@(1,5) (99.4%, 100.0%)
05/24 08:47:19PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [39][300/469]	Step 18760	Loss 0.0225	Prec@(1,5) (99.4%, 100.0%)
05/24 08:47:23PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [39][350/469]	Step 18760	Loss 0.0234	Prec@(1,5) (99.4%, 100.0%)
05/24 08:47:26PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [39][400/469]	Step 18760	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/24 08:47:30PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [39][450/469]	Step 18760	Loss 0.0246	Prec@(1,5) (99.4%, 100.0%)
05/24 08:47:31PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [39][468/469]	Step 18760	Loss 0.0249	Prec@(1,5) (99.3%, 100.0%)
05/24 08:47:31PM searchShareStage_trainer.py:251 [INFO] Valid: [ 39/49] Final Prec@1 99.3433%
05/24 08:47:31PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:47:32PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3433%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3082, 0.2597, 0.2159],
        [0.2298, 0.2641, 0.2270, 0.2791]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3010, 0.2628, 0.2142],
        [0.1938, 0.3034, 0.2817, 0.2212],
        [0.2436, 0.3236, 0.2051, 0.2276]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2734, 0.2445, 0.2363],
        [0.2562, 0.3131, 0.2189, 0.2117],
        [0.2648, 0.2982, 0.2168, 0.2202]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3009, 0.2286, 0.2239],
        [0.2486, 0.2900, 0.2227, 0.2388],
        [0.2477, 0.2939, 0.2367, 0.2218]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3044, 0.2238, 0.2208],
        [0.2557, 0.3043, 0.2363, 0.2037],
        [0.2489, 0.2878, 0.2181, 0.2452]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2980, 0.2318, 0.2204],
        [0.2514, 0.2839, 0.2317, 0.2331],
        [0.2504, 0.2898, 0.2334, 0.2264]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:47:55PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [40][50/468]	Step 18810	lr 0.00329	Loss 0.0008 (0.0079)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:48:17PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [40][100/468]	Step 18860	lr 0.00329	Loss 0.0005 (0.0074)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:48:40PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [40][150/468]	Step 18910	lr 0.00329	Loss 0.0016 (0.0070)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:49:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [40][200/468]	Step 18960	lr 0.00329	Loss 0.0005 (0.0068)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:49:25PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [40][250/468]	Step 19010	lr 0.00329	Loss 0.0004 (0.0064)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:49:47PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [40][300/468]	Step 19060	lr 0.00329	Loss 0.0014 (0.0066)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:50:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [40][350/468]	Step 19110	lr 0.00329	Loss 0.0003 (0.0061)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:50:32PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [40][400/468]	Step 19160	lr 0.00329	Loss 0.0003 (0.0060)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:50:55PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [40][450/468]	Step 19210	lr 0.00329	Loss 0.0024 (0.0058)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:51:03PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [40][468/468]	Step 19228	lr 0.00329	Loss 0.0261 (0.0057)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:51:03PM searchShareStage_trainer.py:212 [INFO] Train: [ 40/49] Final Prec@1 99.8400%
05/24 08:51:07PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [40][50/469]	Step 19229	Loss 0.0196	Prec@(1,5) (99.6%, 100.0%)
05/24 08:51:10PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [40][100/469]	Step 19229	Loss 0.0239	Prec@(1,5) (99.4%, 100.0%)
05/24 08:51:13PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [40][150/469]	Step 19229	Loss 0.0262	Prec@(1,5) (99.3%, 100.0%)
05/24 08:51:17PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [40][200/469]	Step 19229	Loss 0.0240	Prec@(1,5) (99.4%, 100.0%)
05/24 08:51:20PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [40][250/469]	Step 19229	Loss 0.0228	Prec@(1,5) (99.4%, 100.0%)
05/24 08:51:24PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [40][300/469]	Step 19229	Loss 0.0240	Prec@(1,5) (99.4%, 100.0%)
05/24 08:51:27PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [40][350/469]	Step 19229	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/24 08:51:31PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [40][400/469]	Step 19229	Loss 0.0233	Prec@(1,5) (99.4%, 100.0%)
05/24 08:51:34PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [40][450/469]	Step 19229	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/24 08:51:36PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [40][468/469]	Step 19229	Loss 0.0241	Prec@(1,5) (99.4%, 100.0%)
05/24 08:51:36PM searchShareStage_trainer.py:251 [INFO] Valid: [ 40/49] Final Prec@1 99.3800%
05/24 08:51:36PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:51:36PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3081, 0.2598, 0.2160],
        [0.2298, 0.2640, 0.2271, 0.2791]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3009, 0.2628, 0.2143],
        [0.1938, 0.3033, 0.2817, 0.2212],
        [0.2436, 0.3236, 0.2052, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2734, 0.2445, 0.2364],
        [0.2563, 0.3130, 0.2189, 0.2118],
        [0.2648, 0.2981, 0.2168, 0.2202]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3008, 0.2286, 0.2239],
        [0.2486, 0.2899, 0.2227, 0.2388],
        [0.2477, 0.2938, 0.2367, 0.2218]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3043, 0.2238, 0.2208],
        [0.2557, 0.3042, 0.2363, 0.2037],
        [0.2489, 0.2878, 0.2181, 0.2452]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2980, 0.2318, 0.2204],
        [0.2514, 0.2838, 0.2317, 0.2331],
        [0.2504, 0.2897, 0.2334, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:51:59PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [41][50/468]	Step 19279	lr 0.00287	Loss 0.0033 (0.0034)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:52:22PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [41][100/468]	Step 19329	lr 0.00287	Loss 0.0123 (0.0039)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:52:44PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [41][150/468]	Step 19379	lr 0.00287	Loss 0.0016 (0.0041)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:53:06PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [41][200/468]	Step 19429	lr 0.00287	Loss 0.0003 (0.0043)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:53:29PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [41][250/468]	Step 19479	lr 0.00287	Loss 0.0041 (0.0046)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:53:51PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [41][300/468]	Step 19529	lr 0.00287	Loss 0.0002 (0.0043)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:54:14PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [41][350/468]	Step 19579	lr 0.00287	Loss 0.0005 (0.0046)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:54:36PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [41][400/468]	Step 19629	lr 0.00287	Loss 0.0003 (0.0047)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:54:59PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [41][450/468]	Step 19679	lr 0.00287	Loss 0.0016 (0.0047)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:55:07PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [41][468/468]	Step 19697	lr 0.00287	Loss 0.0035 (0.0046)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:55:07PM searchShareStage_trainer.py:212 [INFO] Train: [ 41/49] Final Prec@1 99.8800%
05/24 08:55:11PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [41][50/469]	Step 19698	Loss 0.0171	Prec@(1,5) (99.6%, 100.0%)
05/24 08:55:14PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [41][100/469]	Step 19698	Loss 0.0231	Prec@(1,5) (99.4%, 100.0%)
05/24 08:55:17PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [41][150/469]	Step 19698	Loss 0.0261	Prec@(1,5) (99.3%, 100.0%)
05/24 08:55:21PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [41][200/469]	Step 19698	Loss 0.0265	Prec@(1,5) (99.3%, 100.0%)
05/24 08:55:24PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [41][250/469]	Step 19698	Loss 0.0242	Prec@(1,5) (99.4%, 100.0%)
05/24 08:55:28PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [41][300/469]	Step 19698	Loss 0.0257	Prec@(1,5) (99.3%, 100.0%)
05/24 08:55:32PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [41][350/469]	Step 19698	Loss 0.0245	Prec@(1,5) (99.4%, 100.0%)
05/24 08:55:35PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [41][400/469]	Step 19698	Loss 0.0238	Prec@(1,5) (99.4%, 100.0%)
05/24 08:55:39PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [41][450/469]	Step 19698	Loss 0.0234	Prec@(1,5) (99.4%, 100.0%)
05/24 08:55:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [41][468/469]	Step 19698	Loss 0.0230	Prec@(1,5) (99.4%, 100.0%)
05/24 08:55:40PM searchShareStage_trainer.py:251 [INFO] Valid: [ 41/49] Final Prec@1 99.3867%
05/24 08:55:40PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:55:41PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3867%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3080, 0.2598, 0.2160],
        [0.2298, 0.2639, 0.2271, 0.2792]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2219, 0.3009, 0.2629, 0.2143],
        [0.1937, 0.3033, 0.2817, 0.2212],
        [0.2437, 0.3234, 0.2052, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2734, 0.2445, 0.2364],
        [0.2563, 0.3130, 0.2190, 0.2118],
        [0.2649, 0.2980, 0.2169, 0.2203]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3008, 0.2286, 0.2239],
        [0.2486, 0.2898, 0.2227, 0.2388],
        [0.2477, 0.2937, 0.2368, 0.2218]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3043, 0.2238, 0.2209],
        [0.2557, 0.3041, 0.2364, 0.2037],
        [0.2489, 0.2877, 0.2181, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2979, 0.2319, 0.2204],
        [0.2514, 0.2838, 0.2317, 0.2331],
        [0.2504, 0.2896, 0.2335, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 08:56:04PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [42][50/468]	Step 19748	lr 0.00248	Loss 0.0002 (0.0028)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:56:26PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [42][100/468]	Step 19798	lr 0.00248	Loss 0.0002 (0.0044)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:56:49PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [42][150/468]	Step 19848	lr 0.00248	Loss 0.0007 (0.0060)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:57:11PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [42][200/468]	Step 19898	lr 0.00248	Loss 0.0020 (0.0056)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:57:34PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [42][250/468]	Step 19948	lr 0.00248	Loss 0.0002 (0.0053)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:57:56PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [42][300/468]	Step 19998	lr 0.00248	Loss 0.0007 (0.0053)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:58:18PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [42][350/468]	Step 20048	lr 0.00248	Loss 0.0010 (0.0051)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:58:41PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [42][400/468]	Step 20098	lr 0.00248	Loss 0.0004 (0.0056)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:59:03PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [42][450/468]	Step 20148	lr 0.00248	Loss 0.0020 (0.0055)	Prec@(1,5) (99.9%, 100.0%)	
05/24 08:59:11PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [42][468/468]	Step 20166	lr 0.00248	Loss 0.0196 (0.0055)	Prec@(1,5) (99.8%, 100.0%)	
05/24 08:59:12PM searchShareStage_trainer.py:212 [INFO] Train: [ 42/49] Final Prec@1 99.8500%
05/24 08:59:16PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [42][50/469]	Step 20167	Loss 0.0347	Prec@(1,5) (99.2%, 100.0%)
05/24 08:59:18PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [42][100/469]	Step 20167	Loss 0.0320	Prec@(1,5) (99.3%, 100.0%)
05/24 08:59:22PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [42][150/469]	Step 20167	Loss 0.0305	Prec@(1,5) (99.3%, 100.0%)
05/24 08:59:26PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [42][200/469]	Step 20167	Loss 0.0293	Prec@(1,5) (99.3%, 100.0%)
05/24 08:59:29PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [42][250/469]	Step 20167	Loss 0.0281	Prec@(1,5) (99.3%, 100.0%)
05/24 08:59:33PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [42][300/469]	Step 20167	Loss 0.0268	Prec@(1,5) (99.3%, 100.0%)
05/24 08:59:36PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [42][350/469]	Step 20167	Loss 0.0263	Prec@(1,5) (99.3%, 100.0%)
05/24 08:59:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [42][400/469]	Step 20167	Loss 0.0262	Prec@(1,5) (99.3%, 100.0%)
05/24 08:59:43PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [42][450/469]	Step 20167	Loss 0.0258	Prec@(1,5) (99.3%, 100.0%)
05/24 08:59:44PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [42][468/469]	Step 20167	Loss 0.0252	Prec@(1,5) (99.3%, 100.0%)
05/24 08:59:45PM searchShareStage_trainer.py:251 [INFO] Valid: [ 42/49] Final Prec@1 99.3333%
05/24 08:59:45PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 08:59:45PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3867%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3079, 0.2599, 0.2160],
        [0.2298, 0.2639, 0.2271, 0.2792]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2220, 0.3008, 0.2629, 0.2143],
        [0.1937, 0.3032, 0.2818, 0.2212],
        [0.2436, 0.3234, 0.2052, 0.2277]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2734, 0.2445, 0.2365],
        [0.2563, 0.3129, 0.2190, 0.2118],
        [0.2649, 0.2979, 0.2169, 0.2203]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3008, 0.2286, 0.2239],
        [0.2486, 0.2898, 0.2227, 0.2388],
        [0.2477, 0.2936, 0.2368, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3042, 0.2238, 0.2209],
        [0.2558, 0.3041, 0.2364, 0.2038],
        [0.2489, 0.2876, 0.2182, 0.2453]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2979, 0.2319, 0.2204],
        [0.2514, 0.2837, 0.2317, 0.2332],
        [0.2504, 0.2895, 0.2335, 0.2265]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 09:00:08PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [43][50/468]	Step 20217	lr 0.00214	Loss 0.0053 (0.0058)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:00:30PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [43][100/468]	Step 20267	lr 0.00214	Loss 0.0040 (0.0043)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:00:53PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [43][150/468]	Step 20317	lr 0.00214	Loss 0.0173 (0.0042)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:01:15PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [43][200/468]	Step 20367	lr 0.00214	Loss 0.0008 (0.0037)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:01:37PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [43][250/468]	Step 20417	lr 0.00214	Loss 0.0006 (0.0040)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:02:00PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [43][300/468]	Step 20467	lr 0.00214	Loss 0.0077 (0.0041)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:02:22PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [43][350/468]	Step 20517	lr 0.00214	Loss 0.0006 (0.0044)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:02:45PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [43][400/468]	Step 20567	lr 0.00214	Loss 0.0015 (0.0045)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:03:07PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [43][450/468]	Step 20617	lr 0.00214	Loss 0.0017 (0.0046)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:03:15PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [43][468/468]	Step 20635	lr 0.00214	Loss 0.0479 (0.0048)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:03:16PM searchShareStage_trainer.py:212 [INFO] Train: [ 43/49] Final Prec@1 99.8400%
05/24 09:03:19PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [43][50/469]	Step 20636	Loss 0.0239	Prec@(1,5) (99.4%, 100.0%)
05/24 09:03:22PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [43][100/469]	Step 20636	Loss 0.0197	Prec@(1,5) (99.5%, 100.0%)
05/24 09:03:26PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [43][150/469]	Step 20636	Loss 0.0232	Prec@(1,5) (99.4%, 100.0%)
05/24 09:03:29PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [43][200/469]	Step 20636	Loss 0.0253	Prec@(1,5) (99.4%, 100.0%)
05/24 09:03:33PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [43][250/469]	Step 20636	Loss 0.0248	Prec@(1,5) (99.4%, 100.0%)
05/24 09:03:36PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [43][300/469]	Step 20636	Loss 0.0243	Prec@(1,5) (99.4%, 100.0%)
05/24 09:03:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [43][350/469]	Step 20636	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/24 09:03:43PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [43][400/469]	Step 20636	Loss 0.0238	Prec@(1,5) (99.4%, 100.0%)
05/24 09:03:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [43][450/469]	Step 20636	Loss 0.0243	Prec@(1,5) (99.3%, 100.0%)
05/24 09:03:48PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [43][468/469]	Step 20636	Loss 0.0243	Prec@(1,5) (99.3%, 100.0%)
05/24 09:03:48PM searchShareStage_trainer.py:251 [INFO] Valid: [ 43/49] Final Prec@1 99.3400%
05/24 09:03:48PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 09:03:49PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3867%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3078, 0.2599, 0.2161],
        [0.2298, 0.2638, 0.2272, 0.2793]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2219, 0.3008, 0.2630, 0.2143],
        [0.1937, 0.3032, 0.2818, 0.2213],
        [0.2437, 0.3233, 0.2052, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2734, 0.2445, 0.2365],
        [0.2563, 0.3128, 0.2190, 0.2118],
        [0.2649, 0.2979, 0.2169, 0.2203]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3007, 0.2287, 0.2239],
        [0.2486, 0.2897, 0.2228, 0.2389],
        [0.2477, 0.2936, 0.2368, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3042, 0.2238, 0.2209],
        [0.2558, 0.3040, 0.2365, 0.2038],
        [0.2489, 0.2875, 0.2182, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2978, 0.2319, 0.2205],
        [0.2514, 0.2836, 0.2318, 0.2332],
        [0.2504, 0.2894, 0.2336, 0.2266]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 09:04:12PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [44][50/468]	Step 20686	lr 0.00184	Loss 0.0012 (0.0058)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:04:34PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [44][100/468]	Step 20736	lr 0.00184	Loss 0.0015 (0.0072)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:04:57PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [44][150/468]	Step 20786	lr 0.00184	Loss 0.0004 (0.0062)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:05:19PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [44][200/468]	Step 20836	lr 0.00184	Loss 0.0056 (0.0059)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:05:42PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [44][250/468]	Step 20886	lr 0.00184	Loss 0.0009 (0.0053)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:06:04PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [44][300/468]	Step 20936	lr 0.00184	Loss 0.0018 (0.0049)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:06:26PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [44][350/468]	Step 20986	lr 0.00184	Loss 0.0005 (0.0047)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:06:49PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [44][400/468]	Step 21036	lr 0.00184	Loss 0.0006 (0.0048)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:07:11PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [44][450/468]	Step 21086	lr 0.00184	Loss 0.0190 (0.0051)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:07:19PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [44][468/468]	Step 21104	lr 0.00184	Loss 0.0004 (0.0051)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:07:20PM searchShareStage_trainer.py:212 [INFO] Train: [ 44/49] Final Prec@1 99.8300%
05/24 09:07:24PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [44][50/469]	Step 21105	Loss 0.0285	Prec@(1,5) (99.1%, 100.0%)
05/24 09:07:26PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [44][100/469]	Step 21105	Loss 0.0291	Prec@(1,5) (99.2%, 100.0%)
05/24 09:07:30PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [44][150/469]	Step 21105	Loss 0.0275	Prec@(1,5) (99.2%, 100.0%)
05/24 09:07:33PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [44][200/469]	Step 21105	Loss 0.0265	Prec@(1,5) (99.2%, 100.0%)
05/24 09:07:37PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [44][250/469]	Step 21105	Loss 0.0244	Prec@(1,5) (99.3%, 100.0%)
05/24 09:07:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [44][300/469]	Step 21105	Loss 0.0245	Prec@(1,5) (99.3%, 100.0%)
05/24 09:07:44PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [44][350/469]	Step 21105	Loss 0.0249	Prec@(1,5) (99.3%, 100.0%)
05/24 09:07:48PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [44][400/469]	Step 21105	Loss 0.0249	Prec@(1,5) (99.3%, 100.0%)
05/24 09:07:51PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [44][450/469]	Step 21105	Loss 0.0252	Prec@(1,5) (99.3%, 100.0%)
05/24 09:07:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [44][468/469]	Step 21105	Loss 0.0251	Prec@(1,5) (99.3%, 100.0%)
05/24 09:07:52PM searchShareStage_trainer.py:251 [INFO] Valid: [ 44/49] Final Prec@1 99.2867%
05/24 09:07:52PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 09:07:53PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3867%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3077, 0.2600, 0.2161],
        [0.2298, 0.2637, 0.2272, 0.2793]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2219, 0.3008, 0.2630, 0.2144],
        [0.1937, 0.3032, 0.2819, 0.2213],
        [0.2437, 0.3233, 0.2053, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2733, 0.2444, 0.2365],
        [0.2563, 0.3128, 0.2190, 0.2119],
        [0.2649, 0.2978, 0.2169, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3007, 0.2287, 0.2239],
        [0.2487, 0.2896, 0.2228, 0.2389],
        [0.2477, 0.2935, 0.2369, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3041, 0.2239, 0.2210],
        [0.2558, 0.3039, 0.2365, 0.2038],
        [0.2489, 0.2875, 0.2182, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2978, 0.2320, 0.2205],
        [0.2514, 0.2835, 0.2318, 0.2332],
        [0.2504, 0.2894, 0.2336, 0.2266]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 09:08:16PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [45][50/468]	Step 21155	lr 0.00159	Loss 0.0036 (0.0019)	Prec@(1,5) (100.0%, 100.0%)	
05/24 09:08:39PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [45][100/468]	Step 21205	lr 0.00159	Loss 0.0012 (0.0022)	Prec@(1,5) (100.0%, 100.0%)	
05/24 09:09:01PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [45][150/468]	Step 21255	lr 0.00159	Loss 0.0024 (0.0034)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:09:23PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [45][200/468]	Step 21305	lr 0.00159	Loss 0.0002 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:09:46PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [45][250/468]	Step 21355	lr 0.00159	Loss 0.0017 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:10:08PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [45][300/468]	Step 21405	lr 0.00159	Loss 0.0109 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:10:31PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [45][350/468]	Step 21455	lr 0.00159	Loss 0.0004 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:10:53PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [45][400/468]	Step 21505	lr 0.00159	Loss 0.0027 (0.0036)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:11:16PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [45][450/468]	Step 21555	lr 0.00159	Loss 0.0006 (0.0037)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:11:24PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [45][468/468]	Step 21573	lr 0.00159	Loss 0.0005 (0.0036)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:11:24PM searchShareStage_trainer.py:212 [INFO] Train: [ 45/49] Final Prec@1 99.9067%
05/24 09:11:28PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [45][50/469]	Step 21574	Loss 0.0265	Prec@(1,5) (99.2%, 100.0%)
05/24 09:11:31PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [45][100/469]	Step 21574	Loss 0.0233	Prec@(1,5) (99.3%, 100.0%)
05/24 09:11:35PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [45][150/469]	Step 21574	Loss 0.0244	Prec@(1,5) (99.3%, 100.0%)
05/24 09:11:38PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [45][200/469]	Step 21574	Loss 0.0249	Prec@(1,5) (99.3%, 100.0%)
05/24 09:11:42PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [45][250/469]	Step 21574	Loss 0.0247	Prec@(1,5) (99.3%, 100.0%)
05/24 09:11:45PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [45][300/469]	Step 21574	Loss 0.0243	Prec@(1,5) (99.3%, 100.0%)
05/24 09:11:49PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [45][350/469]	Step 21574	Loss 0.0239	Prec@(1,5) (99.4%, 100.0%)
05/24 09:11:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [45][400/469]	Step 21574	Loss 0.0247	Prec@(1,5) (99.3%, 100.0%)
05/24 09:11:56PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [45][450/469]	Step 21574	Loss 0.0247	Prec@(1,5) (99.3%, 100.0%)
05/24 09:11:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [45][468/469]	Step 21574	Loss 0.0253	Prec@(1,5) (99.3%, 100.0%)
05/24 09:11:57PM searchShareStage_trainer.py:251 [INFO] Valid: [ 45/49] Final Prec@1 99.3067%
05/24 09:11:57PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 09:11:57PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.3867%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3077, 0.2600, 0.2161],
        [0.2298, 0.2636, 0.2272, 0.2793]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2219, 0.3007, 0.2630, 0.2144],
        [0.1937, 0.3031, 0.2819, 0.2213],
        [0.2437, 0.3231, 0.2053, 0.2278]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2732, 0.2444, 0.2366],
        [0.2563, 0.3128, 0.2190, 0.2119],
        [0.2649, 0.2977, 0.2170, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3007, 0.2287, 0.2239],
        [0.2487, 0.2896, 0.2228, 0.2389],
        [0.2478, 0.2934, 0.2369, 0.2219]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3041, 0.2239, 0.2210],
        [0.2558, 0.3039, 0.2365, 0.2038],
        [0.2489, 0.2874, 0.2183, 0.2454]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2977, 0.2320, 0.2205],
        [0.2514, 0.2835, 0.2318, 0.2333],
        [0.2505, 0.2893, 0.2336, 0.2266]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 09:12:20PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [46][50/468]	Step 21624	lr 0.00138	Loss 0.0030 (0.0028)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:12:43PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [46][100/468]	Step 21674	lr 0.00138	Loss 0.0053 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:13:05PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [46][150/468]	Step 21724	lr 0.00138	Loss 0.0004 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:13:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [46][200/468]	Step 21774	lr 0.00138	Loss 0.0005 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:13:50PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [46][250/468]	Step 21824	lr 0.00138	Loss 0.0034 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:14:13PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [46][300/468]	Step 21874	lr 0.00138	Loss 0.0003 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:14:35PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [46][350/468]	Step 21924	lr 0.00138	Loss 0.0004 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:14:58PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [46][400/468]	Step 21974	lr 0.00138	Loss 0.0016 (0.0036)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:15:20PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [46][450/468]	Step 22024	lr 0.00138	Loss 0.0016 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:15:28PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [46][468/468]	Step 22042	lr 0.00138	Loss 0.0004 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:15:28PM searchShareStage_trainer.py:212 [INFO] Train: [ 46/49] Final Prec@1 99.8900%
05/24 09:15:32PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [46][50/469]	Step 22043	Loss 0.0183	Prec@(1,5) (99.5%, 100.0%)
05/24 09:15:35PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [46][100/469]	Step 22043	Loss 0.0206	Prec@(1,5) (99.4%, 100.0%)
05/24 09:15:39PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [46][150/469]	Step 22043	Loss 0.0217	Prec@(1,5) (99.4%, 100.0%)
05/24 09:15:43PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [46][200/469]	Step 22043	Loss 0.0242	Prec@(1,5) (99.4%, 100.0%)
05/24 09:15:46PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [46][250/469]	Step 22043	Loss 0.0237	Prec@(1,5) (99.4%, 100.0%)
05/24 09:15:50PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [46][300/469]	Step 22043	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/24 09:15:53PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [46][350/469]	Step 22043	Loss 0.0235	Prec@(1,5) (99.4%, 100.0%)
05/24 09:15:57PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [46][400/469]	Step 22043	Loss 0.0225	Prec@(1,5) (99.4%, 100.0%)
05/24 09:16:00PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [46][450/469]	Step 22043	Loss 0.0217	Prec@(1,5) (99.4%, 100.0%)
05/24 09:16:01PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [46][468/469]	Step 22043	Loss 0.0215	Prec@(1,5) (99.4%, 100.0%)
05/24 09:16:01PM searchShareStage_trainer.py:251 [INFO] Valid: [ 46/49] Final Prec@1 99.4333%
05/24 09:16:01PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 09:16:02PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.4333%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3076, 0.2601, 0.2161],
        [0.2298, 0.2636, 0.2273, 0.2794]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2219, 0.3007, 0.2630, 0.2144],
        [0.1937, 0.3031, 0.2819, 0.2214],
        [0.2437, 0.3231, 0.2053, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2732, 0.2444, 0.2366],
        [0.2563, 0.3127, 0.2191, 0.2119],
        [0.2649, 0.2976, 0.2170, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3006, 0.2287, 0.2239],
        [0.2487, 0.2895, 0.2228, 0.2389],
        [0.2478, 0.2933, 0.2370, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3040, 0.2239, 0.2210],
        [0.2558, 0.3038, 0.2366, 0.2038],
        [0.2490, 0.2873, 0.2183, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2977, 0.2320, 0.2205],
        [0.2514, 0.2834, 0.2319, 0.2333],
        [0.2505, 0.2892, 0.2337, 0.2267]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 09:16:25PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [47][50/468]	Step 22093	lr 0.00121	Loss 0.0003 (0.0021)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:16:48PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [47][100/468]	Step 22143	lr 0.00121	Loss 0.0014 (0.0027)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:17:10PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [47][150/468]	Step 22193	lr 0.00121	Loss 0.0003 (0.0033)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:17:33PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [47][200/468]	Step 22243	lr 0.00121	Loss 0.0003 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:17:55PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [47][250/468]	Step 22293	lr 0.00121	Loss 0.0514 (0.0044)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:18:18PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [47][300/468]	Step 22343	lr 0.00121	Loss 0.0011 (0.0042)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:18:40PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [47][350/468]	Step 22393	lr 0.00121	Loss 0.0012 (0.0041)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:19:02PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [47][400/468]	Step 22443	lr 0.00121	Loss 0.0027 (0.0040)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:19:25PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [47][450/468]	Step 22493	lr 0.00121	Loss 0.0014 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:19:33PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [47][468/468]	Step 22511	lr 0.00121	Loss 0.0024 (0.0037)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:19:33PM searchShareStage_trainer.py:212 [INFO] Train: [ 47/49] Final Prec@1 99.8933%
05/24 09:19:37PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [47][50/469]	Step 22512	Loss 0.0331	Prec@(1,5) (99.0%, 100.0%)
05/24 09:19:40PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [47][100/469]	Step 22512	Loss 0.0282	Prec@(1,5) (99.2%, 100.0%)
05/24 09:19:44PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [47][150/469]	Step 22512	Loss 0.0247	Prec@(1,5) (99.3%, 100.0%)
05/24 09:19:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [47][200/469]	Step 22512	Loss 0.0262	Prec@(1,5) (99.3%, 100.0%)
05/24 09:19:51PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [47][250/469]	Step 22512	Loss 0.0246	Prec@(1,5) (99.3%, 100.0%)
05/24 09:19:54PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [47][300/469]	Step 22512	Loss 0.0260	Prec@(1,5) (99.3%, 100.0%)
05/24 09:19:58PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [47][350/469]	Step 22512	Loss 0.0246	Prec@(1,5) (99.3%, 100.0%)
05/24 09:20:01PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [47][400/469]	Step 22512	Loss 0.0234	Prec@(1,5) (99.4%, 100.0%)
05/24 09:20:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [47][450/469]	Step 22512	Loss 0.0232	Prec@(1,5) (99.4%, 100.0%)
05/24 09:20:06PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [47][468/469]	Step 22512	Loss 0.0228	Prec@(1,5) (99.4%, 100.0%)
05/24 09:20:06PM searchShareStage_trainer.py:251 [INFO] Valid: [ 47/49] Final Prec@1 99.3767%
05/24 09:20:06PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 09:20:07PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.4333%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3076, 0.2601, 0.2161],
        [0.2298, 0.2635, 0.2273, 0.2794]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2219, 0.3006, 0.2631, 0.2144],
        [0.1937, 0.3030, 0.2819, 0.2214],
        [0.2437, 0.3231, 0.2054, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2731, 0.2444, 0.2367],
        [0.2563, 0.3127, 0.2191, 0.2119],
        [0.2650, 0.2976, 0.2170, 0.2204]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3006, 0.2287, 0.2239],
        [0.2487, 0.2895, 0.2229, 0.2390],
        [0.2478, 0.2933, 0.2370, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3040, 0.2239, 0.2210],
        [0.2558, 0.3037, 0.2366, 0.2039],
        [0.2489, 0.2872, 0.2183, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2977, 0.2321, 0.2205],
        [0.2514, 0.2834, 0.2319, 0.2333],
        [0.2505, 0.2891, 0.2337, 0.2267]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 09:20:30PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [48][50/468]	Step 22562	lr 0.00109	Loss 0.0011 (0.0034)	Prec@(1,5) (99.8%, 100.0%)	
05/24 09:20:52PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [48][100/468]	Step 22612	lr 0.00109	Loss 0.0065 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:21:15PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [48][150/468]	Step 22662	lr 0.00109	Loss 0.0168 (0.0028)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:21:37PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [48][200/468]	Step 22712	lr 0.00109	Loss 0.0009 (0.0027)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:21:59PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [48][250/468]	Step 22762	lr 0.00109	Loss 0.0028 (0.0033)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:22:22PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [48][300/468]	Step 22812	lr 0.00109	Loss 0.0005 (0.0037)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:22:44PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [48][350/468]	Step 22862	lr 0.00109	Loss 0.0004 (0.0036)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:23:06PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [48][400/468]	Step 22912	lr 0.00109	Loss 0.0044 (0.0040)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:23:29PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [48][450/468]	Step 22962	lr 0.00109	Loss 0.0002 (0.0040)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:23:37PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [48][468/468]	Step 22980	lr 0.00109	Loss 0.0009 (0.0040)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:23:37PM searchShareStage_trainer.py:212 [INFO] Train: [ 48/49] Final Prec@1 99.8667%
05/24 09:23:41PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [48][50/469]	Step 22981	Loss 0.0276	Prec@(1,5) (99.4%, 100.0%)
05/24 09:23:44PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [48][100/469]	Step 22981	Loss 0.0263	Prec@(1,5) (99.3%, 100.0%)
05/24 09:23:47PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [48][150/469]	Step 22981	Loss 0.0228	Prec@(1,5) (99.4%, 100.0%)
05/24 09:23:51PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [48][200/469]	Step 22981	Loss 0.0234	Prec@(1,5) (99.4%, 100.0%)
05/24 09:23:54PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [48][250/469]	Step 22981	Loss 0.0227	Prec@(1,5) (99.4%, 100.0%)
05/24 09:23:58PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [48][300/469]	Step 22981	Loss 0.0241	Prec@(1,5) (99.4%, 100.0%)
05/24 09:24:02PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [48][350/469]	Step 22981	Loss 0.0246	Prec@(1,5) (99.4%, 100.0%)
05/24 09:24:05PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [48][400/469]	Step 22981	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/24 09:24:09PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [48][450/469]	Step 22981	Loss 0.0235	Prec@(1,5) (99.4%, 100.0%)
05/24 09:24:10PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [48][468/469]	Step 22981	Loss 0.0234	Prec@(1,5) (99.4%, 100.0%)
05/24 09:24:10PM searchShareStage_trainer.py:251 [INFO] Valid: [ 48/49] Final Prec@1 99.3967%
05/24 09:24:10PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 09:24:11PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.4333%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2162, 0.3075, 0.2602, 0.2162],
        [0.2298, 0.2634, 0.2273, 0.2795]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2219, 0.3006, 0.2631, 0.2144],
        [0.1937, 0.3030, 0.2820, 0.2214],
        [0.2437, 0.3230, 0.2054, 0.2279]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2731, 0.2444, 0.2367],
        [0.2564, 0.3126, 0.2191, 0.2119],
        [0.2650, 0.2975, 0.2171, 0.2205]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.3006, 0.2288, 0.2239],
        [0.2487, 0.2894, 0.2229, 0.2390],
        [0.2478, 0.2932, 0.2370, 0.2220]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.3040, 0.2240, 0.2210],
        [0.2558, 0.3037, 0.2366, 0.2039],
        [0.2490, 0.2872, 0.2183, 0.2455]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2976, 0.2321, 0.2205],
        [0.2514, 0.2833, 0.2319, 0.2333],
        [0.2505, 0.2891, 0.2337, 0.2267]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
05/24 09:24:34PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [49][50/468]	Step 23031	lr 0.00102	Loss 0.0003 (0.0053)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:24:56PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [49][100/468]	Step 23081	lr 0.00102	Loss 0.0026 (0.0047)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:25:18PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [49][150/468]	Step 23131	lr 0.00102	Loss 0.0012 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:25:41PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [49][200/468]	Step 23181	lr 0.00102	Loss 0.0004 (0.0037)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:26:03PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [49][250/468]	Step 23231	lr 0.00102	Loss 0.0008 (0.0036)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:26:26PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [49][300/468]	Step 23281	lr 0.00102	Loss 0.0003 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:26:48PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [49][350/468]	Step 23331	lr 0.00102	Loss 0.0001 (0.0032)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:27:11PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [49][400/468]	Step 23381	lr 0.00102	Loss 0.0057 (0.0034)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:27:33PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [49][450/468]	Step 23431	lr 0.00102	Loss 0.0195 (0.0036)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:27:41PM searchShareStage_trainer.py:202 [INFO] Train: Epoch: [49][468/468]	Step 23449	lr 0.00102	Loss 0.0006 (0.0037)	Prec@(1,5) (99.9%, 100.0%)	
05/24 09:27:42PM searchShareStage_trainer.py:212 [INFO] Train: [ 49/49] Final Prec@1 99.8733%
05/24 09:27:45PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [49][50/469]	Step 23450	Loss 0.0235	Prec@(1,5) (99.4%, 100.0%)
05/24 09:27:48PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [49][100/469]	Step 23450	Loss 0.0230	Prec@(1,5) (99.5%, 100.0%)
05/24 09:27:52PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [49][150/469]	Step 23450	Loss 0.0222	Prec@(1,5) (99.4%, 100.0%)
05/24 09:27:55PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [49][200/469]	Step 23450	Loss 0.0235	Prec@(1,5) (99.5%, 100.0%)
05/24 09:27:59PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [49][250/469]	Step 23450	Loss 0.0220	Prec@(1,5) (99.5%, 100.0%)
05/24 09:28:02PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [49][300/469]	Step 23450	Loss 0.0219	Prec@(1,5) (99.4%, 100.0%)
05/24 09:28:06PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [49][350/469]	Step 23450	Loss 0.0218	Prec@(1,5) (99.4%, 100.0%)
05/24 09:28:09PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [49][400/469]	Step 23450	Loss 0.0234	Prec@(1,5) (99.4%, 100.0%)
05/24 09:28:13PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [49][450/469]	Step 23450	Loss 0.0239	Prec@(1,5) (99.4%, 100.0%)
05/24 09:28:14PM searchShareStage_trainer.py:240 [INFO] Valid: Epoch: [49][468/469]	Step 23450	Loss 0.0235	Prec@(1,5) (99.4%, 100.0%)
05/24 09:28:14PM searchShareStage_trainer.py:251 [INFO] Valid: [ 49/49] Final Prec@1 99.3900%
05/24 09:28:14PM searchStage_main.py:49 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/24 09:28:15PM searchStage_main.py:64 [INFO] Until now, best Prec@1 = 99.4333%
05/24 09:28:15PM searchStage_main.py:69 [INFO] Final best Prec@1 = 99.4333%
05/24 09:28:15PM searchStage_main.py:70 [INFO] Final Best Genotype = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
