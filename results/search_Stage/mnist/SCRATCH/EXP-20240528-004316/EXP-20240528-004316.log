05/28 12:43:16AM parser.py:28 [INFO] 
05/28 12:43:16AM parser.py:29 [INFO] Parameters:
05/28 12:43:16AM parser.py:31 [INFO] DAG_PATH=results/search_Stage/mnist/SCRATCH/EXP-20240528-004316/DAG
05/28 12:43:16AM parser.py:31 [INFO] ALPHA_LR=0.0003
05/28 12:43:16AM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
05/28 12:43:16AM parser.py:31 [INFO] BATCH_SIZE=64
05/28 12:43:16AM parser.py:31 [INFO] CHECKPOINT_RESET=False
05/28 12:43:16AM parser.py:31 [INFO] DATA_PATH=../data/
05/28 12:43:16AM parser.py:31 [INFO] DATASET=mnist
05/28 12:43:16AM parser.py:31 [INFO] EPOCHS=50
05/28 12:43:16AM parser.py:31 [INFO] EXP_NAME=EXP-20240528-004316
05/28 12:43:16AM parser.py:31 [INFO] GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('skip_connect', 2)]], normal_concat=[2, 3, 4, 5], reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce_concat=[2, 3, 4, 5])
05/28 12:43:16AM parser.py:31 [INFO] GPUS=[1]
05/28 12:43:16AM parser.py:31 [INFO] INIT_CHANNELS=16
05/28 12:43:16AM parser.py:31 [INFO] LAYERS=20
05/28 12:43:16AM parser.py:31 [INFO] LOCAL_RANK=0
05/28 12:43:16AM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/28 12:43:16AM parser.py:31 [INFO] NAME=SCRATCH
05/28 12:43:16AM parser.py:31 [INFO] PATH=results/search_Stage/mnist/SCRATCH/EXP-20240528-004316
05/28 12:43:16AM parser.py:31 [INFO] PRINT_FREQ=50
05/28 12:43:16AM parser.py:31 [INFO] RESUME_PATH=None
05/28 12:43:16AM parser.py:31 [INFO] SAVE=EXP
05/28 12:43:16AM parser.py:31 [INFO] SEED=3
05/28 12:43:16AM parser.py:31 [INFO] SHARE_STAGE=True
05/28 12:43:16AM parser.py:31 [INFO] TRAIN_PORTION=0.5
05/28 12:43:16AM parser.py:31 [INFO] W_GRAD_CLIP=5.0
05/28 12:43:16AM parser.py:31 [INFO] W_LR=0.025
05/28 12:43:16AM parser.py:31 [INFO] W_LR_MIN=0.001
05/28 12:43:16AM parser.py:31 [INFO] W_MOMENTUM=0.9
05/28 12:43:16AM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
05/28 12:43:16AM parser.py:31 [INFO] WORKERS=4
05/28 12:43:16AM parser.py:32 [INFO] 
05/28 12:43:18AM searchStage_trainer.py:103 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2498, 0.2499, 0.2500],
        [0.2499, 0.2499, 0.2500, 0.2502]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2502, 0.2500, 0.2499],
        [0.2501, 0.2500, 0.2501, 0.2499],
        [0.2498, 0.2495, 0.2501, 0.2506]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2500, 0.2496, 0.2505],
        [0.2502, 0.2500, 0.2499, 0.2499],
        [0.2500, 0.2500, 0.2501, 0.2499]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2503, 0.2498, 0.2499],
        [0.2500, 0.2498, 0.2502, 0.2500],
        [0.2498, 0.2503, 0.2496, 0.2503]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2501, 0.2503, 0.2498],
        [0.2502, 0.2499, 0.2501, 0.2499],
        [0.2500, 0.2503, 0.2498, 0.2499]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2503, 0.2501, 0.2500],
        [0.2501, 0.2499, 0.2500, 0.2499],
        [0.2500, 0.2501, 0.2501, 0.2499]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 12:43:39AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][50/468]	Step 50	lr 0.025	Loss 0.5572 (1.5514)	Prec@(1,5) (44.7%, 82.0%)	
05/28 12:43:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][100/468]	Step 100	lr 0.025	Loss 0.6132 (1.0243)	Prec@(1,5) (64.8%, 90.4%)	
05/28 12:44:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][150/468]	Step 150	lr 0.025	Loss 0.2901 (0.7773)	Prec@(1,5) (73.6%, 93.4%)	
05/28 12:44:35AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][200/468]	Step 200	lr 0.025	Loss 0.0592 (0.6281)	Prec@(1,5) (78.9%, 95.0%)	
05/28 12:44:54AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][250/468]	Step 250	lr 0.025	Loss 0.0552 (0.5407)	Prec@(1,5) (81.9%, 96.0%)	
05/28 12:45:13AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][300/468]	Step 300	lr 0.025	Loss 0.1421 (0.4766)	Prec@(1,5) (84.1%, 96.6%)	
05/28 12:45:32AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][350/468]	Step 350	lr 0.025	Loss 0.0618 (0.4283)	Prec@(1,5) (85.8%, 97.1%)	
05/28 12:45:51AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][400/468]	Step 400	lr 0.025	Loss 0.0880 (0.3885)	Prec@(1,5) (87.1%, 97.4%)	
05/28 12:46:10AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][450/468]	Step 450	lr 0.025	Loss 0.1077 (0.3577)	Prec@(1,5) (88.2%, 97.7%)	
05/28 12:46:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][468/468]	Step 468	lr 0.025	Loss 0.1540 (0.3474)	Prec@(1,5) (88.5%, 97.8%)	
05/28 12:46:18AM searchShareStage_trainer.py:134 [INFO] Train: [  0/49] Final Prec@1 88.5367%
05/28 12:46:21AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][50/469]	Step 469	Loss 0.1079	Prec@(1,5) (96.9%, 99.9%)
05/28 12:46:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][100/469]	Step 469	Loss 0.1210	Prec@(1,5) (96.7%, 99.9%)
05/28 12:46:28AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][150/469]	Step 469	Loss 0.1165	Prec@(1,5) (96.7%, 99.9%)
05/28 12:46:31AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][200/469]	Step 469	Loss 0.1221	Prec@(1,5) (96.6%, 99.9%)
05/28 12:46:34AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][250/469]	Step 469	Loss 0.1233	Prec@(1,5) (96.6%, 99.9%)
05/28 12:46:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][300/469]	Step 469	Loss 0.1242	Prec@(1,5) (96.6%, 99.9%)
05/28 12:46:40AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][350/469]	Step 469	Loss 0.1226	Prec@(1,5) (96.6%, 99.9%)
05/28 12:46:43AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][400/469]	Step 469	Loss 0.1239	Prec@(1,5) (96.5%, 99.9%)
05/28 12:46:46AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][450/469]	Step 469	Loss 0.1247	Prec@(1,5) (96.5%, 99.9%)
05/28 12:46:47AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][468/469]	Step 469	Loss 0.1245	Prec@(1,5) (96.5%, 99.9%)
05/28 12:46:47AM searchStage_trainer.py:260 [INFO] Valid: [  0/49] Final Prec@1 96.4900%
05/28 12:46:47AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 12:46:47AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 96.4900%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2484, 0.2513, 0.2502, 0.2501],
        [0.2470, 0.2535, 0.2532, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2540, 0.2504, 0.2460],
        [0.2538, 0.2518, 0.2501, 0.2443],
        [0.2490, 0.2576, 0.2477, 0.2457]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2544, 0.2517, 0.2420],
        [0.2532, 0.2562, 0.2403, 0.2503],
        [0.2530, 0.2584, 0.2457, 0.2428]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2546, 0.2493, 0.2432],
        [0.2509, 0.2571, 0.2471, 0.2449],
        [0.2501, 0.2567, 0.2434, 0.2498]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2569, 0.2445, 0.2462],
        [0.2512, 0.2587, 0.2472, 0.2429],
        [0.2515, 0.2554, 0.2462, 0.2468]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2558, 0.2486, 0.2445],
        [0.2495, 0.2548, 0.2439, 0.2518],
        [0.2488, 0.2565, 0.2515, 0.2432]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 12:47:08AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][50/468]	Step 519	lr 0.02498	Loss 0.1013 (0.0800)	Prec@(1,5) (97.4%, 100.0%)	
05/28 12:47:27AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][100/468]	Step 569	lr 0.02498	Loss 0.0591 (0.0836)	Prec@(1,5) (97.3%, 100.0%)	
05/28 12:47:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][150/468]	Step 619	lr 0.02498	Loss 0.2378 (0.0893)	Prec@(1,5) (97.2%, 100.0%)	
05/28 12:48:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][200/468]	Step 669	lr 0.02498	Loss 0.0310 (0.0870)	Prec@(1,5) (97.4%, 100.0%)	
05/28 12:48:25AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][250/468]	Step 719	lr 0.02498	Loss 0.0820 (0.0872)	Prec@(1,5) (97.4%, 100.0%)	
05/28 12:48:45AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][300/468]	Step 769	lr 0.02498	Loss 0.1916 (0.0855)	Prec@(1,5) (97.4%, 100.0%)	
05/28 12:49:05AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][350/468]	Step 819	lr 0.02498	Loss 0.0218 (0.0877)	Prec@(1,5) (97.4%, 100.0%)	
05/28 12:49:24AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][400/468]	Step 869	lr 0.02498	Loss 0.0744 (0.0859)	Prec@(1,5) (97.5%, 100.0%)	
05/28 12:49:44AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][450/468]	Step 919	lr 0.02498	Loss 0.0426 (0.0842)	Prec@(1,5) (97.5%, 100.0%)	
05/28 12:49:50AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][468/468]	Step 937	lr 0.02498	Loss 0.0554 (0.0834)	Prec@(1,5) (97.5%, 100.0%)	
05/28 12:49:51AM searchShareStage_trainer.py:134 [INFO] Train: [  1/49] Final Prec@1 97.5467%
05/28 12:49:55AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][50/469]	Step 938	Loss 0.0713	Prec@(1,5) (97.8%, 100.0%)
05/28 12:49:57AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][100/469]	Step 938	Loss 0.0734	Prec@(1,5) (97.8%, 100.0%)
05/28 12:50:01AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][150/469]	Step 938	Loss 0.0687	Prec@(1,5) (97.9%, 100.0%)
05/28 12:50:04AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][200/469]	Step 938	Loss 0.0681	Prec@(1,5) (97.9%, 100.0%)
05/28 12:50:06AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][250/469]	Step 938	Loss 0.0704	Prec@(1,5) (97.8%, 100.0%)
05/28 12:50:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][300/469]	Step 938	Loss 0.0738	Prec@(1,5) (97.8%, 100.0%)
05/28 12:50:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][350/469]	Step 938	Loss 0.0747	Prec@(1,5) (97.7%, 100.0%)
05/28 12:50:16AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][400/469]	Step 938	Loss 0.0769	Prec@(1,5) (97.7%, 100.0%)
05/28 12:50:18AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][450/469]	Step 938	Loss 0.0790	Prec@(1,5) (97.7%, 99.9%)
05/28 12:50:20AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][468/469]	Step 938	Loss 0.0793	Prec@(1,5) (97.6%, 99.9%)
05/28 12:50:20AM searchStage_trainer.py:260 [INFO] Valid: [  1/49] Final Prec@1 97.6500%
05/28 12:50:20AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 12:50:20AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 97.6500%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2436, 0.2529, 0.2531, 0.2504],
        [0.2446, 0.2608, 0.2539, 0.2407]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2607, 0.2507, 0.2398],
        [0.2580, 0.2516, 0.2488, 0.2415],
        [0.2523, 0.2664, 0.2427, 0.2386]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2550, 0.2548, 0.2346],
        [0.2571, 0.2636, 0.2348, 0.2446],
        [0.2559, 0.2661, 0.2390, 0.2389]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2613, 0.2486, 0.2365],
        [0.2497, 0.2645, 0.2463, 0.2395],
        [0.2514, 0.2635, 0.2367, 0.2483]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2647, 0.2403, 0.2412],
        [0.2530, 0.2659, 0.2447, 0.2364],
        [0.2514, 0.2627, 0.2415, 0.2444]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2594, 0.2457, 0.2430],
        [0.2489, 0.2607, 0.2413, 0.2490],
        [0.2490, 0.2645, 0.2500, 0.2366]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 12:50:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][50/468]	Step 988	lr 0.02491	Loss 0.0139 (0.0495)	Prec@(1,5) (98.4%, 100.0%)	
05/28 12:50:59AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][100/468]	Step 1038	lr 0.02491	Loss 0.0128 (0.0475)	Prec@(1,5) (98.5%, 100.0%)	
05/28 12:51:19AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][150/468]	Step 1088	lr 0.02491	Loss 0.0093 (0.0590)	Prec@(1,5) (98.3%, 99.9%)	
05/28 12:51:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][200/468]	Step 1138	lr 0.02491	Loss 0.2286 (0.0592)	Prec@(1,5) (98.3%, 100.0%)	
05/28 12:51:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][250/468]	Step 1188	lr 0.02491	Loss 0.0628 (0.0600)	Prec@(1,5) (98.2%, 100.0%)	
05/28 12:52:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][300/468]	Step 1238	lr 0.02491	Loss 0.0704 (0.0634)	Prec@(1,5) (98.2%, 100.0%)	
05/28 12:52:37AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][350/468]	Step 1288	lr 0.02491	Loss 0.0877 (0.0608)	Prec@(1,5) (98.2%, 100.0%)	
05/28 12:52:56AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][400/468]	Step 1338	lr 0.02491	Loss 0.1061 (0.0604)	Prec@(1,5) (98.2%, 100.0%)	
05/28 12:53:15AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][450/468]	Step 1388	lr 0.02491	Loss 0.1210 (0.0603)	Prec@(1,5) (98.2%, 100.0%)	
05/28 12:53:23AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][468/468]	Step 1406	lr 0.02491	Loss 0.0764 (0.0600)	Prec@(1,5) (98.2%, 100.0%)	
05/28 12:53:23AM searchShareStage_trainer.py:134 [INFO] Train: [  2/49] Final Prec@1 98.2133%
05/28 12:53:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][50/469]	Step 1407	Loss 0.0405	Prec@(1,5) (98.8%, 100.0%)
05/28 12:53:30AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][100/469]	Step 1407	Loss 0.0443	Prec@(1,5) (98.7%, 100.0%)
05/28 12:53:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][150/469]	Step 1407	Loss 0.0473	Prec@(1,5) (98.6%, 100.0%)
05/28 12:53:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][200/469]	Step 1407	Loss 0.0502	Prec@(1,5) (98.5%, 100.0%)
05/28 12:53:38AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][250/469]	Step 1407	Loss 0.0508	Prec@(1,5) (98.5%, 100.0%)
05/28 12:53:41AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][300/469]	Step 1407	Loss 0.0496	Prec@(1,5) (98.5%, 100.0%)
05/28 12:53:44AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][350/469]	Step 1407	Loss 0.0497	Prec@(1,5) (98.5%, 100.0%)
05/28 12:53:47AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][400/469]	Step 1407	Loss 0.0512	Prec@(1,5) (98.5%, 100.0%)
05/28 12:53:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][450/469]	Step 1407	Loss 0.0516	Prec@(1,5) (98.5%, 100.0%)
05/28 12:53:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][468/469]	Step 1407	Loss 0.0511	Prec@(1,5) (98.5%, 100.0%)
05/28 12:53:52AM searchStage_trainer.py:260 [INFO] Valid: [  2/49] Final Prec@1 98.5033%
05/28 12:53:52AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 12:53:52AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 98.5033%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2449, 0.2549, 0.2501, 0.2500],
        [0.2412, 0.2658, 0.2584, 0.2346]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2663, 0.2516, 0.2374],
        [0.2601, 0.2550, 0.2499, 0.2350],
        [0.2515, 0.2764, 0.2408, 0.2313]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2582, 0.2593, 0.2571, 0.2253],
        [0.2620, 0.2691, 0.2265, 0.2423],
        [0.2571, 0.2756, 0.2350, 0.2323]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2669, 0.2472, 0.2305],
        [0.2522, 0.2712, 0.2432, 0.2335],
        [0.2525, 0.2678, 0.2326, 0.2471]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2559, 0.2711, 0.2392, 0.2338],
        [0.2555, 0.2737, 0.2399, 0.2310],
        [0.2521, 0.2678, 0.2371, 0.2430]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2629, 0.2441, 0.2405],
        [0.2486, 0.2659, 0.2365, 0.2489],
        [0.2488, 0.2713, 0.2510, 0.2289]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 12:54:13AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][50/468]	Step 1457	lr 0.02479	Loss 0.0431 (0.0537)	Prec@(1,5) (98.3%, 100.0%)	
05/28 12:54:32AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][100/468]	Step 1507	lr 0.02479	Loss 0.0482 (0.0567)	Prec@(1,5) (98.2%, 100.0%)	
05/28 12:54:52AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][150/468]	Step 1557	lr 0.02479	Loss 0.0660 (0.0596)	Prec@(1,5) (98.2%, 99.9%)	
05/28 12:55:11AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][200/468]	Step 1607	lr 0.02479	Loss 0.0401 (0.0577)	Prec@(1,5) (98.2%, 99.9%)	
05/28 12:55:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][250/468]	Step 1657	lr 0.02479	Loss 0.0138 (0.0546)	Prec@(1,5) (98.3%, 99.9%)	
05/28 12:55:50AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][300/468]	Step 1707	lr 0.02479	Loss 0.0182 (0.0557)	Prec@(1,5) (98.2%, 100.0%)	
05/28 12:56:10AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][350/468]	Step 1757	lr 0.02479	Loss 0.0713 (0.0557)	Prec@(1,5) (98.3%, 100.0%)	
05/28 12:56:29AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][400/468]	Step 1807	lr 0.02479	Loss 0.1116 (0.0549)	Prec@(1,5) (98.3%, 100.0%)	
05/28 12:56:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][450/468]	Step 1857	lr 0.02479	Loss 0.0675 (0.0545)	Prec@(1,5) (98.3%, 100.0%)	
05/28 12:56:56AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][468/468]	Step 1875	lr 0.02479	Loss 0.0036 (0.0538)	Prec@(1,5) (98.3%, 100.0%)	
05/28 12:56:56AM searchShareStage_trainer.py:134 [INFO] Train: [  3/49] Final Prec@1 98.3067%
05/28 12:57:00AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][50/469]	Step 1876	Loss 0.0425	Prec@(1,5) (98.7%, 100.0%)
05/28 12:57:03AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][100/469]	Step 1876	Loss 0.0430	Prec@(1,5) (98.6%, 100.0%)
05/28 12:57:06AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][150/469]	Step 1876	Loss 0.0427	Prec@(1,5) (98.8%, 100.0%)
05/28 12:57:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][200/469]	Step 1876	Loss 0.0444	Prec@(1,5) (98.7%, 100.0%)
05/28 12:57:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][250/469]	Step 1876	Loss 0.0444	Prec@(1,5) (98.8%, 100.0%)
05/28 12:57:15AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][300/469]	Step 1876	Loss 0.0446	Prec@(1,5) (98.8%, 100.0%)
05/28 12:57:18AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][350/469]	Step 1876	Loss 0.0476	Prec@(1,5) (98.7%, 100.0%)
05/28 12:57:21AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][400/469]	Step 1876	Loss 0.0476	Prec@(1,5) (98.7%, 100.0%)
05/28 12:57:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][450/469]	Step 1876	Loss 0.0473	Prec@(1,5) (98.7%, 100.0%)
05/28 12:57:25AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][468/469]	Step 1876	Loss 0.0471	Prec@(1,5) (98.7%, 100.0%)
05/28 12:57:25AM searchStage_trainer.py:260 [INFO] Valid: [  3/49] Final Prec@1 98.6767%
05/28 12:57:25AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 12:57:26AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 98.6767%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2450, 0.2584, 0.2465, 0.2501],
        [0.2432, 0.2705, 0.2584, 0.2279]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2711, 0.2480, 0.2354],
        [0.2642, 0.2567, 0.2496, 0.2294],
        [0.2540, 0.2849, 0.2372, 0.2239]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2607, 0.2614, 0.2594, 0.2185],
        [0.2678, 0.2732, 0.2187, 0.2403],
        [0.2586, 0.2838, 0.2322, 0.2254]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2554, 0.2712, 0.2455, 0.2279],
        [0.2513, 0.2779, 0.2438, 0.2269],
        [0.2538, 0.2717, 0.2292, 0.2453]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2581, 0.2768, 0.2376, 0.2275],
        [0.2567, 0.2790, 0.2356, 0.2286],
        [0.2534, 0.2724, 0.2347, 0.2396]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2666, 0.2426, 0.2389],
        [0.2485, 0.2693, 0.2350, 0.2472],
        [0.2488, 0.2771, 0.2507, 0.2234]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 12:57:46AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][50/468]	Step 1926	lr 0.02462	Loss 0.1108 (0.0442)	Prec@(1,5) (98.6%, 100.0%)	
05/28 12:58:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][100/468]	Step 1976	lr 0.02462	Loss 0.0275 (0.0412)	Prec@(1,5) (98.6%, 100.0%)	
05/28 12:58:25AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][150/468]	Step 2026	lr 0.02462	Loss 0.0835 (0.0433)	Prec@(1,5) (98.7%, 100.0%)	
05/28 12:58:45AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][200/468]	Step 2076	lr 0.02462	Loss 0.0028 (0.0457)	Prec@(1,5) (98.6%, 100.0%)	
05/28 12:59:05AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][250/468]	Step 2126	lr 0.02462	Loss 0.0071 (0.0451)	Prec@(1,5) (98.7%, 100.0%)	
05/28 12:59:24AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][300/468]	Step 2176	lr 0.02462	Loss 0.0428 (0.0453)	Prec@(1,5) (98.6%, 100.0%)	
05/28 12:59:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][350/468]	Step 2226	lr 0.02462	Loss 0.0153 (0.0445)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:00:03AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][400/468]	Step 2276	lr 0.02462	Loss 0.0170 (0.0452)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:00:23AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][450/468]	Step 2326	lr 0.02462	Loss 0.0045 (0.0461)	Prec@(1,5) (98.6%, 100.0%)	
05/28 01:00:30AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][468/468]	Step 2344	lr 0.02462	Loss 0.0303 (0.0465)	Prec@(1,5) (98.6%, 100.0%)	
05/28 01:00:30AM searchShareStage_trainer.py:134 [INFO] Train: [  4/49] Final Prec@1 98.6200%
05/28 01:00:34AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][50/469]	Step 2345	Loss 0.0494	Prec@(1,5) (98.7%, 100.0%)
05/28 01:00:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][100/469]	Step 2345	Loss 0.0461	Prec@(1,5) (98.6%, 100.0%)
05/28 01:00:40AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][150/469]	Step 2345	Loss 0.0482	Prec@(1,5) (98.6%, 100.0%)
05/28 01:00:43AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][200/469]	Step 2345	Loss 0.0493	Prec@(1,5) (98.6%, 100.0%)
05/28 01:00:46AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][250/469]	Step 2345	Loss 0.0497	Prec@(1,5) (98.6%, 100.0%)
05/28 01:00:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][300/469]	Step 2345	Loss 0.0507	Prec@(1,5) (98.5%, 100.0%)
05/28 01:00:53AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][350/469]	Step 2345	Loss 0.0511	Prec@(1,5) (98.5%, 100.0%)
05/28 01:00:56AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][400/469]	Step 2345	Loss 0.0495	Prec@(1,5) (98.5%, 100.0%)
05/28 01:00:59AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][450/469]	Step 2345	Loss 0.0480	Prec@(1,5) (98.6%, 100.0%)
05/28 01:01:00AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][468/469]	Step 2345	Loss 0.0480	Prec@(1,5) (98.6%, 100.0%)
05/28 01:01:00AM searchStage_trainer.py:260 [INFO] Valid: [  4/49] Final Prec@1 98.5800%
05/28 01:01:00AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:01:00AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 98.6767%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2442, 0.2636, 0.2449, 0.2473],
        [0.2445, 0.2756, 0.2562, 0.2236]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2749, 0.2474, 0.2334],
        [0.2692, 0.2579, 0.2475, 0.2254],
        [0.2569, 0.2917, 0.2346, 0.2167]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2631, 0.2636, 0.2605, 0.2128],
        [0.2709, 0.2775, 0.2149, 0.2367],
        [0.2605, 0.2886, 0.2281, 0.2228]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2552, 0.2748, 0.2453, 0.2247],
        [0.2520, 0.2820, 0.2423, 0.2237],
        [0.2550, 0.2745, 0.2269, 0.2436]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2602, 0.2803, 0.2337, 0.2257],
        [0.2586, 0.2834, 0.2344, 0.2236],
        [0.2535, 0.2757, 0.2336, 0.2372]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2693, 0.2407, 0.2370],
        [0.2483, 0.2720, 0.2333, 0.2464],
        [0.2492, 0.2817, 0.2499, 0.2192]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:01:21AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][50/468]	Step 2395	lr 0.02441	Loss 0.0023 (0.0333)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:01:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][100/468]	Step 2445	lr 0.02441	Loss 0.0085 (0.0419)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:01:59AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][150/468]	Step 2495	lr 0.02441	Loss 0.0536 (0.0406)	Prec@(1,5) (98.8%, 100.0%)	
05/28 01:02:19AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][200/468]	Step 2545	lr 0.02441	Loss 0.0141 (0.0427)	Prec@(1,5) (98.8%, 100.0%)	
05/28 01:02:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][250/468]	Step 2595	lr 0.02441	Loss 0.0093 (0.0446)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:02:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][300/468]	Step 2645	lr 0.02441	Loss 0.0481 (0.0448)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:03:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][350/468]	Step 2695	lr 0.02441	Loss 0.0110 (0.0456)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:03:37AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][400/468]	Step 2745	lr 0.02441	Loss 0.0081 (0.0442)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:03:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][450/468]	Step 2795	lr 0.02441	Loss 0.0278 (0.0435)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:04:04AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [5][468/468]	Step 2813	lr 0.02441	Loss 0.0315 (0.0440)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:04:04AM searchShareStage_trainer.py:134 [INFO] Train: [  5/49] Final Prec@1 98.7000%
05/28 01:04:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][50/469]	Step 2814	Loss 0.0708	Prec@(1,5) (98.0%, 99.9%)
05/28 01:04:11AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][100/469]	Step 2814	Loss 0.0727	Prec@(1,5) (98.0%, 100.0%)
05/28 01:04:14AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][150/469]	Step 2814	Loss 0.0696	Prec@(1,5) (98.1%, 100.0%)
05/28 01:04:17AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][200/469]	Step 2814	Loss 0.0737	Prec@(1,5) (98.0%, 100.0%)
05/28 01:04:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][250/469]	Step 2814	Loss 0.0745	Prec@(1,5) (98.0%, 100.0%)
05/28 01:04:23AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][300/469]	Step 2814	Loss 0.0715	Prec@(1,5) (98.0%, 100.0%)
05/28 01:04:26AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][350/469]	Step 2814	Loss 0.0726	Prec@(1,5) (98.0%, 100.0%)
05/28 01:04:29AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][400/469]	Step 2814	Loss 0.0725	Prec@(1,5) (98.0%, 100.0%)
05/28 01:04:32AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][450/469]	Step 2814	Loss 0.0724	Prec@(1,5) (98.0%, 100.0%)
05/28 01:04:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [5][468/469]	Step 2814	Loss 0.0718	Prec@(1,5) (98.0%, 100.0%)
05/28 01:04:33AM searchStage_trainer.py:260 [INFO] Valid: [  5/49] Final Prec@1 98.0367%
05/28 01:04:33AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:04:33AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 98.6767%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2432, 0.2653, 0.2433, 0.2482],
        [0.2439, 0.2822, 0.2565, 0.2174]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2430, 0.2793, 0.2473, 0.2305],
        [0.2728, 0.2580, 0.2456, 0.2237],
        [0.2579, 0.2968, 0.2333, 0.2119]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2656, 0.2646, 0.2609, 0.2089],
        [0.2721, 0.2798, 0.2142, 0.2339],
        [0.2613, 0.2908, 0.2269, 0.2211]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2560, 0.2770, 0.2447, 0.2223],
        [0.2517, 0.2849, 0.2429, 0.2205],
        [0.2553, 0.2759, 0.2254, 0.2434]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2616, 0.2829, 0.2322, 0.2233],
        [0.2593, 0.2862, 0.2345, 0.2200],
        [0.2541, 0.2770, 0.2323, 0.2365]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2711, 0.2400, 0.2359],
        [0.2482, 0.2738, 0.2317, 0.2463],
        [0.2492, 0.2842, 0.2506, 0.2160]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:04:54AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][50/468]	Step 2864	lr 0.02416	Loss 0.0515 (0.0356)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:05:14AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][100/468]	Step 2914	lr 0.02416	Loss 0.0548 (0.0348)	Prec@(1,5) (98.8%, 100.0%)	
05/28 01:05:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][150/468]	Step 2964	lr 0.02416	Loss 0.0016 (0.0348)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:05:53AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][200/468]	Step 3014	lr 0.02416	Loss 0.0331 (0.0385)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:06:12AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][250/468]	Step 3064	lr 0.02416	Loss 0.0123 (0.0405)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:06:32AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][300/468]	Step 3114	lr 0.02416	Loss 0.0075 (0.0385)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:06:52AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][350/468]	Step 3164	lr 0.02416	Loss 0.0254 (0.0401)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:07:12AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][400/468]	Step 3214	lr 0.02416	Loss 0.0351 (0.0407)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:07:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][450/468]	Step 3264	lr 0.02416	Loss 0.0523 (0.0414)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:07:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [6][468/468]	Step 3282	lr 0.02416	Loss 0.0579 (0.0420)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:07:39AM searchShareStage_trainer.py:134 [INFO] Train: [  6/49] Final Prec@1 98.6900%
05/28 01:07:42AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][50/469]	Step 3283	Loss 0.0531	Prec@(1,5) (98.4%, 99.9%)
05/28 01:07:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][100/469]	Step 3283	Loss 0.0581	Prec@(1,5) (98.3%, 100.0%)
05/28 01:07:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][150/469]	Step 3283	Loss 0.0577	Prec@(1,5) (98.4%, 100.0%)
05/28 01:07:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][200/469]	Step 3283	Loss 0.0568	Prec@(1,5) (98.4%, 100.0%)
05/28 01:07:54AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][250/469]	Step 3283	Loss 0.0572	Prec@(1,5) (98.3%, 100.0%)
05/28 01:07:57AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][300/469]	Step 3283	Loss 0.0564	Prec@(1,5) (98.3%, 100.0%)
05/28 01:08:00AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][350/469]	Step 3283	Loss 0.0560	Prec@(1,5) (98.4%, 100.0%)
05/28 01:08:03AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][400/469]	Step 3283	Loss 0.0561	Prec@(1,5) (98.4%, 100.0%)
05/28 01:08:06AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][450/469]	Step 3283	Loss 0.0551	Prec@(1,5) (98.4%, 100.0%)
05/28 01:08:07AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [6][468/469]	Step 3283	Loss 0.0551	Prec@(1,5) (98.4%, 100.0%)
05/28 01:08:07AM searchStage_trainer.py:260 [INFO] Valid: [  6/49] Final Prec@1 98.3833%
05/28 01:08:07AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:08:07AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 98.6767%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2440, 0.2679, 0.2420, 0.2461],
        [0.2441, 0.2836, 0.2561, 0.2162]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2425, 0.2818, 0.2477, 0.2280],
        [0.2762, 0.2582, 0.2444, 0.2212],
        [0.2587, 0.2992, 0.2321, 0.2100]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2677, 0.2667, 0.2628, 0.2028],
        [0.2730, 0.2803, 0.2119, 0.2348],
        [0.2621, 0.2917, 0.2264, 0.2197]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2558, 0.2781, 0.2436, 0.2225],
        [0.2519, 0.2868, 0.2437, 0.2176],
        [0.2554, 0.2765, 0.2250, 0.2430]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2632, 0.2845, 0.2297, 0.2226],
        [0.2591, 0.2882, 0.2345, 0.2182],
        [0.2546, 0.2777, 0.2323, 0.2354]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2724, 0.2401, 0.2351],
        [0.2481, 0.2749, 0.2318, 0.2452],
        [0.2488, 0.2855, 0.2507, 0.2150]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:08:28AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][50/468]	Step 3333	lr 0.02386	Loss 0.0050 (0.0404)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:08:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][100/468]	Step 3383	lr 0.02386	Loss 0.0329 (0.0415)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:09:07AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][150/468]	Step 3433	lr 0.02386	Loss 0.0327 (0.0414)	Prec@(1,5) (98.7%, 100.0%)	
05/28 01:09:26AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][200/468]	Step 3483	lr 0.02386	Loss 0.0081 (0.0399)	Prec@(1,5) (98.8%, 100.0%)	
05/28 01:09:46AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][250/468]	Step 3533	lr 0.02386	Loss 0.0286 (0.0388)	Prec@(1,5) (98.8%, 100.0%)	
05/28 01:10:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][300/468]	Step 3583	lr 0.02386	Loss 0.0230 (0.0377)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:10:25AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][350/468]	Step 3633	lr 0.02386	Loss 0.0300 (0.0378)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:10:45AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][400/468]	Step 3683	lr 0.02386	Loss 0.0128 (0.0379)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:11:04AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][450/468]	Step 3733	lr 0.02386	Loss 0.0116 (0.0382)	Prec@(1,5) (98.8%, 100.0%)	
05/28 01:11:11AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [7][468/468]	Step 3751	lr 0.02386	Loss 0.0319 (0.0384)	Prec@(1,5) (98.8%, 100.0%)	
05/28 01:11:12AM searchShareStage_trainer.py:134 [INFO] Train: [  7/49] Final Prec@1 98.8367%
05/28 01:11:16AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][50/469]	Step 3752	Loss 0.0655	Prec@(1,5) (98.3%, 100.0%)
05/28 01:11:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][100/469]	Step 3752	Loss 0.0600	Prec@(1,5) (98.3%, 100.0%)
05/28 01:11:22AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][150/469]	Step 3752	Loss 0.0579	Prec@(1,5) (98.4%, 100.0%)
05/28 01:11:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][200/469]	Step 3752	Loss 0.0565	Prec@(1,5) (98.4%, 100.0%)
05/28 01:11:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][250/469]	Step 3752	Loss 0.0565	Prec@(1,5) (98.4%, 100.0%)
05/28 01:11:30AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][300/469]	Step 3752	Loss 0.0556	Prec@(1,5) (98.4%, 100.0%)
05/28 01:11:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][350/469]	Step 3752	Loss 0.0557	Prec@(1,5) (98.4%, 100.0%)
05/28 01:11:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][400/469]	Step 3752	Loss 0.0540	Prec@(1,5) (98.5%, 100.0%)
05/28 01:11:39AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][450/469]	Step 3752	Loss 0.0532	Prec@(1,5) (98.5%, 100.0%)
05/28 01:11:40AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [7][468/469]	Step 3752	Loss 0.0532	Prec@(1,5) (98.5%, 100.0%)
05/28 01:11:40AM searchStage_trainer.py:260 [INFO] Valid: [  7/49] Final Prec@1 98.4867%
05/28 01:11:40AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:11:41AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 98.6767%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2442, 0.2676, 0.2417, 0.2465],
        [0.2440, 0.2859, 0.2561, 0.2140]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2417, 0.2834, 0.2481, 0.2268],
        [0.2776, 0.2583, 0.2445, 0.2196],
        [0.2588, 0.3001, 0.2321, 0.2090]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2701, 0.2668, 0.2622, 0.2009],
        [0.2731, 0.2810, 0.2112, 0.2346],
        [0.2628, 0.2922, 0.2259, 0.2191]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2786, 0.2447, 0.2214],
        [0.2516, 0.2880, 0.2441, 0.2164],
        [0.2554, 0.2768, 0.2242, 0.2435]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2628, 0.2857, 0.2301, 0.2214],
        [0.2593, 0.2891, 0.2335, 0.2181],
        [0.2550, 0.2780, 0.2323, 0.2347]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2733, 0.2409, 0.2339],
        [0.2479, 0.2750, 0.2310, 0.2461],
        [0.2489, 0.2862, 0.2510, 0.2139]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:12:01AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][50/468]	Step 3802	lr 0.02352	Loss 0.0475 (0.0268)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:12:21AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][100/468]	Step 3852	lr 0.02352	Loss 0.0064 (0.0301)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:12:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][150/468]	Step 3902	lr 0.02352	Loss 0.0137 (0.0309)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:13:00AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][200/468]	Step 3952	lr 0.02352	Loss 0.0080 (0.0319)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:13:19AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][250/468]	Step 4002	lr 0.02352	Loss 0.0297 (0.0326)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:13:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][300/468]	Step 4052	lr 0.02352	Loss 0.0327 (0.0333)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:13:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][350/468]	Step 4102	lr 0.02352	Loss 0.0105 (0.0346)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:14:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][400/468]	Step 4152	lr 0.02352	Loss 0.0174 (0.0356)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:14:36AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][450/468]	Step 4202	lr 0.02352	Loss 0.0783 (0.0369)	Prec@(1,5) (98.8%, 100.0%)	
05/28 01:14:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [8][468/468]	Step 4220	lr 0.02352	Loss 0.0113 (0.0373)	Prec@(1,5) (98.8%, 100.0%)	
05/28 01:14:43AM searchShareStage_trainer.py:134 [INFO] Train: [  8/49] Final Prec@1 98.8400%
05/28 01:14:47AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][50/469]	Step 4221	Loss 0.0496	Prec@(1,5) (98.6%, 100.0%)
05/28 01:14:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][100/469]	Step 4221	Loss 0.0450	Prec@(1,5) (98.7%, 100.0%)
05/28 01:14:53AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][150/469]	Step 4221	Loss 0.0461	Prec@(1,5) (98.7%, 100.0%)
05/28 01:14:56AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][200/469]	Step 4221	Loss 0.0463	Prec@(1,5) (98.6%, 100.0%)
05/28 01:14:59AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][250/469]	Step 4221	Loss 0.0457	Prec@(1,5) (98.6%, 100.0%)
05/28 01:15:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][300/469]	Step 4221	Loss 0.0472	Prec@(1,5) (98.5%, 100.0%)
05/28 01:15:05AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][350/469]	Step 4221	Loss 0.0471	Prec@(1,5) (98.5%, 100.0%)
05/28 01:15:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][400/469]	Step 4221	Loss 0.0461	Prec@(1,5) (98.6%, 100.0%)
05/28 01:15:11AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][450/469]	Step 4221	Loss 0.0472	Prec@(1,5) (98.6%, 100.0%)
05/28 01:15:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [8][468/469]	Step 4221	Loss 0.0476	Prec@(1,5) (98.6%, 100.0%)
05/28 01:15:12AM searchStage_trainer.py:260 [INFO] Valid: [  8/49] Final Prec@1 98.5567%
05/28 01:15:12AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:15:12AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 98.6767%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2445, 0.2677, 0.2415, 0.2463],
        [0.2456, 0.2858, 0.2552, 0.2134]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2423, 0.2844, 0.2479, 0.2254],
        [0.2789, 0.2588, 0.2442, 0.2180],
        [0.2592, 0.3003, 0.2313, 0.2092]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2715, 0.2678, 0.2621, 0.1986],
        [0.2737, 0.2810, 0.2111, 0.2342],
        [0.2630, 0.2920, 0.2256, 0.2195]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2560, 0.2787, 0.2434, 0.2220],
        [0.2518, 0.2883, 0.2446, 0.2153],
        [0.2553, 0.2768, 0.2244, 0.2436]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2632, 0.2862, 0.2299, 0.2207],
        [0.2596, 0.2892, 0.2337, 0.2175],
        [0.2551, 0.2779, 0.2321, 0.2350]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2737, 0.2410, 0.2336],
        [0.2479, 0.2751, 0.2300, 0.2470],
        [0.2486, 0.2864, 0.2518, 0.2132]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:15:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][50/468]	Step 4271	lr 0.02313	Loss 0.0746 (0.0369)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:15:52AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][100/468]	Step 4321	lr 0.02313	Loss 0.0442 (0.0290)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:16:12AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][150/468]	Step 4371	lr 0.02313	Loss 0.0816 (0.0290)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:16:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][200/468]	Step 4421	lr 0.02313	Loss 0.0448 (0.0312)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:16:51AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][250/468]	Step 4471	lr 0.02313	Loss 0.0905 (0.0320)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:17:10AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][300/468]	Step 4521	lr 0.02313	Loss 0.0203 (0.0336)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:17:30AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][350/468]	Step 4571	lr 0.02313	Loss 0.0024 (0.0340)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:17:50AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][400/468]	Step 4621	lr 0.02313	Loss 0.0205 (0.0342)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:18:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][450/468]	Step 4671	lr 0.02313	Loss 0.0187 (0.0339)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:18:16AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [9][468/468]	Step 4689	lr 0.02313	Loss 0.0115 (0.0333)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:18:17AM searchShareStage_trainer.py:134 [INFO] Train: [  9/49] Final Prec@1 99.0000%
05/28 01:18:21AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][50/469]	Step 4690	Loss 0.0424	Prec@(1,5) (98.6%, 100.0%)
05/28 01:18:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][100/469]	Step 4690	Loss 0.0371	Prec@(1,5) (98.8%, 100.0%)
05/28 01:18:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][150/469]	Step 4690	Loss 0.0387	Prec@(1,5) (98.7%, 100.0%)
05/28 01:18:30AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][200/469]	Step 4690	Loss 0.0390	Prec@(1,5) (98.7%, 100.0%)
05/28 01:18:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][250/469]	Step 4690	Loss 0.0366	Prec@(1,5) (98.8%, 100.0%)
05/28 01:18:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][300/469]	Step 4690	Loss 0.0372	Prec@(1,5) (98.8%, 100.0%)
05/28 01:18:39AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][350/469]	Step 4690	Loss 0.0379	Prec@(1,5) (98.8%, 100.0%)
05/28 01:18:42AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][400/469]	Step 4690	Loss 0.0391	Prec@(1,5) (98.8%, 100.0%)
05/28 01:18:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][450/469]	Step 4690	Loss 0.0406	Prec@(1,5) (98.7%, 100.0%)
05/28 01:18:46AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [9][468/469]	Step 4690	Loss 0.0404	Prec@(1,5) (98.8%, 100.0%)
05/28 01:18:46AM searchStage_trainer.py:260 [INFO] Valid: [  9/49] Final Prec@1 98.7500%
05/28 01:18:46AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:18:47AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 98.7500%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2446, 0.2672, 0.2414, 0.2469],
        [0.2462, 0.2862, 0.2549, 0.2127]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2417, 0.2844, 0.2479, 0.2260],
        [0.2798, 0.2585, 0.2448, 0.2168],
        [0.2591, 0.3004, 0.2315, 0.2090]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2723, 0.2678, 0.2622, 0.1976],
        [0.2738, 0.2807, 0.2108, 0.2347],
        [0.2630, 0.2919, 0.2259, 0.2192]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2562, 0.2786, 0.2433, 0.2219],
        [0.2518, 0.2885, 0.2452, 0.2145],
        [0.2552, 0.2764, 0.2242, 0.2442]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2636, 0.2864, 0.2295, 0.2205],
        [0.2598, 0.2894, 0.2336, 0.2173],
        [0.2550, 0.2776, 0.2322, 0.2352]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2737, 0.2411, 0.2336],
        [0.2478, 0.2750, 0.2298, 0.2474],
        [0.2485, 0.2864, 0.2520, 0.2130]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:19:07AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][50/468]	Step 4740	lr 0.02271	Loss 0.0551 (0.0297)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:19:26AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][100/468]	Step 4790	lr 0.02271	Loss 0.0293 (0.0317)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:19:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][150/468]	Step 4840	lr 0.02271	Loss 0.0056 (0.0285)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:20:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][200/468]	Step 4890	lr 0.02271	Loss 0.0878 (0.0304)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:20:25AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][250/468]	Step 4940	lr 0.02271	Loss 0.0659 (0.0295)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:20:45AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][300/468]	Step 4990	lr 0.02271	Loss 0.0017 (0.0296)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:21:05AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][350/468]	Step 5040	lr 0.02271	Loss 0.0463 (0.0314)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:21:25AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][400/468]	Step 5090	lr 0.02271	Loss 0.0141 (0.0315)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:21:44AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][450/468]	Step 5140	lr 0.02271	Loss 0.0414 (0.0316)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:21:51AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [10][468/468]	Step 5158	lr 0.02271	Loss 0.0178 (0.0318)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:21:52AM searchShareStage_trainer.py:134 [INFO] Train: [ 10/49] Final Prec@1 99.0267%
05/28 01:21:55AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][50/469]	Step 5159	Loss 0.0419	Prec@(1,5) (98.8%, 100.0%)
05/28 01:21:58AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][100/469]	Step 5159	Loss 0.0343	Prec@(1,5) (99.1%, 100.0%)
05/28 01:22:01AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][150/469]	Step 5159	Loss 0.0343	Prec@(1,5) (99.1%, 100.0%)
05/28 01:22:04AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][200/469]	Step 5159	Loss 0.0353	Prec@(1,5) (99.0%, 100.0%)
05/28 01:22:07AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][250/469]	Step 5159	Loss 0.0350	Prec@(1,5) (99.0%, 100.0%)
05/28 01:22:10AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][300/469]	Step 5159	Loss 0.0357	Prec@(1,5) (99.0%, 100.0%)
05/28 01:22:13AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][350/469]	Step 5159	Loss 0.0360	Prec@(1,5) (99.0%, 100.0%)
05/28 01:22:16AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][400/469]	Step 5159	Loss 0.0353	Prec@(1,5) (99.0%, 100.0%)
05/28 01:22:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][450/469]	Step 5159	Loss 0.0347	Prec@(1,5) (99.0%, 100.0%)
05/28 01:22:20AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [10][468/469]	Step 5159	Loss 0.0344	Prec@(1,5) (99.0%, 100.0%)
05/28 01:22:20AM searchStage_trainer.py:260 [INFO] Valid: [ 10/49] Final Prec@1 99.0000%
05/28 01:22:20AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:22:21AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2444, 0.2671, 0.2417, 0.2468],
        [0.2468, 0.2858, 0.2547, 0.2127]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2416, 0.2850, 0.2472, 0.2262],
        [0.2804, 0.2585, 0.2453, 0.2158],
        [0.2591, 0.3003, 0.2314, 0.2092]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2744, 0.2683, 0.2615, 0.1958],
        [0.2741, 0.2802, 0.2105, 0.2352],
        [0.2632, 0.2916, 0.2259, 0.2193]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2561, 0.2787, 0.2433, 0.2218],
        [0.2516, 0.2885, 0.2455, 0.2143],
        [0.2551, 0.2761, 0.2243, 0.2445]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2641, 0.2865, 0.2289, 0.2206],
        [0.2596, 0.2892, 0.2338, 0.2174],
        [0.2550, 0.2774, 0.2323, 0.2353]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2735, 0.2416, 0.2336],
        [0.2475, 0.2747, 0.2298, 0.2480],
        [0.2484, 0.2862, 0.2524, 0.2130]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:22:41AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][50/468]	Step 5209	lr 0.02225	Loss 0.0036 (0.0301)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:23:01AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][100/468]	Step 5259	lr 0.02225	Loss 0.0343 (0.0354)	Prec@(1,5) (98.9%, 100.0%)	
05/28 01:23:21AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][150/468]	Step 5309	lr 0.02225	Loss 0.0092 (0.0329)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:23:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][200/468]	Step 5359	lr 0.02225	Loss 0.0096 (0.0315)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:24:00AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][250/468]	Step 5409	lr 0.02225	Loss 0.0022 (0.0309)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:24:20AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][300/468]	Step 5459	lr 0.02225	Loss 0.0425 (0.0303)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:24:39AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][350/468]	Step 5509	lr 0.02225	Loss 0.0017 (0.0298)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:24:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][400/468]	Step 5559	lr 0.02225	Loss 0.0485 (0.0300)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:25:18AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][450/468]	Step 5609	lr 0.02225	Loss 0.0237 (0.0307)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:25:25AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [11][468/468]	Step 5627	lr 0.02225	Loss 0.0160 (0.0306)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:25:25AM searchShareStage_trainer.py:134 [INFO] Train: [ 11/49] Final Prec@1 99.0433%
05/28 01:25:29AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][50/469]	Step 5628	Loss 0.0456	Prec@(1,5) (98.7%, 100.0%)
05/28 01:25:32AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][100/469]	Step 5628	Loss 0.0457	Prec@(1,5) (98.7%, 100.0%)
05/28 01:25:35AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][150/469]	Step 5628	Loss 0.0516	Prec@(1,5) (98.5%, 100.0%)
05/28 01:25:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][200/469]	Step 5628	Loss 0.0486	Prec@(1,5) (98.6%, 100.0%)
05/28 01:25:40AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][250/469]	Step 5628	Loss 0.0479	Prec@(1,5) (98.6%, 100.0%)
05/28 01:25:43AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][300/469]	Step 5628	Loss 0.0486	Prec@(1,5) (98.6%, 100.0%)
05/28 01:25:46AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][350/469]	Step 5628	Loss 0.0483	Prec@(1,5) (98.6%, 100.0%)
05/28 01:25:49AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][400/469]	Step 5628	Loss 0.0485	Prec@(1,5) (98.6%, 100.0%)
05/28 01:25:52AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][450/469]	Step 5628	Loss 0.0488	Prec@(1,5) (98.6%, 100.0%)
05/28 01:25:53AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [11][468/469]	Step 5628	Loss 0.0480	Prec@(1,5) (98.6%, 100.0%)
05/28 01:25:54AM searchStage_trainer.py:260 [INFO] Valid: [ 11/49] Final Prec@1 98.6000%
05/28 01:25:54AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:25:54AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2442, 0.2666, 0.2419, 0.2474],
        [0.2471, 0.2858, 0.2547, 0.2124]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.2852, 0.2471, 0.2259],
        [0.2812, 0.2576, 0.2454, 0.2158],
        [0.2592, 0.3000, 0.2314, 0.2093]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2741, 0.2683, 0.2616, 0.1960],
        [0.2742, 0.2798, 0.2106, 0.2354],
        [0.2631, 0.2915, 0.2261, 0.2193]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2559, 0.2786, 0.2434, 0.2221],
        [0.2516, 0.2884, 0.2459, 0.2140],
        [0.2550, 0.2758, 0.2244, 0.2449]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2642, 0.2865, 0.2289, 0.2204],
        [0.2597, 0.2890, 0.2339, 0.2174],
        [0.2549, 0.2771, 0.2323, 0.2357]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2733, 0.2417, 0.2336],
        [0.2474, 0.2744, 0.2299, 0.2483],
        [0.2483, 0.2859, 0.2527, 0.2131]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:26:14AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][50/468]	Step 5678	lr 0.02175	Loss 0.0447 (0.0272)	Prec@(1,5) (99.2%, 99.9%)	
05/28 01:26:34AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][100/468]	Step 5728	lr 0.02175	Loss 0.0020 (0.0233)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:26:54AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][150/468]	Step 5778	lr 0.02175	Loss 0.0138 (0.0305)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:27:14AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][200/468]	Step 5828	lr 0.02175	Loss 0.0160 (0.0305)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:27:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][250/468]	Step 5878	lr 0.02175	Loss 0.0160 (0.0304)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:27:53AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][300/468]	Step 5928	lr 0.02175	Loss 0.0449 (0.0320)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:28:12AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][350/468]	Step 5978	lr 0.02175	Loss 0.0180 (0.0320)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:28:32AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][400/468]	Step 6028	lr 0.02175	Loss 0.0402 (0.0316)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:28:51AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][450/468]	Step 6078	lr 0.02175	Loss 0.0270 (0.0317)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:28:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [12][468/468]	Step 6096	lr 0.02175	Loss 0.0032 (0.0314)	Prec@(1,5) (99.0%, 100.0%)	
05/28 01:28:59AM searchShareStage_trainer.py:134 [INFO] Train: [ 12/49] Final Prec@1 99.0500%
05/28 01:29:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][50/469]	Step 6097	Loss 0.0564	Prec@(1,5) (98.5%, 100.0%)
05/28 01:29:05AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][100/469]	Step 6097	Loss 0.0460	Prec@(1,5) (98.7%, 100.0%)
05/28 01:29:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][150/469]	Step 6097	Loss 0.0448	Prec@(1,5) (98.8%, 100.0%)
05/28 01:29:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][200/469]	Step 6097	Loss 0.0430	Prec@(1,5) (98.9%, 100.0%)
05/28 01:29:15AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][250/469]	Step 6097	Loss 0.0425	Prec@(1,5) (98.9%, 100.0%)
05/28 01:29:18AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][300/469]	Step 6097	Loss 0.0413	Prec@(1,5) (98.9%, 100.0%)
05/28 01:29:21AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][350/469]	Step 6097	Loss 0.0423	Prec@(1,5) (98.9%, 100.0%)
05/28 01:29:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][400/469]	Step 6097	Loss 0.0417	Prec@(1,5) (98.8%, 100.0%)
05/28 01:29:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][450/469]	Step 6097	Loss 0.0415	Prec@(1,5) (98.8%, 100.0%)
05/28 01:29:28AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [12][468/469]	Step 6097	Loss 0.0417	Prec@(1,5) (98.8%, 100.0%)
05/28 01:29:28AM searchStage_trainer.py:260 [INFO] Valid: [ 12/49] Final Prec@1 98.8433%
05/28 01:29:28AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:29:28AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2437, 0.2660, 0.2420, 0.2482],
        [0.2472, 0.2859, 0.2550, 0.2119]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2417, 0.2852, 0.2473, 0.2259],
        [0.2818, 0.2569, 0.2452, 0.2162],
        [0.2591, 0.2998, 0.2318, 0.2093]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2746, 0.2680, 0.2615, 0.1958],
        [0.2741, 0.2796, 0.2105, 0.2357],
        [0.2630, 0.2912, 0.2264, 0.2194]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2557, 0.2786, 0.2437, 0.2219],
        [0.2515, 0.2883, 0.2463, 0.2139],
        [0.2548, 0.2755, 0.2244, 0.2453]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2642, 0.2863, 0.2290, 0.2205],
        [0.2594, 0.2890, 0.2341, 0.2175],
        [0.2548, 0.2768, 0.2324, 0.2360]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2731, 0.2419, 0.2336],
        [0.2472, 0.2742, 0.2300, 0.2487],
        [0.2481, 0.2856, 0.2530, 0.2133]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:29:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][50/468]	Step 6147	lr 0.02121	Loss 0.0065 (0.0159)	Prec@(1,5) (99.5%, 100.0%)	
05/28 01:30:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][100/468]	Step 6197	lr 0.02121	Loss 0.0404 (0.0227)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:30:28AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][150/468]	Step 6247	lr 0.02121	Loss 0.0931 (0.0228)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:30:48AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][200/468]	Step 6297	lr 0.02121	Loss 0.0058 (0.0259)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:31:08AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][250/468]	Step 6347	lr 0.02121	Loss 0.0564 (0.0271)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:31:27AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][300/468]	Step 6397	lr 0.02121	Loss 0.0164 (0.0277)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:31:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][350/468]	Step 6447	lr 0.02121	Loss 0.0265 (0.0277)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:32:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][400/468]	Step 6497	lr 0.02121	Loss 0.0485 (0.0284)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:32:26AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][450/468]	Step 6547	lr 0.02121	Loss 0.0027 (0.0289)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:32:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [13][468/468]	Step 6565	lr 0.02121	Loss 0.0011 (0.0288)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:32:33AM searchShareStage_trainer.py:134 [INFO] Train: [ 13/49] Final Prec@1 99.0900%
05/28 01:32:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][50/469]	Step 6566	Loss 0.0475	Prec@(1,5) (98.5%, 99.9%)
05/28 01:32:40AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][100/469]	Step 6566	Loss 0.0430	Prec@(1,5) (98.7%, 100.0%)
05/28 01:32:43AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][150/469]	Step 6566	Loss 0.0425	Prec@(1,5) (98.7%, 100.0%)
05/28 01:32:46AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][200/469]	Step 6566	Loss 0.0416	Prec@(1,5) (98.7%, 100.0%)
05/28 01:32:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][250/469]	Step 6566	Loss 0.0397	Prec@(1,5) (98.7%, 100.0%)
05/28 01:32:53AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][300/469]	Step 6566	Loss 0.0397	Prec@(1,5) (98.7%, 100.0%)
05/28 01:32:56AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][350/469]	Step 6566	Loss 0.0414	Prec@(1,5) (98.7%, 100.0%)
05/28 01:32:59AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][400/469]	Step 6566	Loss 0.0403	Prec@(1,5) (98.7%, 100.0%)
05/28 01:33:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][450/469]	Step 6566	Loss 0.0406	Prec@(1,5) (98.7%, 100.0%)
05/28 01:33:03AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [13][468/469]	Step 6566	Loss 0.0402	Prec@(1,5) (98.7%, 100.0%)
05/28 01:33:03AM searchStage_trainer.py:260 [INFO] Valid: [ 13/49] Final Prec@1 98.7367%
05/28 01:33:03AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:33:03AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2437, 0.2657, 0.2423, 0.2484],
        [0.2473, 0.2855, 0.2552, 0.2121]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2416, 0.2852, 0.2471, 0.2261],
        [0.2823, 0.2563, 0.2452, 0.2162],
        [0.2590, 0.2996, 0.2319, 0.2095]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2754, 0.2681, 0.2614, 0.1950],
        [0.2741, 0.2794, 0.2106, 0.2360],
        [0.2630, 0.2909, 0.2265, 0.2196]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2556, 0.2786, 0.2441, 0.2217],
        [0.2515, 0.2881, 0.2465, 0.2139],
        [0.2547, 0.2752, 0.2245, 0.2456]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2643, 0.2862, 0.2288, 0.2207],
        [0.2594, 0.2888, 0.2343, 0.2175],
        [0.2547, 0.2766, 0.2325, 0.2362]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2729, 0.2420, 0.2339],
        [0.2469, 0.2740, 0.2302, 0.2489],
        [0.2480, 0.2853, 0.2532, 0.2134]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:33:23AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][50/468]	Step 6616	lr 0.02065	Loss 0.0312 (0.0258)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:33:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][100/468]	Step 6666	lr 0.02065	Loss 0.0095 (0.0241)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:34:02AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][150/468]	Step 6716	lr 0.02065	Loss 0.0011 (0.0269)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:34:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][200/468]	Step 6766	lr 0.02065	Loss 0.0029 (0.0254)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:34:41AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][250/468]	Step 6816	lr 0.02065	Loss 0.1173 (0.0271)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:35:01AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][300/468]	Step 6866	lr 0.02065	Loss 0.0083 (0.0278)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:35:21AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][350/468]	Step 6916	lr 0.02065	Loss 0.0206 (0.0278)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:35:41AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][400/468]	Step 6966	lr 0.02065	Loss 0.0315 (0.0281)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:36:01AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][450/468]	Step 7016	lr 0.02065	Loss 0.0108 (0.0279)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:36:08AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [14][468/468]	Step 7034	lr 0.02065	Loss 0.0052 (0.0284)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:36:08AM searchShareStage_trainer.py:134 [INFO] Train: [ 14/49] Final Prec@1 99.1133%
05/28 01:36:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][50/469]	Step 7035	Loss 0.0666	Prec@(1,5) (98.1%, 100.0%)
05/28 01:36:15AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][100/469]	Step 7035	Loss 0.0607	Prec@(1,5) (98.2%, 100.0%)
05/28 01:36:18AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][150/469]	Step 7035	Loss 0.0620	Prec@(1,5) (98.2%, 100.0%)
05/28 01:36:21AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][200/469]	Step 7035	Loss 0.0600	Prec@(1,5) (98.3%, 100.0%)
05/28 01:36:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][250/469]	Step 7035	Loss 0.0563	Prec@(1,5) (98.3%, 100.0%)
05/28 01:36:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][300/469]	Step 7035	Loss 0.0570	Prec@(1,5) (98.3%, 100.0%)
05/28 01:36:31AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][350/469]	Step 7035	Loss 0.0552	Prec@(1,5) (98.3%, 100.0%)
05/28 01:36:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][400/469]	Step 7035	Loss 0.0545	Prec@(1,5) (98.3%, 100.0%)
05/28 01:36:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][450/469]	Step 7035	Loss 0.0544	Prec@(1,5) (98.3%, 100.0%)
05/28 01:36:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [14][468/469]	Step 7035	Loss 0.0547	Prec@(1,5) (98.3%, 100.0%)
05/28 01:36:37AM searchStage_trainer.py:260 [INFO] Valid: [ 14/49] Final Prec@1 98.3033%
05/28 01:36:37AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:36:38AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2435, 0.2654, 0.2424, 0.2487],
        [0.2473, 0.2853, 0.2554, 0.2120]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2416, 0.2850, 0.2469, 0.2265],
        [0.2828, 0.2560, 0.2452, 0.2160],
        [0.2590, 0.2993, 0.2320, 0.2096]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2761, 0.2681, 0.2615, 0.1943],
        [0.2742, 0.2791, 0.2105, 0.2362],
        [0.2629, 0.2906, 0.2266, 0.2198]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2555, 0.2784, 0.2441, 0.2220],
        [0.2514, 0.2880, 0.2468, 0.2139],
        [0.2546, 0.2749, 0.2246, 0.2458]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2642, 0.2860, 0.2290, 0.2208],
        [0.2592, 0.2886, 0.2347, 0.2175],
        [0.2546, 0.2762, 0.2326, 0.2365]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2728, 0.2425, 0.2338],
        [0.2468, 0.2737, 0.2303, 0.2493],
        [0.2479, 0.2851, 0.2534, 0.2136]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:36:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][50/468]	Step 7085	lr 0.02005	Loss 0.0065 (0.0307)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:37:18AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][100/468]	Step 7135	lr 0.02005	Loss 0.0014 (0.0278)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:37:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][150/468]	Step 7185	lr 0.02005	Loss 0.0033 (0.0275)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:37:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][200/468]	Step 7235	lr 0.02005	Loss 0.0259 (0.0271)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:38:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][250/468]	Step 7285	lr 0.02005	Loss 0.0067 (0.0268)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:38:37AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][300/468]	Step 7335	lr 0.02005	Loss 0.0181 (0.0263)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:38:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][350/468]	Step 7385	lr 0.02005	Loss 0.0342 (0.0259)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:39:16AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][400/468]	Step 7435	lr 0.02005	Loss 0.0285 (0.0258)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:39:36AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][450/468]	Step 7485	lr 0.02005	Loss 0.0028 (0.0253)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:39:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [15][468/468]	Step 7503	lr 0.02005	Loss 0.1107 (0.0259)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:39:44AM searchShareStage_trainer.py:134 [INFO] Train: [ 15/49] Final Prec@1 99.1500%
05/28 01:39:47AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][50/469]	Step 7504	Loss 0.0347	Prec@(1,5) (99.1%, 100.0%)
05/28 01:39:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][100/469]	Step 7504	Loss 0.0427	Prec@(1,5) (98.7%, 100.0%)
05/28 01:39:54AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][150/469]	Step 7504	Loss 0.0407	Prec@(1,5) (98.7%, 100.0%)
05/28 01:39:57AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][200/469]	Step 7504	Loss 0.0413	Prec@(1,5) (98.7%, 100.0%)
05/28 01:40:00AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][250/469]	Step 7504	Loss 0.0388	Prec@(1,5) (98.8%, 100.0%)
05/28 01:40:03AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][300/469]	Step 7504	Loss 0.0392	Prec@(1,5) (98.8%, 100.0%)
05/28 01:40:06AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][350/469]	Step 7504	Loss 0.0397	Prec@(1,5) (98.8%, 100.0%)
05/28 01:40:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][400/469]	Step 7504	Loss 0.0390	Prec@(1,5) (98.8%, 100.0%)
05/28 01:40:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][450/469]	Step 7504	Loss 0.0398	Prec@(1,5) (98.8%, 100.0%)
05/28 01:40:14AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [15][468/469]	Step 7504	Loss 0.0396	Prec@(1,5) (98.8%, 100.0%)
05/28 01:40:14AM searchStage_trainer.py:260 [INFO] Valid: [ 15/49] Final Prec@1 98.8133%
05/28 01:40:14AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:40:14AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2435, 0.2651, 0.2423, 0.2491],
        [0.2472, 0.2851, 0.2557, 0.2120]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.2847, 0.2468, 0.2266],
        [0.2833, 0.2555, 0.2453, 0.2160],
        [0.2590, 0.2991, 0.2321, 0.2098]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2760, 0.2682, 0.2616, 0.1941],
        [0.2742, 0.2788, 0.2105, 0.2365],
        [0.2629, 0.2904, 0.2268, 0.2199]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2553, 0.2783, 0.2444, 0.2220],
        [0.2513, 0.2879, 0.2469, 0.2139],
        [0.2545, 0.2747, 0.2247, 0.2461]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2642, 0.2859, 0.2290, 0.2210],
        [0.2591, 0.2884, 0.2348, 0.2177],
        [0.2545, 0.2760, 0.2327, 0.2367]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2725, 0.2427, 0.2340],
        [0.2467, 0.2735, 0.2304, 0.2495],
        [0.2478, 0.2848, 0.2537, 0.2137]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:40:35AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][50/468]	Step 7554	lr 0.01943	Loss 0.0013 (0.0225)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:40:54AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][100/468]	Step 7604	lr 0.01943	Loss 0.1035 (0.0212)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:41:14AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][150/468]	Step 7654	lr 0.01943	Loss 0.0336 (0.0229)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:41:34AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][200/468]	Step 7704	lr 0.01943	Loss 0.0046 (0.0247)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:41:53AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][250/468]	Step 7754	lr 0.01943	Loss 0.0004 (0.0248)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:42:13AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][300/468]	Step 7804	lr 0.01943	Loss 0.0010 (0.0244)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:42:32AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][350/468]	Step 7854	lr 0.01943	Loss 0.0101 (0.0255)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:42:52AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][400/468]	Step 7904	lr 0.01943	Loss 0.0797 (0.0266)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:43:11AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][450/468]	Step 7954	lr 0.01943	Loss 0.0168 (0.0268)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:43:18AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [16][468/468]	Step 7972	lr 0.01943	Loss 0.0785 (0.0269)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:43:19AM searchShareStage_trainer.py:134 [INFO] Train: [ 16/49] Final Prec@1 99.1167%
05/28 01:43:22AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][50/469]	Step 7973	Loss 0.0348	Prec@(1,5) (99.1%, 100.0%)
05/28 01:43:25AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][100/469]	Step 7973	Loss 0.0343	Prec@(1,5) (99.0%, 100.0%)
05/28 01:43:28AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][150/469]	Step 7973	Loss 0.0379	Prec@(1,5) (98.8%, 100.0%)
05/28 01:43:31AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][200/469]	Step 7973	Loss 0.0380	Prec@(1,5) (98.8%, 100.0%)
05/28 01:43:34AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][250/469]	Step 7973	Loss 0.0376	Prec@(1,5) (98.8%, 100.0%)
05/28 01:43:38AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][300/469]	Step 7973	Loss 0.0379	Prec@(1,5) (98.8%, 100.0%)
05/28 01:43:41AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][350/469]	Step 7973	Loss 0.0376	Prec@(1,5) (98.9%, 100.0%)
05/28 01:43:44AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][400/469]	Step 7973	Loss 0.0376	Prec@(1,5) (98.9%, 100.0%)
05/28 01:43:47AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][450/469]	Step 7973	Loss 0.0387	Prec@(1,5) (98.8%, 100.0%)
05/28 01:43:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [16][468/469]	Step 7973	Loss 0.0382	Prec@(1,5) (98.8%, 100.0%)
05/28 01:43:48AM searchStage_trainer.py:260 [INFO] Valid: [ 16/49] Final Prec@1 98.8433%
05/28 01:43:48AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:43:48AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2435, 0.2648, 0.2424, 0.2493],
        [0.2472, 0.2849, 0.2559, 0.2121]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2847, 0.2470, 0.2265],
        [0.2836, 0.2551, 0.2451, 0.2162],
        [0.2590, 0.2989, 0.2322, 0.2100]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2765, 0.2683, 0.2615, 0.1937],
        [0.2744, 0.2785, 0.2105, 0.2366],
        [0.2629, 0.2902, 0.2269, 0.2201]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2783, 0.2446, 0.2220],
        [0.2513, 0.2877, 0.2470, 0.2140],
        [0.2544, 0.2745, 0.2248, 0.2463]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2642, 0.2859, 0.2291, 0.2209],
        [0.2591, 0.2881, 0.2348, 0.2179],
        [0.2545, 0.2758, 0.2328, 0.2369]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2722, 0.2429, 0.2342],
        [0.2466, 0.2733, 0.2304, 0.2498],
        [0.2477, 0.2846, 0.2539, 0.2139]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:44:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][50/468]	Step 8023	lr 0.01878	Loss 0.0033 (0.0203)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:44:29AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][100/468]	Step 8073	lr 0.01878	Loss 0.0571 (0.0177)	Prec@(1,5) (99.5%, 100.0%)	
05/28 01:44:48AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][150/468]	Step 8123	lr 0.01878	Loss 0.0026 (0.0186)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:45:08AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][200/468]	Step 8173	lr 0.01878	Loss 0.0365 (0.0201)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:45:28AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][250/468]	Step 8223	lr 0.01878	Loss 0.0006 (0.0196)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:45:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][300/468]	Step 8273	lr 0.01878	Loss 0.0137 (0.0202)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:46:07AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][350/468]	Step 8323	lr 0.01878	Loss 0.0040 (0.0206)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:46:27AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][400/468]	Step 8373	lr 0.01878	Loss 0.0860 (0.0218)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:46:46AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][450/468]	Step 8423	lr 0.01878	Loss 0.0167 (0.0228)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:46:53AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [17][468/468]	Step 8441	lr 0.01878	Loss 0.0197 (0.0229)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:46:54AM searchShareStage_trainer.py:134 [INFO] Train: [ 17/49] Final Prec@1 99.2567%
05/28 01:46:57AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][50/469]	Step 8442	Loss 0.0582	Prec@(1,5) (98.3%, 100.0%)
05/28 01:47:01AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][100/469]	Step 8442	Loss 0.0555	Prec@(1,5) (98.4%, 100.0%)
05/28 01:47:04AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][150/469]	Step 8442	Loss 0.0557	Prec@(1,5) (98.4%, 100.0%)
05/28 01:47:07AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][200/469]	Step 8442	Loss 0.0565	Prec@(1,5) (98.3%, 100.0%)
05/28 01:47:10AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][250/469]	Step 8442	Loss 0.0571	Prec@(1,5) (98.2%, 100.0%)
05/28 01:47:13AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][300/469]	Step 8442	Loss 0.0585	Prec@(1,5) (98.2%, 100.0%)
05/28 01:47:16AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][350/469]	Step 8442	Loss 0.0607	Prec@(1,5) (98.2%, 100.0%)
05/28 01:47:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][400/469]	Step 8442	Loss 0.0607	Prec@(1,5) (98.2%, 100.0%)
05/28 01:47:23AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][450/469]	Step 8442	Loss 0.0608	Prec@(1,5) (98.2%, 100.0%)
05/28 01:47:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [17][468/469]	Step 8442	Loss 0.0606	Prec@(1,5) (98.2%, 100.0%)
05/28 01:47:24AM searchStage_trainer.py:260 [INFO] Valid: [ 17/49] Final Prec@1 98.2233%
05/28 01:47:24AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:47:24AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2435, 0.2645, 0.2425, 0.2496],
        [0.2471, 0.2848, 0.2560, 0.2121]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2848, 0.2471, 0.2262],
        [0.2839, 0.2547, 0.2450, 0.2164],
        [0.2590, 0.2986, 0.2322, 0.2102]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2765, 0.2683, 0.2616, 0.1936],
        [0.2743, 0.2782, 0.2105, 0.2369],
        [0.2628, 0.2900, 0.2270, 0.2202]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2551, 0.2782, 0.2447, 0.2220],
        [0.2512, 0.2876, 0.2471, 0.2141],
        [0.2544, 0.2743, 0.2249, 0.2465]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2642, 0.2858, 0.2291, 0.2209],
        [0.2590, 0.2880, 0.2351, 0.2180],
        [0.2544, 0.2755, 0.2329, 0.2372]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2720, 0.2431, 0.2342],
        [0.2465, 0.2731, 0.2304, 0.2500],
        [0.2476, 0.2844, 0.2540, 0.2140]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:47:45AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][50/468]	Step 8492	lr 0.01811	Loss 0.0690 (0.0214)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:48:04AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][100/468]	Step 8542	lr 0.01811	Loss 0.0017 (0.0213)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:48:24AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][150/468]	Step 8592	lr 0.01811	Loss 0.0004 (0.0189)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:48:44AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][200/468]	Step 8642	lr 0.01811	Loss 0.0008 (0.0189)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:49:04AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][250/468]	Step 8692	lr 0.01811	Loss 0.0294 (0.0187)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:49:23AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][300/468]	Step 8742	lr 0.01811	Loss 0.1427 (0.0205)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:49:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][350/468]	Step 8792	lr 0.01811	Loss 0.0692 (0.0223)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:50:02AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][400/468]	Step 8842	lr 0.01811	Loss 0.0189 (0.0220)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:50:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][450/468]	Step 8892	lr 0.01811	Loss 0.0329 (0.0221)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:50:29AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [18][468/468]	Step 8910	lr 0.01811	Loss 0.0042 (0.0217)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:50:30AM searchShareStage_trainer.py:134 [INFO] Train: [ 18/49] Final Prec@1 99.3100%
05/28 01:50:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][50/469]	Step 8911	Loss 0.0346	Prec@(1,5) (99.0%, 100.0%)
05/28 01:50:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][100/469]	Step 8911	Loss 0.0439	Prec@(1,5) (98.7%, 100.0%)
05/28 01:50:39AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][150/469]	Step 8911	Loss 0.0400	Prec@(1,5) (98.8%, 100.0%)
05/28 01:50:42AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][200/469]	Step 8911	Loss 0.0374	Prec@(1,5) (98.9%, 100.0%)
05/28 01:50:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][250/469]	Step 8911	Loss 0.0355	Prec@(1,5) (98.9%, 100.0%)
05/28 01:50:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][300/469]	Step 8911	Loss 0.0354	Prec@(1,5) (98.9%, 100.0%)
05/28 01:50:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][350/469]	Step 8911	Loss 0.0374	Prec@(1,5) (98.8%, 100.0%)
05/28 01:50:54AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][400/469]	Step 8911	Loss 0.0380	Prec@(1,5) (98.8%, 100.0%)
05/28 01:50:57AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][450/469]	Step 8911	Loss 0.0379	Prec@(1,5) (98.8%, 100.0%)
05/28 01:50:58AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [18][468/469]	Step 8911	Loss 0.0382	Prec@(1,5) (98.8%, 100.0%)
05/28 01:50:59AM searchStage_trainer.py:260 [INFO] Valid: [ 18/49] Final Prec@1 98.8433%
05/28 01:50:59AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:50:59AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2434, 0.2642, 0.2426, 0.2498],
        [0.2471, 0.2846, 0.2561, 0.2121]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2415, 0.2848, 0.2473, 0.2263],
        [0.2839, 0.2545, 0.2451, 0.2164],
        [0.2589, 0.2984, 0.2323, 0.2103]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2768, 0.2683, 0.2615, 0.1935],
        [0.2743, 0.2781, 0.2105, 0.2371],
        [0.2628, 0.2898, 0.2271, 0.2203]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2781, 0.2448, 0.2221],
        [0.2512, 0.2875, 0.2472, 0.2141],
        [0.2543, 0.2741, 0.2250, 0.2466]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2642, 0.2857, 0.2291, 0.2210],
        [0.2589, 0.2878, 0.2352, 0.2181],
        [0.2544, 0.2753, 0.2330, 0.2373]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2719, 0.2435, 0.2342],
        [0.2464, 0.2729, 0.2304, 0.2503],
        [0.2475, 0.2842, 0.2542, 0.2141]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:51:19AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][50/468]	Step 8961	lr 0.01742	Loss 0.0056 (0.0195)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:51:39AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][100/468]	Step 9011	lr 0.01742	Loss 0.0314 (0.0216)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:51:59AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][150/468]	Step 9061	lr 0.01742	Loss 0.0203 (0.0242)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:52:19AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][200/468]	Step 9111	lr 0.01742	Loss 0.0218 (0.0255)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:52:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][250/468]	Step 9161	lr 0.01742	Loss 0.0412 (0.0262)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:52:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][300/468]	Step 9211	lr 0.01742	Loss 0.0072 (0.0264)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:53:18AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][350/468]	Step 9261	lr 0.01742	Loss 0.0423 (0.0253)	Prec@(1,5) (99.1%, 100.0%)	
05/28 01:53:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][400/468]	Step 9311	lr 0.01742	Loss 0.1600 (0.0256)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:53:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][450/468]	Step 9361	lr 0.01742	Loss 0.0037 (0.0254)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:54:04AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [19][468/468]	Step 9379	lr 0.01742	Loss 0.0054 (0.0257)	Prec@(1,5) (99.2%, 100.0%)	
05/28 01:54:05AM searchShareStage_trainer.py:134 [INFO] Train: [ 19/49] Final Prec@1 99.1600%
05/28 01:54:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][50/469]	Step 9380	Loss 0.0306	Prec@(1,5) (99.2%, 100.0%)
05/28 01:54:11AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][100/469]	Step 9380	Loss 0.0276	Prec@(1,5) (99.2%, 100.0%)
05/28 01:54:15AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][150/469]	Step 9380	Loss 0.0319	Prec@(1,5) (99.1%, 100.0%)
05/28 01:54:18AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][200/469]	Step 9380	Loss 0.0322	Prec@(1,5) (99.2%, 100.0%)
05/28 01:54:21AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][250/469]	Step 9380	Loss 0.0318	Prec@(1,5) (99.1%, 100.0%)
05/28 01:54:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][300/469]	Step 9380	Loss 0.0326	Prec@(1,5) (99.1%, 100.0%)
05/28 01:54:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][350/469]	Step 9380	Loss 0.0342	Prec@(1,5) (99.0%, 100.0%)
05/28 01:54:30AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][400/469]	Step 9380	Loss 0.0332	Prec@(1,5) (99.1%, 100.0%)
05/28 01:54:34AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][450/469]	Step 9380	Loss 0.0327	Prec@(1,5) (99.1%, 100.0%)
05/28 01:54:35AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [19][468/469]	Step 9380	Loss 0.0328	Prec@(1,5) (99.1%, 100.0%)
05/28 01:54:35AM searchStage_trainer.py:260 [INFO] Valid: [ 19/49] Final Prec@1 99.0900%
05/28 01:54:35AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:54:35AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0900%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2433, 0.2640, 0.2427, 0.2500],
        [0.2471, 0.2844, 0.2563, 0.2122]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2417, 0.2847, 0.2471, 0.2265],
        [0.2842, 0.2543, 0.2451, 0.2164],
        [0.2589, 0.2982, 0.2324, 0.2104]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2770, 0.2684, 0.2614, 0.1932],
        [0.2743, 0.2779, 0.2106, 0.2372],
        [0.2628, 0.2896, 0.2272, 0.2204]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2780, 0.2448, 0.2223],
        [0.2512, 0.2874, 0.2474, 0.2141],
        [0.2542, 0.2739, 0.2251, 0.2468]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2641, 0.2855, 0.2292, 0.2213],
        [0.2588, 0.2877, 0.2354, 0.2181],
        [0.2543, 0.2752, 0.2331, 0.2375]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2717, 0.2438, 0.2342],
        [0.2463, 0.2727, 0.2304, 0.2505],
        [0.2475, 0.2840, 0.2543, 0.2143]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:54:56AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][50/468]	Step 9430	lr 0.01671	Loss 0.0507 (0.0186)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:55:16AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][100/468]	Step 9480	lr 0.01671	Loss 0.0391 (0.0187)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:55:35AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][150/468]	Step 9530	lr 0.01671	Loss 0.0487 (0.0167)	Prec@(1,5) (99.5%, 100.0%)	
05/28 01:55:55AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][200/468]	Step 9580	lr 0.01671	Loss 0.0216 (0.0163)	Prec@(1,5) (99.5%, 100.0%)	
05/28 01:56:15AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][250/468]	Step 9630	lr 0.01671	Loss 0.0051 (0.0169)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:56:34AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][300/468]	Step 9680	lr 0.01671	Loss 0.0578 (0.0182)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:56:54AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][350/468]	Step 9730	lr 0.01671	Loss 0.0039 (0.0191)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:57:14AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][400/468]	Step 9780	lr 0.01671	Loss 0.0010 (0.0190)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:57:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][450/468]	Step 9830	lr 0.01671	Loss 0.0028 (0.0198)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:57:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [20][468/468]	Step 9848	lr 0.01671	Loss 0.0212 (0.0196)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:57:40AM searchShareStage_trainer.py:134 [INFO] Train: [ 20/49] Final Prec@1 99.3867%
05/28 01:57:44AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][50/469]	Step 9849	Loss 0.0329	Prec@(1,5) (99.2%, 100.0%)
05/28 01:57:47AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][100/469]	Step 9849	Loss 0.0342	Prec@(1,5) (99.1%, 100.0%)
05/28 01:57:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][150/469]	Step 9849	Loss 0.0348	Prec@(1,5) (99.0%, 100.0%)
05/28 01:57:53AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][200/469]	Step 9849	Loss 0.0339	Prec@(1,5) (99.0%, 100.0%)
05/28 01:57:56AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][250/469]	Step 9849	Loss 0.0334	Prec@(1,5) (99.0%, 100.0%)
05/28 01:57:58AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][300/469]	Step 9849	Loss 0.0323	Prec@(1,5) (99.1%, 100.0%)
05/28 01:58:01AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][350/469]	Step 9849	Loss 0.0313	Prec@(1,5) (99.1%, 100.0%)
05/28 01:58:05AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][400/469]	Step 9849	Loss 0.0315	Prec@(1,5) (99.1%, 100.0%)
05/28 01:58:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][450/469]	Step 9849	Loss 0.0314	Prec@(1,5) (99.1%, 100.0%)
05/28 01:58:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [20][468/469]	Step 9849	Loss 0.0307	Prec@(1,5) (99.1%, 100.0%)
05/28 01:58:09AM searchStage_trainer.py:260 [INFO] Valid: [ 20/49] Final Prec@1 99.0767%
05/28 01:58:09AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 01:58:09AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0900%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2432, 0.2639, 0.2428, 0.2502],
        [0.2471, 0.2842, 0.2564, 0.2123]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2846, 0.2471, 0.2266],
        [0.2847, 0.2540, 0.2449, 0.2164],
        [0.2590, 0.2981, 0.2324, 0.2106]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2772, 0.2684, 0.2614, 0.1931],
        [0.2743, 0.2777, 0.2106, 0.2374],
        [0.2627, 0.2895, 0.2273, 0.2205]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2780, 0.2450, 0.2224],
        [0.2511, 0.2873, 0.2475, 0.2141],
        [0.2542, 0.2738, 0.2251, 0.2469]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2640, 0.2854, 0.2292, 0.2213],
        [0.2588, 0.2875, 0.2354, 0.2183],
        [0.2543, 0.2750, 0.2331, 0.2376]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2715, 0.2441, 0.2342],
        [0.2463, 0.2726, 0.2304, 0.2507],
        [0.2474, 0.2838, 0.2544, 0.2144]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 01:58:30AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][50/468]	Step 9899	lr 0.01598	Loss 0.0860 (0.0212)	Prec@(1,5) (99.3%, 100.0%)	
05/28 01:58:50AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][100/468]	Step 9949	lr 0.01598	Loss 0.0007 (0.0189)	Prec@(1,5) (99.4%, 100.0%)	
05/28 01:59:10AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][150/468]	Step 9999	lr 0.01598	Loss 0.0096 (0.0168)	Prec@(1,5) (99.5%, 100.0%)	
05/28 01:59:29AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][200/468]	Step 10049	lr 0.01598	Loss 0.0041 (0.0173)	Prec@(1,5) (99.5%, 100.0%)	
05/28 01:59:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][250/468]	Step 10099	lr 0.01598	Loss 0.0031 (0.0179)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:00:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][300/468]	Step 10149	lr 0.01598	Loss 0.0090 (0.0186)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:00:29AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][350/468]	Step 10199	lr 0.01598	Loss 0.1251 (0.0189)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:00:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][400/468]	Step 10249	lr 0.01598	Loss 0.0307 (0.0184)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:01:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][450/468]	Step 10299	lr 0.01598	Loss 0.0123 (0.0190)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:01:16AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [21][468/468]	Step 10317	lr 0.01598	Loss 0.0028 (0.0187)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:01:16AM searchShareStage_trainer.py:134 [INFO] Train: [ 21/49] Final Prec@1 99.4133%
05/28 02:01:20AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][50/469]	Step 10318	Loss 0.0402	Prec@(1,5) (98.8%, 100.0%)
05/28 02:01:23AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][100/469]	Step 10318	Loss 0.0414	Prec@(1,5) (98.8%, 100.0%)
05/28 02:01:26AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][150/469]	Step 10318	Loss 0.0422	Prec@(1,5) (98.8%, 100.0%)
05/28 02:01:29AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][200/469]	Step 10318	Loss 0.0461	Prec@(1,5) (98.7%, 100.0%)
05/28 02:01:32AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][250/469]	Step 10318	Loss 0.0439	Prec@(1,5) (98.7%, 100.0%)
05/28 02:01:34AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][300/469]	Step 10318	Loss 0.0418	Prec@(1,5) (98.8%, 100.0%)
05/28 02:01:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][350/469]	Step 10318	Loss 0.0427	Prec@(1,5) (98.8%, 100.0%)
05/28 02:01:40AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][400/469]	Step 10318	Loss 0.0402	Prec@(1,5) (98.8%, 100.0%)
05/28 02:01:43AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][450/469]	Step 10318	Loss 0.0404	Prec@(1,5) (98.8%, 100.0%)
05/28 02:01:44AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [21][468/469]	Step 10318	Loss 0.0404	Prec@(1,5) (98.8%, 100.0%)
05/28 02:01:45AM searchStage_trainer.py:260 [INFO] Valid: [ 21/49] Final Prec@1 98.8200%
05/28 02:01:45AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:01:45AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0900%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2433, 0.2636, 0.2428, 0.2504],
        [0.2471, 0.2841, 0.2566, 0.2123]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2845, 0.2471, 0.2267],
        [0.2850, 0.2538, 0.2448, 0.2163],
        [0.2590, 0.2979, 0.2324, 0.2107]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2775, 0.2685, 0.2614, 0.1926],
        [0.2743, 0.2776, 0.2105, 0.2376],
        [0.2627, 0.2893, 0.2274, 0.2206]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2779, 0.2450, 0.2224],
        [0.2511, 0.2872, 0.2476, 0.2141],
        [0.2541, 0.2736, 0.2252, 0.2471]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2639, 0.2854, 0.2295, 0.2213],
        [0.2587, 0.2874, 0.2355, 0.2184],
        [0.2542, 0.2748, 0.2332, 0.2378]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2714, 0.2441, 0.2344],
        [0.2462, 0.2724, 0.2305, 0.2509],
        [0.2474, 0.2837, 0.2545, 0.2144]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:02:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][50/468]	Step 10368	lr 0.01525	Loss 0.0011 (0.0210)	Prec@(1,5) (99.3%, 100.0%)	
05/28 02:02:25AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][100/468]	Step 10418	lr 0.01525	Loss 0.0091 (0.0195)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:02:45AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][150/468]	Step 10468	lr 0.01525	Loss 0.0277 (0.0199)	Prec@(1,5) (99.3%, 100.0%)	
05/28 02:03:04AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][200/468]	Step 10518	lr 0.01525	Loss 0.0230 (0.0189)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:03:24AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][250/468]	Step 10568	lr 0.01525	Loss 0.1418 (0.0200)	Prec@(1,5) (99.3%, 100.0%)	
05/28 02:03:44AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][300/468]	Step 10618	lr 0.01525	Loss 0.0050 (0.0206)	Prec@(1,5) (99.3%, 100.0%)	
05/28 02:04:03AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][350/468]	Step 10668	lr 0.01525	Loss 0.0016 (0.0199)	Prec@(1,5) (99.3%, 100.0%)	
05/28 02:04:23AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][400/468]	Step 10718	lr 0.01525	Loss 0.0011 (0.0191)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:04:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][450/468]	Step 10768	lr 0.01525	Loss 0.0526 (0.0193)	Prec@(1,5) (99.3%, 100.0%)	
05/28 02:04:50AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [22][468/468]	Step 10786	lr 0.01525	Loss 0.0069 (0.0198)	Prec@(1,5) (99.3%, 100.0%)	
05/28 02:04:51AM searchShareStage_trainer.py:134 [INFO] Train: [ 22/49] Final Prec@1 99.3300%
05/28 02:04:54AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][50/469]	Step 10787	Loss 0.0292	Prec@(1,5) (99.2%, 100.0%)
05/28 02:04:57AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][100/469]	Step 10787	Loss 0.0353	Prec@(1,5) (99.0%, 100.0%)
05/28 02:05:00AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][150/469]	Step 10787	Loss 0.0362	Prec@(1,5) (98.9%, 100.0%)
05/28 02:05:03AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][200/469]	Step 10787	Loss 0.0373	Prec@(1,5) (98.9%, 100.0%)
05/28 02:05:06AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][250/469]	Step 10787	Loss 0.0373	Prec@(1,5) (98.9%, 100.0%)
05/28 02:05:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][300/469]	Step 10787	Loss 0.0363	Prec@(1,5) (98.9%, 100.0%)
05/28 02:05:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][350/469]	Step 10787	Loss 0.0358	Prec@(1,5) (98.9%, 100.0%)
05/28 02:05:15AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][400/469]	Step 10787	Loss 0.0361	Prec@(1,5) (98.9%, 100.0%)
05/28 02:05:18AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][450/469]	Step 10787	Loss 0.0359	Prec@(1,5) (98.9%, 100.0%)
05/28 02:05:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [22][468/469]	Step 10787	Loss 0.0357	Prec@(1,5) (98.9%, 100.0%)
05/28 02:05:19AM searchStage_trainer.py:260 [INFO] Valid: [ 22/49] Final Prec@1 98.9133%
05/28 02:05:19AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:05:19AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0900%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2432, 0.2635, 0.2429, 0.2505],
        [0.2470, 0.2840, 0.2567, 0.2123]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2417, 0.2844, 0.2471, 0.2267],
        [0.2850, 0.2537, 0.2449, 0.2164],
        [0.2590, 0.2978, 0.2325, 0.2108]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2779, 0.2684, 0.2615, 0.1922],
        [0.2743, 0.2775, 0.2105, 0.2377],
        [0.2627, 0.2892, 0.2274, 0.2207]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2777, 0.2451, 0.2225],
        [0.2511, 0.2871, 0.2477, 0.2142],
        [0.2541, 0.2735, 0.2252, 0.2472]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2852, 0.2296, 0.2213],
        [0.2587, 0.2872, 0.2356, 0.2185],
        [0.2542, 0.2747, 0.2332, 0.2379]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2712, 0.2443, 0.2344],
        [0.2462, 0.2723, 0.2305, 0.2510],
        [0.2473, 0.2835, 0.2546, 0.2145]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:05:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][50/468]	Step 10837	lr 0.0145	Loss 0.0022 (0.0179)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:06:00AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][100/468]	Step 10887	lr 0.0145	Loss 0.0119 (0.0190)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:06:19AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][150/468]	Step 10937	lr 0.0145	Loss 0.0388 (0.0214)	Prec@(1,5) (99.2%, 100.0%)	
05/28 02:06:39AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][200/468]	Step 10987	lr 0.0145	Loss 0.0964 (0.0214)	Prec@(1,5) (99.3%, 100.0%)	
05/28 02:06:59AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][250/468]	Step 11037	lr 0.0145	Loss 0.0006 (0.0205)	Prec@(1,5) (99.3%, 100.0%)	
05/28 02:07:18AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][300/468]	Step 11087	lr 0.0145	Loss 0.0003 (0.0196)	Prec@(1,5) (99.3%, 100.0%)	
05/28 02:07:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][350/468]	Step 11137	lr 0.0145	Loss 0.0018 (0.0198)	Prec@(1,5) (99.3%, 100.0%)	
05/28 02:07:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][400/468]	Step 11187	lr 0.0145	Loss 0.0141 (0.0196)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:08:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][450/468]	Step 11237	lr 0.0145	Loss 0.0107 (0.0190)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:08:24AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [23][468/468]	Step 11255	lr 0.0145	Loss 0.0173 (0.0190)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:08:25AM searchShareStage_trainer.py:134 [INFO] Train: [ 23/49] Final Prec@1 99.3767%
05/28 02:08:28AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][50/469]	Step 11256	Loss 0.0237	Prec@(1,5) (99.1%, 100.0%)
05/28 02:08:31AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][100/469]	Step 11256	Loss 0.0292	Prec@(1,5) (99.1%, 100.0%)
05/28 02:08:34AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][150/469]	Step 11256	Loss 0.0324	Prec@(1,5) (99.1%, 100.0%)
05/28 02:08:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][200/469]	Step 11256	Loss 0.0314	Prec@(1,5) (99.1%, 100.0%)
05/28 02:08:40AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][250/469]	Step 11256	Loss 0.0307	Prec@(1,5) (99.1%, 100.0%)
05/28 02:08:43AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][300/469]	Step 11256	Loss 0.0334	Prec@(1,5) (99.1%, 100.0%)
05/28 02:08:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][350/469]	Step 11256	Loss 0.0329	Prec@(1,5) (99.1%, 100.0%)
05/28 02:08:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][400/469]	Step 11256	Loss 0.0339	Prec@(1,5) (99.1%, 100.0%)
05/28 02:08:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][450/469]	Step 11256	Loss 0.0339	Prec@(1,5) (99.1%, 100.0%)
05/28 02:08:52AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [23][468/469]	Step 11256	Loss 0.0335	Prec@(1,5) (99.1%, 100.0%)
05/28 02:08:52AM searchStage_trainer.py:260 [INFO] Valid: [ 23/49] Final Prec@1 99.0833%
05/28 02:08:52AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:08:53AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0900%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2431, 0.2633, 0.2429, 0.2506],
        [0.2470, 0.2838, 0.2568, 0.2124]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2416, 0.2844, 0.2472, 0.2267],
        [0.2852, 0.2535, 0.2447, 0.2165],
        [0.2590, 0.2976, 0.2325, 0.2109]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2780, 0.2683, 0.2615, 0.1921],
        [0.2744, 0.2773, 0.2105, 0.2378],
        [0.2627, 0.2890, 0.2275, 0.2208]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2777, 0.2452, 0.2225],
        [0.2511, 0.2870, 0.2478, 0.2142],
        [0.2541, 0.2733, 0.2253, 0.2473]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2851, 0.2296, 0.2215],
        [0.2586, 0.2871, 0.2356, 0.2186],
        [0.2542, 0.2745, 0.2333, 0.2380]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2711, 0.2444, 0.2345],
        [0.2461, 0.2722, 0.2305, 0.2512],
        [0.2473, 0.2834, 0.2547, 0.2146]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:09:13AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][50/468]	Step 11306	lr 0.01375	Loss 0.0026 (0.0146)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:09:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][100/468]	Step 11356	lr 0.01375	Loss 0.0022 (0.0178)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:09:52AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][150/468]	Step 11406	lr 0.01375	Loss 0.0124 (0.0194)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:10:12AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][200/468]	Step 11456	lr 0.01375	Loss 0.0017 (0.0192)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:10:32AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][250/468]	Step 11506	lr 0.01375	Loss 0.0008 (0.0174)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:10:51AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][300/468]	Step 11556	lr 0.01375	Loss 0.0038 (0.0162)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:11:11AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][350/468]	Step 11606	lr 0.01375	Loss 0.0017 (0.0167)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:11:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][400/468]	Step 11656	lr 0.01375	Loss 0.0022 (0.0162)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:11:50AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][450/468]	Step 11706	lr 0.01375	Loss 0.0171 (0.0166)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:11:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [24][468/468]	Step 11724	lr 0.01375	Loss 0.0074 (0.0165)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:11:58AM searchShareStage_trainer.py:134 [INFO] Train: [ 24/49] Final Prec@1 99.4633%
05/28 02:12:01AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][50/469]	Step 11725	Loss 0.0280	Prec@(1,5) (99.1%, 100.0%)
05/28 02:12:04AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][100/469]	Step 11725	Loss 0.0394	Prec@(1,5) (99.0%, 100.0%)
05/28 02:12:07AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][150/469]	Step 11725	Loss 0.0328	Prec@(1,5) (99.1%, 100.0%)
05/28 02:12:10AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][200/469]	Step 11725	Loss 0.0356	Prec@(1,5) (99.0%, 100.0%)
05/28 02:12:13AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][250/469]	Step 11725	Loss 0.0328	Prec@(1,5) (99.1%, 100.0%)
05/28 02:12:16AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][300/469]	Step 11725	Loss 0.0333	Prec@(1,5) (99.1%, 100.0%)
05/28 02:12:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][350/469]	Step 11725	Loss 0.0331	Prec@(1,5) (99.1%, 100.0%)
05/28 02:12:22AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][400/469]	Step 11725	Loss 0.0328	Prec@(1,5) (99.1%, 100.0%)
05/28 02:12:25AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][450/469]	Step 11725	Loss 0.0336	Prec@(1,5) (99.1%, 100.0%)
05/28 02:12:26AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [24][468/469]	Step 11725	Loss 0.0338	Prec@(1,5) (99.1%, 100.0%)
05/28 02:12:27AM searchStage_trainer.py:260 [INFO] Valid: [ 24/49] Final Prec@1 99.0533%
05/28 02:12:27AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:12:27AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0900%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2431, 0.2632, 0.2430, 0.2508],
        [0.2470, 0.2837, 0.2569, 0.2124]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2417, 0.2843, 0.2472, 0.2268],
        [0.2853, 0.2534, 0.2447, 0.2165],
        [0.2590, 0.2975, 0.2326, 0.2110]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2782, 0.2684, 0.2616, 0.1919],
        [0.2744, 0.2772, 0.2105, 0.2379],
        [0.2627, 0.2889, 0.2275, 0.2209]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2776, 0.2453, 0.2226],
        [0.2511, 0.2869, 0.2478, 0.2142],
        [0.2540, 0.2732, 0.2253, 0.2474]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2850, 0.2297, 0.2215],
        [0.2586, 0.2870, 0.2357, 0.2186],
        [0.2541, 0.2744, 0.2333, 0.2381]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2710, 0.2445, 0.2346],
        [0.2461, 0.2721, 0.2306, 0.2513],
        [0.2473, 0.2832, 0.2548, 0.2147]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:12:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][50/468]	Step 11775	lr 0.013	Loss 0.0005 (0.0128)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:13:07AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][100/468]	Step 11825	lr 0.013	Loss 0.0389 (0.0143)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:13:27AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][150/468]	Step 11875	lr 0.013	Loss 0.0014 (0.0150)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:13:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][200/468]	Step 11925	lr 0.013	Loss 0.0025 (0.0177)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:14:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][250/468]	Step 11975	lr 0.013	Loss 0.0033 (0.0172)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:14:26AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][300/468]	Step 12025	lr 0.013	Loss 0.0017 (0.0164)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:14:46AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][350/468]	Step 12075	lr 0.013	Loss 0.0024 (0.0163)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:15:05AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][400/468]	Step 12125	lr 0.013	Loss 0.0022 (0.0156)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:15:25AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][450/468]	Step 12175	lr 0.013	Loss 0.0008 (0.0154)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:15:32AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [25][468/468]	Step 12193	lr 0.013	Loss 0.0039 (0.0155)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:15:33AM searchShareStage_trainer.py:134 [INFO] Train: [ 25/49] Final Prec@1 99.5200%
05/28 02:15:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][50/469]	Step 12194	Loss 0.0343	Prec@(1,5) (98.8%, 100.0%)
05/28 02:15:39AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][100/469]	Step 12194	Loss 0.0322	Prec@(1,5) (98.9%, 100.0%)
05/28 02:15:42AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][150/469]	Step 12194	Loss 0.0363	Prec@(1,5) (98.9%, 100.0%)
05/28 02:15:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][200/469]	Step 12194	Loss 0.0352	Prec@(1,5) (98.9%, 100.0%)
05/28 02:15:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][250/469]	Step 12194	Loss 0.0374	Prec@(1,5) (98.9%, 100.0%)
05/28 02:15:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][300/469]	Step 12194	Loss 0.0377	Prec@(1,5) (98.8%, 100.0%)
05/28 02:15:53AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][350/469]	Step 12194	Loss 0.0388	Prec@(1,5) (98.8%, 100.0%)
05/28 02:15:57AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][400/469]	Step 12194	Loss 0.0377	Prec@(1,5) (98.8%, 100.0%)
05/28 02:16:00AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][450/469]	Step 12194	Loss 0.0385	Prec@(1,5) (98.8%, 100.0%)
05/28 02:16:01AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [25][468/469]	Step 12194	Loss 0.0381	Prec@(1,5) (98.8%, 100.0%)
05/28 02:16:01AM searchStage_trainer.py:260 [INFO] Valid: [ 25/49] Final Prec@1 98.8133%
05/28 02:16:01AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:16:01AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.0900%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2431, 0.2630, 0.2430, 0.2509],
        [0.2470, 0.2836, 0.2570, 0.2125]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2842, 0.2471, 0.2268],
        [0.2854, 0.2532, 0.2448, 0.2166],
        [0.2589, 0.2974, 0.2326, 0.2111]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2783, 0.2684, 0.2615, 0.1917],
        [0.2744, 0.2771, 0.2106, 0.2380],
        [0.2626, 0.2888, 0.2276, 0.2210]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2775, 0.2453, 0.2226],
        [0.2510, 0.2868, 0.2479, 0.2142],
        [0.2540, 0.2731, 0.2254, 0.2475]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2849, 0.2297, 0.2216],
        [0.2586, 0.2869, 0.2358, 0.2187],
        [0.2541, 0.2743, 0.2334, 0.2382]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2709, 0.2446, 0.2347],
        [0.2460, 0.2720, 0.2306, 0.2514],
        [0.2472, 0.2831, 0.2549, 0.2148]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:16:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][50/468]	Step 12244	lr 0.01225	Loss 0.0015 (0.0143)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:16:41AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][100/468]	Step 12294	lr 0.01225	Loss 0.0016 (0.0136)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:17:01AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][150/468]	Step 12344	lr 0.01225	Loss 0.0008 (0.0128)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:17:21AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][200/468]	Step 12394	lr 0.01225	Loss 0.0051 (0.0139)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:17:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][250/468]	Step 12444	lr 0.01225	Loss 0.0156 (0.0151)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:18:00AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][300/468]	Step 12494	lr 0.01225	Loss 0.0005 (0.0147)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:18:19AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][350/468]	Step 12544	lr 0.01225	Loss 0.0282 (0.0147)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:18:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][400/468]	Step 12594	lr 0.01225	Loss 0.0041 (0.0146)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:18:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][450/468]	Step 12644	lr 0.01225	Loss 0.0007 (0.0144)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:19:05AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [26][468/468]	Step 12662	lr 0.01225	Loss 0.0102 (0.0147)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:19:05AM searchShareStage_trainer.py:134 [INFO] Train: [ 26/49] Final Prec@1 99.5467%
05/28 02:19:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][50/469]	Step 12663	Loss 0.0349	Prec@(1,5) (98.9%, 99.9%)
05/28 02:19:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][100/469]	Step 12663	Loss 0.0306	Prec@(1,5) (99.0%, 100.0%)
05/28 02:19:15AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][150/469]	Step 12663	Loss 0.0312	Prec@(1,5) (99.0%, 100.0%)
05/28 02:19:18AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][200/469]	Step 12663	Loss 0.0288	Prec@(1,5) (99.1%, 100.0%)
05/28 02:19:21AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][250/469]	Step 12663	Loss 0.0295	Prec@(1,5) (99.1%, 100.0%)
05/28 02:19:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][300/469]	Step 12663	Loss 0.0299	Prec@(1,5) (99.0%, 100.0%)
05/28 02:19:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][350/469]	Step 12663	Loss 0.0301	Prec@(1,5) (99.1%, 100.0%)
05/28 02:19:30AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][400/469]	Step 12663	Loss 0.0291	Prec@(1,5) (99.1%, 100.0%)
05/28 02:19:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][450/469]	Step 12663	Loss 0.0297	Prec@(1,5) (99.1%, 100.0%)
05/28 02:19:35AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [26][468/469]	Step 12663	Loss 0.0296	Prec@(1,5) (99.1%, 100.0%)
05/28 02:19:35AM searchStage_trainer.py:260 [INFO] Valid: [ 26/49] Final Prec@1 99.1100%
05/28 02:19:35AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:19:35AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.1100%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2430, 0.2629, 0.2430, 0.2510],
        [0.2470, 0.2835, 0.2570, 0.2125]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2841, 0.2470, 0.2270],
        [0.2855, 0.2531, 0.2448, 0.2165],
        [0.2589, 0.2973, 0.2326, 0.2111]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2785, 0.2683, 0.2617, 0.1915],
        [0.2744, 0.2770, 0.2106, 0.2381],
        [0.2626, 0.2887, 0.2276, 0.2210]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2775, 0.2453, 0.2227],
        [0.2510, 0.2867, 0.2480, 0.2142],
        [0.2540, 0.2730, 0.2255, 0.2476]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2849, 0.2297, 0.2217],
        [0.2586, 0.2868, 0.2358, 0.2188],
        [0.2541, 0.2742, 0.2334, 0.2383]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2708, 0.2447, 0.2347],
        [0.2460, 0.2719, 0.2306, 0.2515],
        [0.2472, 0.2830, 0.2549, 0.2149]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:19:56AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][50/468]	Step 12713	lr 0.0115	Loss 0.0020 (0.0102)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:20:16AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][100/468]	Step 12763	lr 0.0115	Loss 0.0007 (0.0100)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:20:36AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][150/468]	Step 12813	lr 0.0115	Loss 0.0005 (0.0131)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:20:55AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][200/468]	Step 12863	lr 0.0115	Loss 0.0415 (0.0128)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:21:15AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][250/468]	Step 12913	lr 0.0115	Loss 0.0024 (0.0125)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:21:34AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][300/468]	Step 12963	lr 0.0115	Loss 0.0019 (0.0126)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:21:53AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][350/468]	Step 13013	lr 0.0115	Loss 0.0412 (0.0122)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:22:13AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][400/468]	Step 13063	lr 0.0115	Loss 0.0424 (0.0119)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:22:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][450/468]	Step 13113	lr 0.0115	Loss 0.0030 (0.0121)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:22:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [27][468/468]	Step 13131	lr 0.0115	Loss 0.0008 (0.0122)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:22:41AM searchShareStage_trainer.py:134 [INFO] Train: [ 27/49] Final Prec@1 99.6300%
05/28 02:22:44AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][50/469]	Step 13132	Loss 0.0333	Prec@(1,5) (99.0%, 100.0%)
05/28 02:22:47AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][100/469]	Step 13132	Loss 0.0292	Prec@(1,5) (99.1%, 100.0%)
05/28 02:22:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][150/469]	Step 13132	Loss 0.0268	Prec@(1,5) (99.2%, 100.0%)
05/28 02:22:54AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][200/469]	Step 13132	Loss 0.0276	Prec@(1,5) (99.2%, 100.0%)
05/28 02:22:56AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][250/469]	Step 13132	Loss 0.0273	Prec@(1,5) (99.2%, 100.0%)
05/28 02:22:59AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][300/469]	Step 13132	Loss 0.0264	Prec@(1,5) (99.2%, 100.0%)
05/28 02:23:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][350/469]	Step 13132	Loss 0.0275	Prec@(1,5) (99.2%, 100.0%)
05/28 02:23:05AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][400/469]	Step 13132	Loss 0.0270	Prec@(1,5) (99.2%, 100.0%)
05/28 02:23:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][450/469]	Step 13132	Loss 0.0274	Prec@(1,5) (99.2%, 100.0%)
05/28 02:23:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [27][468/469]	Step 13132	Loss 0.0279	Prec@(1,5) (99.2%, 100.0%)
05/28 02:23:10AM searchStage_trainer.py:260 [INFO] Valid: [ 27/49] Final Prec@1 99.2200%
05/28 02:23:10AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:23:10AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.2200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2430, 0.2628, 0.2431, 0.2511],
        [0.2470, 0.2834, 0.2571, 0.2125]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2841, 0.2471, 0.2270],
        [0.2856, 0.2530, 0.2448, 0.2166],
        [0.2589, 0.2972, 0.2327, 0.2112]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2785, 0.2682, 0.2618, 0.1915],
        [0.2744, 0.2769, 0.2106, 0.2381],
        [0.2626, 0.2886, 0.2277, 0.2211]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2774, 0.2454, 0.2227],
        [0.2510, 0.2867, 0.2481, 0.2143],
        [0.2539, 0.2729, 0.2255, 0.2477]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2848, 0.2297, 0.2217],
        [0.2585, 0.2867, 0.2359, 0.2188],
        [0.2541, 0.2741, 0.2335, 0.2384]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2707, 0.2448, 0.2348],
        [0.2460, 0.2718, 0.2307, 0.2516],
        [0.2472, 0.2829, 0.2550, 0.2149]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:23:30AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][50/468]	Step 13182	lr 0.01075	Loss 0.0068 (0.0071)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:23:50AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][100/468]	Step 13232	lr 0.01075	Loss 0.0701 (0.0090)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:24:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][150/468]	Step 13282	lr 0.01075	Loss 0.0083 (0.0098)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:24:29AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][200/468]	Step 13332	lr 0.01075	Loss 0.0037 (0.0101)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:24:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][250/468]	Step 13382	lr 0.01075	Loss 0.0016 (0.0111)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:25:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][300/468]	Step 13432	lr 0.01075	Loss 0.0038 (0.0112)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:25:28AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][350/468]	Step 13482	lr 0.01075	Loss 0.0030 (0.0122)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:25:48AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][400/468]	Step 13532	lr 0.01075	Loss 0.0102 (0.0127)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:26:08AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][450/468]	Step 13582	lr 0.01075	Loss 0.0663 (0.0127)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:26:15AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [28][468/468]	Step 13600	lr 0.01075	Loss 0.0365 (0.0126)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:26:16AM searchShareStage_trainer.py:134 [INFO] Train: [ 28/49] Final Prec@1 99.5667%
05/28 02:26:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][50/469]	Step 13601	Loss 0.0318	Prec@(1,5) (99.1%, 100.0%)
05/28 02:26:23AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][100/469]	Step 13601	Loss 0.0378	Prec@(1,5) (98.8%, 100.0%)
05/28 02:26:26AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][150/469]	Step 13601	Loss 0.0376	Prec@(1,5) (98.8%, 100.0%)
05/28 02:26:29AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][200/469]	Step 13601	Loss 0.0361	Prec@(1,5) (98.8%, 100.0%)
05/28 02:26:32AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][250/469]	Step 13601	Loss 0.0368	Prec@(1,5) (98.9%, 100.0%)
05/28 02:26:35AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][300/469]	Step 13601	Loss 0.0365	Prec@(1,5) (98.9%, 100.0%)
05/28 02:26:38AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][350/469]	Step 13601	Loss 0.0364	Prec@(1,5) (98.9%, 100.0%)
05/28 02:26:41AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][400/469]	Step 13601	Loss 0.0369	Prec@(1,5) (98.9%, 100.0%)
05/28 02:26:44AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][450/469]	Step 13601	Loss 0.0363	Prec@(1,5) (98.9%, 100.0%)
05/28 02:26:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [28][468/469]	Step 13601	Loss 0.0368	Prec@(1,5) (98.9%, 100.0%)
05/28 02:26:45AM searchStage_trainer.py:260 [INFO] Valid: [ 28/49] Final Prec@1 98.9167%
05/28 02:26:45AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:26:45AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.2200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2430, 0.2626, 0.2431, 0.2512],
        [0.2470, 0.2833, 0.2572, 0.2126]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.2840, 0.2471, 0.2270],
        [0.2857, 0.2529, 0.2448, 0.2166],
        [0.2589, 0.2971, 0.2327, 0.2113]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2787, 0.2682, 0.2618, 0.1913],
        [0.2744, 0.2769, 0.2106, 0.2382],
        [0.2626, 0.2885, 0.2277, 0.2212]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2774, 0.2454, 0.2227],
        [0.2510, 0.2866, 0.2481, 0.2143],
        [0.2539, 0.2728, 0.2255, 0.2478]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2847, 0.2297, 0.2218],
        [0.2585, 0.2867, 0.2360, 0.2189],
        [0.2541, 0.2740, 0.2335, 0.2385]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2706, 0.2449, 0.2348],
        [0.2460, 0.2717, 0.2307, 0.2517],
        [0.2472, 0.2828, 0.2550, 0.2150]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:27:05AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][50/468]	Step 13651	lr 0.01002	Loss 0.0149 (0.0125)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:27:25AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][100/468]	Step 13701	lr 0.01002	Loss 0.0132 (0.0102)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:27:45AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][150/468]	Step 13751	lr 0.01002	Loss 0.0059 (0.0088)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:28:05AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][200/468]	Step 13801	lr 0.01002	Loss 0.0011 (0.0098)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:28:24AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][250/468]	Step 13851	lr 0.01002	Loss 0.0118 (0.0122)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:28:44AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][300/468]	Step 13901	lr 0.01002	Loss 0.0774 (0.0131)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:29:03AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][350/468]	Step 13951	lr 0.01002	Loss 0.0003 (0.0128)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:29:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][400/468]	Step 14001	lr 0.01002	Loss 0.0022 (0.0123)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:29:42AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][450/468]	Step 14051	lr 0.01002	Loss 0.0206 (0.0119)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:29:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [29][468/468]	Step 14069	lr 0.01002	Loss 0.0017 (0.0116)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:29:49AM searchShareStage_trainer.py:134 [INFO] Train: [ 29/49] Final Prec@1 99.6467%
05/28 02:29:52AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][50/469]	Step 14070	Loss 0.0228	Prec@(1,5) (99.4%, 100.0%)
05/28 02:29:56AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][100/469]	Step 14070	Loss 0.0174	Prec@(1,5) (99.5%, 100.0%)
05/28 02:29:59AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][150/469]	Step 14070	Loss 0.0274	Prec@(1,5) (99.3%, 100.0%)
05/28 02:30:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][200/469]	Step 14070	Loss 0.0266	Prec@(1,5) (99.3%, 100.0%)
05/28 02:30:05AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][250/469]	Step 14070	Loss 0.0245	Prec@(1,5) (99.3%, 100.0%)
05/28 02:30:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][300/469]	Step 14070	Loss 0.0259	Prec@(1,5) (99.3%, 100.0%)
05/28 02:30:11AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][350/469]	Step 14070	Loss 0.0262	Prec@(1,5) (99.3%, 100.0%)
05/28 02:30:14AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][400/469]	Step 14070	Loss 0.0263	Prec@(1,5) (99.3%, 100.0%)
05/28 02:30:17AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][450/469]	Step 14070	Loss 0.0259	Prec@(1,5) (99.3%, 100.0%)
05/28 02:30:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [29][468/469]	Step 14070	Loss 0.0259	Prec@(1,5) (99.3%, 100.0%)
05/28 02:30:19AM searchStage_trainer.py:260 [INFO] Valid: [ 29/49] Final Prec@1 99.2833%
05/28 02:30:19AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:30:19AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.2833%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2430, 0.2625, 0.2432, 0.2513],
        [0.2469, 0.2832, 0.2573, 0.2126]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.2840, 0.2471, 0.2271],
        [0.2857, 0.2528, 0.2448, 0.2167],
        [0.2589, 0.2970, 0.2327, 0.2114]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2789, 0.2681, 0.2618, 0.1912],
        [0.2744, 0.2768, 0.2106, 0.2382],
        [0.2626, 0.2884, 0.2277, 0.2213]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2773, 0.2454, 0.2228],
        [0.2510, 0.2865, 0.2482, 0.2143],
        [0.2539, 0.2727, 0.2256, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2847, 0.2297, 0.2218],
        [0.2585, 0.2866, 0.2360, 0.2189],
        [0.2540, 0.2739, 0.2335, 0.2386]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2705, 0.2450, 0.2349],
        [0.2459, 0.2716, 0.2307, 0.2518],
        [0.2471, 0.2827, 0.2551, 0.2151]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:30:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][50/468]	Step 14120	lr 0.00929	Loss 0.0010 (0.0120)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:30:59AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][100/468]	Step 14170	lr 0.00929	Loss 0.0006 (0.0116)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:31:19AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][150/468]	Step 14220	lr 0.00929	Loss 0.0021 (0.0110)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:31:39AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][200/468]	Step 14270	lr 0.00929	Loss 0.0058 (0.0117)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:31:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][250/468]	Step 14320	lr 0.00929	Loss 0.0041 (0.0115)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:32:18AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][300/468]	Step 14370	lr 0.00929	Loss 0.0152 (0.0114)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:32:37AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][350/468]	Step 14420	lr 0.00929	Loss 0.0169 (0.0118)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:32:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][400/468]	Step 14470	lr 0.00929	Loss 0.0056 (0.0119)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:33:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][450/468]	Step 14520	lr 0.00929	Loss 0.0026 (0.0116)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:33:24AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [30][468/468]	Step 14538	lr 0.00929	Loss 0.0009 (0.0115)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:33:24AM searchShareStage_trainer.py:134 [INFO] Train: [ 30/49] Final Prec@1 99.6167%
05/28 02:33:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][50/469]	Step 14539	Loss 0.0252	Prec@(1,5) (99.2%, 100.0%)
05/28 02:33:30AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][100/469]	Step 14539	Loss 0.0259	Prec@(1,5) (99.2%, 100.0%)
05/28 02:33:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][150/469]	Step 14539	Loss 0.0238	Prec@(1,5) (99.2%, 100.0%)
05/28 02:33:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][200/469]	Step 14539	Loss 0.0262	Prec@(1,5) (99.2%, 100.0%)
05/28 02:33:39AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][250/469]	Step 14539	Loss 0.0267	Prec@(1,5) (99.3%, 100.0%)
05/28 02:33:42AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][300/469]	Step 14539	Loss 0.0278	Prec@(1,5) (99.2%, 100.0%)
05/28 02:33:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][350/469]	Step 14539	Loss 0.0279	Prec@(1,5) (99.3%, 100.0%)
05/28 02:33:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][400/469]	Step 14539	Loss 0.0268	Prec@(1,5) (99.3%, 100.0%)
05/28 02:33:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][450/469]	Step 14539	Loss 0.0267	Prec@(1,5) (99.3%, 100.0%)
05/28 02:33:52AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [30][468/469]	Step 14539	Loss 0.0264	Prec@(1,5) (99.3%, 100.0%)
05/28 02:33:52AM searchStage_trainer.py:260 [INFO] Valid: [ 30/49] Final Prec@1 99.2967%
05/28 02:33:52AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:33:53AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.2967%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2430, 0.2624, 0.2432, 0.2514],
        [0.2469, 0.2831, 0.2573, 0.2127]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.2839, 0.2471, 0.2271],
        [0.2858, 0.2526, 0.2448, 0.2168],
        [0.2589, 0.2969, 0.2328, 0.2114]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2790, 0.2682, 0.2619, 0.1909],
        [0.2744, 0.2767, 0.2106, 0.2383],
        [0.2626, 0.2883, 0.2278, 0.2213]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2772, 0.2455, 0.2229],
        [0.2510, 0.2865, 0.2482, 0.2144],
        [0.2539, 0.2726, 0.2256, 0.2479]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2846, 0.2298, 0.2219],
        [0.2584, 0.2865, 0.2361, 0.2190],
        [0.2540, 0.2738, 0.2336, 0.2386]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2705, 0.2450, 0.2350],
        [0.2459, 0.2715, 0.2307, 0.2518],
        [0.2471, 0.2826, 0.2552, 0.2151]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:34:14AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][50/468]	Step 14589	lr 0.00858	Loss 0.0014 (0.0146)	Prec@(1,5) (99.4%, 100.0%)	
05/28 02:34:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][100/468]	Step 14639	lr 0.00858	Loss 0.0018 (0.0114)	Prec@(1,5) (99.5%, 100.0%)	
05/28 02:34:53AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][150/468]	Step 14689	lr 0.00858	Loss 0.0097 (0.0107)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:35:12AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][200/468]	Step 14739	lr 0.00858	Loss 0.0003 (0.0106)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:35:32AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][250/468]	Step 14789	lr 0.00858	Loss 0.0052 (0.0115)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:35:52AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][300/468]	Step 14839	lr 0.00858	Loss 0.0240 (0.0113)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:36:12AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][350/468]	Step 14889	lr 0.00858	Loss 0.0150 (0.0109)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:36:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][400/468]	Step 14939	lr 0.00858	Loss 0.0020 (0.0106)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:36:51AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][450/468]	Step 14989	lr 0.00858	Loss 0.0004 (0.0115)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:36:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [31][468/468]	Step 15007	lr 0.00858	Loss 0.0157 (0.0114)	Prec@(1,5) (99.6%, 100.0%)	
05/28 02:36:59AM searchShareStage_trainer.py:134 [INFO] Train: [ 31/49] Final Prec@1 99.6033%
05/28 02:37:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][50/469]	Step 15008	Loss 0.0253	Prec@(1,5) (99.3%, 100.0%)
05/28 02:37:06AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][100/469]	Step 15008	Loss 0.0274	Prec@(1,5) (99.3%, 100.0%)
05/28 02:37:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][150/469]	Step 15008	Loss 0.0282	Prec@(1,5) (99.2%, 100.0%)
05/28 02:37:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][200/469]	Step 15008	Loss 0.0289	Prec@(1,5) (99.2%, 100.0%)
05/28 02:37:15AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][250/469]	Step 15008	Loss 0.0273	Prec@(1,5) (99.2%, 100.0%)
05/28 02:37:18AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][300/469]	Step 15008	Loss 0.0271	Prec@(1,5) (99.2%, 100.0%)
05/28 02:37:21AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][350/469]	Step 15008	Loss 0.0270	Prec@(1,5) (99.2%, 100.0%)
05/28 02:37:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][400/469]	Step 15008	Loss 0.0279	Prec@(1,5) (99.2%, 100.0%)
05/28 02:37:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][450/469]	Step 15008	Loss 0.0272	Prec@(1,5) (99.2%, 100.0%)
05/28 02:37:28AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [31][468/469]	Step 15008	Loss 0.0273	Prec@(1,5) (99.2%, 100.0%)
05/28 02:37:28AM searchStage_trainer.py:260 [INFO] Valid: [ 31/49] Final Prec@1 99.2233%
05/28 02:37:28AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:37:28AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.2967%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2430, 0.2623, 0.2433, 0.2514],
        [0.2469, 0.2830, 0.2573, 0.2127]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.2838, 0.2471, 0.2271],
        [0.2858, 0.2525, 0.2448, 0.2168],
        [0.2589, 0.2968, 0.2328, 0.2115]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2790, 0.2681, 0.2620, 0.1908],
        [0.2744, 0.2766, 0.2106, 0.2383],
        [0.2626, 0.2882, 0.2278, 0.2214]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2772, 0.2455, 0.2229],
        [0.2509, 0.2864, 0.2483, 0.2144],
        [0.2538, 0.2725, 0.2256, 0.2480]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2846, 0.2298, 0.2219],
        [0.2584, 0.2864, 0.2361, 0.2190],
        [0.2540, 0.2737, 0.2336, 0.2387]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2704, 0.2451, 0.2350],
        [0.2459, 0.2715, 0.2308, 0.2519],
        [0.2471, 0.2825, 0.2552, 0.2152]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:37:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][50/468]	Step 15058	lr 0.00789	Loss 0.0005 (0.0113)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:38:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][100/468]	Step 15108	lr 0.00789	Loss 0.0031 (0.0105)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:38:28AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][150/468]	Step 15158	lr 0.00789	Loss 0.0126 (0.0101)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:38:48AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][200/468]	Step 15208	lr 0.00789	Loss 0.0067 (0.0100)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:39:08AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][250/468]	Step 15258	lr 0.00789	Loss 0.0146 (0.0111)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:39:27AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][300/468]	Step 15308	lr 0.00789	Loss 0.0057 (0.0109)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:39:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][350/468]	Step 15358	lr 0.00789	Loss 0.0009 (0.0111)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:40:07AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][400/468]	Step 15408	lr 0.00789	Loss 0.0006 (0.0109)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:40:27AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][450/468]	Step 15458	lr 0.00789	Loss 0.0002 (0.0111)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:40:34AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [32][468/468]	Step 15476	lr 0.00789	Loss 0.0033 (0.0111)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:40:34AM searchShareStage_trainer.py:134 [INFO] Train: [ 32/49] Final Prec@1 99.6667%
05/28 02:40:38AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][50/469]	Step 15477	Loss 0.0169	Prec@(1,5) (99.4%, 100.0%)
05/28 02:40:41AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][100/469]	Step 15477	Loss 0.0184	Prec@(1,5) (99.3%, 100.0%)
05/28 02:40:44AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][150/469]	Step 15477	Loss 0.0256	Prec@(1,5) (99.2%, 100.0%)
05/28 02:40:47AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][200/469]	Step 15477	Loss 0.0273	Prec@(1,5) (99.1%, 100.0%)
05/28 02:40:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][250/469]	Step 15477	Loss 0.0263	Prec@(1,5) (99.2%, 100.0%)
05/28 02:40:52AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][300/469]	Step 15477	Loss 0.0270	Prec@(1,5) (99.2%, 100.0%)
05/28 02:40:55AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][350/469]	Step 15477	Loss 0.0267	Prec@(1,5) (99.2%, 100.0%)
05/28 02:40:58AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][400/469]	Step 15477	Loss 0.0272	Prec@(1,5) (99.2%, 100.0%)
05/28 02:41:01AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][450/469]	Step 15477	Loss 0.0265	Prec@(1,5) (99.2%, 100.0%)
05/28 02:41:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [32][468/469]	Step 15477	Loss 0.0257	Prec@(1,5) (99.2%, 100.0%)
05/28 02:41:02AM searchStage_trainer.py:260 [INFO] Valid: [ 32/49] Final Prec@1 99.2500%
05/28 02:41:02AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:41:02AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.2967%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2429, 0.2622, 0.2433, 0.2515],
        [0.2469, 0.2829, 0.2574, 0.2128]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.2838, 0.2472, 0.2272],
        [0.2859, 0.2524, 0.2449, 0.2168],
        [0.2589, 0.2967, 0.2328, 0.2115]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2791, 0.2680, 0.2621, 0.1908],
        [0.2744, 0.2766, 0.2106, 0.2384],
        [0.2626, 0.2881, 0.2278, 0.2214]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2771, 0.2455, 0.2229],
        [0.2509, 0.2863, 0.2483, 0.2144],
        [0.2538, 0.2724, 0.2257, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2845, 0.2298, 0.2220],
        [0.2584, 0.2864, 0.2361, 0.2191],
        [0.2540, 0.2736, 0.2336, 0.2388]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2703, 0.2451, 0.2351],
        [0.2459, 0.2714, 0.2308, 0.2519],
        [0.2471, 0.2824, 0.2553, 0.2153]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:41:23AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][50/468]	Step 15527	lr 0.00722	Loss 0.0006 (0.0120)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:41:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][100/468]	Step 15577	lr 0.00722	Loss 0.0007 (0.0102)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:42:02AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][150/468]	Step 15627	lr 0.00722	Loss 0.0098 (0.0104)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:42:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][200/468]	Step 15677	lr 0.00722	Loss 0.0105 (0.0107)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:42:42AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][250/468]	Step 15727	lr 0.00722	Loss 0.0010 (0.0105)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:43:01AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][300/468]	Step 15777	lr 0.00722	Loss 0.0234 (0.0101)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:43:21AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][350/468]	Step 15827	lr 0.00722	Loss 0.0123 (0.0097)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:43:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][400/468]	Step 15877	lr 0.00722	Loss 0.0014 (0.0093)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:43:59AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][450/468]	Step 15927	lr 0.00722	Loss 0.0045 (0.0102)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:44:07AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [33][468/468]	Step 15945	lr 0.00722	Loss 0.0014 (0.0106)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:44:07AM searchShareStage_trainer.py:134 [INFO] Train: [ 33/49] Final Prec@1 99.6867%
05/28 02:44:11AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][50/469]	Step 15946	Loss 0.0282	Prec@(1,5) (99.2%, 100.0%)
05/28 02:44:14AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][100/469]	Step 15946	Loss 0.0280	Prec@(1,5) (99.2%, 100.0%)
05/28 02:44:17AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][150/469]	Step 15946	Loss 0.0261	Prec@(1,5) (99.2%, 100.0%)
05/28 02:44:20AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][200/469]	Step 15946	Loss 0.0259	Prec@(1,5) (99.3%, 100.0%)
05/28 02:44:23AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][250/469]	Step 15946	Loss 0.0270	Prec@(1,5) (99.2%, 100.0%)
05/28 02:44:26AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][300/469]	Step 15946	Loss 0.0272	Prec@(1,5) (99.2%, 100.0%)
05/28 02:44:29AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][350/469]	Step 15946	Loss 0.0282	Prec@(1,5) (99.2%, 100.0%)
05/28 02:44:32AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][400/469]	Step 15946	Loss 0.0285	Prec@(1,5) (99.2%, 100.0%)
05/28 02:44:35AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][450/469]	Step 15946	Loss 0.0284	Prec@(1,5) (99.2%, 100.0%)
05/28 02:44:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [33][468/469]	Step 15946	Loss 0.0287	Prec@(1,5) (99.2%, 100.0%)
05/28 02:44:36AM searchStage_trainer.py:260 [INFO] Valid: [ 33/49] Final Prec@1 99.2200%
05/28 02:44:36AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:44:36AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.2967%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2429, 0.2622, 0.2434, 0.2516],
        [0.2469, 0.2828, 0.2574, 0.2128]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.2838, 0.2472, 0.2272],
        [0.2859, 0.2523, 0.2448, 0.2170],
        [0.2589, 0.2966, 0.2329, 0.2116]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2792, 0.2680, 0.2621, 0.1907],
        [0.2744, 0.2765, 0.2106, 0.2384],
        [0.2626, 0.2881, 0.2279, 0.2215]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2771, 0.2455, 0.2229],
        [0.2509, 0.2863, 0.2483, 0.2145],
        [0.2538, 0.2724, 0.2257, 0.2481]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2844, 0.2298, 0.2221],
        [0.2584, 0.2863, 0.2362, 0.2191],
        [0.2540, 0.2735, 0.2336, 0.2388]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2702, 0.2451, 0.2352],
        [0.2458, 0.2713, 0.2308, 0.2520],
        [0.2471, 0.2823, 0.2553, 0.2153]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:44:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][50/468]	Step 15996	lr 0.00657	Loss 0.0034 (0.0073)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:45:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][100/468]	Step 16046	lr 0.00657	Loss 0.0005 (0.0089)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:45:36AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][150/468]	Step 16096	lr 0.00657	Loss 0.0068 (0.0083)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:45:56AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][200/468]	Step 16146	lr 0.00657	Loss 0.0009 (0.0073)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:46:16AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][250/468]	Step 16196	lr 0.00657	Loss 0.0004 (0.0080)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:46:35AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][300/468]	Step 16246	lr 0.00657	Loss 0.0273 (0.0079)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:46:55AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][350/468]	Step 16296	lr 0.00657	Loss 0.0246 (0.0080)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:47:14AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][400/468]	Step 16346	lr 0.00657	Loss 0.0097 (0.0079)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:47:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][450/468]	Step 16396	lr 0.00657	Loss 0.0030 (0.0083)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:47:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [34][468/468]	Step 16414	lr 0.00657	Loss 0.0061 (0.0084)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:47:40AM searchShareStage_trainer.py:134 [INFO] Train: [ 34/49] Final Prec@1 99.7233%
05/28 02:47:44AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][50/469]	Step 16415	Loss 0.0251	Prec@(1,5) (99.2%, 100.0%)
05/28 02:47:47AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][100/469]	Step 16415	Loss 0.0197	Prec@(1,5) (99.3%, 100.0%)
05/28 02:47:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][150/469]	Step 16415	Loss 0.0234	Prec@(1,5) (99.2%, 100.0%)
05/28 02:47:53AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][200/469]	Step 16415	Loss 0.0220	Prec@(1,5) (99.3%, 100.0%)
05/28 02:47:56AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][250/469]	Step 16415	Loss 0.0201	Prec@(1,5) (99.4%, 100.0%)
05/28 02:47:59AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][300/469]	Step 16415	Loss 0.0199	Prec@(1,5) (99.4%, 100.0%)
05/28 02:48:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][350/469]	Step 16415	Loss 0.0193	Prec@(1,5) (99.4%, 100.0%)
05/28 02:48:05AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][400/469]	Step 16415	Loss 0.0197	Prec@(1,5) (99.4%, 100.0%)
05/28 02:48:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][450/469]	Step 16415	Loss 0.0198	Prec@(1,5) (99.4%, 100.0%)
05/28 02:48:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [34][468/469]	Step 16415	Loss 0.0204	Prec@(1,5) (99.4%, 100.0%)
05/28 02:48:09AM searchStage_trainer.py:260 [INFO] Valid: [ 34/49] Final Prec@1 99.3833%
05/28 02:48:09AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:48:10AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.3833%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2429, 0.2621, 0.2434, 0.2516],
        [0.2469, 0.2828, 0.2575, 0.2129]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2837, 0.2473, 0.2272],
        [0.2859, 0.2522, 0.2448, 0.2170],
        [0.2589, 0.2966, 0.2329, 0.2116]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2792, 0.2679, 0.2622, 0.1906],
        [0.2744, 0.2765, 0.2106, 0.2385],
        [0.2626, 0.2880, 0.2279, 0.2215]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2771, 0.2456, 0.2229],
        [0.2509, 0.2862, 0.2484, 0.2145],
        [0.2538, 0.2723, 0.2257, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2843, 0.2298, 0.2221],
        [0.2583, 0.2863, 0.2362, 0.2192],
        [0.2540, 0.2735, 0.2337, 0.2389]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2702, 0.2452, 0.2352],
        [0.2458, 0.2713, 0.2309, 0.2521],
        [0.2470, 0.2823, 0.2553, 0.2154]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:48:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][50/468]	Step 16465	lr 0.00595	Loss 0.0011 (0.0057)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:48:50AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][100/468]	Step 16515	lr 0.00595	Loss 0.0073 (0.0070)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:49:10AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][150/468]	Step 16565	lr 0.00595	Loss 0.0020 (0.0076)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:49:30AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][200/468]	Step 16615	lr 0.00595	Loss 0.0003 (0.0072)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:49:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][250/468]	Step 16665	lr 0.00595	Loss 0.0316 (0.0073)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:50:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][300/468]	Step 16715	lr 0.00595	Loss 0.0013 (0.0074)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:50:28AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][350/468]	Step 16765	lr 0.00595	Loss 0.0102 (0.0077)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:50:48AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][400/468]	Step 16815	lr 0.00595	Loss 0.0058 (0.0074)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:51:08AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][450/468]	Step 16865	lr 0.00595	Loss 0.0236 (0.0078)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:51:15AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [35][468/468]	Step 16883	lr 0.00595	Loss 0.0023 (0.0076)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:51:15AM searchShareStage_trainer.py:134 [INFO] Train: [ 35/49] Final Prec@1 99.7533%
05/28 02:51:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][50/469]	Step 16884	Loss 0.0245	Prec@(1,5) (99.3%, 100.0%)
05/28 02:51:22AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][100/469]	Step 16884	Loss 0.0248	Prec@(1,5) (99.4%, 100.0%)
05/28 02:51:25AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][150/469]	Step 16884	Loss 0.0224	Prec@(1,5) (99.4%, 100.0%)
05/28 02:51:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][200/469]	Step 16884	Loss 0.0219	Prec@(1,5) (99.4%, 100.0%)
05/28 02:51:30AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][250/469]	Step 16884	Loss 0.0218	Prec@(1,5) (99.4%, 100.0%)
05/28 02:51:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][300/469]	Step 16884	Loss 0.0230	Prec@(1,5) (99.4%, 100.0%)
05/28 02:51:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][350/469]	Step 16884	Loss 0.0226	Prec@(1,5) (99.4%, 100.0%)
05/28 02:51:39AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][400/469]	Step 16884	Loss 0.0223	Prec@(1,5) (99.4%, 100.0%)
05/28 02:51:42AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][450/469]	Step 16884	Loss 0.0228	Prec@(1,5) (99.4%, 100.0%)
05/28 02:51:43AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [35][468/469]	Step 16884	Loss 0.0233	Prec@(1,5) (99.4%, 100.0%)
05/28 02:51:43AM searchStage_trainer.py:260 [INFO] Valid: [ 35/49] Final Prec@1 99.3533%
05/28 02:51:43AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:51:43AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.3833%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2429, 0.2620, 0.2434, 0.2517],
        [0.2469, 0.2827, 0.2575, 0.2129]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2836, 0.2473, 0.2273],
        [0.2859, 0.2522, 0.2449, 0.2170],
        [0.2589, 0.2965, 0.2329, 0.2117]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2793, 0.2679, 0.2622, 0.1906],
        [0.2744, 0.2764, 0.2107, 0.2385],
        [0.2625, 0.2879, 0.2279, 0.2216]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2770, 0.2456, 0.2230],
        [0.2509, 0.2862, 0.2484, 0.2145],
        [0.2538, 0.2722, 0.2258, 0.2482]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2843, 0.2298, 0.2222],
        [0.2583, 0.2862, 0.2363, 0.2192],
        [0.2540, 0.2734, 0.2337, 0.2390]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2701, 0.2452, 0.2353],
        [0.2458, 0.2712, 0.2309, 0.2521],
        [0.2470, 0.2822, 0.2554, 0.2154]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:52:04AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][50/468]	Step 16934	lr 0.00535	Loss 0.0070 (0.0064)	Prec@(1,5) (99.7%, 100.0%)	
05/28 02:52:23AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][100/468]	Step 16984	lr 0.00535	Loss 0.0020 (0.0058)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:52:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][150/468]	Step 17034	lr 0.00535	Loss 0.0018 (0.0054)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:53:02AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][200/468]	Step 17084	lr 0.00535	Loss 0.0020 (0.0051)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:53:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][250/468]	Step 17134	lr 0.00535	Loss 0.0063 (0.0053)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:53:41AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][300/468]	Step 17184	lr 0.00535	Loss 0.0010 (0.0053)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:54:01AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][350/468]	Step 17234	lr 0.00535	Loss 0.0038 (0.0050)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:54:20AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][400/468]	Step 17284	lr 0.00535	Loss 0.0013 (0.0054)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:54:40AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][450/468]	Step 17334	lr 0.00535	Loss 0.0001 (0.0053)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:54:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [36][468/468]	Step 17352	lr 0.00535	Loss 0.0006 (0.0053)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:54:48AM searchShareStage_trainer.py:134 [INFO] Train: [ 36/49] Final Prec@1 99.8367%
05/28 02:54:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][50/469]	Step 17353	Loss 0.0270	Prec@(1,5) (99.3%, 100.0%)
05/28 02:54:54AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][100/469]	Step 17353	Loss 0.0242	Prec@(1,5) (99.4%, 100.0%)
05/28 02:54:58AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][150/469]	Step 17353	Loss 0.0264	Prec@(1,5) (99.3%, 100.0%)
05/28 02:55:01AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][200/469]	Step 17353	Loss 0.0254	Prec@(1,5) (99.3%, 100.0%)
05/28 02:55:04AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][250/469]	Step 17353	Loss 0.0242	Prec@(1,5) (99.3%, 100.0%)
05/28 02:55:07AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][300/469]	Step 17353	Loss 0.0235	Prec@(1,5) (99.3%, 100.0%)
05/28 02:55:10AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][350/469]	Step 17353	Loss 0.0248	Prec@(1,5) (99.3%, 100.0%)
05/28 02:55:13AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][400/469]	Step 17353	Loss 0.0241	Prec@(1,5) (99.3%, 100.0%)
05/28 02:55:16AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][450/469]	Step 17353	Loss 0.0238	Prec@(1,5) (99.3%, 100.0%)
05/28 02:55:17AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [36][468/469]	Step 17353	Loss 0.0235	Prec@(1,5) (99.3%, 100.0%)
05/28 02:55:17AM searchStage_trainer.py:260 [INFO] Valid: [ 36/49] Final Prec@1 99.3400%
05/28 02:55:17AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:55:18AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.3833%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2429, 0.2619, 0.2435, 0.2517],
        [0.2469, 0.2826, 0.2576, 0.2129]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2836, 0.2473, 0.2273],
        [0.2860, 0.2521, 0.2449, 0.2171],
        [0.2589, 0.2964, 0.2330, 0.2117]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2794, 0.2678, 0.2622, 0.1906],
        [0.2744, 0.2763, 0.2107, 0.2386],
        [0.2625, 0.2879, 0.2280, 0.2216]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2770, 0.2456, 0.2230],
        [0.2509, 0.2861, 0.2484, 0.2145],
        [0.2537, 0.2722, 0.2258, 0.2483]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2842, 0.2297, 0.2223],
        [0.2583, 0.2861, 0.2363, 0.2192],
        [0.2539, 0.2733, 0.2337, 0.2390]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2701, 0.2453, 0.2353],
        [0.2458, 0.2711, 0.2309, 0.2522],
        [0.2470, 0.2821, 0.2554, 0.2155]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:55:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][50/468]	Step 17403	lr 0.00479	Loss 0.0013 (0.0052)	Prec@(1,5) (99.9%, 100.0%)	
05/28 02:55:58AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][100/468]	Step 17453	lr 0.00479	Loss 0.0003 (0.0050)	Prec@(1,5) (99.9%, 100.0%)	
05/28 02:56:18AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][150/468]	Step 17503	lr 0.00479	Loss 0.0263 (0.0052)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:56:37AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][200/468]	Step 17553	lr 0.00479	Loss 0.0024 (0.0051)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:56:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][250/468]	Step 17603	lr 0.00479	Loss 0.0012 (0.0055)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:57:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][300/468]	Step 17653	lr 0.00479	Loss 0.0030 (0.0055)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:57:36AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][350/468]	Step 17703	lr 0.00479	Loss 0.0024 (0.0056)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:57:56AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][400/468]	Step 17753	lr 0.00479	Loss 0.0021 (0.0059)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:58:16AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][450/468]	Step 17803	lr 0.00479	Loss 0.0011 (0.0058)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:58:23AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [37][468/468]	Step 17821	lr 0.00479	Loss 0.0008 (0.0058)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:58:23AM searchShareStage_trainer.py:134 [INFO] Train: [ 37/49] Final Prec@1 99.8100%
05/28 02:58:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][50/469]	Step 17822	Loss 0.0199	Prec@(1,5) (99.5%, 100.0%)
05/28 02:58:30AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][100/469]	Step 17822	Loss 0.0202	Prec@(1,5) (99.4%, 100.0%)
05/28 02:58:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][150/469]	Step 17822	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/28 02:58:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][200/469]	Step 17822	Loss 0.0253	Prec@(1,5) (99.4%, 100.0%)
05/28 02:58:39AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][250/469]	Step 17822	Loss 0.0251	Prec@(1,5) (99.4%, 100.0%)
05/28 02:58:42AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][300/469]	Step 17822	Loss 0.0268	Prec@(1,5) (99.3%, 100.0%)
05/28 02:58:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][350/469]	Step 17822	Loss 0.0250	Prec@(1,5) (99.4%, 100.0%)
05/28 02:58:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][400/469]	Step 17822	Loss 0.0242	Prec@(1,5) (99.4%, 100.0%)
05/28 02:58:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][450/469]	Step 17822	Loss 0.0239	Prec@(1,5) (99.4%, 100.0%)
05/28 02:58:52AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [37][468/469]	Step 17822	Loss 0.0234	Prec@(1,5) (99.4%, 100.0%)
05/28 02:58:53AM searchStage_trainer.py:260 [INFO] Valid: [ 37/49] Final Prec@1 99.3733%
05/28 02:58:53AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 02:58:53AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.3833%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2429, 0.2619, 0.2435, 0.2518],
        [0.2469, 0.2826, 0.2576, 0.2130]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2836, 0.2473, 0.2273],
        [0.2860, 0.2520, 0.2449, 0.2171],
        [0.2589, 0.2964, 0.2330, 0.2117]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2795, 0.2677, 0.2623, 0.1905],
        [0.2744, 0.2763, 0.2107, 0.2386],
        [0.2625, 0.2878, 0.2280, 0.2217]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2769, 0.2456, 0.2231],
        [0.2509, 0.2861, 0.2485, 0.2145],
        [0.2537, 0.2721, 0.2258, 0.2483]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2842, 0.2298, 0.2223],
        [0.2583, 0.2861, 0.2363, 0.2193],
        [0.2539, 0.2733, 0.2337, 0.2391]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2700, 0.2454, 0.2353],
        [0.2458, 0.2711, 0.2309, 0.2522],
        [0.2470, 0.2821, 0.2554, 0.2155]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 02:59:14AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][50/468]	Step 17872	lr 0.00425	Loss 0.0010 (0.0047)	Prec@(1,5) (99.8%, 100.0%)	
05/28 02:59:34AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][100/468]	Step 17922	lr 0.00425	Loss 0.0007 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/28 02:59:54AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][150/468]	Step 17972	lr 0.00425	Loss 0.0005 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:00:13AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][200/468]	Step 18022	lr 0.00425	Loss 0.0123 (0.0048)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:00:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][250/468]	Step 18072	lr 0.00425	Loss 0.0384 (0.0045)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:00:53AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][300/468]	Step 18122	lr 0.00425	Loss 0.0036 (0.0043)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:01:12AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][350/468]	Step 18172	lr 0.00425	Loss 0.0557 (0.0042)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:01:32AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][400/468]	Step 18222	lr 0.00425	Loss 0.0009 (0.0041)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:01:51AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][450/468]	Step 18272	lr 0.00425	Loss 0.0004 (0.0045)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:01:59AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [38][468/468]	Step 18290	lr 0.00425	Loss 0.0060 (0.0045)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:01:59AM searchShareStage_trainer.py:134 [INFO] Train: [ 38/49] Final Prec@1 99.8600%
05/28 03:02:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][50/469]	Step 18291	Loss 0.0272	Prec@(1,5) (99.3%, 100.0%)
05/28 03:02:05AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][100/469]	Step 18291	Loss 0.0259	Prec@(1,5) (99.4%, 100.0%)
05/28 03:02:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][150/469]	Step 18291	Loss 0.0223	Prec@(1,5) (99.4%, 100.0%)
05/28 03:02:11AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][200/469]	Step 18291	Loss 0.0248	Prec@(1,5) (99.4%, 100.0%)
05/28 03:02:14AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][250/469]	Step 18291	Loss 0.0247	Prec@(1,5) (99.4%, 100.0%)
05/28 03:02:17AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][300/469]	Step 18291	Loss 0.0245	Prec@(1,5) (99.4%, 100.0%)
05/28 03:02:20AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][350/469]	Step 18291	Loss 0.0247	Prec@(1,5) (99.4%, 100.0%)
05/28 03:02:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][400/469]	Step 18291	Loss 0.0244	Prec@(1,5) (99.4%, 100.0%)
05/28 03:02:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][450/469]	Step 18291	Loss 0.0250	Prec@(1,5) (99.4%, 100.0%)
05/28 03:02:28AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [38][468/469]	Step 18291	Loss 0.0251	Prec@(1,5) (99.4%, 100.0%)
05/28 03:02:28AM searchStage_trainer.py:260 [INFO] Valid: [ 38/49] Final Prec@1 99.3700%
05/28 03:02:28AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:02:28AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.3833%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2429, 0.2618, 0.2435, 0.2518],
        [0.2468, 0.2825, 0.2577, 0.2130]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2835, 0.2473, 0.2274],
        [0.2860, 0.2520, 0.2449, 0.2171],
        [0.2589, 0.2963, 0.2330, 0.2118]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2796, 0.2677, 0.2623, 0.1904],
        [0.2744, 0.2762, 0.2107, 0.2387],
        [0.2625, 0.2877, 0.2280, 0.2217]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2769, 0.2457, 0.2230],
        [0.2509, 0.2860, 0.2485, 0.2146],
        [0.2537, 0.2720, 0.2259, 0.2484]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2841, 0.2298, 0.2223],
        [0.2583, 0.2860, 0.2364, 0.2193],
        [0.2539, 0.2732, 0.2337, 0.2391]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2700, 0.2454, 0.2354],
        [0.2457, 0.2710, 0.2310, 0.2523],
        [0.2470, 0.2820, 0.2555, 0.2155]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 03:02:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][50/468]	Step 18341	lr 0.00375	Loss 0.0062 (0.0091)	Prec@(1,5) (99.7%, 100.0%)	
05/28 03:03:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][100/468]	Step 18391	lr 0.00375	Loss 0.0005 (0.0075)	Prec@(1,5) (99.7%, 100.0%)	
05/28 03:03:28AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][150/468]	Step 18441	lr 0.00375	Loss 0.0007 (0.0067)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:03:48AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][200/468]	Step 18491	lr 0.00375	Loss 0.0259 (0.0058)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:04:08AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][250/468]	Step 18541	lr 0.00375	Loss 0.0034 (0.0052)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:04:27AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][300/468]	Step 18591	lr 0.00375	Loss 0.0002 (0.0052)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:04:47AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][350/468]	Step 18641	lr 0.00375	Loss 0.0026 (0.0051)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:05:07AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][400/468]	Step 18691	lr 0.00375	Loss 0.0110 (0.0052)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:05:26AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][450/468]	Step 18741	lr 0.00375	Loss 0.0033 (0.0053)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:05:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [39][468/468]	Step 18759	lr 0.00375	Loss 0.0092 (0.0053)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:05:34AM searchShareStage_trainer.py:134 [INFO] Train: [ 39/49] Final Prec@1 99.8500%
05/28 03:05:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][50/469]	Step 18760	Loss 0.0255	Prec@(1,5) (99.2%, 100.0%)
05/28 03:05:40AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][100/469]	Step 18760	Loss 0.0230	Prec@(1,5) (99.3%, 100.0%)
05/28 03:05:43AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][150/469]	Step 18760	Loss 0.0237	Prec@(1,5) (99.3%, 100.0%)
05/28 03:05:46AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][200/469]	Step 18760	Loss 0.0239	Prec@(1,5) (99.3%, 100.0%)
05/28 03:05:49AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][250/469]	Step 18760	Loss 0.0234	Prec@(1,5) (99.3%, 100.0%)
05/28 03:05:52AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][300/469]	Step 18760	Loss 0.0248	Prec@(1,5) (99.3%, 100.0%)
05/28 03:05:55AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][350/469]	Step 18760	Loss 0.0271	Prec@(1,5) (99.3%, 100.0%)
05/28 03:05:58AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][400/469]	Step 18760	Loss 0.0258	Prec@(1,5) (99.3%, 100.0%)
05/28 03:06:01AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][450/469]	Step 18760	Loss 0.0246	Prec@(1,5) (99.3%, 100.0%)
05/28 03:06:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [39][468/469]	Step 18760	Loss 0.0240	Prec@(1,5) (99.3%, 100.0%)
05/28 03:06:02AM searchStage_trainer.py:260 [INFO] Valid: [ 39/49] Final Prec@1 99.3433%
05/28 03:06:02AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:06:02AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.3833%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2428, 0.2617, 0.2436, 0.2519],
        [0.2468, 0.2824, 0.2577, 0.2130]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2835, 0.2473, 0.2274],
        [0.2860, 0.2519, 0.2449, 0.2172],
        [0.2589, 0.2963, 0.2330, 0.2118]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2797, 0.2677, 0.2623, 0.1903],
        [0.2744, 0.2762, 0.2107, 0.2387],
        [0.2625, 0.2877, 0.2280, 0.2217]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2769, 0.2457, 0.2231],
        [0.2509, 0.2860, 0.2485, 0.2146],
        [0.2537, 0.2720, 0.2259, 0.2484]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2841, 0.2298, 0.2224],
        [0.2583, 0.2860, 0.2364, 0.2193],
        [0.2539, 0.2731, 0.2338, 0.2392]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2699, 0.2454, 0.2354],
        [0.2457, 0.2710, 0.2310, 0.2523],
        [0.2470, 0.2819, 0.2555, 0.2156]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 03:06:23AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][50/468]	Step 18810	lr 0.00329	Loss 0.0003 (0.0020)	Prec@(1,5) (100.0%, 100.0%)	
05/28 03:06:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][100/468]	Step 18860	lr 0.00329	Loss 0.0008 (0.0021)	Prec@(1,5) (100.0%, 100.0%)	
05/28 03:07:02AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][150/468]	Step 18910	lr 0.00329	Loss 0.0003 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:07:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][200/468]	Step 18960	lr 0.00329	Loss 0.0013 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:07:42AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][250/468]	Step 19010	lr 0.00329	Loss 0.0002 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:08:02AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][300/468]	Step 19060	lr 0.00329	Loss 0.0052 (0.0032)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:08:21AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][350/468]	Step 19110	lr 0.00329	Loss 0.0037 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:08:41AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][400/468]	Step 19160	lr 0.00329	Loss 0.0150 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:09:01AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][450/468]	Step 19210	lr 0.00329	Loss 0.0008 (0.0032)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:09:08AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [40][468/468]	Step 19228	lr 0.00329	Loss 0.0114 (0.0032)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:09:08AM searchShareStage_trainer.py:134 [INFO] Train: [ 40/49] Final Prec@1 99.9100%
05/28 03:09:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][50/469]	Step 19229	Loss 0.0191	Prec@(1,5) (99.4%, 100.0%)
05/28 03:09:15AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][100/469]	Step 19229	Loss 0.0176	Prec@(1,5) (99.4%, 100.0%)
05/28 03:09:18AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][150/469]	Step 19229	Loss 0.0183	Prec@(1,5) (99.5%, 100.0%)
05/28 03:09:21AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][200/469]	Step 19229	Loss 0.0202	Prec@(1,5) (99.4%, 100.0%)
05/28 03:09:24AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][250/469]	Step 19229	Loss 0.0217	Prec@(1,5) (99.4%, 100.0%)
05/28 03:09:27AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][300/469]	Step 19229	Loss 0.0225	Prec@(1,5) (99.4%, 100.0%)
05/28 03:09:31AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][350/469]	Step 19229	Loss 0.0217	Prec@(1,5) (99.4%, 100.0%)
05/28 03:09:34AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][400/469]	Step 19229	Loss 0.0223	Prec@(1,5) (99.4%, 100.0%)
05/28 03:09:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][450/469]	Step 19229	Loss 0.0212	Prec@(1,5) (99.4%, 100.0%)
05/28 03:09:38AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [40][468/469]	Step 19229	Loss 0.0222	Prec@(1,5) (99.4%, 100.0%)
05/28 03:09:38AM searchStage_trainer.py:260 [INFO] Valid: [ 40/49] Final Prec@1 99.4100%
05/28 03:09:38AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:09:39AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.4100%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2428, 0.2617, 0.2436, 0.2519],
        [0.2468, 0.2824, 0.2577, 0.2131]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2835, 0.2473, 0.2274],
        [0.2861, 0.2518, 0.2449, 0.2172],
        [0.2589, 0.2962, 0.2330, 0.2119]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2797, 0.2677, 0.2624, 0.1902],
        [0.2744, 0.2761, 0.2107, 0.2388],
        [0.2625, 0.2877, 0.2281, 0.2218]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2768, 0.2457, 0.2231],
        [0.2509, 0.2860, 0.2485, 0.2146],
        [0.2537, 0.2719, 0.2259, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2840, 0.2298, 0.2224],
        [0.2583, 0.2859, 0.2364, 0.2194],
        [0.2539, 0.2731, 0.2338, 0.2392]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2699, 0.2454, 0.2354],
        [0.2457, 0.2709, 0.2310, 0.2524],
        [0.2470, 0.2819, 0.2555, 0.2156]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 03:09:59AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][50/468]	Step 19279	lr 0.00287	Loss 0.0005 (0.0047)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:10:19AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][100/468]	Step 19329	lr 0.00287	Loss 0.0007 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:10:39AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][150/468]	Step 19379	lr 0.00287	Loss 0.0003 (0.0034)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:10:59AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][200/468]	Step 19429	lr 0.00287	Loss 0.0057 (0.0032)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:11:18AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][250/468]	Step 19479	lr 0.00287	Loss 0.0004 (0.0033)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:11:38AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][300/468]	Step 19529	lr 0.00287	Loss 0.0019 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:11:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][350/468]	Step 19579	lr 0.00287	Loss 0.0010 (0.0040)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:12:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][400/468]	Step 19629	lr 0.00287	Loss 0.0005 (0.0042)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:12:36AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][450/468]	Step 19679	lr 0.00287	Loss 0.0027 (0.0043)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:12:44AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [41][468/468]	Step 19697	lr 0.00287	Loss 0.0009 (0.0043)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:12:44AM searchShareStage_trainer.py:134 [INFO] Train: [ 41/49] Final Prec@1 99.8933%
05/28 03:12:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][50/469]	Step 19698	Loss 0.0226	Prec@(1,5) (99.5%, 100.0%)
05/28 03:12:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][100/469]	Step 19698	Loss 0.0217	Prec@(1,5) (99.4%, 100.0%)
05/28 03:12:53AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][150/469]	Step 19698	Loss 0.0234	Prec@(1,5) (99.4%, 100.0%)
05/28 03:12:56AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][200/469]	Step 19698	Loss 0.0247	Prec@(1,5) (99.4%, 100.0%)
05/28 03:12:59AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][250/469]	Step 19698	Loss 0.0238	Prec@(1,5) (99.4%, 100.0%)
05/28 03:13:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][300/469]	Step 19698	Loss 0.0229	Prec@(1,5) (99.4%, 100.0%)
05/28 03:13:05AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][350/469]	Step 19698	Loss 0.0233	Prec@(1,5) (99.5%, 100.0%)
05/28 03:13:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][400/469]	Step 19698	Loss 0.0228	Prec@(1,5) (99.5%, 100.0%)
05/28 03:13:11AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][450/469]	Step 19698	Loss 0.0233	Prec@(1,5) (99.4%, 100.0%)
05/28 03:13:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [41][468/469]	Step 19698	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/28 03:13:12AM searchStage_trainer.py:260 [INFO] Valid: [ 41/49] Final Prec@1 99.4300%
05/28 03:13:12AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:13:13AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.4300%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2428, 0.2616, 0.2436, 0.2519],
        [0.2468, 0.2823, 0.2578, 0.2131]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2834, 0.2473, 0.2275],
        [0.2861, 0.2518, 0.2449, 0.2172],
        [0.2589, 0.2961, 0.2331, 0.2119]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2798, 0.2676, 0.2624, 0.1902],
        [0.2744, 0.2761, 0.2107, 0.2388],
        [0.2625, 0.2876, 0.2281, 0.2218]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2768, 0.2457, 0.2231],
        [0.2509, 0.2859, 0.2486, 0.2146],
        [0.2537, 0.2719, 0.2259, 0.2485]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2840, 0.2298, 0.2224],
        [0.2583, 0.2859, 0.2364, 0.2194],
        [0.2539, 0.2730, 0.2338, 0.2393]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2698, 0.2454, 0.2355],
        [0.2457, 0.2709, 0.2310, 0.2524],
        [0.2470, 0.2818, 0.2556, 0.2157]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 03:13:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][50/468]	Step 19748	lr 0.00248	Loss 0.0168 (0.0032)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:13:53AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][100/468]	Step 19798	lr 0.00248	Loss 0.0003 (0.0025)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:14:12AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][150/468]	Step 19848	lr 0.00248	Loss 0.0032 (0.0027)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:14:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][200/468]	Step 19898	lr 0.00248	Loss 0.0005 (0.0032)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:14:53AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][250/468]	Step 19948	lr 0.00248	Loss 0.0024 (0.0033)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:15:12AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][300/468]	Step 19998	lr 0.00248	Loss 0.0008 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:15:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][350/468]	Step 20048	lr 0.00248	Loss 0.0018 (0.0039)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:15:52AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][400/468]	Step 20098	lr 0.00248	Loss 0.0005 (0.0040)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:16:11AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][450/468]	Step 20148	lr 0.00248	Loss 0.0020 (0.0042)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:16:18AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [42][468/468]	Step 20166	lr 0.00248	Loss 0.0003 (0.0042)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:16:19AM searchShareStage_trainer.py:134 [INFO] Train: [ 42/49] Final Prec@1 99.8833%
05/28 03:16:23AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][50/469]	Step 20167	Loss 0.0236	Prec@(1,5) (99.3%, 100.0%)
05/28 03:16:26AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][100/469]	Step 20167	Loss 0.0194	Prec@(1,5) (99.4%, 100.0%)
05/28 03:16:29AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][150/469]	Step 20167	Loss 0.0242	Prec@(1,5) (99.3%, 100.0%)
05/28 03:16:32AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][200/469]	Step 20167	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/28 03:16:35AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][250/469]	Step 20167	Loss 0.0257	Prec@(1,5) (99.4%, 100.0%)
05/28 03:16:39AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][300/469]	Step 20167	Loss 0.0268	Prec@(1,5) (99.3%, 100.0%)
05/28 03:16:42AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][350/469]	Step 20167	Loss 0.0265	Prec@(1,5) (99.3%, 100.0%)
05/28 03:16:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][400/469]	Step 20167	Loss 0.0261	Prec@(1,5) (99.3%, 100.0%)
05/28 03:16:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][450/469]	Step 20167	Loss 0.0260	Prec@(1,5) (99.3%, 100.0%)
05/28 03:16:49AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [42][468/469]	Step 20167	Loss 0.0255	Prec@(1,5) (99.3%, 100.0%)
05/28 03:16:50AM searchStage_trainer.py:260 [INFO] Valid: [ 42/49] Final Prec@1 99.3333%
05/28 03:16:50AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:16:50AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.4300%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2428, 0.2616, 0.2436, 0.2520],
        [0.2468, 0.2823, 0.2578, 0.2131]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2834, 0.2473, 0.2275],
        [0.2861, 0.2517, 0.2449, 0.2172],
        [0.2589, 0.2961, 0.2331, 0.2119]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2799, 0.2676, 0.2624, 0.1901],
        [0.2744, 0.2761, 0.2107, 0.2388],
        [0.2625, 0.2875, 0.2281, 0.2219]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2768, 0.2457, 0.2232],
        [0.2509, 0.2859, 0.2486, 0.2146],
        [0.2537, 0.2718, 0.2260, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2840, 0.2298, 0.2225],
        [0.2582, 0.2858, 0.2365, 0.2195],
        [0.2539, 0.2730, 0.2338, 0.2393]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2698, 0.2455, 0.2355],
        [0.2457, 0.2708, 0.2310, 0.2524],
        [0.2470, 0.2817, 0.2556, 0.2157]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 03:17:11AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][50/468]	Step 20217	lr 0.00214	Loss 0.0008 (0.0041)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:17:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][100/468]	Step 20267	lr 0.00214	Loss 0.0004 (0.0047)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:17:50AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][150/468]	Step 20317	lr 0.00214	Loss 0.0005 (0.0043)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:18:10AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][200/468]	Step 20367	lr 0.00214	Loss 0.0088 (0.0047)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:18:30AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][250/468]	Step 20417	lr 0.00214	Loss 0.0006 (0.0044)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:18:50AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][300/468]	Step 20467	lr 0.00214	Loss 0.0004 (0.0045)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:19:10AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][350/468]	Step 20517	lr 0.00214	Loss 0.0008 (0.0041)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:19:29AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][400/468]	Step 20567	lr 0.00214	Loss 0.0034 (0.0039)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:19:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][450/468]	Step 20617	lr 0.00214	Loss 0.0018 (0.0041)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:19:56AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [43][468/468]	Step 20635	lr 0.00214	Loss 0.0014 (0.0041)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:19:57AM searchShareStage_trainer.py:134 [INFO] Train: [ 43/49] Final Prec@1 99.8700%
05/28 03:20:00AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][50/469]	Step 20636	Loss 0.0184	Prec@(1,5) (99.5%, 100.0%)
05/28 03:20:03AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][100/469]	Step 20636	Loss 0.0190	Prec@(1,5) (99.5%, 100.0%)
05/28 03:20:06AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][150/469]	Step 20636	Loss 0.0216	Prec@(1,5) (99.4%, 100.0%)
05/28 03:20:10AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][200/469]	Step 20636	Loss 0.0219	Prec@(1,5) (99.4%, 100.0%)
05/28 03:20:13AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][250/469]	Step 20636	Loss 0.0219	Prec@(1,5) (99.4%, 100.0%)
05/28 03:20:16AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][300/469]	Step 20636	Loss 0.0219	Prec@(1,5) (99.4%, 100.0%)
05/28 03:20:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][350/469]	Step 20636	Loss 0.0222	Prec@(1,5) (99.5%, 100.0%)
05/28 03:20:22AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][400/469]	Step 20636	Loss 0.0219	Prec@(1,5) (99.5%, 100.0%)
05/28 03:20:25AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][450/469]	Step 20636	Loss 0.0214	Prec@(1,5) (99.5%, 100.0%)
05/28 03:20:26AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [43][468/469]	Step 20636	Loss 0.0212	Prec@(1,5) (99.5%, 100.0%)
05/28 03:20:26AM searchStage_trainer.py:260 [INFO] Valid: [ 43/49] Final Prec@1 99.4667%
05/28 03:20:26AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:20:27AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.4667%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2428, 0.2615, 0.2437, 0.2520],
        [0.2468, 0.2822, 0.2578, 0.2132]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2833, 0.2474, 0.2275],
        [0.2862, 0.2517, 0.2450, 0.2172],
        [0.2589, 0.2961, 0.2331, 0.2119]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2800, 0.2675, 0.2624, 0.1901],
        [0.2744, 0.2760, 0.2107, 0.2389],
        [0.2625, 0.2875, 0.2281, 0.2219]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2768, 0.2458, 0.2232],
        [0.2509, 0.2858, 0.2486, 0.2147],
        [0.2537, 0.2718, 0.2260, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2839, 0.2298, 0.2225],
        [0.2582, 0.2858, 0.2365, 0.2195],
        [0.2539, 0.2730, 0.2338, 0.2393]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2697, 0.2455, 0.2355],
        [0.2457, 0.2708, 0.2311, 0.2524],
        [0.2469, 0.2817, 0.2556, 0.2157]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 03:20:48AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][50/468]	Step 20686	lr 0.00184	Loss 0.0006 (0.0015)	Prec@(1,5) (100.0%, 100.0%)	
05/28 03:21:07AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][100/468]	Step 20736	lr 0.00184	Loss 0.0008 (0.0031)	Prec@(1,5) (100.0%, 100.0%)	
05/28 03:21:27AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][150/468]	Step 20786	lr 0.00184	Loss 0.0008 (0.0034)	Prec@(1,5) (100.0%, 100.0%)	
05/28 03:21:46AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][200/468]	Step 20836	lr 0.00184	Loss 0.0005 (0.0033)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:22:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][250/468]	Step 20886	lr 0.00184	Loss 0.0036 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:22:26AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][300/468]	Step 20936	lr 0.00184	Loss 0.0108 (0.0030)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:22:46AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][350/468]	Step 20986	lr 0.00184	Loss 0.0031 (0.0029)	Prec@(1,5) (100.0%, 100.0%)	
05/28 03:23:05AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][400/468]	Step 21036	lr 0.00184	Loss 0.0177 (0.0030)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:23:26AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][450/468]	Step 21086	lr 0.00184	Loss 0.0054 (0.0028)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:23:33AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [44][468/468]	Step 21104	lr 0.00184	Loss 0.0233 (0.0028)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:23:34AM searchShareStage_trainer.py:134 [INFO] Train: [ 44/49] Final Prec@1 99.9467%
05/28 03:23:37AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][50/469]	Step 21105	Loss 0.0172	Prec@(1,5) (99.3%, 100.0%)
05/28 03:23:40AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][100/469]	Step 21105	Loss 0.0219	Prec@(1,5) (99.3%, 100.0%)
05/28 03:23:44AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][150/469]	Step 21105	Loss 0.0192	Prec@(1,5) (99.4%, 100.0%)
05/28 03:23:47AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][200/469]	Step 21105	Loss 0.0220	Prec@(1,5) (99.3%, 100.0%)
05/28 03:23:50AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][250/469]	Step 21105	Loss 0.0206	Prec@(1,5) (99.4%, 100.0%)
05/28 03:23:53AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][300/469]	Step 21105	Loss 0.0210	Prec@(1,5) (99.4%, 100.0%)
05/28 03:23:56AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][350/469]	Step 21105	Loss 0.0217	Prec@(1,5) (99.4%, 100.0%)
05/28 03:23:59AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][400/469]	Step 21105	Loss 0.0223	Prec@(1,5) (99.4%, 100.0%)
05/28 03:24:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][450/469]	Step 21105	Loss 0.0226	Prec@(1,5) (99.4%, 100.0%)
05/28 03:24:03AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [44][468/469]	Step 21105	Loss 0.0222	Prec@(1,5) (99.4%, 100.0%)
05/28 03:24:03AM searchStage_trainer.py:260 [INFO] Valid: [ 44/49] Final Prec@1 99.3700%
05/28 03:24:03AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:24:03AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.4667%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2428, 0.2615, 0.2437, 0.2521],
        [0.2468, 0.2822, 0.2578, 0.2132]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2833, 0.2474, 0.2276],
        [0.2862, 0.2516, 0.2450, 0.2172],
        [0.2589, 0.2960, 0.2331, 0.2120]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2800, 0.2675, 0.2624, 0.1901],
        [0.2744, 0.2760, 0.2107, 0.2389],
        [0.2625, 0.2875, 0.2281, 0.2219]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2767, 0.2458, 0.2232],
        [0.2509, 0.2858, 0.2487, 0.2147],
        [0.2537, 0.2717, 0.2260, 0.2486]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2839, 0.2298, 0.2226],
        [0.2582, 0.2857, 0.2365, 0.2195],
        [0.2539, 0.2729, 0.2339, 0.2394]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2697, 0.2455, 0.2356],
        [0.2457, 0.2707, 0.2311, 0.2525],
        [0.2469, 0.2817, 0.2556, 0.2157]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 03:24:24AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][50/468]	Step 21155	lr 0.00159	Loss 0.0005 (0.0025)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:24:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][100/468]	Step 21205	lr 0.00159	Loss 0.0010 (0.0026)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:25:02AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][150/468]	Step 21255	lr 0.00159	Loss 0.0010 (0.0024)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:25:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][200/468]	Step 21305	lr 0.00159	Loss 0.0075 (0.0026)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:25:42AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][250/468]	Step 21355	lr 0.00159	Loss 0.0006 (0.0027)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:26:02AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][300/468]	Step 21405	lr 0.00159	Loss 0.0009 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:26:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][350/468]	Step 21455	lr 0.00159	Loss 0.0002 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:26:41AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][400/468]	Step 21505	lr 0.00159	Loss 0.0002 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:27:01AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][450/468]	Step 21555	lr 0.00159	Loss 0.0066 (0.0030)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:27:08AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [45][468/468]	Step 21573	lr 0.00159	Loss 0.0003 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:27:08AM searchShareStage_trainer.py:134 [INFO] Train: [ 45/49] Final Prec@1 99.9067%
05/28 03:27:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][50/469]	Step 21574	Loss 0.0131	Prec@(1,5) (99.7%, 100.0%)
05/28 03:27:15AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][100/469]	Step 21574	Loss 0.0173	Prec@(1,5) (99.6%, 100.0%)
05/28 03:27:18AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][150/469]	Step 21574	Loss 0.0192	Prec@(1,5) (99.5%, 100.0%)
05/28 03:27:21AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][200/469]	Step 21574	Loss 0.0210	Prec@(1,5) (99.5%, 100.0%)
05/28 03:27:23AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][250/469]	Step 21574	Loss 0.0215	Prec@(1,5) (99.5%, 100.0%)
05/28 03:27:26AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][300/469]	Step 21574	Loss 0.0225	Prec@(1,5) (99.5%, 100.0%)
05/28 03:27:29AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][350/469]	Step 21574	Loss 0.0228	Prec@(1,5) (99.5%, 100.0%)
05/28 03:27:32AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][400/469]	Step 21574	Loss 0.0225	Prec@(1,5) (99.5%, 100.0%)
05/28 03:27:35AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][450/469]	Step 21574	Loss 0.0237	Prec@(1,5) (99.4%, 100.0%)
05/28 03:27:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [45][468/469]	Step 21574	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/28 03:27:36AM searchStage_trainer.py:260 [INFO] Valid: [ 45/49] Final Prec@1 99.4367%
05/28 03:27:36AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:27:36AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.4667%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2428, 0.2614, 0.2437, 0.2521],
        [0.2468, 0.2821, 0.2579, 0.2132]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2833, 0.2474, 0.2276],
        [0.2862, 0.2516, 0.2450, 0.2173],
        [0.2589, 0.2959, 0.2332, 0.2120]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2800, 0.2675, 0.2624, 0.1901],
        [0.2744, 0.2760, 0.2107, 0.2389],
        [0.2625, 0.2874, 0.2282, 0.2219]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2767, 0.2458, 0.2232],
        [0.2509, 0.2858, 0.2487, 0.2147],
        [0.2537, 0.2717, 0.2260, 0.2487]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2839, 0.2298, 0.2226],
        [0.2582, 0.2857, 0.2365, 0.2195],
        [0.2539, 0.2728, 0.2339, 0.2394]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2697, 0.2456, 0.2356],
        [0.2456, 0.2707, 0.2311, 0.2525],
        [0.2469, 0.2816, 0.2557, 0.2158]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 03:27:57AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][50/468]	Step 21624	lr 0.00138	Loss 0.0068 (0.0041)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:28:17AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][100/468]	Step 21674	lr 0.00138	Loss 0.0017 (0.0045)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:28:36AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][150/468]	Step 21724	lr 0.00138	Loss 0.0006 (0.0040)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:28:56AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][200/468]	Step 21774	lr 0.00138	Loss 0.0013 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:29:16AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][250/468]	Step 21824	lr 0.00138	Loss 0.0001 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:29:36AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][300/468]	Step 21874	lr 0.00138	Loss 0.0015 (0.0039)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:29:55AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][350/468]	Step 21924	lr 0.00138	Loss 0.0006 (0.0038)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:30:15AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][400/468]	Step 21974	lr 0.00138	Loss 0.0002 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:30:34AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][450/468]	Step 22024	lr 0.00138	Loss 0.0059 (0.0036)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:30:41AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [46][468/468]	Step 22042	lr 0.00138	Loss 0.0006 (0.0035)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:30:42AM searchShareStage_trainer.py:134 [INFO] Train: [ 46/49] Final Prec@1 99.9000%
05/28 03:30:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][50/469]	Step 22043	Loss 0.0261	Prec@(1,5) (99.3%, 100.0%)
05/28 03:30:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][100/469]	Step 22043	Loss 0.0232	Prec@(1,5) (99.5%, 100.0%)
05/28 03:30:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][150/469]	Step 22043	Loss 0.0241	Prec@(1,5) (99.4%, 100.0%)
05/28 03:30:54AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][200/469]	Step 22043	Loss 0.0239	Prec@(1,5) (99.4%, 100.0%)
05/28 03:30:57AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][250/469]	Step 22043	Loss 0.0220	Prec@(1,5) (99.5%, 100.0%)
05/28 03:30:59AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][300/469]	Step 22043	Loss 0.0213	Prec@(1,5) (99.5%, 100.0%)
05/28 03:31:02AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][350/469]	Step 22043	Loss 0.0216	Prec@(1,5) (99.5%, 100.0%)
05/28 03:31:05AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][400/469]	Step 22043	Loss 0.0213	Prec@(1,5) (99.5%, 100.0%)
05/28 03:31:08AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][450/469]	Step 22043	Loss 0.0218	Prec@(1,5) (99.5%, 100.0%)
05/28 03:31:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [46][468/469]	Step 22043	Loss 0.0215	Prec@(1,5) (99.5%, 100.0%)
05/28 03:31:09AM searchStage_trainer.py:260 [INFO] Valid: [ 46/49] Final Prec@1 99.4700%
05/28 03:31:09AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:31:10AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.4700%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2428, 0.2614, 0.2437, 0.2521],
        [0.2468, 0.2821, 0.2579, 0.2133]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2832, 0.2474, 0.2276],
        [0.2862, 0.2515, 0.2450, 0.2173],
        [0.2589, 0.2959, 0.2332, 0.2120]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2801, 0.2674, 0.2624, 0.1900],
        [0.2744, 0.2759, 0.2107, 0.2389],
        [0.2625, 0.2874, 0.2282, 0.2220]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2767, 0.2458, 0.2232],
        [0.2509, 0.2857, 0.2487, 0.2147],
        [0.2536, 0.2716, 0.2260, 0.2487]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2637, 0.2839, 0.2298, 0.2226],
        [0.2582, 0.2857, 0.2366, 0.2196],
        [0.2539, 0.2728, 0.2339, 0.2395]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2696, 0.2456, 0.2356],
        [0.2457, 0.2707, 0.2311, 0.2525],
        [0.2469, 0.2815, 0.2557, 0.2158]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 03:31:31AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][50/468]	Step 22093	lr 0.00121	Loss 0.0009 (0.0038)	Prec@(1,5) (99.8%, 100.0%)	
05/28 03:31:51AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][100/468]	Step 22143	lr 0.00121	Loss 0.0014 (0.0034)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:32:10AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][150/468]	Step 22193	lr 0.00121	Loss 0.0003 (0.0031)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:32:30AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][200/468]	Step 22243	lr 0.00121	Loss 0.0006 (0.0029)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:32:50AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][250/468]	Step 22293	lr 0.00121	Loss 0.0018 (0.0028)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:33:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][300/468]	Step 22343	lr 0.00121	Loss 0.0003 (0.0026)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:33:29AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][350/468]	Step 22393	lr 0.00121	Loss 0.0002 (0.0025)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:33:49AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][400/468]	Step 22443	lr 0.00121	Loss 0.0099 (0.0028)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:34:09AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][450/468]	Step 22493	lr 0.00121	Loss 0.0007 (0.0026)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:34:15AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [47][468/468]	Step 22511	lr 0.00121	Loss 0.0004 (0.0026)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:34:16AM searchShareStage_trainer.py:134 [INFO] Train: [ 47/49] Final Prec@1 99.9133%
05/28 03:34:20AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][50/469]	Step 22512	Loss 0.0197	Prec@(1,5) (99.5%, 100.0%)
05/28 03:34:23AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][100/469]	Step 22512	Loss 0.0214	Prec@(1,5) (99.5%, 100.0%)
05/28 03:34:26AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][150/469]	Step 22512	Loss 0.0280	Prec@(1,5) (99.4%, 100.0%)
05/28 03:34:29AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][200/469]	Step 22512	Loss 0.0254	Prec@(1,5) (99.4%, 100.0%)
05/28 03:34:32AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][250/469]	Step 22512	Loss 0.0238	Prec@(1,5) (99.4%, 100.0%)
05/28 03:34:35AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][300/469]	Step 22512	Loss 0.0234	Prec@(1,5) (99.4%, 100.0%)
05/28 03:34:38AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][350/469]	Step 22512	Loss 0.0232	Prec@(1,5) (99.4%, 100.0%)
05/28 03:34:41AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][400/469]	Step 22512	Loss 0.0224	Prec@(1,5) (99.4%, 100.0%)
05/28 03:34:44AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][450/469]	Step 22512	Loss 0.0230	Prec@(1,5) (99.4%, 100.0%)
05/28 03:34:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [47][468/469]	Step 22512	Loss 0.0230	Prec@(1,5) (99.4%, 100.0%)
05/28 03:34:46AM searchStage_trainer.py:260 [INFO] Valid: [ 47/49] Final Prec@1 99.4300%
05/28 03:34:46AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:34:46AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.4700%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2427, 0.2613, 0.2438, 0.2522],
        [0.2468, 0.2820, 0.2579, 0.2133]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2832, 0.2474, 0.2276],
        [0.2862, 0.2515, 0.2450, 0.2173],
        [0.2589, 0.2959, 0.2332, 0.2121]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2802, 0.2674, 0.2624, 0.1900],
        [0.2744, 0.2759, 0.2107, 0.2389],
        [0.2625, 0.2873, 0.2282, 0.2220]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2767, 0.2459, 0.2232],
        [0.2509, 0.2857, 0.2487, 0.2147],
        [0.2536, 0.2716, 0.2260, 0.2487]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2838, 0.2298, 0.2226],
        [0.2582, 0.2856, 0.2366, 0.2196],
        [0.2539, 0.2728, 0.2339, 0.2395]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2696, 0.2456, 0.2356],
        [0.2456, 0.2707, 0.2311, 0.2526],
        [0.2469, 0.2815, 0.2557, 0.2159]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 03:35:07AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][50/468]	Step 22562	lr 0.00109	Loss 0.0023 (0.0018)	Prec@(1,5) (100.0%, 100.0%)	
05/28 03:35:27AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][100/468]	Step 22612	lr 0.00109	Loss 0.0037 (0.0027)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:35:46AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][150/468]	Step 22662	lr 0.00109	Loss 0.0011 (0.0028)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:36:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][200/468]	Step 22712	lr 0.00109	Loss 0.0005 (0.0026)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:36:26AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][250/468]	Step 22762	lr 0.00109	Loss 0.0014 (0.0024)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:36:45AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][300/468]	Step 22812	lr 0.00109	Loss 0.0003 (0.0024)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:37:06AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][350/468]	Step 22862	lr 0.00109	Loss 0.0002 (0.0025)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:37:26AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][400/468]	Step 22912	lr 0.00109	Loss 0.0016 (0.0027)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:37:46AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][450/468]	Step 22962	lr 0.00109	Loss 0.0004 (0.0027)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:37:53AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [48][468/468]	Step 22980	lr 0.00109	Loss 0.0009 (0.0027)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:37:53AM searchShareStage_trainer.py:134 [INFO] Train: [ 48/49] Final Prec@1 99.9167%
05/28 03:37:57AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][50/469]	Step 22981	Loss 0.0234	Prec@(1,5) (99.4%, 100.0%)
05/28 03:38:00AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][100/469]	Step 22981	Loss 0.0243	Prec@(1,5) (99.3%, 100.0%)
05/28 03:38:03AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][150/469]	Step 22981	Loss 0.0258	Prec@(1,5) (99.3%, 100.0%)
05/28 03:38:06AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][200/469]	Step 22981	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/28 03:38:09AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][250/469]	Step 22981	Loss 0.0237	Prec@(1,5) (99.4%, 100.0%)
05/28 03:38:12AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][300/469]	Step 22981	Loss 0.0232	Prec@(1,5) (99.4%, 100.0%)
05/28 03:38:16AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][350/469]	Step 22981	Loss 0.0248	Prec@(1,5) (99.4%, 100.0%)
05/28 03:38:19AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][400/469]	Step 22981	Loss 0.0241	Prec@(1,5) (99.4%, 100.0%)
05/28 03:38:22AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][450/469]	Step 22981	Loss 0.0248	Prec@(1,5) (99.4%, 100.0%)
05/28 03:38:23AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [48][468/469]	Step 22981	Loss 0.0241	Prec@(1,5) (99.4%, 100.0%)
05/28 03:38:23AM searchStage_trainer.py:260 [INFO] Valid: [ 48/49] Final Prec@1 99.3867%
05/28 03:38:23AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:38:23AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.4700%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2427, 0.2613, 0.2438, 0.2522],
        [0.2468, 0.2820, 0.2579, 0.2133]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2418, 0.2832, 0.2474, 0.2277],
        [0.2862, 0.2515, 0.2450, 0.2173],
        [0.2589, 0.2959, 0.2332, 0.2121]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2802, 0.2673, 0.2624, 0.1900],
        [0.2745, 0.2758, 0.2107, 0.2390],
        [0.2625, 0.2873, 0.2282, 0.2220]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2767, 0.2459, 0.2232],
        [0.2509, 0.2857, 0.2487, 0.2148],
        [0.2536, 0.2716, 0.2261, 0.2488]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2638, 0.2838, 0.2298, 0.2227],
        [0.2582, 0.2856, 0.2366, 0.2196],
        [0.2539, 0.2727, 0.2339, 0.2395]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2696, 0.2456, 0.2357],
        [0.2456, 0.2706, 0.2311, 0.2526],
        [0.2469, 0.2815, 0.2557, 0.2159]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/28 03:38:44AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][50/468]	Step 23031	lr 0.00102	Loss 0.0001 (0.0033)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:39:04AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][100/468]	Step 23081	lr 0.00102	Loss 0.0005 (0.0034)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:39:24AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][150/468]	Step 23131	lr 0.00102	Loss 0.0011 (0.0027)	Prec@(1,5) (100.0%, 100.0%)	
05/28 03:39:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][200/468]	Step 23181	lr 0.00102	Loss 0.0002 (0.0027)	Prec@(1,5) (100.0%, 100.0%)	
05/28 03:40:04AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][250/468]	Step 23231	lr 0.00102	Loss 0.0001 (0.0030)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:40:23AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][300/468]	Step 23281	lr 0.00102	Loss 0.0012 (0.0028)	Prec@(1,5) (100.0%, 100.0%)	
05/28 03:40:43AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][350/468]	Step 23331	lr 0.00102	Loss 0.0004 (0.0028)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:41:02AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][400/468]	Step 23381	lr 0.00102	Loss 0.0007 (0.0027)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:41:22AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][450/468]	Step 23431	lr 0.00102	Loss 0.0007 (0.0026)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:41:29AM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [49][468/468]	Step 23449	lr 0.00102	Loss 0.0067 (0.0027)	Prec@(1,5) (99.9%, 100.0%)	
05/28 03:41:30AM searchShareStage_trainer.py:134 [INFO] Train: [ 49/49] Final Prec@1 99.9433%
05/28 03:41:33AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][50/469]	Step 23450	Loss 0.0252	Prec@(1,5) (99.4%, 100.0%)
05/28 03:41:36AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][100/469]	Step 23450	Loss 0.0233	Prec@(1,5) (99.4%, 100.0%)
05/28 03:41:39AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][150/469]	Step 23450	Loss 0.0245	Prec@(1,5) (99.4%, 100.0%)
05/28 03:41:42AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][200/469]	Step 23450	Loss 0.0262	Prec@(1,5) (99.4%, 100.0%)
05/28 03:41:45AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][250/469]	Step 23450	Loss 0.0262	Prec@(1,5) (99.3%, 100.0%)
05/28 03:41:48AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][300/469]	Step 23450	Loss 0.0258	Prec@(1,5) (99.3%, 100.0%)
05/28 03:41:51AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][350/469]	Step 23450	Loss 0.0249	Prec@(1,5) (99.4%, 100.0%)
05/28 03:41:54AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][400/469]	Step 23450	Loss 0.0245	Prec@(1,5) (99.4%, 100.0%)
05/28 03:41:56AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][450/469]	Step 23450	Loss 0.0236	Prec@(1,5) (99.4%, 100.0%)
05/28 03:41:58AM searchStage_trainer.py:249 [INFO] Valid: Epoch: [49][468/469]	Step 23450	Loss 0.0233	Prec@(1,5) (99.4%, 100.0%)
05/28 03:41:58AM searchStage_trainer.py:260 [INFO] Valid: [ 49/49] Final Prec@1 99.3967%
05/28 03:41:58AM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/28 03:41:58AM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 99.4700%
05/28 03:41:58AM searchStage_main.py:73 [INFO] Final best Prec@1 = 99.4700%
05/28 03:41:58AM searchStage_main.py:74 [INFO] Final Best Genotype = Genotype2(DAG1=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
