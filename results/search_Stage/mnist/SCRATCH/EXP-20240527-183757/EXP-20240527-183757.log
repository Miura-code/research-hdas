05/27 06:37:57PM parser.py:28 [INFO] 
05/27 06:37:57PM parser.py:29 [INFO] Parameters:
05/27 06:37:57PM parser.py:31 [INFO] DAG_PATH=results/search_Stage/mnist/SCRATCH/EXP-20240527-183757/DAG
05/27 06:37:57PM parser.py:31 [INFO] ALPHA_LR=0.0003
05/27 06:37:57PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
05/27 06:37:57PM parser.py:31 [INFO] BATCH_SIZE=64
05/27 06:37:57PM parser.py:31 [INFO] CHECKPOINT_RESET=False
05/27 06:37:57PM parser.py:31 [INFO] DATA_PATH=../data/
05/27 06:37:57PM parser.py:31 [INFO] DATASET=mnist
05/27 06:37:57PM parser.py:31 [INFO] EPOCHS=50
05/27 06:37:57PM parser.py:31 [INFO] EXP_NAME=EXP-20240527-183757
05/27 06:37:57PM parser.py:31 [INFO] GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('skip_connect', 2)]], normal_concat=[2, 3, 4, 5], reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce_concat=[2, 3, 4, 5])
05/27 06:37:57PM parser.py:31 [INFO] GPUS=[1]
05/27 06:37:57PM parser.py:31 [INFO] INIT_CHANNELS=16
05/27 06:37:57PM parser.py:31 [INFO] LAYERS=20
05/27 06:37:57PM parser.py:31 [INFO] LOCAL_RANK=0
05/27 06:37:57PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/27 06:37:57PM parser.py:31 [INFO] NAME=SCRATCH
05/27 06:37:57PM parser.py:31 [INFO] PATH=results/search_Stage/mnist/SCRATCH/EXP-20240527-183757
05/27 06:37:57PM parser.py:31 [INFO] PRINT_FREQ=50
05/27 06:37:57PM parser.py:31 [INFO] RESUME_PATH=None
05/27 06:37:57PM parser.py:31 [INFO] SAVE=EXP
05/27 06:37:57PM parser.py:31 [INFO] SEED=1
05/27 06:37:57PM parser.py:31 [INFO] SHARE_STAGE=True
05/27 06:37:57PM parser.py:31 [INFO] TRAIN_PORTION=0.5
05/27 06:37:57PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
05/27 06:37:57PM parser.py:31 [INFO] W_LR=0.025
05/27 06:37:57PM parser.py:31 [INFO] W_LR_MIN=0.001
05/27 06:37:57PM parser.py:31 [INFO] W_MOMENTUM=0.9
05/27 06:37:57PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
05/27 06:37:57PM parser.py:31 [INFO] WORKERS=4
05/27 06:37:57PM parser.py:32 [INFO] 
05/27 06:37:58PM searchStage_trainer.py:103 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2501, 0.2497, 0.2498, 0.2503],
        [0.2499, 0.2500, 0.2498, 0.2503]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2500, 0.2497, 0.2503],
        [0.2501, 0.2499, 0.2500, 0.2500],
        [0.2500, 0.2502, 0.2499, 0.2499]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2501, 0.2499, 0.2500],
        [0.2501, 0.2497, 0.2500, 0.2502],
        [0.2500, 0.2501, 0.2502, 0.2497]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2498, 0.2502, 0.2501],
        [0.2500, 0.2499, 0.2501, 0.2500],
        [0.2503, 0.2500, 0.2501, 0.2496]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2502, 0.2500, 0.2499],
        [0.2500, 0.2496, 0.2503, 0.2500],
        [0.2502, 0.2494, 0.2501, 0.2503]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2503, 0.2502, 0.2495],
        [0.2501, 0.2496, 0.2503, 0.2501],
        [0.2501, 0.2500, 0.2498, 0.2501]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 06:38:22PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][50/468]	Step 50	lr 0.025	Loss 0.6563 (1.6208)	Prec@(1,5) (42.9%, 81.2%)	
05/27 06:38:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][100/468]	Step 100	lr 0.025	Loss 0.4449 (1.0456)	Prec@(1,5) (64.5%, 90.0%)	
05/27 06:39:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][150/468]	Step 150	lr 0.025	Loss 0.5137 (0.8025)	Prec@(1,5) (73.3%, 93.1%)	
05/27 06:39:27PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][200/468]	Step 200	lr 0.025	Loss 0.2485 (0.6641)	Prec@(1,5) (78.2%, 94.7%)	
05/27 06:39:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][250/468]	Step 250	lr 0.025	Loss 0.1360 (0.5666)	Prec@(1,5) (81.5%, 95.8%)	
05/27 06:40:09PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][300/468]	Step 300	lr 0.025	Loss 0.1119 (0.4925)	Prec@(1,5) (83.9%, 96.5%)	
05/27 06:40:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][350/468]	Step 350	lr 0.025	Loss 0.0956 (0.4410)	Prec@(1,5) (85.7%, 96.9%)	
05/27 06:40:52PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][400/468]	Step 400	lr 0.025	Loss 0.1640 (0.3982)	Prec@(1,5) (87.1%, 97.3%)	
05/27 06:41:13PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][450/468]	Step 450	lr 0.025	Loss 0.1781 (0.3673)	Prec@(1,5) (88.1%, 97.6%)	
05/27 06:41:21PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [0][468/468]	Step 468	lr 0.025	Loss 0.0585 (0.3568)	Prec@(1,5) (88.5%, 97.7%)	
05/27 06:41:22PM searchShareStage_trainer.py:134 [INFO] Train: [  0/49] Final Prec@1 88.5033%
05/27 06:41:26PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][50/469]	Step 469	Loss 0.0873	Prec@(1,5) (97.4%, 100.0%)
05/27 06:41:30PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][100/469]	Step 469	Loss 0.0912	Prec@(1,5) (97.2%, 100.0%)
05/27 06:41:33PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][150/469]	Step 469	Loss 0.0914	Prec@(1,5) (97.1%, 99.9%)
05/27 06:41:36PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][200/469]	Step 469	Loss 0.0907	Prec@(1,5) (97.2%, 100.0%)
05/27 06:41:39PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][250/469]	Step 469	Loss 0.0954	Prec@(1,5) (97.1%, 99.9%)
05/27 06:41:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][300/469]	Step 469	Loss 0.0975	Prec@(1,5) (97.0%, 99.9%)
05/27 06:41:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][350/469]	Step 469	Loss 0.0953	Prec@(1,5) (97.1%, 100.0%)
05/27 06:41:49PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][400/469]	Step 469	Loss 0.0941	Prec@(1,5) (97.1%, 99.9%)
05/27 06:41:52PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][450/469]	Step 469	Loss 0.0934	Prec@(1,5) (97.1%, 99.9%)
05/27 06:41:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [0][468/469]	Step 469	Loss 0.0931	Prec@(1,5) (97.1%, 99.9%)
05/27 06:41:54PM searchStage_trainer.py:260 [INFO] Valid: [  0/49] Final Prec@1 97.1067%
05/27 06:41:54PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 6)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 6)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 6)]], DAG3_concat=range(6, 8))
05/27 06:41:54PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 97.1067%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2550, 0.2518, 0.2439],
        [0.2494, 0.2495, 0.2496, 0.2514]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2534, 0.2487, 0.2475],
        [0.2487, 0.2532, 0.2541, 0.2440],
        [0.2512, 0.2570, 0.2449, 0.2469]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2525, 0.2507, 0.2454],
        [0.2518, 0.2600, 0.2435, 0.2447],
        [0.2536, 0.2581, 0.2453, 0.2430]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2616, 0.2493, 0.2356],
        [0.2563, 0.2540, 0.2406, 0.2491],
        [0.2538, 0.2557, 0.2421, 0.2484]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2556, 0.2454, 0.2460],
        [0.2527, 0.2582, 0.2470, 0.2421],
        [0.2504, 0.2559, 0.2453, 0.2483]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2561, 0.2495, 0.2435],
        [0.2526, 0.2539, 0.2447, 0.2487],
        [0.2508, 0.2552, 0.2482, 0.2458]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 06:42:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][50/468]	Step 519	lr 0.02498	Loss 0.1945 (0.1005)	Prec@(1,5) (97.0%, 99.9%)	
05/27 06:42:40PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][100/468]	Step 569	lr 0.02498	Loss 0.0168 (0.0985)	Prec@(1,5) (97.1%, 99.9%)	
05/27 06:43:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][150/468]	Step 619	lr 0.02498	Loss 0.1005 (0.0955)	Prec@(1,5) (97.2%, 99.9%)	
05/27 06:43:24PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][200/468]	Step 669	lr 0.02498	Loss 0.0181 (0.0914)	Prec@(1,5) (97.3%, 99.9%)	
05/27 06:43:47PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][250/468]	Step 719	lr 0.02498	Loss 0.1265 (0.0877)	Prec@(1,5) (97.4%, 99.9%)	
05/27 06:44:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][300/468]	Step 769	lr 0.02498	Loss 0.0116 (0.0836)	Prec@(1,5) (97.5%, 99.9%)	
05/27 06:44:31PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][350/468]	Step 819	lr 0.02498	Loss 0.1624 (0.0845)	Prec@(1,5) (97.5%, 99.9%)	
05/27 06:44:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][400/468]	Step 869	lr 0.02498	Loss 0.1255 (0.0836)	Prec@(1,5) (97.5%, 99.9%)	
05/27 06:45:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][450/468]	Step 919	lr 0.02498	Loss 0.0899 (0.0835)	Prec@(1,5) (97.5%, 99.9%)	
05/27 06:45:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [1][468/468]	Step 937	lr 0.02498	Loss 0.0328 (0.0830)	Prec@(1,5) (97.5%, 99.9%)	
05/27 06:45:24PM searchShareStage_trainer.py:134 [INFO] Train: [  1/49] Final Prec@1 97.4700%
05/27 06:45:27PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][50/469]	Step 938	Loss 0.0657	Prec@(1,5) (98.0%, 100.0%)
05/27 06:45:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][100/469]	Step 938	Loss 0.0671	Prec@(1,5) (98.0%, 100.0%)
05/27 06:45:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][150/469]	Step 938	Loss 0.0677	Prec@(1,5) (98.0%, 100.0%)
05/27 06:45:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][200/469]	Step 938	Loss 0.0681	Prec@(1,5) (98.0%, 100.0%)
05/27 06:45:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][250/469]	Step 938	Loss 0.0684	Prec@(1,5) (97.9%, 100.0%)
05/27 06:45:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][300/469]	Step 938	Loss 0.0672	Prec@(1,5) (97.9%, 100.0%)
05/27 06:45:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][350/469]	Step 938	Loss 0.0675	Prec@(1,5) (97.9%, 100.0%)
05/27 06:45:51PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][400/469]	Step 938	Loss 0.0690	Prec@(1,5) (97.9%, 99.9%)
05/27 06:45:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][450/469]	Step 938	Loss 0.0683	Prec@(1,5) (97.9%, 99.9%)
05/27 06:45:56PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [1][468/469]	Step 938	Loss 0.0680	Prec@(1,5) (97.9%, 99.9%)
05/27 06:45:56PM searchStage_trainer.py:260 [INFO] Valid: [  1/49] Final Prec@1 97.9433%
05/27 06:45:56PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 06:45:56PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 97.9433%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2461, 0.2602, 0.2543, 0.2394],
        [0.2482, 0.2513, 0.2509, 0.2496]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2596, 0.2507, 0.2385],
        [0.2480, 0.2583, 0.2557, 0.2380],
        [0.2504, 0.2672, 0.2368, 0.2457]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2541, 0.2514, 0.2410],
        [0.2526, 0.2714, 0.2373, 0.2387],
        [0.2542, 0.2701, 0.2406, 0.2351]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2695, 0.2476, 0.2285],
        [0.2602, 0.2626, 0.2346, 0.2427],
        [0.2585, 0.2631, 0.2338, 0.2447]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2549, 0.2652, 0.2417, 0.2383],
        [0.2555, 0.2674, 0.2396, 0.2376],
        [0.2543, 0.2631, 0.2372, 0.2455]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2605, 0.2475, 0.2390],
        [0.2558, 0.2600, 0.2380, 0.2462],
        [0.2530, 0.2619, 0.2448, 0.2403]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 06:46:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][50/468]	Step 988	lr 0.02491	Loss 0.0253 (0.0537)	Prec@(1,5) (98.3%, 100.0%)	
05/27 06:46:42PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][100/468]	Step 1038	lr 0.02491	Loss 0.2621 (0.0632)	Prec@(1,5) (98.1%, 100.0%)	
05/27 06:47:03PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][150/468]	Step 1088	lr 0.02491	Loss 0.0337 (0.0694)	Prec@(1,5) (98.0%, 100.0%)	
05/27 06:47:26PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][200/468]	Step 1138	lr 0.02491	Loss 0.1057 (0.0674)	Prec@(1,5) (98.0%, 100.0%)	
05/27 06:47:48PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][250/468]	Step 1188	lr 0.02491	Loss 0.0684 (0.0654)	Prec@(1,5) (98.1%, 100.0%)	
05/27 06:48:10PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][300/468]	Step 1238	lr 0.02491	Loss 0.0441 (0.0650)	Prec@(1,5) (98.1%, 100.0%)	
05/27 06:48:32PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][350/468]	Step 1288	lr 0.02491	Loss 0.0117 (0.0651)	Prec@(1,5) (98.0%, 100.0%)	
05/27 06:48:53PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][400/468]	Step 1338	lr 0.02491	Loss 0.1330 (0.0641)	Prec@(1,5) (98.0%, 100.0%)	
05/27 06:49:15PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][450/468]	Step 1388	lr 0.02491	Loss 0.0672 (0.0630)	Prec@(1,5) (98.1%, 100.0%)	
05/27 06:49:23PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [2][468/468]	Step 1406	lr 0.02491	Loss 0.1251 (0.0636)	Prec@(1,5) (98.1%, 100.0%)	
05/27 06:49:24PM searchShareStage_trainer.py:134 [INFO] Train: [  2/49] Final Prec@1 98.0567%
05/27 06:49:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][50/469]	Step 1407	Loss 0.0711	Prec@(1,5) (98.3%, 100.0%)
05/27 06:49:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][100/469]	Step 1407	Loss 0.0570	Prec@(1,5) (98.5%, 100.0%)
05/27 06:49:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][150/469]	Step 1407	Loss 0.0592	Prec@(1,5) (98.4%, 100.0%)
05/27 06:49:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][200/469]	Step 1407	Loss 0.0564	Prec@(1,5) (98.5%, 100.0%)
05/27 06:49:41PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][250/469]	Step 1407	Loss 0.0583	Prec@(1,5) (98.4%, 100.0%)
05/27 06:49:44PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][300/469]	Step 1407	Loss 0.0567	Prec@(1,5) (98.4%, 100.0%)
05/27 06:49:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][350/469]	Step 1407	Loss 0.0588	Prec@(1,5) (98.4%, 100.0%)
05/27 06:49:50PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][400/469]	Step 1407	Loss 0.0590	Prec@(1,5) (98.3%, 100.0%)
05/27 06:49:53PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][450/469]	Step 1407	Loss 0.0585	Prec@(1,5) (98.3%, 100.0%)
05/27 06:49:54PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [2][468/469]	Step 1407	Loss 0.0583	Prec@(1,5) (98.3%, 100.0%)
05/27 06:49:54PM searchStage_trainer.py:260 [INFO] Valid: [  2/49] Final Prec@1 98.3133%
05/27 06:49:54PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 06:49:55PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 98.3133%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2460, 0.2633, 0.2542, 0.2365],
        [0.2438, 0.2562, 0.2550, 0.2450]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2638, 0.2495, 0.2324],
        [0.2486, 0.2628, 0.2566, 0.2321],
        [0.2539, 0.2763, 0.2300, 0.2398]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2576, 0.2524, 0.2367],
        [0.2535, 0.2825, 0.2336, 0.2305],
        [0.2575, 0.2791, 0.2340, 0.2295]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2575, 0.2767, 0.2430, 0.2228],
        [0.2642, 0.2710, 0.2280, 0.2367],
        [0.2639, 0.2698, 0.2267, 0.2396]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2559, 0.2741, 0.2385, 0.2316],
        [0.2590, 0.2770, 0.2312, 0.2327],
        [0.2574, 0.2702, 0.2310, 0.2413]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2670, 0.2452, 0.2344],
        [0.2566, 0.2658, 0.2333, 0.2442],
        [0.2550, 0.2689, 0.2415, 0.2345]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 06:50:17PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][50/468]	Step 1457	lr 0.02479	Loss 0.0554 (0.0533)	Prec@(1,5) (98.4%, 100.0%)	
05/27 06:50:37PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][100/468]	Step 1507	lr 0.02479	Loss 0.2086 (0.0533)	Prec@(1,5) (98.3%, 100.0%)	
05/27 06:50:58PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][150/468]	Step 1557	lr 0.02479	Loss 0.0623 (0.0503)	Prec@(1,5) (98.5%, 100.0%)	
05/27 06:51:20PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][200/468]	Step 1607	lr 0.02479	Loss 0.0747 (0.0494)	Prec@(1,5) (98.5%, 100.0%)	
05/27 06:51:43PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][250/468]	Step 1657	lr 0.02479	Loss 0.0040 (0.0497)	Prec@(1,5) (98.5%, 100.0%)	
05/27 06:52:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][300/468]	Step 1707	lr 0.02479	Loss 0.1471 (0.0504)	Prec@(1,5) (98.5%, 100.0%)	
05/27 06:52:27PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][350/468]	Step 1757	lr 0.02479	Loss 0.0049 (0.0517)	Prec@(1,5) (98.5%, 100.0%)	
05/27 06:52:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][400/468]	Step 1807	lr 0.02479	Loss 0.1032 (0.0545)	Prec@(1,5) (98.4%, 100.0%)	
05/27 06:53:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][450/468]	Step 1857	lr 0.02479	Loss 0.0376 (0.0543)	Prec@(1,5) (98.4%, 100.0%)	
05/27 06:53:18PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [3][468/468]	Step 1875	lr 0.02479	Loss 0.0442 (0.0541)	Prec@(1,5) (98.4%, 100.0%)	
05/27 06:53:19PM searchShareStage_trainer.py:134 [INFO] Train: [  3/49] Final Prec@1 98.3833%
05/27 06:53:22PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][50/469]	Step 1876	Loss 0.0478	Prec@(1,5) (98.7%, 99.9%)
05/27 06:53:25PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][100/469]	Step 1876	Loss 0.0442	Prec@(1,5) (98.7%, 100.0%)
05/27 06:53:28PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][150/469]	Step 1876	Loss 0.0479	Prec@(1,5) (98.6%, 100.0%)
05/27 06:53:31PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][200/469]	Step 1876	Loss 0.0498	Prec@(1,5) (98.5%, 100.0%)
05/27 06:53:34PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][250/469]	Step 1876	Loss 0.0513	Prec@(1,5) (98.5%, 100.0%)
05/27 06:53:37PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][300/469]	Step 1876	Loss 0.0515	Prec@(1,5) (98.5%, 100.0%)
05/27 06:53:40PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][350/469]	Step 1876	Loss 0.0527	Prec@(1,5) (98.5%, 100.0%)
05/27 06:53:43PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][400/469]	Step 1876	Loss 0.0523	Prec@(1,5) (98.5%, 100.0%)
05/27 06:53:46PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][450/469]	Step 1876	Loss 0.0526	Prec@(1,5) (98.5%, 100.0%)
05/27 06:53:47PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [3][468/469]	Step 1876	Loss 0.0522	Prec@(1,5) (98.5%, 100.0%)
05/27 06:53:47PM searchStage_trainer.py:260 [INFO] Valid: [  3/49] Final Prec@1 98.4800%
05/27 06:53:47PM searchStage_main.py:53 [INFO] DAG = Genotype2(DAG1=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG2_concat=range(6, 8), DAG3=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
05/27 06:53:48PM searchStage_main.py:68 [INFO] Until now, best Prec@1 = 98.4800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2430, 0.2712, 0.2542, 0.2316],
        [0.2440, 0.2599, 0.2553, 0.2407]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2702, 0.2483, 0.2267],
        [0.2480, 0.2693, 0.2575, 0.2252],
        [0.2545, 0.2873, 0.2242, 0.2341]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2609, 0.2532, 0.2319],
        [0.2550, 0.2923, 0.2286, 0.2241],
        [0.2589, 0.2893, 0.2286, 0.2233]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2583, 0.2838, 0.2419, 0.2159],
        [0.2679, 0.2768, 0.2206, 0.2347],
        [0.2662, 0.2780, 0.2221, 0.2337]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2571, 0.2811, 0.2368, 0.2249],
        [0.2618, 0.2849, 0.2250, 0.2282],
        [0.2598, 0.2772, 0.2253, 0.2377]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2720, 0.2441, 0.2294],
        [0.2586, 0.2712, 0.2286, 0.2416],
        [0.2570, 0.2740, 0.2387, 0.2303]], device='cuda:1',
       grad_fn=<SoftmaxBackward0>)
#####################
05/27 06:54:08PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][50/468]	Step 1926	lr 0.02462	Loss 0.0226 (0.0430)	Prec@(1,5) (98.5%, 100.0%)	
05/27 06:54:28PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][100/468]	Step 1976	lr 0.02462	Loss 0.0337 (0.0432)	Prec@(1,5) (98.6%, 100.0%)	
05/27 06:54:47PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][150/468]	Step 2026	lr 0.02462	Loss 0.0678 (0.0431)	Prec@(1,5) (98.7%, 100.0%)	
05/27 06:55:05PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][200/468]	Step 2076	lr 0.02462	Loss 0.0013 (0.0428)	Prec@(1,5) (98.7%, 100.0%)	
05/27 06:55:27PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][250/468]	Step 2126	lr 0.02462	Loss 0.0333 (0.0423)	Prec@(1,5) (98.8%, 100.0%)	
05/27 06:55:49PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][300/468]	Step 2176	lr 0.02462	Loss 0.1377 (0.0450)	Prec@(1,5) (98.7%, 100.0%)	
05/27 06:56:11PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][350/468]	Step 2226	lr 0.02462	Loss 0.0994 (0.0471)	Prec@(1,5) (98.6%, 100.0%)	
05/27 06:56:34PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][400/468]	Step 2276	lr 0.02462	Loss 0.0552 (0.0483)	Prec@(1,5) (98.6%, 100.0%)	
05/27 06:56:55PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][450/468]	Step 2326	lr 0.02462	Loss 0.1073 (0.0489)	Prec@(1,5) (98.6%, 100.0%)	
05/27 06:57:02PM searchShareStage_trainer.py:124 [INFO] Train: Epoch: [4][468/468]	Step 2344	lr 0.02462	Loss 0.0119 (0.0485)	Prec@(1,5) (98.6%, 100.0%)	
05/27 06:57:02PM searchShareStage_trainer.py:134 [INFO] Train: [  4/49] Final Prec@1 98.5633%
05/27 06:57:06PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][50/469]	Step 2345	Loss 0.0404	Prec@(1,5) (98.6%, 100.0%)
05/27 06:57:09PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][100/469]	Step 2345	Loss 0.0419	Prec@(1,5) (98.7%, 100.0%)
05/27 06:57:12PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][150/469]	Step 2345	Loss 0.0415	Prec@(1,5) (98.8%, 100.0%)
05/27 06:57:14PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][200/469]	Step 2345	Loss 0.0402	Prec@(1,5) (98.9%, 100.0%)
05/27 06:57:17PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][250/469]	Step 2345	Loss 0.0381	Prec@(1,5) (98.9%, 100.0%)
05/27 06:57:20PM searchStage_trainer.py:249 [INFO] Valid: Epoch: [4][300/469]	Step 2345	Loss 0.0375	Prec@(1,5) (98.9%, 100.0%)
