07/04 03:10:17PM parser.py:28 [INFO] 
07/04 03:10:17PM parser.py:29 [INFO] Parameters:
07/04 03:10:17PM parser.py:31 [INFO] DAG_PATH=results/search_Stage/CIFAR100/BASELINE/BASELINE1-20240704-151017/DAG
07/04 03:10:17PM parser.py:31 [INFO] ALPHA_LR=0.0003
07/04 03:10:17PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
07/04 03:10:17PM parser.py:31 [INFO] BATCH_SIZE=64
07/04 03:10:17PM parser.py:31 [INFO] CHECKPOINT_RESET=False
07/04 03:10:17PM parser.py:31 [INFO] DATA_PATH=../data/
07/04 03:10:17PM parser.py:31 [INFO] DATASET=CIFAR100
07/04 03:10:17PM parser.py:31 [INFO] EPOCHS=50
07/04 03:10:17PM parser.py:31 [INFO] EXP_NAME=BASELINE1-20240704-151017
07/04 03:10:17PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('skip_connect', 1), ('avg_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 3)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('sep_conv_3x3', 2)], [('skip_connect', 0), ('skip_connect', 1)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('skip_connect', 2)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
07/04 03:10:17PM parser.py:31 [INFO] GPUS=[0]
07/04 03:10:17PM parser.py:31 [INFO] INIT_CHANNELS=16
07/04 03:10:17PM parser.py:31 [INFO] LAYERS=20
07/04 03:10:17PM parser.py:31 [INFO] LOCAL_RANK=0
07/04 03:10:17PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
07/04 03:10:17PM parser.py:31 [INFO] NAME=BASELINE
07/04 03:10:17PM parser.py:31 [INFO] PATH=results/search_Stage/CIFAR100/BASELINE/BASELINE1-20240704-151017
07/04 03:10:17PM parser.py:31 [INFO] PLOT_PATH=results/search_Stage/CIFAR100/BASELINE/BASELINE1-20240704-151017/plots
07/04 03:10:17PM parser.py:31 [INFO] PRINT_FREQ=50
07/04 03:10:17PM parser.py:31 [INFO] RESUME_PATH=None
07/04 03:10:17PM parser.py:31 [INFO] SAVE=BASELINE1
07/04 03:10:17PM parser.py:31 [INFO] SEED=2
07/04 03:10:17PM parser.py:31 [INFO] SHARE_STAGE=False
07/04 03:10:17PM parser.py:31 [INFO] SPEC_CELL=True
07/04 03:10:17PM parser.py:31 [INFO] TRAIN_PORTION=0.5
07/04 03:10:17PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
07/04 03:10:17PM parser.py:31 [INFO] W_LR=0.025
07/04 03:10:17PM parser.py:31 [INFO] W_LR_MIN=0.001
07/04 03:10:17PM parser.py:31 [INFO] W_MOMENTUM=0.9
07/04 03:10:17PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
07/04 03:10:17PM parser.py:31 [INFO] WORKERS=4
07/04 03:10:17PM parser.py:32 [INFO] 
07/04 03:10:19PM searchStage_trainer.py:103 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2500, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2500, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2497, 0.2503, 0.2501],
        [0.2499, 0.2499, 0.2502, 0.2500],
        [0.2502, 0.2499, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2498, 0.2503, 0.2498],
        [0.2499, 0.2501, 0.2501, 0.2500],
        [0.2500, 0.2501, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2501, 0.2499],
        [0.2497, 0.2504, 0.2499, 0.2501],
        [0.2501, 0.2501, 0.2501, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2501, 0.2499, 0.2502],
        [0.2502, 0.2499, 0.2499, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2496, 0.2502],
        [0.2504, 0.2499, 0.2499, 0.2498],
        [0.2501, 0.2501, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2500, 0.2501, 0.2503],
        [0.2501, 0.2498, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2497, 0.2503, 0.2502],
        [0.2497, 0.2501, 0.2500, 0.2502],
        [0.2504, 0.2500, 0.2498, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2500, 0.2502],
        [0.2500, 0.2503, 0.2497, 0.2499],
        [0.2497, 0.2498, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2503, 0.2498, 0.2500],
        [0.2497, 0.2501, 0.2501, 0.2501],
        [0.2498, 0.2499, 0.2500, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2502, 0.2501],
        [0.2503, 0.2500, 0.2499, 0.2498],
        [0.2502, 0.2501, 0.2497, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2505, 0.2499],
        [0.2497, 0.2500, 0.2501, 0.2501],
        [0.2501, 0.2503, 0.2496, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2497, 0.2502, 0.2501],
        [0.2497, 0.2500, 0.2502, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2498, 0.2502, 0.2502],
        [0.2500, 0.2500, 0.2498, 0.2502],
        [0.2496, 0.2503, 0.2503, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2502, 0.2498],
        [0.2497, 0.2498, 0.2504, 0.2501],
        [0.2501, 0.2500, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2503, 0.2497, 0.2498],
        [0.2501, 0.2495, 0.2503, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2501, 0.2499, 0.2498],
        [0.2502, 0.2500, 0.2496, 0.2502],
        [0.2498, 0.2502, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2499, 0.2502],
        [0.2499, 0.2504, 0.2497, 0.2500],
        [0.2497, 0.2500, 0.2501, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:10:43PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 4.8906 (4.7071)	Architecture Loss 4.8576 (4.7461)	Prec@(1,5) (1.7%, 8.2%)	
07/04 03:11:04PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.4958 (4.6175)	Architecture Loss 4.3742 (4.6367)	Prec@(1,5) (2.5%, 10.5%)	
07/04 03:11:25PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 4.0657 (4.5157)	Architecture Loss 4.2211 (4.5247)	Prec@(1,5) (2.8%, 12.7%)	
07/04 03:11:47PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.0313 (4.4409)	Architecture Loss 3.9577 (4.4410)	Prec@(1,5) (3.6%, 14.6%)	
07/04 03:12:09PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 4.1483 (4.3715)	Architecture Loss 4.2378 (4.3748)	Prec@(1,5) (4.1%, 16.6%)	
07/04 03:12:30PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 3.6936 (4.3162)	Architecture Loss 3.9878 (4.3235)	Prec@(1,5) (4.5%, 18.0%)	
07/04 03:12:52PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 4.0349 (4.2757)	Architecture Loss 3.7954 (4.2743)	Prec@(1,5) (4.9%, 19.1%)	
07/04 03:13:08PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 4.0663 (4.2476)	Architecture Loss 3.8807 (4.2437)	Prec@(1,5) (5.2%, 19.9%)	
07/04 03:13:09PM searchStage_trainer.py:225 [INFO] Train: [  0/49] Final Prec@1 5.1960%
07/04 03:13:12PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 3.9260	Prec@(1,5) (8.2%, 28.1%)
07/04 03:13:15PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 3.9304	Prec@(1,5) (8.3%, 28.2%)
07/04 03:13:18PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 3.9305	Prec@(1,5) (8.0%, 28.3%)
07/04 03:13:21PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 3.9433	Prec@(1,5) (7.9%, 28.0%)
07/04 03:13:23PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 3.9427	Prec@(1,5) (7.9%, 28.1%)
07/04 03:13:26PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 3.9445	Prec@(1,5) (7.8%, 28.0%)
07/04 03:13:29PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 3.9454	Prec@(1,5) (7.9%, 28.0%)
07/04 03:13:31PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 3.9474	Prec@(1,5) (7.9%, 28.0%)
07/04 03:13:31PM searchStage_trainer.py:264 [INFO] Valid: [  0/49] Final Prec@1 7.9160%
07/04 03:13:31PM searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 3), ('max_pool_3x3', 2)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('skip_connect', 4), ('avg_pool_3x3', 6)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 3)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:13:32PM searchStage_main.py:82 [INFO] Until now, best Prec@1 = 7.9160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2501, 0.2501, 0.2503, 0.2495],
        [0.2497, 0.2498, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2507, 0.2512, 0.2483],
        [0.2503, 0.2506, 0.2499, 0.2492],
        [0.2505, 0.2511, 0.2479, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2503, 0.2507, 0.2483],
        [0.2502, 0.2514, 0.2495, 0.2489],
        [0.2496, 0.2507, 0.2499, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2506, 0.2497, 0.2490],
        [0.2501, 0.2511, 0.2492, 0.2496],
        [0.2502, 0.2508, 0.2494, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2507, 0.2494, 0.2493],
        [0.2501, 0.2502, 0.2498, 0.2499],
        [0.2500, 0.2503, 0.2498, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2504, 0.2495, 0.2493],
        [0.2506, 0.2500, 0.2497, 0.2497],
        [0.2504, 0.2503, 0.2494, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2499, 0.2504, 0.2502],
        [0.2499, 0.2496, 0.2506, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2496, 0.2504, 0.2499],
        [0.2498, 0.2499, 0.2501, 0.2502],
        [0.2504, 0.2500, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2501, 0.2501, 0.2501],
        [0.2502, 0.2502, 0.2499, 0.2498],
        [0.2497, 0.2498, 0.2504, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2502, 0.2499, 0.2500],
        [0.2497, 0.2500, 0.2501, 0.2502],
        [0.2498, 0.2500, 0.2502, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2499, 0.2504, 0.2498],
        [0.2501, 0.2501, 0.2501, 0.2497],
        [0.2500, 0.2501, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2507, 0.2497],
        [0.2498, 0.2500, 0.2501, 0.2500],
        [0.2500, 0.2504, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2495, 0.2505, 0.2500],
        [0.2496, 0.2498, 0.2505, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2496, 0.2507, 0.2500],
        [0.2498, 0.2499, 0.2502, 0.2501],
        [0.2496, 0.2501, 0.2502, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2498, 0.2504, 0.2496],
        [0.2495, 0.2498, 0.2505, 0.2502],
        [0.2499, 0.2500, 0.2502, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2501, 0.2497],
        [0.2500, 0.2494, 0.2507, 0.2500],
        [0.2499, 0.2500, 0.2501, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2502, 0.2504, 0.2493],
        [0.2499, 0.2499, 0.2500, 0.2502],
        [0.2496, 0.2499, 0.2501, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2502, 0.2501],
        [0.2498, 0.2503, 0.2497, 0.2502],
        [0.2494, 0.2500, 0.2506, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:13:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 3.9821 (3.9070)	Architecture Loss 4.1824 (3.9538)	Prec@(1,5) (8.5%, 30.1%)	
07/04 03:14:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 3.8951 (3.9164)	Architecture Loss 3.5814 (3.9444)	Prec@(1,5) (8.8%, 29.4%)	
07/04 03:14:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 3.7739 (3.9025)	Architecture Loss 3.8409 (3.9221)	Prec@(1,5) (9.1%, 29.9%)	
07/04 03:15:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 3.4870 (3.8905)	Architecture Loss 3.5482 (3.8912)	Prec@(1,5) (9.2%, 30.1%)	
07/04 03:15:20午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 3.9298 (3.8692)	Architecture Loss 3.9447 (3.8686)	Prec@(1,5) (9.6%, 30.8%)	
07/04 03:15:41午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.6255 (3.8591)	Architecture Loss 3.4859 (3.8550)	Prec@(1,5) (9.6%, 30.9%)	
07/04 03:16:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 3.9987 (3.8413)	Architecture Loss 3.9095 (3.8350)	Prec@(1,5) (9.8%, 31.2%)	
07/04 03:16:20午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.8584 (3.8313)	Architecture Loss 3.8098 (3.8206)	Prec@(1,5) (10.0%, 31.5%)	
07/04 03:16:21午後 searchStage_trainer.py:225 [INFO] Train: [  1/49] Final Prec@1 9.9520%
07/04 03:16:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 3.7346	Prec@(1,5) (11.7%, 34.6%)
07/04 03:16:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.7455	Prec@(1,5) (11.4%, 34.2%)
07/04 03:16:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 3.7404	Prec@(1,5) (11.2%, 34.3%)
07/04 03:16:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.7281	Prec@(1,5) (11.6%, 34.7%)
07/04 03:16:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 3.7285	Prec@(1,5) (11.6%, 34.8%)
07/04 03:16:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.7282	Prec@(1,5) (11.5%, 34.6%)
07/04 03:16:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 3.7270	Prec@(1,5) (11.5%, 34.7%)
07/04 03:16:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.7277	Prec@(1,5) (11.5%, 34.7%)
07/04 03:16:43午後 searchStage_trainer.py:264 [INFO] Valid: [  1/49] Final Prec@1 11.5200%
07/04 03:16:43午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('max_pool_3x3', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 3), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:16:44午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 11.5200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2501, 0.2499, 0.2504, 0.2495],
        [0.2497, 0.2496, 0.2504, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2507, 0.2514, 0.2479],
        [0.2505, 0.2506, 0.2501, 0.2489],
        [0.2505, 0.2512, 0.2476, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2502, 0.2516, 0.2474],
        [0.2500, 0.2514, 0.2493, 0.2492],
        [0.2497, 0.2508, 0.2499, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2497, 0.2489],
        [0.2503, 0.2511, 0.2491, 0.2495],
        [0.2503, 0.2509, 0.2492, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2508, 0.2496, 0.2490],
        [0.2501, 0.2503, 0.2500, 0.2496],
        [0.2499, 0.2503, 0.2497, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2503, 0.2497, 0.2486],
        [0.2505, 0.2499, 0.2496, 0.2500],
        [0.2503, 0.2503, 0.2493, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2498, 0.2506, 0.2502],
        [0.2498, 0.2495, 0.2508, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2494, 0.2506, 0.2497],
        [0.2499, 0.2497, 0.2503, 0.2502],
        [0.2504, 0.2499, 0.2497, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2499, 0.2503, 0.2501],
        [0.2502, 0.2501, 0.2499, 0.2499],
        [0.2498, 0.2497, 0.2505, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2500, 0.2500],
        [0.2496, 0.2499, 0.2502, 0.2502],
        [0.2497, 0.2499, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2497, 0.2508, 0.2496],
        [0.2502, 0.2499, 0.2502, 0.2497],
        [0.2498, 0.2500, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2498, 0.2508, 0.2496],
        [0.2498, 0.2499, 0.2502, 0.2501],
        [0.2500, 0.2503, 0.2497, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2491, 0.2506, 0.2500],
        [0.2499, 0.2494, 0.2506, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2491, 0.2508, 0.2499],
        [0.2501, 0.2494, 0.2502, 0.2503],
        [0.2500, 0.2499, 0.2502, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2494, 0.2504, 0.2495],
        [0.2498, 0.2495, 0.2504, 0.2503],
        [0.2502, 0.2497, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2500, 0.2500, 0.2496],
        [0.2503, 0.2492, 0.2505, 0.2500],
        [0.2501, 0.2499, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2503, 0.2504, 0.2487],
        [0.2502, 0.2498, 0.2497, 0.2503],
        [0.2499, 0.2498, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2498, 0.2500, 0.2499],
        [0.2502, 0.2502, 0.2493, 0.2503],
        [0.2496, 0.2500, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:17:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 3.4369 (3.6788)	Architecture Loss 3.5323 (3.6373)	Prec@(1,5) (11.5%, 36.4%)	
07/04 03:17:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.6364 (3.6605)	Architecture Loss 3.5834 (3.6292)	Prec@(1,5) (12.2%, 36.5%)	
07/04 03:17:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 3.4731 (3.6573)	Architecture Loss 3.2655 (3.6345)	Prec@(1,5) (12.4%, 36.8%)	
07/04 03:18:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.4788 (3.6478)	Architecture Loss 3.5769 (3.6227)	Prec@(1,5) (12.6%, 37.2%)	
07/04 03:18:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 3.9343 (3.6412)	Architecture Loss 3.5264 (3.6179)	Prec@(1,5) (12.8%, 37.4%)	
07/04 03:18:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.5770 (3.6295)	Architecture Loss 3.5646 (3.6060)	Prec@(1,5) (13.1%, 37.8%)	
07/04 03:19:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 3.6461 (3.6156)	Architecture Loss 3.7604 (3.5993)	Prec@(1,5) (13.3%, 38.2%)	
07/04 03:19:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.2654 (3.6095)	Architecture Loss 3.4684 (3.5937)	Prec@(1,5) (13.4%, 38.4%)	
07/04 03:19:33午後 searchStage_trainer.py:225 [INFO] Train: [  2/49] Final Prec@1 13.4400%
07/04 03:19:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 3.4645	Prec@(1,5) (16.0%, 43.3%)
07/04 03:19:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.4843	Prec@(1,5) (15.6%, 42.3%)
07/04 03:19:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 3.4907	Prec@(1,5) (15.5%, 42.5%)
07/04 03:19:45午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.4931	Prec@(1,5) (15.3%, 42.3%)
07/04 03:19:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 3.4913	Prec@(1,5) (15.3%, 42.2%)
07/04 03:19:50午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.4981	Prec@(1,5) (15.1%, 41.9%)
07/04 03:19:53午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 3.4980	Prec@(1,5) (15.1%, 41.9%)
07/04 03:19:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.5000	Prec@(1,5) (15.0%, 41.9%)
07/04 03:19:56午後 searchStage_trainer.py:264 [INFO] Valid: [  2/49] Final Prec@1 15.0000%
07/04 03:19:56午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 5), ('skip_connect', 6)]], DAG3_concat=range(6, 8))
07/04 03:19:56午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 15.0000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2498, 0.2505, 0.2495],
        [0.2497, 0.2495, 0.2505, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2506, 0.2516, 0.2477],
        [0.2506, 0.2504, 0.2502, 0.2488],
        [0.2504, 0.2512, 0.2475, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2498, 0.2528, 0.2466],
        [0.2500, 0.2514, 0.2492, 0.2494],
        [0.2496, 0.2508, 0.2497, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2506, 0.2498, 0.2488],
        [0.2503, 0.2511, 0.2491, 0.2496],
        [0.2503, 0.2508, 0.2493, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2508, 0.2499, 0.2487],
        [0.2500, 0.2504, 0.2501, 0.2495],
        [0.2496, 0.2504, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2502, 0.2498, 0.2481],
        [0.2506, 0.2499, 0.2495, 0.2500],
        [0.2502, 0.2502, 0.2492, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2496, 0.2508, 0.2502],
        [0.2498, 0.2493, 0.2509, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2491, 0.2510, 0.2496],
        [0.2499, 0.2493, 0.2506, 0.2502],
        [0.2504, 0.2498, 0.2497, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2496, 0.2506, 0.2499],
        [0.2502, 0.2499, 0.2500, 0.2500],
        [0.2499, 0.2496, 0.2505, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2501, 0.2500],
        [0.2497, 0.2498, 0.2503, 0.2502],
        [0.2497, 0.2498, 0.2504, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2495, 0.2512, 0.2494],
        [0.2503, 0.2497, 0.2504, 0.2497],
        [0.2499, 0.2498, 0.2498, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2497, 0.2509, 0.2494],
        [0.2499, 0.2498, 0.2503, 0.2500],
        [0.2501, 0.2500, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2486, 0.2507, 0.2500],
        [0.2501, 0.2490, 0.2507, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2485, 0.2508, 0.2499],
        [0.2505, 0.2489, 0.2503, 0.2503],
        [0.2503, 0.2498, 0.2501, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2489, 0.2503, 0.2493],
        [0.2500, 0.2492, 0.2503, 0.2504],
        [0.2506, 0.2495, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2498, 0.2499, 0.2497],
        [0.2507, 0.2490, 0.2503, 0.2500],
        [0.2503, 0.2497, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2503, 0.2503, 0.2481],
        [0.2507, 0.2498, 0.2493, 0.2503],
        [0.2502, 0.2497, 0.2488, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2497, 0.2499, 0.2496],
        [0.2507, 0.2501, 0.2489, 0.2503],
        [0.2498, 0.2499, 0.2500, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:20:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 3.6808 (3.4907)	Architecture Loss 3.5895 (3.4558)	Prec@(1,5) (14.5%, 41.4%)	
07/04 03:20:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.5420 (3.4648)	Architecture Loss 3.5908 (3.4821)	Prec@(1,5) (15.6%, 42.2%)	
07/04 03:21:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 3.3595 (3.4485)	Architecture Loss 3.5336 (3.4667)	Prec@(1,5) (16.2%, 42.9%)	
07/04 03:21:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.4321 (3.4334)	Architecture Loss 3.0641 (3.4702)	Prec@(1,5) (16.4%, 43.5%)	
07/04 03:21:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 3.6976 (3.4235)	Architecture Loss 3.1984 (3.4576)	Prec@(1,5) (16.6%, 43.7%)	
07/04 03:22:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 3.1983 (3.4184)	Architecture Loss 3.4584 (3.4464)	Prec@(1,5) (16.6%, 43.9%)	
07/04 03:22:26午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 3.3337 (3.4073)	Architecture Loss 3.3871 (3.4254)	Prec@(1,5) (17.0%, 44.3%)	
07/04 03:22:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 3.2500 (3.4008)	Architecture Loss 3.1647 (3.4141)	Prec@(1,5) (17.1%, 44.5%)	
07/04 03:22:43午後 searchStage_trainer.py:225 [INFO] Train: [  3/49] Final Prec@1 17.0600%
07/04 03:22:46午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 3.3300	Prec@(1,5) (19.7%, 46.6%)
07/04 03:22:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.3323	Prec@(1,5) (19.7%, 46.8%)
07/04 03:22:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 3.3219	Prec@(1,5) (19.7%, 46.9%)
07/04 03:22:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.3275	Prec@(1,5) (19.6%, 46.9%)
07/04 03:22:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 3.3281	Prec@(1,5) (19.5%, 46.8%)
07/04 03:23:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.3220	Prec@(1,5) (19.6%, 46.9%)
07/04 03:23:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 3.3228	Prec@(1,5) (19.4%, 46.8%)
07/04 03:23:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.3171	Prec@(1,5) (19.4%, 47.0%)
07/04 03:23:06午後 searchStage_trainer.py:264 [INFO] Valid: [  3/49] Final Prec@1 19.3960%
07/04 03:23:06午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('max_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:23:06午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 19.3960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2501, 0.2497, 0.2506, 0.2496],
        [0.2497, 0.2494, 0.2506, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2505, 0.2519, 0.2475],
        [0.2505, 0.2503, 0.2504, 0.2488],
        [0.2502, 0.2512, 0.2476, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2492, 0.2550, 0.2454],
        [0.2498, 0.2513, 0.2492, 0.2497],
        [0.2494, 0.2508, 0.2496, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2506, 0.2499, 0.2488],
        [0.2502, 0.2511, 0.2492, 0.2495],
        [0.2502, 0.2508, 0.2494, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2509, 0.2505, 0.2483],
        [0.2497, 0.2504, 0.2501, 0.2498],
        [0.2494, 0.2505, 0.2498, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2500, 0.2500, 0.2477],
        [0.2505, 0.2499, 0.2496, 0.2499],
        [0.2499, 0.2502, 0.2491, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2495, 0.2509, 0.2502],
        [0.2497, 0.2492, 0.2511, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2488, 0.2512, 0.2495],
        [0.2499, 0.2491, 0.2509, 0.2500],
        [0.2504, 0.2497, 0.2497, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2495, 0.2510, 0.2495],
        [0.2501, 0.2497, 0.2500, 0.2501],
        [0.2498, 0.2495, 0.2506, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2500, 0.2503, 0.2498],
        [0.2496, 0.2497, 0.2504, 0.2503],
        [0.2497, 0.2498, 0.2504, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2494, 0.2516, 0.2492],
        [0.2501, 0.2495, 0.2507, 0.2497],
        [0.2498, 0.2496, 0.2498, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2496, 0.2513, 0.2490],
        [0.2499, 0.2497, 0.2503, 0.2501],
        [0.2501, 0.2499, 0.2496, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2483, 0.2510, 0.2501],
        [0.2502, 0.2487, 0.2510, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2480, 0.2511, 0.2499],
        [0.2508, 0.2483, 0.2506, 0.2503],
        [0.2504, 0.2496, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2484, 0.2505, 0.2491],
        [0.2502, 0.2490, 0.2504, 0.2504],
        [0.2507, 0.2493, 0.2500, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2495, 0.2499, 0.2496],
        [0.2510, 0.2487, 0.2503, 0.2500],
        [0.2505, 0.2494, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2500, 0.2504, 0.2476],
        [0.2512, 0.2494, 0.2491, 0.2503],
        [0.2505, 0.2493, 0.2486, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2494, 0.2498, 0.2494],
        [0.2510, 0.2498, 0.2488, 0.2503],
        [0.2499, 0.2496, 0.2499, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:23:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 3.6280 (3.2376)	Architecture Loss 3.4302 (3.2672)	Prec@(1,5) (20.4%, 48.5%)	
07/04 03:23:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 3.3275 (3.2554)	Architecture Loss 3.3076 (3.2788)	Prec@(1,5) (20.0%, 48.2%)	
07/04 03:24:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 3.2638 (3.2628)	Architecture Loss 3.1622 (3.2800)	Prec@(1,5) (19.6%, 48.0%)	
07/04 03:24:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 3.1509 (3.2620)	Architecture Loss 3.2895 (3.2754)	Prec@(1,5) (19.5%, 48.0%)	
07/04 03:24:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 3.3208 (3.2571)	Architecture Loss 3.6589 (3.2690)	Prec@(1,5) (19.5%, 48.3%)	
07/04 03:25:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 3.2848 (3.2500)	Architecture Loss 3.3012 (3.2624)	Prec@(1,5) (19.8%, 48.6%)	
07/04 03:25:37午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 3.1577 (3.2384)	Architecture Loss 3.1849 (3.2600)	Prec@(1,5) (20.1%, 48.9%)	
07/04 03:25:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 2.9272 (3.2267)	Architecture Loss 3.7959 (3.2527)	Prec@(1,5) (20.3%, 49.2%)	
07/04 03:25:55午後 searchStage_trainer.py:225 [INFO] Train: [  4/49] Final Prec@1 20.3000%
07/04 03:25:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 3.1547	Prec@(1,5) (22.5%, 51.6%)
07/04 03:26:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.1554	Prec@(1,5) (22.4%, 52.0%)
07/04 03:26:04午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 3.1286	Prec@(1,5) (23.1%, 52.5%)
07/04 03:26:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.1338	Prec@(1,5) (22.9%, 52.5%)
07/04 03:26:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 3.1371	Prec@(1,5) (23.1%, 52.2%)
07/04 03:26:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.1397	Prec@(1,5) (22.9%, 52.1%)
07/04 03:26:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 3.1414	Prec@(1,5) (22.8%, 52.1%)
07/04 03:26:16午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.1427	Prec@(1,5) (22.7%, 52.0%)
07/04 03:26:16午後 searchStage_trainer.py:264 [INFO] Valid: [  4/49] Final Prec@1 22.7320%
07/04 03:26:17午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('max_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:26:17午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 22.7320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2500, 0.2496, 0.2507, 0.2496],
        [0.2496, 0.2493, 0.2507, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2504, 0.2525, 0.2472],
        [0.2502, 0.2502, 0.2509, 0.2487],
        [0.2501, 0.2511, 0.2476, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2483, 0.2577, 0.2446],
        [0.2497, 0.2513, 0.2492, 0.2498],
        [0.2492, 0.2507, 0.2495, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2506, 0.2500, 0.2488],
        [0.2502, 0.2510, 0.2492, 0.2496],
        [0.2501, 0.2508, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2509, 0.2482],
        [0.2497, 0.2503, 0.2503, 0.2497],
        [0.2492, 0.2504, 0.2498, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2498, 0.2502, 0.2473],
        [0.2505, 0.2498, 0.2497, 0.2501],
        [0.2497, 0.2501, 0.2491, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2493, 0.2510, 0.2502],
        [0.2497, 0.2491, 0.2513, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2486, 0.2515, 0.2495],
        [0.2499, 0.2489, 0.2512, 0.2500],
        [0.2504, 0.2496, 0.2496, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2492, 0.2514, 0.2494],
        [0.2501, 0.2496, 0.2502, 0.2501],
        [0.2498, 0.2494, 0.2505, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2504, 0.2498],
        [0.2496, 0.2496, 0.2505, 0.2503],
        [0.2496, 0.2497, 0.2505, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2492, 0.2521, 0.2489],
        [0.2501, 0.2493, 0.2507, 0.2499],
        [0.2498, 0.2495, 0.2499, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2494, 0.2516, 0.2489],
        [0.2499, 0.2495, 0.2504, 0.2501],
        [0.2501, 0.2497, 0.2496, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2480, 0.2512, 0.2501],
        [0.2503, 0.2484, 0.2512, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2473, 0.2514, 0.2501],
        [0.2511, 0.2478, 0.2508, 0.2502],
        [0.2505, 0.2495, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2480, 0.2507, 0.2488],
        [0.2503, 0.2488, 0.2504, 0.2505],
        [0.2508, 0.2491, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2494, 0.2500, 0.2495],
        [0.2512, 0.2485, 0.2502, 0.2501],
        [0.2506, 0.2493, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2498, 0.2506, 0.2471],
        [0.2517, 0.2490, 0.2491, 0.2503],
        [0.2506, 0.2490, 0.2483, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2492, 0.2498, 0.2492],
        [0.2513, 0.2497, 0.2487, 0.2504],
        [0.2500, 0.2494, 0.2499, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:26:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 3.2396 (3.1086)	Architecture Loss 3.2839 (3.1838)	Prec@(1,5) (21.8%, 51.8%)	
07/04 03:27:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.3284 (3.0932)	Architecture Loss 2.9257 (3.1579)	Prec@(1,5) (22.2%, 52.6%)	
07/04 03:27:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 3.2083 (3.0760)	Architecture Loss 3.4660 (3.1633)	Prec@(1,5) (22.5%, 53.0%)	
07/04 03:27:44午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 3.1433 (3.0876)	Architecture Loss 3.0848 (3.1585)	Prec@(1,5) (22.4%, 52.5%)	
07/04 03:28:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 2.7819 (3.0774)	Architecture Loss 3.2970 (3.1338)	Prec@(1,5) (22.7%, 52.9%)	
07/04 03:28:26午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 2.9400 (3.0724)	Architecture Loss 2.9326 (3.1269)	Prec@(1,5) (22.7%, 53.1%)	
07/04 03:28:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 3.2921 (3.0716)	Architecture Loss 2.8390 (3.1191)	Prec@(1,5) (22.8%, 53.1%)	
07/04 03:29:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 3.0025 (3.0604)	Architecture Loss 2.6205 (3.1026)	Prec@(1,5) (23.0%, 53.4%)	
07/04 03:29:06午後 searchStage_trainer.py:225 [INFO] Train: [  5/49] Final Prec@1 23.0320%
07/04 03:29:09午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 3.0701	Prec@(1,5) (23.1%, 54.7%)
07/04 03:29:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 3.0618	Prec@(1,5) (23.6%, 54.6%)
07/04 03:29:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 3.0597	Prec@(1,5) (23.8%, 54.6%)
07/04 03:29:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 3.0540	Prec@(1,5) (24.0%, 54.6%)
07/04 03:29:20午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 3.0554	Prec@(1,5) (24.1%, 54.3%)
07/04 03:29:23午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 3.0633	Prec@(1,5) (24.0%, 54.1%)
07/04 03:29:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 3.0689	Prec@(1,5) (23.9%, 54.0%)
07/04 03:29:29午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 3.0759	Prec@(1,5) (23.8%, 53.9%)
07/04 03:29:29午後 searchStage_trainer.py:264 [INFO] Valid: [  5/49] Final Prec@1 23.7560%
07/04 03:29:29午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:29:29午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 23.7560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2499, 0.2496, 0.2509, 0.2496],
        [0.2495, 0.2493, 0.2508, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2503, 0.2531, 0.2471],
        [0.2498, 0.2500, 0.2514, 0.2488],
        [0.2499, 0.2511, 0.2477, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2473, 0.2609, 0.2436],
        [0.2495, 0.2513, 0.2493, 0.2499],
        [0.2489, 0.2506, 0.2494, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2505, 0.2500, 0.2488],
        [0.2501, 0.2509, 0.2493, 0.2497],
        [0.2500, 0.2507, 0.2496, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2507, 0.2515, 0.2480],
        [0.2494, 0.2503, 0.2506, 0.2498],
        [0.2489, 0.2504, 0.2499, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2497, 0.2508, 0.2467],
        [0.2502, 0.2498, 0.2497, 0.2502],
        [0.2494, 0.2501, 0.2491, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2492, 0.2511, 0.2503],
        [0.2497, 0.2490, 0.2514, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2484, 0.2518, 0.2495],
        [0.2498, 0.2487, 0.2515, 0.2500],
        [0.2504, 0.2496, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2491, 0.2517, 0.2491],
        [0.2501, 0.2496, 0.2502, 0.2501],
        [0.2498, 0.2493, 0.2505, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2499, 0.2506, 0.2498],
        [0.2495, 0.2496, 0.2506, 0.2504],
        [0.2496, 0.2497, 0.2506, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2491, 0.2526, 0.2487],
        [0.2500, 0.2491, 0.2508, 0.2500],
        [0.2497, 0.2494, 0.2500, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2493, 0.2518, 0.2488],
        [0.2500, 0.2494, 0.2505, 0.2500],
        [0.2501, 0.2496, 0.2496, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2478, 0.2515, 0.2501],
        [0.2502, 0.2482, 0.2515, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2468, 0.2517, 0.2501],
        [0.2512, 0.2474, 0.2512, 0.2503],
        [0.2506, 0.2494, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2476, 0.2511, 0.2484],
        [0.2503, 0.2487, 0.2504, 0.2506],
        [0.2509, 0.2490, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2492, 0.2501, 0.2494],
        [0.2513, 0.2482, 0.2503, 0.2502],
        [0.2506, 0.2491, 0.2502, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2496, 0.2507, 0.2467],
        [0.2520, 0.2487, 0.2492, 0.2502],
        [0.2506, 0.2487, 0.2482, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2489, 0.2499, 0.2491],
        [0.2515, 0.2495, 0.2488, 0.2503],
        [0.2500, 0.2492, 0.2501, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:29:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 2.8202 (2.9051)	Architecture Loss 3.1365 (3.0008)	Prec@(1,5) (25.7%, 57.2%)	
07/04 03:30:13午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 3.0042 (2.9203)	Architecture Loss 2.5464 (2.9888)	Prec@(1,5) (25.3%, 57.1%)	
07/04 03:30:34午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 2.9159 (2.9345)	Architecture Loss 3.0416 (2.9869)	Prec@(1,5) (25.1%, 56.9%)	
07/04 03:30:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.7536 (2.9239)	Architecture Loss 3.2077 (2.9813)	Prec@(1,5) (25.3%, 57.1%)	
07/04 03:31:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 2.9650 (2.9174)	Architecture Loss 2.7763 (2.9735)	Prec@(1,5) (25.6%, 57.2%)	
07/04 03:31:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 2.8293 (2.9125)	Architecture Loss 2.6686 (2.9561)	Prec@(1,5) (25.8%, 57.4%)	
07/04 03:32:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 2.6997 (2.9045)	Architecture Loss 2.9684 (2.9528)	Prec@(1,5) (25.8%, 57.6%)	
07/04 03:32:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 2.6808 (2.9000)	Architecture Loss 3.1960 (2.9479)	Prec@(1,5) (25.8%, 57.8%)	
07/04 03:32:18午後 searchStage_trainer.py:225 [INFO] Train: [  6/49] Final Prec@1 25.8160%
07/04 03:32:21午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 3.0707	Prec@(1,5) (23.7%, 53.2%)
07/04 03:32:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 3.0720	Prec@(1,5) (24.0%, 53.2%)
07/04 03:32:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 3.0537	Prec@(1,5) (24.3%, 53.7%)
07/04 03:32:29午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 3.0509	Prec@(1,5) (24.3%, 53.8%)
07/04 03:32:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 3.0525	Prec@(1,5) (24.4%, 53.9%)
07/04 03:32:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 3.0553	Prec@(1,5) (24.2%, 53.8%)
07/04 03:32:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 3.0556	Prec@(1,5) (24.2%, 53.8%)
07/04 03:32:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 3.0519	Prec@(1,5) (24.4%, 53.9%)
07/04 03:32:39午後 searchStage_trainer.py:264 [INFO] Valid: [  6/49] Final Prec@1 24.3560%
07/04 03:32:39午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:32:40午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 24.3560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2499, 0.2495, 0.2510, 0.2496],
        [0.2494, 0.2493, 0.2509, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2502, 0.2537, 0.2469],
        [0.2495, 0.2499, 0.2518, 0.2488],
        [0.2498, 0.2511, 0.2478, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2454, 0.2664, 0.2425],
        [0.2493, 0.2513, 0.2493, 0.2501],
        [0.2486, 0.2506, 0.2494, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2505, 0.2501, 0.2487],
        [0.2500, 0.2509, 0.2494, 0.2497],
        [0.2499, 0.2507, 0.2496, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2521, 0.2478],
        [0.2494, 0.2501, 0.2508, 0.2498],
        [0.2487, 0.2504, 0.2500, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2496, 0.2514, 0.2460],
        [0.2501, 0.2498, 0.2498, 0.2503],
        [0.2491, 0.2500, 0.2491, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2491, 0.2513, 0.2503],
        [0.2496, 0.2489, 0.2516, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2484, 0.2520, 0.2494],
        [0.2496, 0.2487, 0.2518, 0.2499],
        [0.2503, 0.2496, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2491, 0.2521, 0.2488],
        [0.2501, 0.2495, 0.2503, 0.2502],
        [0.2498, 0.2492, 0.2506, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2498, 0.2506, 0.2498],
        [0.2494, 0.2496, 0.2506, 0.2504],
        [0.2496, 0.2496, 0.2507, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2490, 0.2530, 0.2484],
        [0.2499, 0.2491, 0.2509, 0.2501],
        [0.2497, 0.2493, 0.2500, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2492, 0.2520, 0.2487],
        [0.2501, 0.2493, 0.2505, 0.2500],
        [0.2501, 0.2495, 0.2496, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2476, 0.2516, 0.2501],
        [0.2502, 0.2481, 0.2517, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2464, 0.2521, 0.2502],
        [0.2512, 0.2470, 0.2515, 0.2503],
        [0.2506, 0.2493, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2533, 0.2472, 0.2514, 0.2481],
        [0.2503, 0.2485, 0.2504, 0.2507],
        [0.2509, 0.2489, 0.2500, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2491, 0.2502, 0.2494],
        [0.2513, 0.2480, 0.2503, 0.2504],
        [0.2507, 0.2491, 0.2502, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2493, 0.2508, 0.2465],
        [0.2521, 0.2484, 0.2493, 0.2502],
        [0.2507, 0.2486, 0.2482, 0.2525]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2488, 0.2501, 0.2489],
        [0.2516, 0.2493, 0.2488, 0.2503],
        [0.2500, 0.2490, 0.2502, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:33:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 2.5128 (2.7607)	Architecture Loss 2.7933 (2.8917)	Prec@(1,5) (28.1%, 61.1%)	
07/04 03:33:26午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.9649 (2.7717)	Architecture Loss 2.6714 (2.8576)	Prec@(1,5) (28.4%, 60.7%)	
07/04 03:33:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 2.5369 (2.7693)	Architecture Loss 3.1731 (2.8505)	Prec@(1,5) (28.8%, 60.7%)	
07/04 03:34:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.9209 (2.7718)	Architecture Loss 3.0495 (2.8437)	Prec@(1,5) (28.7%, 60.6%)	
07/04 03:34:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 2.8074 (2.7596)	Architecture Loss 3.0731 (2.8407)	Prec@(1,5) (29.2%, 60.9%)	
07/04 03:35:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 2.5355 (2.7556)	Architecture Loss 2.7844 (2.8384)	Prec@(1,5) (29.2%, 60.9%)	
07/04 03:35:41午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 2.4758 (2.7494)	Architecture Loss 2.7181 (2.8323)	Prec@(1,5) (29.4%, 61.2%)	
07/04 03:36:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.7839 (2.7500)	Architecture Loss 2.7479 (2.8206)	Prec@(1,5) (29.4%, 61.2%)	
07/04 03:36:03午後 searchStage_trainer.py:225 [INFO] Train: [  7/49] Final Prec@1 29.3920%
07/04 03:36:07午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 2.8148	Prec@(1,5) (28.5%, 59.4%)
07/04 03:36:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.8055	Prec@(1,5) (28.3%, 59.8%)
07/04 03:36:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 2.7840	Prec@(1,5) (28.5%, 60.7%)
07/04 03:36:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.7843	Prec@(1,5) (28.3%, 60.8%)
07/04 03:36:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 2.7703	Prec@(1,5) (28.7%, 61.1%)
07/04 03:36:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.7727	Prec@(1,5) (28.5%, 61.2%)
07/04 03:36:29午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 2.7718	Prec@(1,5) (28.7%, 61.2%)
07/04 03:36:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.7714	Prec@(1,5) (28.7%, 61.2%)
07/04 03:36:32午後 searchStage_trainer.py:264 [INFO] Valid: [  7/49] Final Prec@1 28.7240%
07/04 03:36:32午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('max_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:36:33午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 28.7240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2498, 0.2495, 0.2511, 0.2497],
        [0.2493, 0.2492, 0.2510, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2542, 0.2468],
        [0.2493, 0.2498, 0.2523, 0.2487],
        [0.2497, 0.2510, 0.2478, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2432, 0.2431, 0.2717, 0.2420],
        [0.2492, 0.2512, 0.2493, 0.2503],
        [0.2485, 0.2506, 0.2495, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2505, 0.2501, 0.2486],
        [0.2499, 0.2508, 0.2495, 0.2498],
        [0.2498, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2506, 0.2527, 0.2476],
        [0.2492, 0.2499, 0.2510, 0.2499],
        [0.2485, 0.2504, 0.2501, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2532, 0.2494, 0.2519, 0.2454],
        [0.2499, 0.2498, 0.2498, 0.2505],
        [0.2488, 0.2500, 0.2491, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2490, 0.2514, 0.2503],
        [0.2495, 0.2488, 0.2517, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2483, 0.2522, 0.2494],
        [0.2495, 0.2486, 0.2520, 0.2500],
        [0.2503, 0.2496, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2489, 0.2524, 0.2488],
        [0.2500, 0.2495, 0.2503, 0.2502],
        [0.2497, 0.2492, 0.2506, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2497, 0.2507, 0.2498],
        [0.2494, 0.2495, 0.2507, 0.2503],
        [0.2495, 0.2496, 0.2507, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2490, 0.2532, 0.2483],
        [0.2499, 0.2490, 0.2510, 0.2501],
        [0.2497, 0.2493, 0.2500, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2491, 0.2522, 0.2485],
        [0.2501, 0.2493, 0.2505, 0.2501],
        [0.2501, 0.2495, 0.2496, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2474, 0.2518, 0.2501],
        [0.2501, 0.2479, 0.2518, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2460, 0.2524, 0.2501],
        [0.2512, 0.2467, 0.2519, 0.2502],
        [0.2506, 0.2493, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2470, 0.2517, 0.2478],
        [0.2503, 0.2484, 0.2505, 0.2508],
        [0.2509, 0.2488, 0.2501, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2490, 0.2504, 0.2493],
        [0.2513, 0.2479, 0.2503, 0.2505],
        [0.2507, 0.2490, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2492, 0.2510, 0.2463],
        [0.2522, 0.2482, 0.2494, 0.2502],
        [0.2507, 0.2484, 0.2483, 0.2526]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2486, 0.2503, 0.2487],
        [0.2517, 0.2492, 0.2488, 0.2504],
        [0.2500, 0.2488, 0.2503, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:37:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 2.5611 (2.6310)	Architecture Loss 2.9610 (2.7728)	Prec@(1,5) (32.2%, 64.1%)	
07/04 03:37:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.2417 (2.6136)	Architecture Loss 2.7252 (2.7677)	Prec@(1,5) (32.0%, 64.2%)	
07/04 03:37:57午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 3.0106 (2.6243)	Architecture Loss 2.8813 (2.7476)	Prec@(1,5) (31.6%, 63.9%)	
07/04 03:38:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 2.6367 (2.6143)	Architecture Loss 3.0865 (2.7312)	Prec@(1,5) (31.8%, 64.4%)	
07/04 03:38:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 2.5346 (2.6105)	Architecture Loss 2.8396 (2.7273)	Prec@(1,5) (31.9%, 64.7%)	
07/04 03:39:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.9299 (2.6113)	Architecture Loss 2.7285 (2.7201)	Prec@(1,5) (32.1%, 64.6%)	
07/04 03:39:49午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 2.7029 (2.6137)	Architecture Loss 2.2524 (2.7046)	Prec@(1,5) (32.0%, 64.6%)	
07/04 03:40:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.2820 (2.6180)	Architecture Loss 2.6429 (2.7023)	Prec@(1,5) (32.0%, 64.5%)	
07/04 03:40:12午後 searchStage_trainer.py:225 [INFO] Train: [  8/49] Final Prec@1 32.0080%
07/04 03:40:16午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 2.7636	Prec@(1,5) (30.5%, 61.3%)
07/04 03:40:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.6964	Prec@(1,5) (31.2%, 63.0%)
07/04 03:40:23午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 2.6850	Prec@(1,5) (31.3%, 63.1%)
07/04 03:40:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.6960	Prec@(1,5) (31.0%, 63.0%)
07/04 03:40:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 2.6962	Prec@(1,5) (31.2%, 63.1%)
07/04 03:40:34午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.6895	Prec@(1,5) (31.5%, 63.2%)
07/04 03:40:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 2.6876	Prec@(1,5) (31.6%, 63.2%)
07/04 03:40:40午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.6854	Prec@(1,5) (31.6%, 63.3%)
07/04 03:40:41午後 searchStage_trainer.py:264 [INFO] Valid: [  8/49] Final Prec@1 31.5360%
07/04 03:40:41午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:40:41午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 31.5360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2497, 0.2495, 0.2512, 0.2497],
        [0.2493, 0.2492, 0.2511, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2498, 0.2547, 0.2468],
        [0.2491, 0.2496, 0.2527, 0.2486],
        [0.2497, 0.2510, 0.2478, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2406, 0.2411, 0.2770, 0.2412],
        [0.2490, 0.2512, 0.2493, 0.2504],
        [0.2483, 0.2506, 0.2494, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2502, 0.2487],
        [0.2498, 0.2508, 0.2495, 0.2498],
        [0.2498, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2505, 0.2533, 0.2474],
        [0.2491, 0.2498, 0.2512, 0.2499],
        [0.2484, 0.2504, 0.2501, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2494, 0.2523, 0.2450],
        [0.2498, 0.2498, 0.2498, 0.2506],
        [0.2487, 0.2500, 0.2491, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2490, 0.2515, 0.2503],
        [0.2495, 0.2488, 0.2518, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2483, 0.2524, 0.2493],
        [0.2493, 0.2485, 0.2522, 0.2499],
        [0.2503, 0.2495, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2488, 0.2528, 0.2487],
        [0.2500, 0.2495, 0.2503, 0.2502],
        [0.2497, 0.2492, 0.2506, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2497, 0.2508, 0.2498],
        [0.2494, 0.2495, 0.2508, 0.2504],
        [0.2495, 0.2496, 0.2507, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2489, 0.2535, 0.2481],
        [0.2498, 0.2490, 0.2511, 0.2501],
        [0.2496, 0.2492, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2491, 0.2524, 0.2484],
        [0.2502, 0.2492, 0.2505, 0.2501],
        [0.2501, 0.2494, 0.2496, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2473, 0.2520, 0.2502],
        [0.2501, 0.2478, 0.2520, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2456, 0.2526, 0.2501],
        [0.2513, 0.2464, 0.2521, 0.2502],
        [0.2506, 0.2492, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2466, 0.2519, 0.2476],
        [0.2503, 0.2483, 0.2505, 0.2509],
        [0.2509, 0.2487, 0.2501, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2489, 0.2505, 0.2492],
        [0.2513, 0.2478, 0.2504, 0.2505],
        [0.2508, 0.2489, 0.2503, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2490, 0.2511, 0.2462],
        [0.2523, 0.2480, 0.2496, 0.2501],
        [0.2507, 0.2483, 0.2483, 0.2527]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2484, 0.2505, 0.2486],
        [0.2517, 0.2490, 0.2488, 0.2505],
        [0.2500, 0.2487, 0.2504, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:41:09午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 2.3185 (2.4875)	Architecture Loss 2.6737 (2.6913)	Prec@(1,5) (35.8%, 66.9%)	
07/04 03:41:37午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.6447 (2.5070)	Architecture Loss 2.9892 (2.6531)	Prec@(1,5) (34.6%, 66.7%)	
07/04 03:42:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 2.5253 (2.4898)	Architecture Loss 2.7164 (2.6196)	Prec@(1,5) (34.3%, 67.3%)	
07/04 03:42:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.2532 (2.4936)	Architecture Loss 2.2405 (2.6197)	Prec@(1,5) (34.3%, 67.4%)	
07/04 03:43:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 2.2198 (2.4890)	Architecture Loss 2.7462 (2.6089)	Prec@(1,5) (34.5%, 67.4%)	
07/04 03:43:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.3149 (2.4792)	Architecture Loss 2.8186 (2.6036)	Prec@(1,5) (34.8%, 67.8%)	
07/04 03:43:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 2.7418 (2.4785)	Architecture Loss 2.6385 (2.5912)	Prec@(1,5) (34.8%, 67.8%)	
07/04 03:44:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.5451 (2.4761)	Architecture Loss 2.5147 (2.5821)	Prec@(1,5) (34.8%, 67.9%)	
07/04 03:44:18午後 searchStage_trainer.py:225 [INFO] Train: [  9/49] Final Prec@1 34.7840%
07/04 03:44:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 2.6103	Prec@(1,5) (32.2%, 65.6%)
07/04 03:44:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.6339	Prec@(1,5) (32.3%, 64.7%)
07/04 03:44:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 2.6277	Prec@(1,5) (32.6%, 64.8%)
07/04 03:44:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.6255	Prec@(1,5) (32.7%, 64.8%)
07/04 03:44:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 2.6194	Prec@(1,5) (32.9%, 64.9%)
07/04 03:44:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.6173	Prec@(1,5) (32.7%, 65.0%)
07/04 03:44:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 2.6161	Prec@(1,5) (32.6%, 65.1%)
07/04 03:44:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.6175	Prec@(1,5) (32.5%, 65.0%)
07/04 03:44:48午後 searchStage_trainer.py:264 [INFO] Valid: [  9/49] Final Prec@1 32.4920%
07/04 03:44:48午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:44:48午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 32.4920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2494, 0.2512, 0.2497],
        [0.2492, 0.2491, 0.2512, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2496, 0.2551, 0.2467],
        [0.2489, 0.2495, 0.2530, 0.2486],
        [0.2496, 0.2510, 0.2478, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2386, 0.2394, 0.2816, 0.2404],
        [0.2489, 0.2511, 0.2494, 0.2506],
        [0.2482, 0.2505, 0.2494, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2502, 0.2486],
        [0.2498, 0.2508, 0.2495, 0.2498],
        [0.2497, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2503, 0.2537, 0.2474],
        [0.2490, 0.2497, 0.2514, 0.2499],
        [0.2483, 0.2504, 0.2502, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2534, 0.2493, 0.2526, 0.2447],
        [0.2496, 0.2498, 0.2499, 0.2507],
        [0.2485, 0.2500, 0.2491, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2490, 0.2515, 0.2503],
        [0.2495, 0.2488, 0.2518, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2483, 0.2526, 0.2493],
        [0.2492, 0.2485, 0.2524, 0.2499],
        [0.2503, 0.2495, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2487, 0.2531, 0.2485],
        [0.2500, 0.2494, 0.2504, 0.2502],
        [0.2497, 0.2492, 0.2506, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2497, 0.2509, 0.2497],
        [0.2493, 0.2495, 0.2508, 0.2504],
        [0.2495, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2489, 0.2539, 0.2479],
        [0.2497, 0.2489, 0.2512, 0.2501],
        [0.2496, 0.2492, 0.2500, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2490, 0.2526, 0.2483],
        [0.2502, 0.2491, 0.2505, 0.2502],
        [0.2501, 0.2494, 0.2496, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2472, 0.2521, 0.2502],
        [0.2501, 0.2477, 0.2521, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2453, 0.2528, 0.2501],
        [0.2513, 0.2461, 0.2523, 0.2503],
        [0.2507, 0.2492, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2462, 0.2522, 0.2474],
        [0.2503, 0.2482, 0.2505, 0.2510],
        [0.2510, 0.2487, 0.2502, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2488, 0.2506, 0.2492],
        [0.2513, 0.2477, 0.2504, 0.2506],
        [0.2508, 0.2489, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2489, 0.2513, 0.2460],
        [0.2523, 0.2479, 0.2497, 0.2501],
        [0.2507, 0.2482, 0.2482, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2483, 0.2507, 0.2484],
        [0.2518, 0.2489, 0.2488, 0.2505],
        [0.2500, 0.2486, 0.2504, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:45:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 2.0902 (2.3944)	Architecture Loss 2.2199 (2.5799)	Prec@(1,5) (36.3%, 69.7%)	
07/04 03:45:45午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 2.5759 (2.3667)	Architecture Loss 2.7948 (2.5445)	Prec@(1,5) (37.2%, 69.9%)	
07/04 03:46:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 1.9837 (2.3611)	Architecture Loss 2.3740 (2.5248)	Prec@(1,5) (37.3%, 70.1%)	
07/04 03:46:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.0213 (2.3604)	Architecture Loss 2.4002 (2.5198)	Prec@(1,5) (37.6%, 70.1%)	
07/04 03:47:07午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 2.0381 (2.3576)	Architecture Loss 2.5949 (2.5207)	Prec@(1,5) (37.4%, 70.3%)	
07/04 03:47:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.2259 (2.3564)	Architecture Loss 2.2805 (2.5042)	Prec@(1,5) (37.3%, 70.2%)	
07/04 03:48:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 2.2321 (2.3581)	Architecture Loss 2.2534 (2.4960)	Prec@(1,5) (37.2%, 70.3%)	
07/04 03:48:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.4128 (2.3615)	Architecture Loss 2.8647 (2.4901)	Prec@(1,5) (37.2%, 70.1%)	
07/04 03:48:26午後 searchStage_trainer.py:225 [INFO] Train: [ 10/49] Final Prec@1 37.1600%
07/04 03:48:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 2.4699	Prec@(1,5) (35.7%, 69.5%)
07/04 03:48:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.4619	Prec@(1,5) (34.9%, 69.4%)
07/04 03:48:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 2.4696	Prec@(1,5) (35.0%, 69.1%)
07/04 03:48:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.4710	Prec@(1,5) (35.0%, 68.9%)
07/04 03:48:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 2.4839	Prec@(1,5) (34.8%, 68.4%)
07/04 03:48:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.4856	Prec@(1,5) (34.8%, 68.4%)
07/04 03:48:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 2.4828	Prec@(1,5) (34.8%, 68.4%)
07/04 03:48:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.4746	Prec@(1,5) (35.0%, 68.7%)
07/04 03:48:55午後 searchStage_trainer.py:264 [INFO] Valid: [ 10/49] Final Prec@1 34.9520%
07/04 03:48:55午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:48:55午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 34.9520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2494, 0.2512, 0.2497],
        [0.2492, 0.2491, 0.2512, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2495, 0.2555, 0.2468],
        [0.2488, 0.2493, 0.2533, 0.2486],
        [0.2496, 0.2510, 0.2478, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2366, 0.2380, 0.2856, 0.2399],
        [0.2489, 0.2511, 0.2494, 0.2506],
        [0.2481, 0.2505, 0.2494, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2502, 0.2486],
        [0.2498, 0.2508, 0.2495, 0.2498],
        [0.2497, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2502, 0.2541, 0.2474],
        [0.2489, 0.2496, 0.2517, 0.2498],
        [0.2482, 0.2504, 0.2502, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2535, 0.2493, 0.2527, 0.2445],
        [0.2496, 0.2498, 0.2499, 0.2507],
        [0.2485, 0.2499, 0.2491, 0.2525]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2490, 0.2516, 0.2503],
        [0.2494, 0.2487, 0.2519, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2483, 0.2528, 0.2492],
        [0.2491, 0.2484, 0.2525, 0.2499],
        [0.2503, 0.2495, 0.2497, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2487, 0.2534, 0.2483],
        [0.2499, 0.2494, 0.2504, 0.2503],
        [0.2497, 0.2492, 0.2506, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2497, 0.2509, 0.2497],
        [0.2493, 0.2495, 0.2508, 0.2504],
        [0.2495, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2488, 0.2541, 0.2479],
        [0.2497, 0.2489, 0.2513, 0.2501],
        [0.2496, 0.2492, 0.2500, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2490, 0.2527, 0.2482],
        [0.2502, 0.2491, 0.2505, 0.2502],
        [0.2501, 0.2494, 0.2496, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2471, 0.2522, 0.2502],
        [0.2500, 0.2477, 0.2522, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2451, 0.2530, 0.2501],
        [0.2514, 0.2458, 0.2525, 0.2503],
        [0.2507, 0.2492, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2460, 0.2525, 0.2470],
        [0.2503, 0.2482, 0.2505, 0.2510],
        [0.2509, 0.2487, 0.2502, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2487, 0.2506, 0.2492],
        [0.2513, 0.2476, 0.2505, 0.2506],
        [0.2508, 0.2489, 0.2505, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2488, 0.2515, 0.2457],
        [0.2524, 0.2478, 0.2498, 0.2500],
        [0.2507, 0.2481, 0.2482, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2482, 0.2509, 0.2481],
        [0.2518, 0.2488, 0.2489, 0.2505],
        [0.2500, 0.2485, 0.2505, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:49:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 2.1516 (2.1713)	Architecture Loss 2.4843 (2.3916)	Prec@(1,5) (42.2%, 73.6%)	
07/04 03:49:51午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 2.4062 (2.2033)	Architecture Loss 2.4003 (2.4034)	Prec@(1,5) (40.7%, 73.7%)	
07/04 03:50:19午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 2.0989 (2.2278)	Architecture Loss 2.1384 (2.4181)	Prec@(1,5) (39.8%, 72.9%)	
07/04 03:50:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 1.9859 (2.2288)	Architecture Loss 2.1191 (2.4138)	Prec@(1,5) (39.9%, 73.0%)	
07/04 03:51:15午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 2.0391 (2.2337)	Architecture Loss 2.3674 (2.4092)	Prec@(1,5) (39.8%, 72.9%)	
07/04 03:51:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 1.9233 (2.2391)	Architecture Loss 2.0345 (2.4045)	Prec@(1,5) (39.7%, 73.0%)	
07/04 03:52:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 2.4049 (2.2421)	Architecture Loss 2.0396 (2.4103)	Prec@(1,5) (39.7%, 72.9%)	
07/04 03:52:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 2.3014 (2.2402)	Architecture Loss 2.2396 (2.4041)	Prec@(1,5) (39.6%, 72.9%)	
07/04 03:52:32午後 searchStage_trainer.py:225 [INFO] Train: [ 11/49] Final Prec@1 39.5880%
07/04 03:52:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 2.4403	Prec@(1,5) (36.2%, 69.7%)
07/04 03:52:40午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.4316	Prec@(1,5) (36.8%, 69.6%)
07/04 03:52:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 2.4226	Prec@(1,5) (37.0%, 69.4%)
07/04 03:52:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.4335	Prec@(1,5) (36.8%, 69.3%)
07/04 03:52:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 2.4356	Prec@(1,5) (36.7%, 69.3%)
07/04 03:52:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.4357	Prec@(1,5) (36.5%, 69.3%)
07/04 03:52:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 2.4325	Prec@(1,5) (36.7%, 69.4%)
07/04 03:53:02午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.4369	Prec@(1,5) (36.7%, 69.2%)
07/04 03:53:02午後 searchStage_trainer.py:264 [INFO] Valid: [ 11/49] Final Prec@1 36.6760%
07/04 03:53:02午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:53:02午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 36.6760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2494, 0.2513, 0.2497],
        [0.2492, 0.2491, 0.2512, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2494, 0.2558, 0.2467],
        [0.2486, 0.2493, 0.2535, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2346, 0.2367, 0.2892, 0.2395],
        [0.2488, 0.2511, 0.2494, 0.2507],
        [0.2480, 0.2505, 0.2494, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2508, 0.2496, 0.2499],
        [0.2497, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2502, 0.2545, 0.2473],
        [0.2487, 0.2495, 0.2519, 0.2498],
        [0.2482, 0.2504, 0.2502, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2493, 0.2529, 0.2442],
        [0.2495, 0.2498, 0.2499, 0.2508],
        [0.2484, 0.2499, 0.2491, 0.2526]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2489, 0.2516, 0.2503],
        [0.2494, 0.2487, 0.2519, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2483, 0.2529, 0.2492],
        [0.2490, 0.2484, 0.2527, 0.2499],
        [0.2503, 0.2495, 0.2496, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2487, 0.2537, 0.2480],
        [0.2499, 0.2494, 0.2504, 0.2503],
        [0.2497, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2497, 0.2510, 0.2497],
        [0.2493, 0.2495, 0.2508, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2488, 0.2543, 0.2478],
        [0.2496, 0.2489, 0.2514, 0.2501],
        [0.2496, 0.2491, 0.2500, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2490, 0.2529, 0.2481],
        [0.2502, 0.2490, 0.2505, 0.2502],
        [0.2501, 0.2494, 0.2496, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2470, 0.2523, 0.2502],
        [0.2500, 0.2476, 0.2523, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2449, 0.2532, 0.2501],
        [0.2513, 0.2456, 0.2527, 0.2503],
        [0.2507, 0.2492, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2458, 0.2530, 0.2468],
        [0.2502, 0.2481, 0.2506, 0.2511],
        [0.2509, 0.2486, 0.2502, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2487, 0.2508, 0.2491],
        [0.2512, 0.2475, 0.2505, 0.2507],
        [0.2508, 0.2488, 0.2505, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2488, 0.2517, 0.2455],
        [0.2523, 0.2477, 0.2500, 0.2500],
        [0.2506, 0.2480, 0.2482, 0.2531]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2481, 0.2511, 0.2480],
        [0.2518, 0.2487, 0.2489, 0.2505],
        [0.2499, 0.2484, 0.2505, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:53:31午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 2.4037 (2.1461)	Architecture Loss 2.0313 (2.3069)	Prec@(1,5) (41.3%, 75.8%)	
07/04 03:53:59午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 2.2224 (2.1480)	Architecture Loss 2.0460 (2.3224)	Prec@(1,5) (41.6%, 75.6%)	
07/04 03:54:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 2.3409 (2.1507)	Architecture Loss 2.5937 (2.3401)	Prec@(1,5) (41.7%, 75.2%)	
07/04 03:54:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 2.2536 (2.1518)	Architecture Loss 2.2120 (2.3349)	Prec@(1,5) (41.6%, 75.1%)	
07/04 03:55:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 2.3837 (2.1549)	Architecture Loss 2.3993 (2.3281)	Prec@(1,5) (41.7%, 74.9%)	
07/04 03:55:49午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 2.4702 (2.1611)	Architecture Loss 2.2683 (2.3296)	Prec@(1,5) (41.5%, 74.9%)	
07/04 03:56:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 1.9961 (2.1614)	Architecture Loss 2.0999 (2.3222)	Prec@(1,5) (41.6%, 74.8%)	
07/04 03:56:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 2.2930 (2.1576)	Architecture Loss 2.3516 (2.3191)	Prec@(1,5) (41.7%, 74.8%)	
07/04 03:56:41午後 searchStage_trainer.py:225 [INFO] Train: [ 12/49] Final Prec@1 41.7120%
07/04 03:56:45午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 2.3305	Prec@(1,5) (38.1%, 71.5%)
07/04 03:56:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.3820	Prec@(1,5) (37.9%, 70.5%)
07/04 03:56:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 2.3694	Prec@(1,5) (38.2%, 70.8%)
07/04 03:56:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.3764	Prec@(1,5) (38.2%, 70.5%)
07/04 03:56:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 2.3690	Prec@(1,5) (37.9%, 70.7%)
07/04 03:57:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.3667	Prec@(1,5) (37.7%, 70.8%)
07/04 03:57:07午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 2.3594	Prec@(1,5) (37.9%, 70.9%)
07/04 03:57:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.3547	Prec@(1,5) (38.0%, 71.0%)
07/04 03:57:10午後 searchStage_trainer.py:264 [INFO] Valid: [ 12/49] Final Prec@1 38.0000%
07/04 03:57:10午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 03:57:10午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 38.0000%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2494, 0.2513, 0.2497],
        [0.2492, 0.2491, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2493, 0.2560, 0.2466],
        [0.2485, 0.2492, 0.2537, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2325, 0.2354, 0.2931, 0.2390],
        [0.2488, 0.2511, 0.2494, 0.2507],
        [0.2479, 0.2505, 0.2494, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2508, 0.2496, 0.2499],
        [0.2497, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2501, 0.2549, 0.2473],
        [0.2486, 0.2495, 0.2521, 0.2498],
        [0.2481, 0.2504, 0.2502, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2493, 0.2530, 0.2440],
        [0.2495, 0.2498, 0.2499, 0.2508],
        [0.2484, 0.2499, 0.2491, 0.2527]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2489, 0.2517, 0.2503],
        [0.2494, 0.2487, 0.2520, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2483, 0.2531, 0.2491],
        [0.2489, 0.2484, 0.2528, 0.2499],
        [0.2503, 0.2495, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2486, 0.2540, 0.2479],
        [0.2499, 0.2494, 0.2504, 0.2503],
        [0.2497, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2497, 0.2511, 0.2496],
        [0.2493, 0.2495, 0.2508, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2488, 0.2545, 0.2477],
        [0.2496, 0.2489, 0.2515, 0.2501],
        [0.2496, 0.2491, 0.2500, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2490, 0.2530, 0.2480],
        [0.2502, 0.2490, 0.2505, 0.2503],
        [0.2501, 0.2494, 0.2496, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2470, 0.2524, 0.2502],
        [0.2499, 0.2476, 0.2523, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2446, 0.2534, 0.2501],
        [0.2513, 0.2455, 0.2529, 0.2503],
        [0.2507, 0.2491, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2456, 0.2533, 0.2466],
        [0.2502, 0.2481, 0.2506, 0.2511],
        [0.2509, 0.2486, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2486, 0.2509, 0.2491],
        [0.2512, 0.2475, 0.2506, 0.2508],
        [0.2507, 0.2488, 0.2505, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2487, 0.2519, 0.2452],
        [0.2523, 0.2476, 0.2501, 0.2500],
        [0.2506, 0.2479, 0.2482, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2480, 0.2512, 0.2479],
        [0.2519, 0.2487, 0.2490, 0.2505],
        [0.2499, 0.2484, 0.2506, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 03:57:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 2.1975 (1.9980)	Architecture Loss 2.1068 (2.2596)	Prec@(1,5) (45.1%, 77.2%)	
07/04 03:58:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 2.1670 (2.0475)	Architecture Loss 2.5904 (2.2855)	Prec@(1,5) (44.3%, 76.4%)	
07/04 03:58:34午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 1.8515 (2.0640)	Architecture Loss 2.4304 (2.2687)	Prec@(1,5) (44.0%, 76.1%)	
07/04 03:59:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 1.9928 (2.0607)	Architecture Loss 2.4341 (2.2736)	Prec@(1,5) (44.2%, 76.2%)	
07/04 03:59:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 2.3049 (2.0644)	Architecture Loss 2.2859 (2.2742)	Prec@(1,5) (43.9%, 76.4%)	
07/04 03:59:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 1.8014 (2.0714)	Architecture Loss 2.3334 (2.2702)	Prec@(1,5) (43.8%, 76.3%)	
07/04 04:00:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 2.0995 (2.0681)	Architecture Loss 2.3855 (2.2703)	Prec@(1,5) (43.8%, 76.4%)	
07/04 04:00:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 2.1296 (2.0673)	Architecture Loss 2.2167 (2.2674)	Prec@(1,5) (43.7%, 76.4%)	
07/04 04:00:48午後 searchStage_trainer.py:225 [INFO] Train: [ 13/49] Final Prec@1 43.7520%
07/04 04:00:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 2.3305	Prec@(1,5) (38.1%, 71.5%)
07/04 04:00:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.2895	Prec@(1,5) (39.1%, 72.2%)
07/04 04:00:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 2.2809	Prec@(1,5) (39.1%, 72.4%)
07/04 04:01:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.2764	Prec@(1,5) (39.4%, 72.5%)
07/04 04:01:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 2.2762	Prec@(1,5) (39.5%, 72.5%)
07/04 04:01:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.2787	Prec@(1,5) (39.8%, 72.5%)
07/04 04:01:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 2.2777	Prec@(1,5) (39.8%, 72.5%)
07/04 04:01:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.2762	Prec@(1,5) (39.8%, 72.5%)
07/04 04:01:17午後 searchStage_trainer.py:264 [INFO] Valid: [ 13/49] Final Prec@1 39.8520%
07/04 04:01:17午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:01:17午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 39.8520%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2494, 0.2513, 0.2497],
        [0.2492, 0.2491, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2492, 0.2562, 0.2466],
        [0.2485, 0.2491, 0.2538, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2309, 0.2343, 0.2959, 0.2388],
        [0.2487, 0.2511, 0.2494, 0.2507],
        [0.2479, 0.2505, 0.2494, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2508, 0.2496, 0.2499],
        [0.2497, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2500, 0.2552, 0.2473],
        [0.2485, 0.2494, 0.2523, 0.2498],
        [0.2481, 0.2503, 0.2502, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2494, 0.2531, 0.2438],
        [0.2494, 0.2498, 0.2499, 0.2509],
        [0.2483, 0.2499, 0.2491, 0.2527]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2489, 0.2517, 0.2503],
        [0.2494, 0.2487, 0.2520, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2482, 0.2532, 0.2491],
        [0.2489, 0.2484, 0.2529, 0.2498],
        [0.2503, 0.2495, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2485, 0.2542, 0.2478],
        [0.2499, 0.2494, 0.2504, 0.2503],
        [0.2497, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2497, 0.2511, 0.2496],
        [0.2492, 0.2495, 0.2508, 0.2505],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2488, 0.2547, 0.2476],
        [0.2495, 0.2489, 0.2515, 0.2500],
        [0.2496, 0.2491, 0.2500, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2491, 0.2531, 0.2478],
        [0.2502, 0.2490, 0.2505, 0.2503],
        [0.2501, 0.2494, 0.2496, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2470, 0.2524, 0.2502],
        [0.2499, 0.2476, 0.2524, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2445, 0.2536, 0.2501],
        [0.2513, 0.2453, 0.2531, 0.2503],
        [0.2507, 0.2491, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2455, 0.2536, 0.2463],
        [0.2502, 0.2480, 0.2506, 0.2511],
        [0.2509, 0.2486, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2486, 0.2510, 0.2490],
        [0.2512, 0.2474, 0.2506, 0.2508],
        [0.2507, 0.2488, 0.2506, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2487, 0.2520, 0.2450],
        [0.2523, 0.2475, 0.2502, 0.2500],
        [0.2506, 0.2479, 0.2482, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2480, 0.2514, 0.2477],
        [0.2519, 0.2486, 0.2490, 0.2506],
        [0.2499, 0.2484, 0.2506, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:01:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 1.6007 (1.9333)	Architecture Loss 2.1232 (2.2265)	Prec@(1,5) (46.3%, 79.6%)	
07/04 04:02:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 2.0344 (1.9442)	Architecture Loss 2.0917 (2.2290)	Prec@(1,5) (46.5%, 78.8%)	
07/04 04:02:42午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 1.6626 (1.9428)	Architecture Loss 2.0523 (2.2211)	Prec@(1,5) (46.4%, 78.9%)	
07/04 04:03:09午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 2.0755 (1.9560)	Architecture Loss 2.0920 (2.2248)	Prec@(1,5) (46.1%, 78.5%)	
07/04 04:03:37午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 1.9336 (1.9608)	Architecture Loss 2.2987 (2.2202)	Prec@(1,5) (46.0%, 78.3%)	
07/04 04:04:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 1.7244 (1.9621)	Architecture Loss 2.0347 (2.2221)	Prec@(1,5) (45.9%, 78.3%)	
07/04 04:04:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 2.0767 (1.9687)	Architecture Loss 2.0534 (2.2183)	Prec@(1,5) (45.9%, 78.2%)	
07/04 04:04:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 1.9132 (1.9683)	Architecture Loss 2.5365 (2.2122)	Prec@(1,5) (45.9%, 78.2%)	
07/04 04:04:56午後 searchStage_trainer.py:225 [INFO] Train: [ 14/49] Final Prec@1 45.9040%
07/04 04:05:00午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 2.1247	Prec@(1,5) (43.2%, 75.7%)
07/04 04:05:04午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.1460	Prec@(1,5) (42.3%, 75.1%)
07/04 04:05:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 2.1531	Prec@(1,5) (42.2%, 74.7%)
07/04 04:05:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.1521	Prec@(1,5) (42.3%, 74.7%)
07/04 04:05:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 2.1503	Prec@(1,5) (42.2%, 74.8%)
07/04 04:05:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.1392	Prec@(1,5) (42.4%, 75.1%)
07/04 04:05:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 2.1397	Prec@(1,5) (42.4%, 75.2%)
07/04 04:05:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.1376	Prec@(1,5) (42.4%, 75.2%)
07/04 04:05:26午後 searchStage_trainer.py:264 [INFO] Valid: [ 14/49] Final Prec@1 42.4040%
07/04 04:05:26午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:05:26午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 42.4040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2494, 0.2513, 0.2497],
        [0.2491, 0.2491, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2492, 0.2565, 0.2466],
        [0.2484, 0.2490, 0.2540, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2293, 0.2332, 0.2990, 0.2385],
        [0.2487, 0.2511, 0.2494, 0.2508],
        [0.2478, 0.2505, 0.2494, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2497, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2499, 0.2554, 0.2473],
        [0.2484, 0.2493, 0.2525, 0.2497],
        [0.2481, 0.2503, 0.2502, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2494, 0.2532, 0.2437],
        [0.2494, 0.2498, 0.2499, 0.2509],
        [0.2483, 0.2498, 0.2491, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2489, 0.2517, 0.2503],
        [0.2494, 0.2487, 0.2520, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2482, 0.2533, 0.2491],
        [0.2488, 0.2483, 0.2530, 0.2498],
        [0.2503, 0.2495, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2485, 0.2544, 0.2477],
        [0.2499, 0.2494, 0.2504, 0.2504],
        [0.2497, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2497, 0.2512, 0.2496],
        [0.2492, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2488, 0.2548, 0.2476],
        [0.2495, 0.2489, 0.2516, 0.2500],
        [0.2496, 0.2491, 0.2500, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2491, 0.2532, 0.2477],
        [0.2502, 0.2490, 0.2505, 0.2504],
        [0.2501, 0.2494, 0.2496, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2469, 0.2525, 0.2502],
        [0.2499, 0.2476, 0.2524, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2443, 0.2537, 0.2501],
        [0.2512, 0.2452, 0.2533, 0.2503],
        [0.2507, 0.2491, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2547, 0.2453, 0.2540, 0.2460],
        [0.2502, 0.2480, 0.2507, 0.2512],
        [0.2509, 0.2486, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2486, 0.2511, 0.2490],
        [0.2511, 0.2474, 0.2506, 0.2509],
        [0.2507, 0.2488, 0.2506, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2487, 0.2522, 0.2449],
        [0.2523, 0.2475, 0.2502, 0.2500],
        [0.2506, 0.2479, 0.2483, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2479, 0.2516, 0.2476],
        [0.2518, 0.2486, 0.2490, 0.2505],
        [0.2498, 0.2483, 0.2506, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:05:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 2.0383 (1.8537)	Architecture Loss 1.8627 (2.1550)	Prec@(1,5) (48.2%, 80.6%)	
07/04 04:06:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 1.8570 (1.8653)	Architecture Loss 2.1863 (2.1508)	Prec@(1,5) (48.4%, 80.4%)	
07/04 04:06:49午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 2.3639 (1.8596)	Architecture Loss 2.5376 (2.1480)	Prec@(1,5) (48.7%, 80.8%)	
07/04 04:07:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 2.0089 (1.8703)	Architecture Loss 2.6172 (2.1694)	Prec@(1,5) (48.2%, 80.4%)	
07/04 04:07:45午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 2.0871 (1.8799)	Architecture Loss 2.1146 (2.1609)	Prec@(1,5) (47.9%, 80.3%)	
07/04 04:08:13午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 1.7599 (1.8875)	Architecture Loss 2.1164 (2.1603)	Prec@(1,5) (47.9%, 80.2%)	
07/04 04:08:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 1.6876 (1.8985)	Architecture Loss 2.2692 (2.1687)	Prec@(1,5) (47.7%, 79.8%)	
07/04 04:09:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 1.8082 (1.9010)	Architecture Loss 2.0439 (2.1659)	Prec@(1,5) (47.7%, 79.8%)	
07/04 04:09:03午後 searchStage_trainer.py:225 [INFO] Train: [ 15/49] Final Prec@1 47.6720%
07/04 04:09:07午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 2.2061	Prec@(1,5) (41.5%, 74.2%)
07/04 04:09:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.2186	Prec@(1,5) (41.6%, 73.9%)
07/04 04:09:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 2.2261	Prec@(1,5) (41.7%, 74.0%)
07/04 04:09:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.2156	Prec@(1,5) (41.9%, 74.4%)
07/04 04:09:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 2.2101	Prec@(1,5) (41.9%, 74.3%)
07/04 04:09:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.2052	Prec@(1,5) (42.1%, 74.5%)
07/04 04:09:29午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 2.1986	Prec@(1,5) (42.2%, 74.4%)
07/04 04:09:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.1884	Prec@(1,5) (42.4%, 74.5%)
07/04 04:09:32午後 searchStage_trainer.py:264 [INFO] Valid: [ 15/49] Final Prec@1 42.4000%
07/04 04:09:32午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:09:32午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 42.4040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2494, 0.2514, 0.2497],
        [0.2491, 0.2491, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2491, 0.2566, 0.2466],
        [0.2483, 0.2490, 0.2541, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2280, 0.2322, 0.3016, 0.2382],
        [0.2487, 0.2511, 0.2494, 0.2508],
        [0.2478, 0.2505, 0.2494, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2497, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2499, 0.2557, 0.2473],
        [0.2483, 0.2493, 0.2527, 0.2497],
        [0.2481, 0.2503, 0.2502, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2494, 0.2533, 0.2435],
        [0.2494, 0.2498, 0.2499, 0.2509],
        [0.2483, 0.2498, 0.2491, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2494, 0.2487, 0.2520, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2482, 0.2533, 0.2491],
        [0.2487, 0.2483, 0.2531, 0.2498],
        [0.2503, 0.2495, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2485, 0.2546, 0.2476],
        [0.2499, 0.2494, 0.2504, 0.2504],
        [0.2497, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2497, 0.2512, 0.2496],
        [0.2492, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2488, 0.2549, 0.2475],
        [0.2495, 0.2489, 0.2516, 0.2500],
        [0.2496, 0.2491, 0.2500, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2491, 0.2533, 0.2476],
        [0.2502, 0.2489, 0.2505, 0.2504],
        [0.2501, 0.2494, 0.2496, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2525, 0.2502],
        [0.2498, 0.2476, 0.2525, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2442, 0.2539, 0.2501],
        [0.2512, 0.2451, 0.2534, 0.2503],
        [0.2506, 0.2491, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2452, 0.2544, 0.2458],
        [0.2501, 0.2480, 0.2507, 0.2512],
        [0.2508, 0.2485, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2485, 0.2511, 0.2490],
        [0.2511, 0.2474, 0.2507, 0.2509],
        [0.2507, 0.2488, 0.2506, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2486, 0.2524, 0.2447],
        [0.2522, 0.2474, 0.2503, 0.2500],
        [0.2506, 0.2478, 0.2483, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2479, 0.2518, 0.2474],
        [0.2518, 0.2485, 0.2491, 0.2506],
        [0.2498, 0.2483, 0.2506, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:10:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 1.5805 (1.7425)	Architecture Loss 2.3656 (2.1444)	Prec@(1,5) (51.4%, 83.3%)	
07/04 04:10:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 1.7985 (1.7789)	Architecture Loss 2.3101 (2.1617)	Prec@(1,5) (50.6%, 82.0%)	
07/04 04:10:57午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 2.1134 (1.8092)	Architecture Loss 2.3846 (2.1691)	Prec@(1,5) (49.7%, 81.2%)	
07/04 04:11:24午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 1.9119 (1.8081)	Architecture Loss 2.3432 (2.1505)	Prec@(1,5) (49.6%, 81.1%)	
07/04 04:11:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 1.8619 (1.8131)	Architecture Loss 1.9677 (2.1519)	Prec@(1,5) (49.5%, 81.1%)	
07/04 04:12:20午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 1.7826 (1.8269)	Architecture Loss 1.8260 (2.1406)	Prec@(1,5) (49.1%, 80.9%)	
07/04 04:12:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 1.8324 (1.8268)	Architecture Loss 2.1382 (2.1277)	Prec@(1,5) (49.2%, 80.9%)	
07/04 04:13:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 2.1420 (1.8275)	Architecture Loss 2.2861 (2.1212)	Prec@(1,5) (49.1%, 80.9%)	
07/04 04:13:11午後 searchStage_trainer.py:225 [INFO] Train: [ 16/49] Final Prec@1 49.1040%
07/04 04:13:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 2.2078	Prec@(1,5) (42.5%, 73.7%)
07/04 04:13:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.1590	Prec@(1,5) (43.1%, 74.9%)
07/04 04:13:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 2.1520	Prec@(1,5) (43.1%, 75.0%)
07/04 04:13:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.1390	Prec@(1,5) (43.2%, 75.2%)
07/04 04:13:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 2.1487	Prec@(1,5) (43.0%, 75.1%)
07/04 04:13:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.1511	Prec@(1,5) (42.9%, 75.1%)
07/04 04:13:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 2.1587	Prec@(1,5) (42.8%, 75.0%)
07/04 04:13:40午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.1608	Prec@(1,5) (42.9%, 75.0%)
07/04 04:13:40午後 searchStage_trainer.py:264 [INFO] Valid: [ 16/49] Final Prec@1 42.8480%
07/04 04:13:40午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('max_pool_3x3', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:13:41午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 42.8480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2494, 0.2514, 0.2497],
        [0.2491, 0.2491, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2490, 0.2568, 0.2466],
        [0.2483, 0.2489, 0.2542, 0.2485],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2266, 0.2312, 0.3042, 0.2380],
        [0.2487, 0.2511, 0.2494, 0.2508],
        [0.2478, 0.2505, 0.2494, 0.2522]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2497, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2498, 0.2559, 0.2473],
        [0.2482, 0.2492, 0.2529, 0.2497],
        [0.2480, 0.2503, 0.2502, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2494, 0.2534, 0.2434],
        [0.2493, 0.2498, 0.2499, 0.2509],
        [0.2483, 0.2498, 0.2490, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2482, 0.2534, 0.2491],
        [0.2487, 0.2483, 0.2531, 0.2498],
        [0.2502, 0.2495, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2485, 0.2549, 0.2475],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2497, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2497, 0.2512, 0.2496],
        [0.2492, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2488, 0.2551, 0.2475],
        [0.2494, 0.2489, 0.2517, 0.2500],
        [0.2495, 0.2491, 0.2500, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2491, 0.2534, 0.2476],
        [0.2502, 0.2489, 0.2504, 0.2504],
        [0.2501, 0.2494, 0.2496, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2526, 0.2502],
        [0.2498, 0.2476, 0.2525, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2441, 0.2540, 0.2501],
        [0.2511, 0.2450, 0.2536, 0.2503],
        [0.2506, 0.2491, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2451, 0.2548, 0.2456],
        [0.2501, 0.2480, 0.2507, 0.2512],
        [0.2508, 0.2485, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2485, 0.2512, 0.2490],
        [0.2510, 0.2473, 0.2507, 0.2509],
        [0.2507, 0.2488, 0.2507, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2487, 0.2525, 0.2445],
        [0.2522, 0.2474, 0.2504, 0.2500],
        [0.2505, 0.2478, 0.2483, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2479, 0.2519, 0.2473],
        [0.2518, 0.2485, 0.2491, 0.2505],
        [0.2497, 0.2483, 0.2507, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:14:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 2.0499 (1.7100)	Architecture Loss 2.5386 (2.0877)	Prec@(1,5) (52.2%, 83.0%)	
07/04 04:14:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 2.2002 (1.7204)	Architecture Loss 2.0143 (2.0805)	Prec@(1,5) (52.1%, 82.8%)	
07/04 04:15:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 1.8591 (1.7313)	Architecture Loss 2.3702 (2.0845)	Prec@(1,5) (51.9%, 82.6%)	
07/04 04:15:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 1.6756 (1.7363)	Architecture Loss 2.1804 (2.0906)	Prec@(1,5) (51.7%, 82.7%)	
07/04 04:15:59午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 1.8308 (1.7385)	Architecture Loss 1.8664 (2.0938)	Prec@(1,5) (51.5%, 82.7%)	
07/04 04:16:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 1.4710 (1.7434)	Architecture Loss 2.0991 (2.0804)	Prec@(1,5) (51.3%, 82.6%)	
07/04 04:16:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 1.7321 (1.7485)	Architecture Loss 2.2227 (2.0789)	Prec@(1,5) (51.2%, 82.6%)	
07/04 04:17:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 1.8807 (1.7491)	Architecture Loss 1.8402 (2.0806)	Prec@(1,5) (51.3%, 82.6%)	
07/04 04:17:17午後 searchStage_trainer.py:225 [INFO] Train: [ 17/49] Final Prec@1 51.2960%
07/04 04:17:21午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 2.1180	Prec@(1,5) (44.6%, 74.5%)
07/04 04:17:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.1195	Prec@(1,5) (44.7%, 75.1%)
07/04 04:17:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 2.1271	Prec@(1,5) (44.2%, 75.5%)
07/04 04:17:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 2.1432	Prec@(1,5) (44.0%, 75.1%)
07/04 04:17:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 2.1337	Prec@(1,5) (44.0%, 75.3%)
07/04 04:17:40午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 2.1396	Prec@(1,5) (43.9%, 75.2%)
07/04 04:17:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 2.1410	Prec@(1,5) (43.8%, 75.2%)
07/04 04:17:46午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 2.1401	Prec@(1,5) (43.7%, 75.2%)
07/04 04:17:46午後 searchStage_trainer.py:264 [INFO] Valid: [ 17/49] Final Prec@1 43.7200%
07/04 04:17:46午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:17:46午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 43.7200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2494, 0.2514, 0.2497],
        [0.2491, 0.2491, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2569, 0.2466],
        [0.2482, 0.2489, 0.2543, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2254, 0.2304, 0.3065, 0.2377],
        [0.2486, 0.2511, 0.2494, 0.2508],
        [0.2478, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2497, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2497, 0.2561, 0.2472],
        [0.2481, 0.2491, 0.2531, 0.2497],
        [0.2480, 0.2503, 0.2502, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2494, 0.2534, 0.2433],
        [0.2493, 0.2498, 0.2499, 0.2510],
        [0.2483, 0.2498, 0.2490, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2482, 0.2535, 0.2490],
        [0.2486, 0.2483, 0.2532, 0.2498],
        [0.2502, 0.2495, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2484, 0.2550, 0.2474],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2497, 0.2512, 0.2496],
        [0.2492, 0.2495, 0.2509, 0.2505],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2488, 0.2552, 0.2474],
        [0.2494, 0.2489, 0.2517, 0.2500],
        [0.2495, 0.2491, 0.2499, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2491, 0.2535, 0.2475],
        [0.2502, 0.2489, 0.2504, 0.2504],
        [0.2501, 0.2494, 0.2496, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2526, 0.2502],
        [0.2498, 0.2475, 0.2526, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2440, 0.2542, 0.2501],
        [0.2511, 0.2449, 0.2537, 0.2503],
        [0.2506, 0.2491, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2546, 0.2449, 0.2552, 0.2453],
        [0.2501, 0.2480, 0.2508, 0.2512],
        [0.2508, 0.2485, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2485, 0.2513, 0.2489],
        [0.2510, 0.2473, 0.2508, 0.2509],
        [0.2507, 0.2488, 0.2507, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2487, 0.2527, 0.2444],
        [0.2521, 0.2474, 0.2505, 0.2500],
        [0.2505, 0.2478, 0.2483, 0.2534]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2479, 0.2521, 0.2471],
        [0.2518, 0.2485, 0.2492, 0.2505],
        [0.2497, 0.2483, 0.2507, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:18:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 1.7278 (1.6310)	Architecture Loss 1.8200 (2.0157)	Prec@(1,5) (53.8%, 84.5%)	
07/04 04:18:44午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 1.1536 (1.6363)	Architecture Loss 1.7356 (2.0053)	Prec@(1,5) (54.0%, 84.5%)	
07/04 04:19:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 1.5751 (1.6329)	Architecture Loss 2.3181 (2.0480)	Prec@(1,5) (53.9%, 84.5%)	
07/04 04:19:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 1.7500 (1.6457)	Architecture Loss 1.9374 (2.0537)	Prec@(1,5) (53.7%, 84.4%)	
07/04 04:20:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 1.8275 (1.6593)	Architecture Loss 1.4982 (2.0602)	Prec@(1,5) (53.3%, 83.9%)	
07/04 04:20:34午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 1.6924 (1.6673)	Architecture Loss 2.3718 (2.0519)	Prec@(1,5) (52.9%, 83.8%)	
07/04 04:21:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 2.0160 (1.6761)	Architecture Loss 2.0046 (2.0485)	Prec@(1,5) (52.7%, 83.7%)	
07/04 04:21:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 1.7021 (1.6828)	Architecture Loss 1.9131 (2.0471)	Prec@(1,5) (52.5%, 83.6%)	
07/04 04:21:25午後 searchStage_trainer.py:225 [INFO] Train: [ 18/49] Final Prec@1 52.5600%
07/04 04:21:29午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 2.0314	Prec@(1,5) (44.9%, 78.0%)
07/04 04:21:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 2.0202	Prec@(1,5) (46.0%, 77.7%)
07/04 04:21:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 2.0347	Prec@(1,5) (45.8%, 77.6%)
07/04 04:21:40午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 2.0367	Prec@(1,5) (45.9%, 77.6%)
07/04 04:21:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 2.0292	Prec@(1,5) (45.9%, 77.7%)
07/04 04:21:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 2.0296	Prec@(1,5) (46.1%, 77.7%)
07/04 04:21:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 2.0190	Prec@(1,5) (46.3%, 77.8%)
07/04 04:21:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 2.0262	Prec@(1,5) (46.1%, 77.6%)
07/04 04:21:54午後 searchStage_trainer.py:264 [INFO] Valid: [ 18/49] Final Prec@1 46.1600%
07/04 04:21:54午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:21:55午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 46.1600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2494, 0.2514, 0.2497],
        [0.2491, 0.2491, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2489, 0.2570, 0.2466],
        [0.2482, 0.2488, 0.2544, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2242, 0.2296, 0.3087, 0.2376],
        [0.2486, 0.2511, 0.2494, 0.2508],
        [0.2478, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2497, 0.2563, 0.2472],
        [0.2480, 0.2490, 0.2532, 0.2497],
        [0.2480, 0.2503, 0.2502, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2494, 0.2534, 0.2433],
        [0.2493, 0.2498, 0.2499, 0.2510],
        [0.2483, 0.2498, 0.2490, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2482, 0.2535, 0.2490],
        [0.2486, 0.2483, 0.2533, 0.2498],
        [0.2503, 0.2495, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2484, 0.2552, 0.2473],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2497, 0.2513, 0.2496],
        [0.2492, 0.2495, 0.2509, 0.2505],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2488, 0.2553, 0.2473],
        [0.2494, 0.2489, 0.2518, 0.2500],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2491, 0.2536, 0.2474],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2501, 0.2494, 0.2496, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2469, 0.2527, 0.2502],
        [0.2498, 0.2475, 0.2526, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2439, 0.2543, 0.2501],
        [0.2510, 0.2449, 0.2538, 0.2503],
        [0.2506, 0.2491, 0.2504, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2448, 0.2555, 0.2451],
        [0.2501, 0.2480, 0.2508, 0.2512],
        [0.2508, 0.2485, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2485, 0.2514, 0.2489],
        [0.2510, 0.2473, 0.2508, 0.2510],
        [0.2506, 0.2488, 0.2507, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2487, 0.2528, 0.2442],
        [0.2521, 0.2473, 0.2506, 0.2500],
        [0.2504, 0.2478, 0.2483, 0.2535]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2479, 0.2522, 0.2470],
        [0.2518, 0.2485, 0.2492, 0.2505],
        [0.2496, 0.2482, 0.2507, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:22:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 1.4438 (1.5624)	Architecture Loss 1.4481 (2.0118)	Prec@(1,5) (56.0%, 85.6%)	
07/04 04:22:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.3809 (1.5746)	Architecture Loss 1.8418 (1.9786)	Prec@(1,5) (55.7%, 85.2%)	
07/04 04:23:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 1.6082 (1.5887)	Architecture Loss 2.4110 (2.0140)	Prec@(1,5) (55.6%, 84.9%)	
07/04 04:23:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 1.5282 (1.5952)	Architecture Loss 1.9420 (2.0289)	Prec@(1,5) (55.2%, 84.8%)	
07/04 04:24:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 1.8412 (1.6019)	Architecture Loss 1.7011 (2.0262)	Prec@(1,5) (54.7%, 84.7%)	
07/04 04:24:42午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 1.6840 (1.6077)	Architecture Loss 1.7440 (2.0217)	Prec@(1,5) (54.5%, 84.8%)	
07/04 04:25:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 1.7954 (1.6203)	Architecture Loss 2.2289 (2.0231)	Prec@(1,5) (54.1%, 84.6%)	
07/04 04:25:31午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 1.7845 (1.6264)	Architecture Loss 1.8730 (2.0277)	Prec@(1,5) (54.1%, 84.5%)	
07/04 04:25:32午後 searchStage_trainer.py:225 [INFO] Train: [ 19/49] Final Prec@1 54.0920%
07/04 04:25:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 2.0184	Prec@(1,5) (46.8%, 77.1%)
07/04 04:25:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.0135	Prec@(1,5) (47.0%, 77.5%)
07/04 04:25:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 2.0192	Prec@(1,5) (46.7%, 77.5%)
07/04 04:25:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.0130	Prec@(1,5) (47.0%, 77.6%)
07/04 04:25:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 2.0074	Prec@(1,5) (47.0%, 77.8%)
07/04 04:25:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 2.0104	Prec@(1,5) (46.9%, 77.7%)
07/04 04:25:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 2.0070	Prec@(1,5) (46.9%, 77.7%)
07/04 04:26:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 2.0007	Prec@(1,5) (46.9%, 77.8%)
07/04 04:26:01午後 searchStage_trainer.py:264 [INFO] Valid: [ 19/49] Final Prec@1 46.8880%
07/04 04:26:01午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:26:02午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 46.8880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2489, 0.2572, 0.2466],
        [0.2481, 0.2488, 0.2545, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2230, 0.2287, 0.3108, 0.2374],
        [0.2486, 0.2511, 0.2494, 0.2508],
        [0.2478, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2497, 0.2565, 0.2471],
        [0.2479, 0.2490, 0.2534, 0.2497],
        [0.2480, 0.2503, 0.2502, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2494, 0.2535, 0.2432],
        [0.2493, 0.2498, 0.2499, 0.2510],
        [0.2483, 0.2498, 0.2490, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2482, 0.2536, 0.2490],
        [0.2486, 0.2483, 0.2533, 0.2498],
        [0.2503, 0.2495, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2483, 0.2554, 0.2473],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2497, 0.2513, 0.2496],
        [0.2492, 0.2495, 0.2509, 0.2505],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2488, 0.2553, 0.2473],
        [0.2493, 0.2489, 0.2518, 0.2500],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2492, 0.2537, 0.2473],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2501, 0.2494, 0.2496, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2469, 0.2527, 0.2502],
        [0.2497, 0.2475, 0.2526, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2438, 0.2544, 0.2500],
        [0.2510, 0.2448, 0.2539, 0.2503],
        [0.2506, 0.2491, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2448, 0.2559, 0.2448],
        [0.2500, 0.2479, 0.2508, 0.2512],
        [0.2508, 0.2485, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2485, 0.2514, 0.2489],
        [0.2509, 0.2473, 0.2508, 0.2510],
        [0.2506, 0.2488, 0.2507, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2487, 0.2529, 0.2441],
        [0.2520, 0.2473, 0.2506, 0.2500],
        [0.2504, 0.2478, 0.2483, 0.2535]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2479, 0.2524, 0.2468],
        [0.2518, 0.2485, 0.2493, 0.2505],
        [0.2496, 0.2482, 0.2507, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:26:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 1.7262 (1.5084)	Architecture Loss 1.6633 (1.9954)	Prec@(1,5) (56.8%, 86.8%)	
07/04 04:26:59午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 1.1119 (1.4983)	Architecture Loss 1.9579 (1.9998)	Prec@(1,5) (57.2%, 86.6%)	
07/04 04:27:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 1.4049 (1.5076)	Architecture Loss 1.9248 (1.9956)	Prec@(1,5) (56.8%, 86.5%)	
07/04 04:27:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 1.8609 (1.5274)	Architecture Loss 1.9453 (1.9932)	Prec@(1,5) (56.0%, 86.3%)	
07/04 04:28:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 1.6704 (1.5469)	Architecture Loss 2.2578 (1.9910)	Prec@(1,5) (55.7%, 86.0%)	
07/04 04:28:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 1.3829 (1.5463)	Architecture Loss 2.0216 (1.9903)	Prec@(1,5) (55.7%, 86.0%)	
07/04 04:29:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 1.5166 (1.5546)	Architecture Loss 2.0089 (1.9876)	Prec@(1,5) (55.5%, 85.9%)	
07/04 04:29:41午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 1.4662 (1.5652)	Architecture Loss 2.0388 (1.9929)	Prec@(1,5) (55.3%, 85.6%)	
07/04 04:29:41午後 searchStage_trainer.py:225 [INFO] Train: [ 20/49] Final Prec@1 55.3160%
07/04 04:29:45午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 1.9902	Prec@(1,5) (45.7%, 78.6%)
07/04 04:29:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 2.0042	Prec@(1,5) (46.1%, 78.1%)
07/04 04:29:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 2.0004	Prec@(1,5) (46.5%, 78.3%)
07/04 04:29:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 1.9886	Prec@(1,5) (46.6%, 78.5%)
07/04 04:30:00午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 1.9873	Prec@(1,5) (46.7%, 78.6%)
07/04 04:30:04午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 1.9837	Prec@(1,5) (46.9%, 78.6%)
07/04 04:30:07午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 1.9907	Prec@(1,5) (47.0%, 78.3%)
07/04 04:30:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 1.9926	Prec@(1,5) (46.9%, 78.3%)
07/04 04:30:10午後 searchStage_trainer.py:264 [INFO] Valid: [ 20/49] Final Prec@1 46.9120%
07/04 04:30:10午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('max_pool_3x3', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:30:11午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 46.9120%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2488, 0.2572, 0.2466],
        [0.2481, 0.2488, 0.2546, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2221, 0.2280, 0.3127, 0.2372],
        [0.2486, 0.2511, 0.2494, 0.2509],
        [0.2478, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2496, 0.2566, 0.2471],
        [0.2479, 0.2489, 0.2535, 0.2497],
        [0.2480, 0.2503, 0.2502, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2494, 0.2535, 0.2431],
        [0.2493, 0.2498, 0.2499, 0.2510],
        [0.2483, 0.2498, 0.2490, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2482, 0.2536, 0.2490],
        [0.2486, 0.2483, 0.2533, 0.2498],
        [0.2503, 0.2495, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2483, 0.2555, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2497, 0.2513, 0.2496],
        [0.2491, 0.2495, 0.2509, 0.2505],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2488, 0.2554, 0.2473],
        [0.2493, 0.2489, 0.2518, 0.2500],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2492, 0.2538, 0.2472],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2496, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2469, 0.2527, 0.2502],
        [0.2497, 0.2475, 0.2526, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2438, 0.2545, 0.2500],
        [0.2509, 0.2447, 0.2541, 0.2503],
        [0.2506, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2545, 0.2447, 0.2563, 0.2446],
        [0.2500, 0.2479, 0.2508, 0.2512],
        [0.2507, 0.2485, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2485, 0.2515, 0.2488],
        [0.2509, 0.2473, 0.2509, 0.2510],
        [0.2506, 0.2488, 0.2507, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2486, 0.2531, 0.2440],
        [0.2520, 0.2473, 0.2507, 0.2500],
        [0.2504, 0.2478, 0.2483, 0.2535]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2480, 0.2525, 0.2466],
        [0.2517, 0.2485, 0.2493, 0.2504],
        [0.2496, 0.2482, 0.2507, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:30:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 1.6460 (1.4629)	Architecture Loss 1.3583 (1.9827)	Prec@(1,5) (58.0%, 87.6%)	
07/04 04:31:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.6188 (1.4620)	Architecture Loss 1.6822 (1.9419)	Prec@(1,5) (57.9%, 87.4%)	
07/04 04:31:34午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 1.4764 (1.4624)	Architecture Loss 1.8682 (1.9647)	Prec@(1,5) (57.6%, 87.5%)	
07/04 04:32:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 1.5994 (1.4719)	Architecture Loss 1.6664 (1.9704)	Prec@(1,5) (57.5%, 87.3%)	
07/04 04:32:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 1.3672 (1.4808)	Architecture Loss 1.8589 (1.9765)	Prec@(1,5) (57.4%, 87.1%)	
07/04 04:32:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 1.6858 (1.4889)	Architecture Loss 2.0497 (1.9791)	Prec@(1,5) (57.2%, 87.0%)	
07/04 04:33:26午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 1.7644 (1.5055)	Architecture Loss 1.7539 (1.9787)	Prec@(1,5) (56.7%, 86.6%)	
07/04 04:33:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.6579 (1.5142)	Architecture Loss 1.8879 (1.9705)	Prec@(1,5) (56.5%, 86.4%)	
07/04 04:33:48午後 searchStage_trainer.py:225 [INFO] Train: [ 21/49] Final Prec@1 56.5160%
07/04 04:33:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 1.9568	Prec@(1,5) (47.2%, 79.3%)
07/04 04:33:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 1.9733	Prec@(1,5) (47.0%, 78.7%)
07/04 04:33:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 1.9633	Prec@(1,5) (47.4%, 78.7%)
07/04 04:34:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 1.9543	Prec@(1,5) (47.5%, 78.8%)
07/04 04:34:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 1.9480	Prec@(1,5) (47.9%, 78.8%)
07/04 04:34:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 1.9468	Prec@(1,5) (47.9%, 78.9%)
07/04 04:34:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 1.9406	Prec@(1,5) (48.1%, 79.0%)
07/04 04:34:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 1.9415	Prec@(1,5) (48.2%, 79.0%)
07/04 04:34:17午後 searchStage_trainer.py:264 [INFO] Valid: [ 21/49] Final Prec@1 48.1560%
07/04 04:34:17午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:34:17午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 48.1560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2488, 0.2573, 0.2465],
        [0.2480, 0.2487, 0.2546, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2211, 0.2272, 0.3147, 0.2370],
        [0.2486, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2496, 0.2567, 0.2471],
        [0.2478, 0.2489, 0.2536, 0.2497],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2494, 0.2535, 0.2430],
        [0.2493, 0.2498, 0.2499, 0.2510],
        [0.2483, 0.2498, 0.2490, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2482, 0.2536, 0.2490],
        [0.2486, 0.2483, 0.2533, 0.2498],
        [0.2503, 0.2495, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2483, 0.2557, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2497, 0.2513, 0.2496],
        [0.2491, 0.2495, 0.2509, 0.2505],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2488, 0.2555, 0.2472],
        [0.2493, 0.2489, 0.2518, 0.2500],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2492, 0.2538, 0.2472],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2469, 0.2527, 0.2502],
        [0.2497, 0.2475, 0.2527, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2437, 0.2547, 0.2500],
        [0.2509, 0.2447, 0.2542, 0.2502],
        [0.2506, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2446, 0.2566, 0.2444],
        [0.2500, 0.2479, 0.2509, 0.2512],
        [0.2507, 0.2485, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2485, 0.2516, 0.2488],
        [0.2509, 0.2472, 0.2509, 0.2510],
        [0.2506, 0.2488, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2486, 0.2532, 0.2439],
        [0.2519, 0.2473, 0.2507, 0.2500],
        [0.2504, 0.2477, 0.2483, 0.2535]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2480, 0.2526, 0.2464],
        [0.2517, 0.2485, 0.2494, 0.2504],
        [0.2495, 0.2482, 0.2507, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:34:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 1.5790 (1.3428)	Architecture Loss 1.8524 (1.9281)	Prec@(1,5) (62.4%, 88.6%)	
07/04 04:35:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 1.3997 (1.3791)	Architecture Loss 2.3942 (1.9545)	Prec@(1,5) (61.2%, 87.9%)	
07/04 04:35:42午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 1.6481 (1.3958)	Architecture Loss 1.9878 (1.9277)	Prec@(1,5) (60.6%, 87.6%)	
07/04 04:36:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.7389 (1.4071)	Architecture Loss 1.6810 (1.9307)	Prec@(1,5) (60.2%, 87.5%)	
07/04 04:36:37午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 1.4654 (1.4159)	Architecture Loss 2.1279 (1.9365)	Prec@(1,5) (59.7%, 87.4%)	
07/04 04:37:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.4570 (1.4215)	Architecture Loss 2.0489 (1.9327)	Prec@(1,5) (59.5%, 87.4%)	
07/04 04:37:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 1.8582 (1.4363)	Architecture Loss 1.7337 (1.9369)	Prec@(1,5) (59.1%, 87.3%)	
07/04 04:37:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.6152 (1.4428)	Architecture Loss 1.8629 (1.9363)	Prec@(1,5) (59.0%, 87.1%)	
07/04 04:37:56午後 searchStage_trainer.py:225 [INFO] Train: [ 22/49] Final Prec@1 58.9600%
07/04 04:38:00午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 1.9881	Prec@(1,5) (47.9%, 78.7%)
07/04 04:38:04午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 2.0012	Prec@(1,5) (47.5%, 78.7%)
07/04 04:38:07午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 1.9979	Prec@(1,5) (47.7%, 78.6%)
07/04 04:38:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 2.0071	Prec@(1,5) (47.3%, 78.4%)
07/04 04:38:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 2.0207	Prec@(1,5) (47.2%, 78.1%)
07/04 04:38:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 2.0061	Prec@(1,5) (47.7%, 78.2%)
07/04 04:38:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 2.0025	Prec@(1,5) (47.7%, 78.4%)
07/04 04:38:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 1.9967	Prec@(1,5) (47.7%, 78.5%)
07/04 04:38:25午後 searchStage_trainer.py:264 [INFO] Valid: [ 22/49] Final Prec@1 47.7040%
07/04 04:38:25午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:38:26午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 48.1560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2488, 0.2574, 0.2465],
        [0.2480, 0.2487, 0.2547, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2203, 0.2267, 0.3162, 0.2368],
        [0.2486, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2496, 0.2568, 0.2471],
        [0.2478, 0.2488, 0.2537, 0.2497],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2494, 0.2536, 0.2430],
        [0.2493, 0.2498, 0.2499, 0.2510],
        [0.2483, 0.2498, 0.2490, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2482, 0.2536, 0.2490],
        [0.2485, 0.2482, 0.2534, 0.2498],
        [0.2503, 0.2495, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2482, 0.2558, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2497, 0.2513, 0.2496],
        [0.2491, 0.2495, 0.2509, 0.2505],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2488, 0.2556, 0.2472],
        [0.2493, 0.2489, 0.2518, 0.2500],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2539, 0.2472],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2469, 0.2528, 0.2502],
        [0.2497, 0.2475, 0.2527, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2436, 0.2548, 0.2500],
        [0.2509, 0.2446, 0.2543, 0.2502],
        [0.2506, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2544, 0.2445, 0.2569, 0.2442],
        [0.2499, 0.2479, 0.2509, 0.2513],
        [0.2507, 0.2485, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2485, 0.2516, 0.2487],
        [0.2508, 0.2472, 0.2509, 0.2510],
        [0.2506, 0.2488, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2486, 0.2533, 0.2438],
        [0.2519, 0.2473, 0.2508, 0.2500],
        [0.2503, 0.2477, 0.2484, 0.2536]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2480, 0.2527, 0.2463],
        [0.2517, 0.2485, 0.2494, 0.2504],
        [0.2495, 0.2482, 0.2507, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:38:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 1.5590 (1.3296)	Architecture Loss 2.3044 (1.8903)	Prec@(1,5) (61.4%, 89.4%)	
07/04 04:39:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.3646 (1.3297)	Architecture Loss 1.6197 (1.9051)	Prec@(1,5) (61.6%, 89.0%)	
07/04 04:39:49午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 1.3011 (1.3612)	Architecture Loss 2.0415 (1.9093)	Prec@(1,5) (60.8%, 88.6%)	
07/04 04:40:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.2499 (1.3616)	Architecture Loss 1.9099 (1.9173)	Prec@(1,5) (60.9%, 88.7%)	
07/04 04:40:45午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 1.4456 (1.3690)	Architecture Loss 1.4091 (1.9220)	Prec@(1,5) (60.7%, 88.5%)	
07/04 04:41:13午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 1.5845 (1.3748)	Architecture Loss 1.9164 (1.9259)	Prec@(1,5) (60.5%, 88.5%)	
07/04 04:41:41午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 1.4334 (1.3824)	Architecture Loss 2.2831 (1.9317)	Prec@(1,5) (60.3%, 88.3%)	
07/04 04:42:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.5124 (1.3865)	Architecture Loss 1.7674 (1.9292)	Prec@(1,5) (60.1%, 88.2%)	
07/04 04:42:03午後 searchStage_trainer.py:225 [INFO] Train: [ 23/49] Final Prec@1 60.1160%
07/04 04:42:07午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 1.9396	Prec@(1,5) (48.4%, 80.0%)
07/04 04:42:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 1.9361	Prec@(1,5) (48.7%, 79.8%)
07/04 04:42:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 1.9221	Prec@(1,5) (49.0%, 80.1%)
07/04 04:42:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 1.9245	Prec@(1,5) (48.8%, 80.0%)
07/04 04:42:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 1.9236	Prec@(1,5) (48.5%, 80.1%)
07/04 04:42:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 1.9307	Prec@(1,5) (48.6%, 79.9%)
07/04 04:42:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 1.9253	Prec@(1,5) (48.8%, 80.0%)
07/04 04:42:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 1.9253	Prec@(1,5) (48.9%, 80.0%)
07/04 04:42:33午後 searchStage_trainer.py:264 [INFO] Valid: [ 23/49] Final Prec@1 48.9240%
07/04 04:42:33午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:42:34午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 48.9240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2488, 0.2574, 0.2465],
        [0.2480, 0.2487, 0.2547, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2195, 0.2261, 0.3177, 0.2367],
        [0.2486, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2495, 0.2569, 0.2471],
        [0.2478, 0.2488, 0.2538, 0.2497],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2536, 0.2429],
        [0.2493, 0.2498, 0.2499, 0.2510],
        [0.2483, 0.2498, 0.2490, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2537, 0.2490],
        [0.2485, 0.2482, 0.2534, 0.2498],
        [0.2503, 0.2496, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2482, 0.2560, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2498, 0.2513, 0.2496],
        [0.2491, 0.2495, 0.2509, 0.2505],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2488, 0.2556, 0.2471],
        [0.2493, 0.2489, 0.2519, 0.2500],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2539, 0.2471],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2468, 0.2528, 0.2502],
        [0.2497, 0.2475, 0.2527, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2436, 0.2548, 0.2500],
        [0.2508, 0.2446, 0.2544, 0.2502],
        [0.2506, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2444, 0.2573, 0.2440],
        [0.2499, 0.2479, 0.2509, 0.2513],
        [0.2507, 0.2485, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2485, 0.2517, 0.2487],
        [0.2508, 0.2472, 0.2509, 0.2510],
        [0.2506, 0.2488, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2486, 0.2534, 0.2437],
        [0.2519, 0.2473, 0.2508, 0.2500],
        [0.2503, 0.2477, 0.2484, 0.2536]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2480, 0.2529, 0.2462],
        [0.2517, 0.2485, 0.2494, 0.2504],
        [0.2495, 0.2482, 0.2507, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:43:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 1.3805 (1.2745)	Architecture Loss 1.6338 (1.9375)	Prec@(1,5) (62.8%, 89.2%)	
07/04 04:43:57午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.1803 (1.2651)	Architecture Loss 1.6952 (1.9090)	Prec@(1,5) (62.8%, 89.8%)	
07/04 04:44:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 1.3530 (1.2764)	Architecture Loss 1.9159 (1.9122)	Prec@(1,5) (62.5%, 90.0%)	
07/04 04:45:19午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.4432 (1.2845)	Architecture Loss 2.2671 (1.9120)	Prec@(1,5) (62.4%, 89.8%)	
07/04 04:46:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 1.3866 (1.3045)	Architecture Loss 1.9706 (1.9288)	Prec@(1,5) (61.8%, 89.5%)	
07/04 04:46:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 1.2302 (1.3180)	Architecture Loss 1.9603 (1.9278)	Prec@(1,5) (61.5%, 89.3%)	
07/04 04:47:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 1.1201 (1.3219)	Architecture Loss 2.1200 (1.9160)	Prec@(1,5) (61.5%, 89.2%)	
07/04 04:47:49午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.4472 (1.3268)	Architecture Loss 2.2068 (1.9126)	Prec@(1,5) (61.4%, 89.1%)	
07/04 04:47:49午後 searchStage_trainer.py:225 [INFO] Train: [ 24/49] Final Prec@1 61.4120%
07/04 04:47:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 1.8689	Prec@(1,5) (50.3%, 81.2%)
07/04 04:48:00午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 1.8897	Prec@(1,5) (49.3%, 81.1%)
07/04 04:48:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 1.9048	Prec@(1,5) (49.2%, 80.6%)
07/04 04:48:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 1.9122	Prec@(1,5) (49.0%, 80.3%)
07/04 04:48:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 1.9160	Prec@(1,5) (49.0%, 80.2%)
07/04 04:48:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 1.9274	Prec@(1,5) (48.8%, 79.9%)
07/04 04:48:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 1.9236	Prec@(1,5) (48.9%, 80.0%)
07/04 04:48:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 1.9232	Prec@(1,5) (48.9%, 80.0%)
07/04 04:48:32午後 searchStage_trainer.py:264 [INFO] Valid: [ 24/49] Final Prec@1 48.9120%
07/04 04:48:32午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:48:33午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 48.9240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2513, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2487, 0.2575, 0.2465],
        [0.2480, 0.2487, 0.2548, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2188, 0.2257, 0.3190, 0.2365],
        [0.2486, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2506, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2495, 0.2569, 0.2471],
        [0.2477, 0.2488, 0.2539, 0.2497],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2536, 0.2429],
        [0.2493, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2537, 0.2490],
        [0.2485, 0.2482, 0.2534, 0.2498],
        [0.2503, 0.2496, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2481, 0.2561, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2497, 0.2513, 0.2496],
        [0.2491, 0.2495, 0.2509, 0.2505],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2488, 0.2557, 0.2471],
        [0.2492, 0.2489, 0.2519, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2540, 0.2471],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2468, 0.2528, 0.2502],
        [0.2496, 0.2475, 0.2527, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2435, 0.2549, 0.2499],
        [0.2508, 0.2445, 0.2545, 0.2502],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2444, 0.2576, 0.2438],
        [0.2499, 0.2479, 0.2509, 0.2513],
        [0.2507, 0.2485, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2485, 0.2517, 0.2486],
        [0.2508, 0.2472, 0.2509, 0.2511],
        [0.2506, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2486, 0.2535, 0.2436],
        [0.2518, 0.2473, 0.2509, 0.2500],
        [0.2503, 0.2477, 0.2484, 0.2536]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2530, 0.2480, 0.2530, 0.2460],
        [0.2516, 0.2485, 0.2495, 0.2504],
        [0.2494, 0.2482, 0.2507, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:49:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 1.2683 (1.2549)	Architecture Loss 2.5466 (1.8954)	Prec@(1,5) (62.7%, 90.5%)	
07/04 04:49:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.0475 (1.2526)	Architecture Loss 1.7424 (1.8933)	Prec@(1,5) (62.8%, 90.4%)	
07/04 04:50:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 0.8524 (1.2635)	Architecture Loss 1.9755 (1.9048)	Prec@(1,5) (62.6%, 90.3%)	
07/04 04:51:19午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 1.1118 (1.2637)	Architecture Loss 1.8668 (1.8959)	Prec@(1,5) (62.7%, 90.2%)	
07/04 04:52:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 1.3076 (1.2631)	Architecture Loss 1.9493 (1.8862)	Prec@(1,5) (62.8%, 90.0%)	
07/04 04:52:41午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.2904 (1.2691)	Architecture Loss 2.0390 (1.8944)	Prec@(1,5) (62.8%, 90.0%)	
07/04 04:53:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 1.4006 (1.2782)	Architecture Loss 1.6932 (1.8961)	Prec@(1,5) (62.6%, 89.8%)	
07/04 04:53:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.2708 (1.2809)	Architecture Loss 2.1157 (1.8997)	Prec@(1,5) (62.4%, 89.8%)	
07/04 04:53:55午後 searchStage_trainer.py:225 [INFO] Train: [ 25/49] Final Prec@1 62.4320%
07/04 04:54:00午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 1.9582	Prec@(1,5) (48.4%, 79.3%)
07/04 04:54:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 1.9665	Prec@(1,5) (48.6%, 79.3%)
07/04 04:54:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 1.9611	Prec@(1,5) (48.9%, 79.6%)
07/04 04:54:16午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 1.9580	Prec@(1,5) (49.3%, 79.6%)
07/04 04:54:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 1.9639	Prec@(1,5) (49.2%, 79.4%)
07/04 04:54:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 1.9514	Prec@(1,5) (49.3%, 79.6%)
07/04 04:54:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 1.9475	Prec@(1,5) (49.4%, 79.7%)
07/04 04:54:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 1.9386	Prec@(1,5) (49.4%, 79.8%)
07/04 04:54:37午後 searchStage_trainer.py:264 [INFO] Valid: [ 25/49] Final Prec@1 49.4360%
07/04 04:54:37午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 04:54:38午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 49.4360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2487, 0.2575, 0.2465],
        [0.2480, 0.2487, 0.2548, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2181, 0.2252, 0.3202, 0.2364],
        [0.2486, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2506, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2495, 0.2570, 0.2471],
        [0.2477, 0.2487, 0.2539, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2537, 0.2428],
        [0.2493, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2537, 0.2490],
        [0.2485, 0.2482, 0.2535, 0.2498],
        [0.2503, 0.2496, 0.2496, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2481, 0.2562, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2497, 0.2513, 0.2497],
        [0.2491, 0.2495, 0.2509, 0.2505],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2488, 0.2557, 0.2471],
        [0.2492, 0.2489, 0.2519, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2540, 0.2471],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2468, 0.2528, 0.2502],
        [0.2496, 0.2475, 0.2527, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2435, 0.2550, 0.2499],
        [0.2508, 0.2445, 0.2546, 0.2502],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2443, 0.2579, 0.2435],
        [0.2499, 0.2479, 0.2509, 0.2513],
        [0.2506, 0.2485, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2486, 0.2518, 0.2486],
        [0.2508, 0.2472, 0.2510, 0.2511],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2486, 0.2536, 0.2435],
        [0.2518, 0.2473, 0.2509, 0.2500],
        [0.2503, 0.2477, 0.2484, 0.2536]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2481, 0.2531, 0.2459],
        [0.2516, 0.2485, 0.2495, 0.2504],
        [0.2494, 0.2482, 0.2507, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 04:55:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 1.4940 (1.1371)	Architecture Loss 2.2711 (1.8256)	Prec@(1,5) (66.1%, 92.2%)	
07/04 04:56:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 1.1082 (1.1525)	Architecture Loss 2.0030 (1.8568)	Prec@(1,5) (66.4%, 91.7%)	
07/04 04:56:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 1.3565 (1.1840)	Architecture Loss 2.0008 (1.8805)	Prec@(1,5) (65.4%, 91.4%)	
07/04 04:57:24午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 1.4998 (1.1886)	Architecture Loss 1.7816 (1.8929)	Prec@(1,5) (65.1%, 91.2%)	
07/04 04:58:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 1.3324 (1.1957)	Architecture Loss 2.4612 (1.8994)	Prec@(1,5) (64.8%, 91.2%)	
07/04 04:58:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.4175 (1.2059)	Architecture Loss 1.7689 (1.8964)	Prec@(1,5) (64.5%, 91.0%)	
07/04 04:59:26午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 1.1571 (1.2138)	Architecture Loss 1.7167 (1.8963)	Prec@(1,5) (64.2%, 90.9%)	
07/04 05:00:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.5633 (1.2166)	Architecture Loss 1.8076 (1.8901)	Prec@(1,5) (64.2%, 90.7%)	
07/04 05:00:00午後 searchStage_trainer.py:225 [INFO] Train: [ 26/49] Final Prec@1 64.1720%
07/04 05:00:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 1.8742	Prec@(1,5) (51.4%, 81.2%)
07/04 05:00:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 1.8841	Prec@(1,5) (51.2%, 81.5%)
07/04 05:00:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 1.8843	Prec@(1,5) (51.1%, 81.2%)
07/04 05:00:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 1.8911	Prec@(1,5) (51.0%, 81.1%)
07/04 05:00:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 1.8794	Prec@(1,5) (51.4%, 81.3%)
07/04 05:00:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 1.8806	Prec@(1,5) (51.3%, 81.2%)
07/04 05:00:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 1.8821	Prec@(1,5) (51.1%, 81.1%)
07/04 05:00:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 1.8861	Prec@(1,5) (51.0%, 81.0%)
07/04 05:00:43午後 searchStage_trainer.py:264 [INFO] Valid: [ 26/49] Final Prec@1 50.9760%
07/04 05:00:43午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 05:00:44午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 50.9760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2487, 0.2576, 0.2465],
        [0.2479, 0.2486, 0.2549, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2175, 0.2248, 0.3214, 0.2363],
        [0.2486, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2506, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2495, 0.2571, 0.2471],
        [0.2476, 0.2487, 0.2540, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2537, 0.2428],
        [0.2493, 0.2497, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2537, 0.2490],
        [0.2485, 0.2482, 0.2535, 0.2498],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2480, 0.2563, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2497, 0.2513, 0.2497],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2488, 0.2558, 0.2471],
        [0.2492, 0.2489, 0.2519, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2492, 0.2541, 0.2471],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2468, 0.2528, 0.2502],
        [0.2496, 0.2475, 0.2527, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2434, 0.2551, 0.2499],
        [0.2507, 0.2445, 0.2547, 0.2502],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2443, 0.2582, 0.2433],
        [0.2498, 0.2479, 0.2509, 0.2513],
        [0.2506, 0.2485, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2486, 0.2518, 0.2485],
        [0.2507, 0.2472, 0.2510, 0.2511],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2487, 0.2537, 0.2434],
        [0.2518, 0.2473, 0.2510, 0.2500],
        [0.2502, 0.2477, 0.2484, 0.2536]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2481, 0.2532, 0.2457],
        [0.2516, 0.2485, 0.2495, 0.2503],
        [0.2494, 0.2482, 0.2507, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 05:01:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 1.0250 (1.0787)	Architecture Loss 2.0884 (1.8441)	Prec@(1,5) (67.8%, 92.8%)	
07/04 05:02:07午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 1.2603 (1.0819)	Architecture Loss 1.9946 (1.8907)	Prec@(1,5) (68.0%, 92.6%)	
07/04 05:02:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 1.4615 (1.1044)	Architecture Loss 1.6840 (1.8971)	Prec@(1,5) (67.5%, 92.4%)	
07/04 05:03:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 1.1382 (1.1209)	Architecture Loss 1.5008 (1.8874)	Prec@(1,5) (67.2%, 92.1%)	
07/04 05:04:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 1.2237 (1.1409)	Architecture Loss 1.6772 (1.8950)	Prec@(1,5) (66.3%, 91.9%)	
07/04 05:04:51午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 1.1506 (1.1481)	Architecture Loss 2.0610 (1.8827)	Prec@(1,5) (66.0%, 91.8%)	
07/04 05:05:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 1.2272 (1.1569)	Architecture Loss 2.0915 (1.8756)	Prec@(1,5) (65.9%, 91.6%)	
07/04 05:06:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 1.0188 (1.1627)	Architecture Loss 1.6503 (1.8770)	Prec@(1,5) (65.8%, 91.5%)	
07/04 05:06:05午後 searchStage_trainer.py:225 [INFO] Train: [ 27/49] Final Prec@1 65.8400%
07/04 05:06:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 1.8094	Prec@(1,5) (52.2%, 81.5%)
07/04 05:06:16午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 1.8337	Prec@(1,5) (51.9%, 81.5%)
07/04 05:06:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 1.8535	Prec@(1,5) (51.6%, 81.1%)
07/04 05:06:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 1.8662	Prec@(1,5) (51.3%, 80.9%)
07/04 05:06:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 1.8671	Prec@(1,5) (51.3%, 80.9%)
07/04 05:06:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 1.8637	Prec@(1,5) (51.2%, 81.0%)
07/04 05:06:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 1.8600	Prec@(1,5) (51.3%, 81.1%)
07/04 05:06:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 1.8631	Prec@(1,5) (51.3%, 81.1%)
07/04 05:06:48午後 searchStage_trainer.py:264 [INFO] Valid: [ 27/49] Final Prec@1 51.2560%
07/04 05:06:48午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 05:06:49午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 51.2560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2487, 0.2576, 0.2465],
        [0.2479, 0.2486, 0.2549, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2169, 0.2245, 0.3225, 0.2361],
        [0.2486, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2506, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2495, 0.2572, 0.2471],
        [0.2476, 0.2487, 0.2541, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2537, 0.2428],
        [0.2493, 0.2497, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2537, 0.2489],
        [0.2484, 0.2482, 0.2535, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2480, 0.2564, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2497],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2488, 0.2558, 0.2471],
        [0.2492, 0.2489, 0.2519, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2493, 0.2541, 0.2470],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2468, 0.2528, 0.2502],
        [0.2496, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2434, 0.2552, 0.2499],
        [0.2507, 0.2444, 0.2547, 0.2501],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2442, 0.2585, 0.2431],
        [0.2498, 0.2479, 0.2510, 0.2513],
        [0.2506, 0.2485, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2486, 0.2518, 0.2485],
        [0.2507, 0.2472, 0.2510, 0.2511],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2487, 0.2538, 0.2433],
        [0.2517, 0.2473, 0.2510, 0.2500],
        [0.2502, 0.2477, 0.2484, 0.2536]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2482, 0.2533, 0.2456],
        [0.2516, 0.2485, 0.2496, 0.2503],
        [0.2494, 0.2482, 0.2507, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 05:07:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 0.8231 (0.9748)	Architecture Loss 1.9864 (1.8112)	Prec@(1,5) (71.2%, 94.2%)	
07/04 05:08:13午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 1.3714 (1.0355)	Architecture Loss 1.4357 (1.8501)	Prec@(1,5) (69.0%, 93.6%)	
07/04 05:08:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 1.0837 (1.0480)	Architecture Loss 1.6071 (1.8554)	Prec@(1,5) (68.8%, 93.2%)	
07/04 05:09:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.8936 (1.0657)	Architecture Loss 1.9658 (1.8667)	Prec@(1,5) (68.3%, 92.8%)	
07/04 05:10:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 0.9379 (1.0761)	Architecture Loss 1.7517 (1.8768)	Prec@(1,5) (68.1%, 92.7%)	
07/04 05:10:57午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 1.4894 (1.0878)	Architecture Loss 1.5718 (1.8694)	Prec@(1,5) (67.7%, 92.5%)	
07/04 05:11:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][350/390]	Step 11298	lr 0.01075	Loss 1.1362 (1.1003)	Architecture Loss 2.0661 (1.8686)	Prec@(1,5) (67.4%, 92.3%)	
07/04 05:12:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 1.1227 (1.1090)	Architecture Loss 1.4759 (1.8592)	Prec@(1,5) (67.2%, 92.2%)	
07/04 05:12:11午後 searchStage_trainer.py:225 [INFO] Train: [ 28/49] Final Prec@1 67.1440%
07/04 05:12:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][50/391]	Step 11339	Loss 1.9198	Prec@(1,5) (51.1%, 80.5%)
07/04 05:12:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 1.9294	Prec@(1,5) (50.8%, 80.7%)
07/04 05:12:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][150/391]	Step 11339	Loss 1.9385	Prec@(1,5) (50.6%, 80.6%)
07/04 05:12:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 1.9367	Prec@(1,5) (50.5%, 80.7%)
07/04 05:12:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][250/391]	Step 11339	Loss 1.9348	Prec@(1,5) (50.4%, 80.7%)
07/04 05:12:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 1.9295	Prec@(1,5) (50.4%, 80.8%)
07/04 05:12:50午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][350/391]	Step 11339	Loss 1.9308	Prec@(1,5) (50.5%, 80.6%)
07/04 05:12:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 1.9303	Prec@(1,5) (50.5%, 80.7%)
07/04 05:12:54午後 searchStage_trainer.py:264 [INFO] Valid: [ 28/49] Final Prec@1 50.5160%
07/04 05:12:54午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 05:12:55午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 51.2560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2487, 0.2577, 0.2465],
        [0.2479, 0.2486, 0.2549, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2164, 0.2241, 0.3236, 0.2360],
        [0.2486, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2506, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2495, 0.2572, 0.2471],
        [0.2476, 0.2487, 0.2541, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2538, 0.2428],
        [0.2493, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2538, 0.2489],
        [0.2484, 0.2482, 0.2535, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2479, 0.2566, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2497],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2488, 0.2559, 0.2471],
        [0.2492, 0.2489, 0.2519, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2493, 0.2542, 0.2470],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2468, 0.2529, 0.2502],
        [0.2496, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2434, 0.2553, 0.2499],
        [0.2507, 0.2444, 0.2548, 0.2501],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2442, 0.2588, 0.2428],
        [0.2498, 0.2479, 0.2510, 0.2513],
        [0.2506, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2486, 0.2519, 0.2485],
        [0.2507, 0.2472, 0.2510, 0.2511],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2487, 0.2538, 0.2432],
        [0.2517, 0.2473, 0.2510, 0.2500],
        [0.2502, 0.2477, 0.2484, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2482, 0.2534, 0.2455],
        [0.2516, 0.2485, 0.2496, 0.2503],
        [0.2494, 0.2482, 0.2507, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 05:13:37午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][50/390]	Step 11389	lr 0.01002	Loss 0.9936 (0.9971)	Architecture Loss 1.3869 (1.8701)	Prec@(1,5) (70.1%, 93.2%)	
07/04 05:14:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 1.0267 (1.0036)	Architecture Loss 1.7620 (1.8724)	Prec@(1,5) (69.5%, 93.6%)	
07/04 05:14:59午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][150/390]	Step 11489	lr 0.01002	Loss 1.1747 (1.0163)	Architecture Loss 1.9005 (1.8737)	Prec@(1,5) (68.9%, 93.7%)	
07/04 05:15:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.7984 (1.0294)	Architecture Loss 2.0009 (1.8963)	Prec@(1,5) (68.8%, 93.4%)	
07/04 05:16:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][250/390]	Step 11589	lr 0.01002	Loss 1.2001 (1.0354)	Architecture Loss 1.8692 (1.8797)	Prec@(1,5) (68.6%, 93.3%)	
07/04 05:17:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 1.0203 (1.0403)	Architecture Loss 1.7491 (1.8777)	Prec@(1,5) (68.4%, 93.2%)	
07/04 05:17:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][350/390]	Step 11689	lr 0.01002	Loss 1.2051 (1.0478)	Architecture Loss 1.5448 (1.8804)	Prec@(1,5) (68.3%, 93.1%)	
07/04 05:18:15午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 1.0955 (1.0505)	Architecture Loss 2.2509 (1.8768)	Prec@(1,5) (68.2%, 93.0%)	
07/04 05:18:16午後 searchStage_trainer.py:225 [INFO] Train: [ 29/49] Final Prec@1 68.2280%
07/04 05:18:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][50/391]	Step 11730	Loss 1.8809	Prec@(1,5) (51.9%, 81.7%)
07/04 05:18:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 1.8788	Prec@(1,5) (51.8%, 81.7%)
07/04 05:18:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][150/391]	Step 11730	Loss 1.8619	Prec@(1,5) (51.9%, 82.3%)
07/04 05:18:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 1.8628	Prec@(1,5) (51.6%, 82.1%)
07/04 05:18:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][250/391]	Step 11730	Loss 1.8697	Prec@(1,5) (51.7%, 81.8%)
07/04 05:18:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 1.8651	Prec@(1,5) (51.8%, 81.8%)
07/04 05:18:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][350/391]	Step 11730	Loss 1.8712	Prec@(1,5) (51.6%, 81.7%)
07/04 05:18:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 1.8660	Prec@(1,5) (51.7%, 81.7%)
07/04 05:18:59午後 searchStage_trainer.py:264 [INFO] Valid: [ 29/49] Final Prec@1 51.6960%
07/04 05:18:59午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 05:19:00午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 51.6960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2486, 0.2577, 0.2465],
        [0.2479, 0.2486, 0.2550, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2158, 0.2237, 0.3246, 0.2359],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2494, 0.2573, 0.2471],
        [0.2476, 0.2486, 0.2542, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2538, 0.2427],
        [0.2493, 0.2497, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2538, 0.2489],
        [0.2484, 0.2482, 0.2535, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2478, 0.2567, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2504],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2497],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2488, 0.2559, 0.2471],
        [0.2492, 0.2489, 0.2519, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2493, 0.2542, 0.2470],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2468, 0.2529, 0.2502],
        [0.2496, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2433, 0.2553, 0.2499],
        [0.2506, 0.2444, 0.2549, 0.2501],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2442, 0.2591, 0.2426],
        [0.2498, 0.2479, 0.2510, 0.2513],
        [0.2506, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2486, 0.2519, 0.2485],
        [0.2507, 0.2472, 0.2510, 0.2511],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2543, 0.2487, 0.2539, 0.2431],
        [0.2517, 0.2473, 0.2511, 0.2500],
        [0.2502, 0.2477, 0.2484, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2483, 0.2535, 0.2453],
        [0.2515, 0.2485, 0.2496, 0.2503],
        [0.2493, 0.2482, 0.2507, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 05:19:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][50/390]	Step 11780	lr 0.00929	Loss 1.1753 (0.9542)	Architecture Loss 1.8512 (1.9240)	Prec@(1,5) (72.1%, 93.9%)	
07/04 05:20:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 0.8506 (0.9485)	Architecture Loss 1.7890 (1.8717)	Prec@(1,5) (71.9%, 94.4%)	
07/04 05:21:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][150/390]	Step 11880	lr 0.00929	Loss 0.7211 (0.9688)	Architecture Loss 2.1326 (1.8796)	Prec@(1,5) (71.3%, 94.1%)	
07/04 05:21:45午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 0.9217 (0.9669)	Architecture Loss 1.5703 (1.8927)	Prec@(1,5) (71.0%, 94.1%)	
07/04 05:22:26午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][250/390]	Step 11980	lr 0.00929	Loss 1.0598 (0.9831)	Architecture Loss 1.7513 (1.8969)	Prec@(1,5) (70.7%, 93.9%)	
07/04 05:23:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 1.2759 (0.9945)	Architecture Loss 1.5366 (1.8879)	Prec@(1,5) (70.2%, 93.8%)	
07/04 05:23:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][350/390]	Step 12080	lr 0.00929	Loss 1.3902 (1.0024)	Architecture Loss 1.9237 (1.8881)	Prec@(1,5) (70.1%, 93.7%)	
07/04 05:24:20午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 0.7317 (1.0014)	Architecture Loss 2.0772 (1.8829)	Prec@(1,5) (70.1%, 93.6%)	
07/04 05:24:21午後 searchStage_trainer.py:225 [INFO] Train: [ 30/49] Final Prec@1 70.1080%
07/04 05:24:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][50/391]	Step 12121	Loss 1.8485	Prec@(1,5) (52.7%, 82.3%)
07/04 05:24:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 1.8451	Prec@(1,5) (52.8%, 82.3%)
07/04 05:24:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][150/391]	Step 12121	Loss 1.8572	Prec@(1,5) (52.7%, 82.2%)
07/04 05:24:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 1.8671	Prec@(1,5) (52.6%, 82.2%)
07/04 05:24:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][250/391]	Step 12121	Loss 1.8771	Prec@(1,5) (52.4%, 81.9%)
07/04 05:24:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 1.8784	Prec@(1,5) (52.3%, 81.9%)
07/04 05:24:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][350/391]	Step 12121	Loss 1.8683	Prec@(1,5) (52.6%, 82.1%)
07/04 05:25:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 1.8692	Prec@(1,5) (52.6%, 82.1%)
07/04 05:25:03午後 searchStage_trainer.py:264 [INFO] Valid: [ 30/49] Final Prec@1 52.5600%
07/04 05:25:04午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 05:25:04午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 52.5600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2486, 0.2578, 0.2465],
        [0.2479, 0.2486, 0.2550, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2153, 0.2233, 0.3256, 0.2358],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2494, 0.2573, 0.2471],
        [0.2475, 0.2486, 0.2542, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2538, 0.2427],
        [0.2493, 0.2497, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2538, 0.2489],
        [0.2484, 0.2482, 0.2536, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2478, 0.2568, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2497],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2488, 0.2560, 0.2471],
        [0.2492, 0.2490, 0.2519, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2493, 0.2542, 0.2470],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2468, 0.2529, 0.2502],
        [0.2496, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2433, 0.2554, 0.2499],
        [0.2506, 0.2444, 0.2550, 0.2501],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2442, 0.2594, 0.2423],
        [0.2497, 0.2479, 0.2510, 0.2513],
        [0.2506, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2486, 0.2520, 0.2484],
        [0.2506, 0.2472, 0.2510, 0.2511],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2488, 0.2540, 0.2430],
        [0.2516, 0.2473, 0.2511, 0.2500],
        [0.2502, 0.2477, 0.2484, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2483, 0.2536, 0.2452],
        [0.2515, 0.2486, 0.2496, 0.2503],
        [0.2493, 0.2482, 0.2507, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 05:25:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][50/390]	Step 12171	lr 0.00858	Loss 0.8689 (0.8869)	Architecture Loss 2.6718 (1.9301)	Prec@(1,5) (73.2%, 94.8%)	
07/04 05:26:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.9756 (0.8957)	Architecture Loss 2.5038 (1.8761)	Prec@(1,5) (73.0%, 94.7%)	
07/04 05:27:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][150/390]	Step 12271	lr 0.00858	Loss 0.8327 (0.8922)	Architecture Loss 1.8811 (1.8648)	Prec@(1,5) (73.2%, 94.7%)	
07/04 05:27:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.7370 (0.9019)	Architecture Loss 2.1775 (1.8796)	Prec@(1,5) (72.8%, 94.7%)	
07/04 05:28:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][250/390]	Step 12371	lr 0.00858	Loss 0.6729 (0.9093)	Architecture Loss 1.9315 (1.8783)	Prec@(1,5) (72.5%, 94.6%)	
07/04 05:29:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 1.1773 (0.9246)	Architecture Loss 1.7704 (1.8751)	Prec@(1,5) (72.0%, 94.5%)	
07/04 05:29:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][350/390]	Step 12471	lr 0.00858	Loss 0.9612 (0.9358)	Architecture Loss 1.4994 (1.8739)	Prec@(1,5) (71.7%, 94.4%)	
07/04 05:30:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 0.8643 (0.9413)	Architecture Loss 1.7317 (1.8756)	Prec@(1,5) (71.6%, 94.3%)	
07/04 05:30:26午後 searchStage_trainer.py:225 [INFO] Train: [ 31/49] Final Prec@1 71.6360%
07/04 05:30:31午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][50/391]	Step 12512	Loss 1.8613	Prec@(1,5) (52.3%, 82.1%)
07/04 05:30:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 1.8402	Prec@(1,5) (52.1%, 82.7%)
07/04 05:30:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][150/391]	Step 12512	Loss 1.8261	Prec@(1,5) (52.6%, 82.6%)
07/04 05:30:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 1.8315	Prec@(1,5) (53.1%, 82.4%)
07/04 05:30:53午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][250/391]	Step 12512	Loss 1.8268	Prec@(1,5) (53.2%, 82.3%)
07/04 05:30:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 1.8313	Prec@(1,5) (53.0%, 82.3%)
07/04 05:31:04午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][350/391]	Step 12512	Loss 1.8350	Prec@(1,5) (52.9%, 82.3%)
07/04 05:31:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 1.8348	Prec@(1,5) (53.0%, 82.3%)
07/04 05:31:09午後 searchStage_trainer.py:264 [INFO] Valid: [ 31/49] Final Prec@1 53.0040%
07/04 05:31:09午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 05:31:09午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 53.0040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2486, 0.2578, 0.2465],
        [0.2479, 0.2486, 0.2550, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2148, 0.2230, 0.3264, 0.2357],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2498, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2494, 0.2574, 0.2472],
        [0.2475, 0.2486, 0.2543, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2538, 0.2427],
        [0.2493, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2538, 0.2489],
        [0.2484, 0.2482, 0.2536, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2478, 0.2569, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2497],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2488, 0.2560, 0.2471],
        [0.2492, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2493, 0.2543, 0.2469],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2468, 0.2529, 0.2502],
        [0.2496, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2433, 0.2555, 0.2499],
        [0.2506, 0.2443, 0.2550, 0.2501],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2441, 0.2597, 0.2420],
        [0.2497, 0.2479, 0.2510, 0.2514],
        [0.2506, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2487, 0.2520, 0.2484],
        [0.2506, 0.2472, 0.2510, 0.2511],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2488, 0.2541, 0.2429],
        [0.2516, 0.2473, 0.2511, 0.2500],
        [0.2502, 0.2477, 0.2484, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2484, 0.2536, 0.2451],
        [0.2515, 0.2486, 0.2497, 0.2503],
        [0.2493, 0.2482, 0.2507, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 05:31:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][50/390]	Step 12562	lr 0.00789	Loss 1.1312 (0.8425)	Architecture Loss 2.1795 (1.8215)	Prec@(1,5) (74.5%, 95.6%)	
07/04 05:32:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.8743 (0.8259)	Architecture Loss 1.8642 (1.8331)	Prec@(1,5) (75.3%, 95.4%)	
07/04 05:33:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][150/390]	Step 12662	lr 0.00789	Loss 0.7490 (0.8387)	Architecture Loss 1.4456 (1.8527)	Prec@(1,5) (74.6%, 95.3%)	
07/04 05:33:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.8408 (0.8361)	Architecture Loss 2.1187 (1.8581)	Prec@(1,5) (74.8%, 95.3%)	
07/04 05:34:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][250/390]	Step 12762	lr 0.00789	Loss 0.9969 (0.8533)	Architecture Loss 1.8304 (1.8679)	Prec@(1,5) (74.3%, 95.2%)	
07/04 05:35:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.7845 (0.8559)	Architecture Loss 1.6825 (1.8656)	Prec@(1,5) (74.2%, 95.2%)	
07/04 05:35:57午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][350/390]	Step 12862	lr 0.00789	Loss 0.8765 (0.8704)	Architecture Loss 1.5153 (1.8631)	Prec@(1,5) (73.8%, 95.0%)	
07/04 05:36:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 1.0805 (0.8793)	Architecture Loss 1.6310 (1.8638)	Prec@(1,5) (73.6%, 94.9%)	
07/04 05:36:31午後 searchStage_trainer.py:225 [INFO] Train: [ 32/49] Final Prec@1 73.6080%
07/04 05:36:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][50/391]	Step 12903	Loss 1.8330	Prec@(1,5) (53.2%, 82.7%)
07/04 05:36:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 1.8109	Prec@(1,5) (53.5%, 83.1%)
07/04 05:36:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][150/391]	Step 12903	Loss 1.8588	Prec@(1,5) (53.4%, 82.6%)
07/04 05:36:53午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 1.8601	Prec@(1,5) (53.6%, 82.5%)
07/04 05:36:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][250/391]	Step 12903	Loss 1.8618	Prec@(1,5) (53.6%, 82.5%)
07/04 05:37:04午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 1.8639	Prec@(1,5) (53.5%, 82.4%)
07/04 05:37:09午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][350/391]	Step 12903	Loss 1.8687	Prec@(1,5) (53.5%, 82.4%)
07/04 05:37:13午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 1.8626	Prec@(1,5) (53.5%, 82.4%)
07/04 05:37:14午後 searchStage_trainer.py:264 [INFO] Valid: [ 32/49] Final Prec@1 53.5400%
07/04 05:37:14午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 05:37:14午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 53.5400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2486, 0.2578, 0.2465],
        [0.2479, 0.2485, 0.2550, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2143, 0.2227, 0.3273, 0.2357],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2494, 0.2574, 0.2472],
        [0.2475, 0.2486, 0.2543, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2538, 0.2427],
        [0.2493, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2538, 0.2489],
        [0.2484, 0.2482, 0.2536, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2477, 0.2570, 0.2472],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2497],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2488, 0.2560, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2493, 0.2543, 0.2469],
        [0.2502, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2529, 0.2502],
        [0.2496, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2433, 0.2555, 0.2499],
        [0.2505, 0.2443, 0.2551, 0.2500],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2441, 0.2600, 0.2418],
        [0.2497, 0.2479, 0.2510, 0.2514],
        [0.2506, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2487, 0.2520, 0.2483],
        [0.2506, 0.2472, 0.2511, 0.2511],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2488, 0.2542, 0.2428],
        [0.2516, 0.2473, 0.2512, 0.2500],
        [0.2501, 0.2477, 0.2484, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2484, 0.2537, 0.2449],
        [0.2515, 0.2486, 0.2497, 0.2503],
        [0.2493, 0.2482, 0.2507, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 05:37:57午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][50/390]	Step 12953	lr 0.00722	Loss 0.7087 (0.7632)	Architecture Loss 1.6407 (1.9193)	Prec@(1,5) (76.9%, 96.6%)	
07/04 05:38:37午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.7059 (0.7847)	Architecture Loss 1.8074 (1.8735)	Prec@(1,5) (76.1%, 96.4%)	
07/04 05:39:19午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][150/390]	Step 13053	lr 0.00722	Loss 0.8434 (0.7850)	Architecture Loss 1.8308 (1.8723)	Prec@(1,5) (76.0%, 96.3%)	
07/04 05:39:59午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.7639 (0.8016)	Architecture Loss 1.9449 (1.8833)	Prec@(1,5) (75.4%, 96.0%)	
07/04 05:40:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][250/390]	Step 13153	lr 0.00722	Loss 0.7822 (0.8139)	Architecture Loss 1.7649 (1.8860)	Prec@(1,5) (74.9%, 95.8%)	
07/04 05:41:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.9186 (0.8213)	Architecture Loss 2.0751 (1.8813)	Prec@(1,5) (74.6%, 95.7%)	
07/04 05:42:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][350/390]	Step 13253	lr 0.00722	Loss 0.9207 (0.8321)	Architecture Loss 2.0707 (1.8840)	Prec@(1,5) (74.3%, 95.5%)	
07/04 05:42:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 0.8552 (0.8380)	Architecture Loss 1.5474 (1.8790)	Prec@(1,5) (74.1%, 95.4%)	
07/04 05:42:36午後 searchStage_trainer.py:225 [INFO] Train: [ 33/49] Final Prec@1 74.1240%
07/04 05:42:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][50/391]	Step 13294	Loss 1.8510	Prec@(1,5) (53.5%, 82.4%)
07/04 05:42:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 1.8544	Prec@(1,5) (53.6%, 82.4%)
07/04 05:42:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][150/391]	Step 13294	Loss 1.8472	Prec@(1,5) (53.7%, 82.4%)
07/04 05:42:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 1.8405	Prec@(1,5) (53.9%, 82.5%)
07/04 05:43:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][250/391]	Step 13294	Loss 1.8346	Prec@(1,5) (53.9%, 82.6%)
07/04 05:43:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 1.8323	Prec@(1,5) (53.9%, 82.7%)
07/04 05:43:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][350/391]	Step 13294	Loss 1.8493	Prec@(1,5) (53.6%, 82.5%)
07/04 05:43:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 1.8456	Prec@(1,5) (53.6%, 82.5%)
07/04 05:43:18午後 searchStage_trainer.py:264 [INFO] Valid: [ 33/49] Final Prec@1 53.6400%
07/04 05:43:18午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 05:43:19午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 53.6400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2486, 0.2578, 0.2465],
        [0.2479, 0.2485, 0.2550, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2139, 0.2224, 0.3281, 0.2356],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2505, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2494, 0.2574, 0.2472],
        [0.2475, 0.2486, 0.2544, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2539, 0.2427],
        [0.2493, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2538, 0.2489],
        [0.2483, 0.2482, 0.2536, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2477, 0.2571, 0.2473],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2497],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2488, 0.2561, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2493, 0.2543, 0.2469],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2529, 0.2502],
        [0.2495, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2432, 0.2556, 0.2499],
        [0.2505, 0.2443, 0.2552, 0.2500],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2441, 0.2604, 0.2416],
        [0.2497, 0.2479, 0.2510, 0.2514],
        [0.2505, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2488, 0.2521, 0.2483],
        [0.2506, 0.2472, 0.2511, 0.2512],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2489, 0.2542, 0.2427],
        [0.2515, 0.2473, 0.2512, 0.2500],
        [0.2501, 0.2477, 0.2484, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2485, 0.2538, 0.2448],
        [0.2514, 0.2486, 0.2497, 0.2503],
        [0.2493, 0.2482, 0.2507, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 05:44:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][50/390]	Step 13344	lr 0.00657	Loss 0.7099 (0.7328)	Architecture Loss 1.3904 (1.9110)	Prec@(1,5) (77.1%, 96.7%)	
07/04 05:44:42午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 1.0053 (0.7312)	Architecture Loss 1.8981 (1.8903)	Prec@(1,5) (77.1%, 96.5%)	
07/04 05:45:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][150/390]	Step 13444	lr 0.00657	Loss 0.8313 (0.7438)	Architecture Loss 2.1654 (1.8826)	Prec@(1,5) (76.5%, 96.4%)	
07/04 05:46:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.6396 (0.7456)	Architecture Loss 1.9804 (1.8732)	Prec@(1,5) (76.5%, 96.4%)	
07/04 05:46:45午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][250/390]	Step 13544	lr 0.00657	Loss 0.7403 (0.7526)	Architecture Loss 1.2916 (1.8767)	Prec@(1,5) (76.5%, 96.3%)	
07/04 05:47:26午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.6618 (0.7578)	Architecture Loss 1.7683 (1.8947)	Prec@(1,5) (76.4%, 96.2%)	
07/04 05:48:07午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][350/390]	Step 13644	lr 0.00657	Loss 0.8225 (0.7672)	Architecture Loss 1.5914 (1.8940)	Prec@(1,5) (76.2%, 96.1%)	
07/04 05:48:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 0.9175 (0.7761)	Architecture Loss 1.1127 (1.8884)	Prec@(1,5) (75.9%, 96.0%)	
07/04 05:48:40午後 searchStage_trainer.py:225 [INFO] Train: [ 34/49] Final Prec@1 75.9040%
07/04 05:48:46午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][50/391]	Step 13685	Loss 1.8410	Prec@(1,5) (53.9%, 83.2%)
07/04 05:48:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 1.8168	Prec@(1,5) (54.0%, 83.2%)
07/04 05:48:57午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][150/391]	Step 13685	Loss 1.8273	Prec@(1,5) (53.6%, 83.2%)
07/04 05:49:02午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 1.8371	Prec@(1,5) (53.5%, 83.1%)
07/04 05:49:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][250/391]	Step 13685	Loss 1.8336	Prec@(1,5) (53.6%, 83.1%)
07/04 05:49:13午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 1.8406	Prec@(1,5) (53.7%, 83.0%)
07/04 05:49:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][350/391]	Step 13685	Loss 1.8471	Prec@(1,5) (53.8%, 82.9%)
07/04 05:49:23午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 1.8501	Prec@(1,5) (53.8%, 82.9%)
07/04 05:49:23午後 searchStage_trainer.py:264 [INFO] Valid: [ 34/49] Final Prec@1 53.8440%
07/04 05:49:23午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 05:49:23午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 53.8440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2485, 0.2579, 0.2465],
        [0.2479, 0.2485, 0.2551, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2135, 0.2221, 0.3288, 0.2356],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2493, 0.2574, 0.2472],
        [0.2475, 0.2486, 0.2544, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2539, 0.2426],
        [0.2493, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2538, 0.2489],
        [0.2483, 0.2482, 0.2536, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2476, 0.2571, 0.2473],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2498],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2488, 0.2561, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2493, 0.2543, 0.2469],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2529, 0.2502],
        [0.2495, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2432, 0.2556, 0.2498],
        [0.2505, 0.2443, 0.2552, 0.2500],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2440, 0.2606, 0.2414],
        [0.2497, 0.2479, 0.2510, 0.2514],
        [0.2505, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2488, 0.2521, 0.2482],
        [0.2506, 0.2472, 0.2511, 0.2512],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2489, 0.2543, 0.2426],
        [0.2515, 0.2473, 0.2512, 0.2500],
        [0.2501, 0.2477, 0.2484, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2485, 0.2539, 0.2447],
        [0.2514, 0.2486, 0.2497, 0.2503],
        [0.2493, 0.2482, 0.2507, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 05:50:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][50/390]	Step 13735	lr 0.00595	Loss 0.6562 (0.6436)	Architecture Loss 1.7301 (1.8740)	Prec@(1,5) (79.8%, 97.4%)	
07/04 05:50:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.7364 (0.6444)	Architecture Loss 1.9635 (1.9206)	Prec@(1,5) (80.1%, 97.4%)	
07/04 05:51:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][150/390]	Step 13835	lr 0.00595	Loss 0.5682 (0.6770)	Architecture Loss 1.7674 (1.9023)	Prec@(1,5) (78.8%, 97.0%)	
07/04 05:51:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 0.8032 (0.6883)	Architecture Loss 1.9766 (1.8967)	Prec@(1,5) (78.5%, 96.9%)	
07/04 05:52:15午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][250/390]	Step 13935	lr 0.00595	Loss 0.6841 (0.6950)	Architecture Loss 2.2661 (1.9162)	Prec@(1,5) (78.2%, 96.9%)	
07/04 05:52:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 0.8221 (0.6974)	Architecture Loss 1.7635 (1.9072)	Prec@(1,5) (78.2%, 96.8%)	
07/04 05:53:37午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][350/390]	Step 14035	lr 0.00595	Loss 0.6687 (0.7087)	Architecture Loss 2.4629 (1.8963)	Prec@(1,5) (78.0%, 96.7%)	
07/04 05:54:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 0.6612 (0.7117)	Architecture Loss 1.7010 (1.8896)	Prec@(1,5) (77.8%, 96.7%)	
07/04 05:54:10午後 searchStage_trainer.py:225 [INFO] Train: [ 35/49] Final Prec@1 77.7880%
07/04 05:54:16午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][50/391]	Step 14076	Loss 1.8815	Prec@(1,5) (54.0%, 82.8%)
07/04 05:54:21午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 1.8842	Prec@(1,5) (53.8%, 83.0%)
07/04 05:54:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][150/391]	Step 14076	Loss 1.8757	Prec@(1,5) (54.0%, 82.9%)
07/04 05:54:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 1.8665	Prec@(1,5) (54.1%, 83.1%)
07/04 05:54:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][250/391]	Step 14076	Loss 1.8638	Prec@(1,5) (54.2%, 83.1%)
07/04 05:54:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 1.8659	Prec@(1,5) (54.1%, 83.1%)
07/04 05:54:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][350/391]	Step 14076	Loss 1.8604	Prec@(1,5) (54.2%, 83.0%)
07/04 05:54:53午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 1.8633	Prec@(1,5) (54.2%, 82.9%)
07/04 05:54:53午後 searchStage_trainer.py:264 [INFO] Valid: [ 35/49] Final Prec@1 54.1400%
07/04 05:54:53午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 05:54:54午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 54.1400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2485, 0.2579, 0.2465],
        [0.2479, 0.2485, 0.2551, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2131, 0.2218, 0.3296, 0.2355],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2493, 0.2575, 0.2472],
        [0.2475, 0.2486, 0.2544, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2494, 0.2539, 0.2426],
        [0.2492, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2489],
        [0.2483, 0.2482, 0.2536, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2476, 0.2573, 0.2473],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2498],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2488, 0.2561, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2493, 0.2544, 0.2469],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2529, 0.2502],
        [0.2495, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2432, 0.2557, 0.2498],
        [0.2505, 0.2443, 0.2553, 0.2500],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2440, 0.2609, 0.2411],
        [0.2497, 0.2479, 0.2511, 0.2514],
        [0.2505, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2488, 0.2521, 0.2482],
        [0.2505, 0.2472, 0.2511, 0.2512],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2490, 0.2544, 0.2425],
        [0.2515, 0.2473, 0.2512, 0.2500],
        [0.2501, 0.2477, 0.2484, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2529, 0.2486, 0.2540, 0.2446],
        [0.2514, 0.2486, 0.2498, 0.2502],
        [0.2492, 0.2482, 0.2507, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 05:55:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][50/390]	Step 14126	lr 0.00535	Loss 0.6981 (0.6232)	Architecture Loss 2.1273 (1.8399)	Prec@(1,5) (80.8%, 97.4%)	
07/04 05:56:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.6497 (0.6366)	Architecture Loss 2.0745 (1.8403)	Prec@(1,5) (80.3%, 97.3%)	
07/04 05:56:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][150/390]	Step 14226	lr 0.00535	Loss 0.6039 (0.6274)	Architecture Loss 1.6918 (1.8437)	Prec@(1,5) (80.5%, 97.3%)	
07/04 05:57:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.4603 (0.6345)	Architecture Loss 1.5232 (1.8747)	Prec@(1,5) (80.1%, 97.3%)	
07/04 05:58:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][250/390]	Step 14326	lr 0.00535	Loss 0.8167 (0.6430)	Architecture Loss 2.2134 (1.8837)	Prec@(1,5) (79.8%, 97.2%)	
07/04 05:58:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.7118 (0.6496)	Architecture Loss 1.9471 (1.8892)	Prec@(1,5) (79.8%, 97.1%)	
07/04 05:59:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][350/390]	Step 14426	lr 0.00535	Loss 0.6583 (0.6574)	Architecture Loss 2.1052 (1.8893)	Prec@(1,5) (79.7%, 97.1%)	
07/04 06:00:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.4523 (0.6604)	Architecture Loss 2.3984 (1.8929)	Prec@(1,5) (79.5%, 97.1%)	
07/04 06:00:09午後 searchStage_trainer.py:225 [INFO] Train: [ 36/49] Final Prec@1 79.5240%
07/04 06:00:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][50/391]	Step 14467	Loss 1.8844	Prec@(1,5) (54.5%, 82.3%)
07/04 06:00:20午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 1.8690	Prec@(1,5) (54.7%, 82.6%)
07/04 06:00:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][150/391]	Step 14467	Loss 1.8598	Prec@(1,5) (54.4%, 83.0%)
07/04 06:00:31午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 1.8692	Prec@(1,5) (54.2%, 82.8%)
07/04 06:00:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][250/391]	Step 14467	Loss 1.8673	Prec@(1,5) (54.2%, 82.7%)
07/04 06:00:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 1.8693	Prec@(1,5) (54.2%, 82.7%)
07/04 06:00:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][350/391]	Step 14467	Loss 1.8647	Prec@(1,5) (54.4%, 82.8%)
07/04 06:00:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 1.8658	Prec@(1,5) (54.5%, 82.8%)
07/04 06:00:52午後 searchStage_trainer.py:264 [INFO] Valid: [ 36/49] Final Prec@1 54.4640%
07/04 06:00:52午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 06:00:53午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 54.4640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2485, 0.2579, 0.2465],
        [0.2479, 0.2485, 0.2551, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2128, 0.2216, 0.3302, 0.2354],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2493, 0.2575, 0.2472],
        [0.2475, 0.2485, 0.2544, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2493, 0.2539, 0.2426],
        [0.2492, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2490, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2489],
        [0.2483, 0.2482, 0.2536, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2475, 0.2574, 0.2473],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2498],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2488, 0.2561, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2494, 0.2544, 0.2469],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2529, 0.2502],
        [0.2495, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2432, 0.2558, 0.2498],
        [0.2504, 0.2443, 0.2553, 0.2499],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2440, 0.2612, 0.2409],
        [0.2496, 0.2479, 0.2511, 0.2514],
        [0.2505, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2488, 0.2522, 0.2482],
        [0.2505, 0.2472, 0.2511, 0.2512],
        [0.2505, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2490, 0.2544, 0.2424],
        [0.2515, 0.2473, 0.2512, 0.2500],
        [0.2501, 0.2477, 0.2484, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2486, 0.2541, 0.2445],
        [0.2514, 0.2486, 0.2498, 0.2502],
        [0.2492, 0.2482, 0.2507, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 06:01:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][50/390]	Step 14517	lr 0.00479	Loss 0.8013 (0.5672)	Architecture Loss 1.8235 (1.8463)	Prec@(1,5) (82.7%, 98.0%)	
07/04 06:02:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.5333 (0.5913)	Architecture Loss 2.2701 (1.8688)	Prec@(1,5) (81.5%, 97.8%)	
07/04 06:02:57午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][150/390]	Step 14617	lr 0.00479	Loss 0.6806 (0.5937)	Architecture Loss 1.3553 (1.8909)	Prec@(1,5) (81.5%, 97.9%)	
07/04 06:03:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.5937 (0.6015)	Architecture Loss 2.5003 (1.8898)	Prec@(1,5) (81.2%, 97.7%)	
07/04 06:04:19午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][250/390]	Step 14717	lr 0.00479	Loss 0.4928 (0.6076)	Architecture Loss 2.4575 (1.9066)	Prec@(1,5) (81.1%, 97.6%)	
07/04 06:05:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.6584 (0.6136)	Architecture Loss 2.2022 (1.9059)	Prec@(1,5) (80.9%, 97.5%)	
07/04 06:05:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][350/390]	Step 14817	lr 0.00479	Loss 0.4989 (0.6199)	Architecture Loss 2.5192 (1.9113)	Prec@(1,5) (80.7%, 97.4%)	
07/04 06:06:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.7510 (0.6235)	Architecture Loss 2.4062 (1.9139)	Prec@(1,5) (80.6%, 97.5%)	
07/04 06:06:14午後 searchStage_trainer.py:225 [INFO] Train: [ 37/49] Final Prec@1 80.6560%
07/04 06:06:20午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][50/391]	Step 14858	Loss 1.8405	Prec@(1,5) (56.0%, 83.2%)
07/04 06:06:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 1.8745	Prec@(1,5) (55.5%, 82.8%)
07/04 06:06:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][150/391]	Step 14858	Loss 1.9008	Prec@(1,5) (54.9%, 82.4%)
07/04 06:06:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 1.8954	Prec@(1,5) (55.1%, 82.5%)
07/04 06:06:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][250/391]	Step 14858	Loss 1.8873	Prec@(1,5) (55.1%, 82.5%)
07/04 06:06:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 1.8846	Prec@(1,5) (55.2%, 82.7%)
07/04 06:06:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][350/391]	Step 14858	Loss 1.8809	Prec@(1,5) (55.3%, 82.7%)
07/04 06:06:57午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 1.8772	Prec@(1,5) (55.2%, 82.8%)
07/04 06:06:57午後 searchStage_trainer.py:264 [INFO] Valid: [ 37/49] Final Prec@1 55.1960%
07/04 06:06:57午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 06:06:58午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.1960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2485, 0.2579, 0.2465],
        [0.2479, 0.2485, 0.2551, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2124, 0.2214, 0.3309, 0.2354],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2493, 0.2576, 0.2473],
        [0.2474, 0.2485, 0.2545, 0.2496],
        [0.2480, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2493, 0.2539, 0.2426],
        [0.2492, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2489],
        [0.2483, 0.2482, 0.2536, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2475, 0.2574, 0.2473],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2498],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2496, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2488, 0.2562, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2494, 0.2544, 0.2469],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2529, 0.2502],
        [0.2495, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2432, 0.2558, 0.2498],
        [0.2504, 0.2443, 0.2554, 0.2499],
        [0.2505, 0.2492, 0.2504, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2440, 0.2615, 0.2406],
        [0.2496, 0.2479, 0.2511, 0.2514],
        [0.2505, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2489, 0.2522, 0.2481],
        [0.2505, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2491, 0.2545, 0.2423],
        [0.2514, 0.2473, 0.2513, 0.2500],
        [0.2501, 0.2477, 0.2484, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2487, 0.2541, 0.2444],
        [0.2514, 0.2486, 0.2498, 0.2502],
        [0.2492, 0.2482, 0.2507, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 06:07:41午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][50/390]	Step 14908	lr 0.00425	Loss 0.3562 (0.5400)	Architecture Loss 1.8236 (1.9332)	Prec@(1,5) (83.0%, 98.1%)	
07/04 06:08:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.4657 (0.5401)	Architecture Loss 1.5352 (1.9246)	Prec@(1,5) (83.4%, 98.2%)	
07/04 06:09:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][150/390]	Step 15008	lr 0.00425	Loss 0.7434 (0.5430)	Architecture Loss 1.3707 (1.9191)	Prec@(1,5) (83.3%, 98.0%)	
07/04 06:09:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.4596 (0.5497)	Architecture Loss 2.2467 (1.9236)	Prec@(1,5) (83.2%, 98.0%)	
07/04 06:10:24午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][250/390]	Step 15108	lr 0.00425	Loss 0.4411 (0.5582)	Architecture Loss 1.8964 (1.9335)	Prec@(1,5) (82.7%, 97.9%)	
07/04 06:11:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.6340 (0.5653)	Architecture Loss 1.7945 (1.9183)	Prec@(1,5) (82.4%, 97.9%)	
07/04 06:11:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][350/390]	Step 15208	lr 0.00425	Loss 0.5239 (0.5700)	Architecture Loss 2.2500 (1.9110)	Prec@(1,5) (82.2%, 97.9%)	
07/04 06:12:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.5230 (0.5737)	Architecture Loss 1.6744 (1.9095)	Prec@(1,5) (82.1%, 97.8%)	
07/04 06:12:19午後 searchStage_trainer.py:225 [INFO] Train: [ 38/49] Final Prec@1 82.1160%
07/04 06:12:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][50/391]	Step 15249	Loss 1.8438	Prec@(1,5) (55.7%, 84.1%)
07/04 06:12:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 1.8924	Prec@(1,5) (55.0%, 83.6%)
07/04 06:12:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][150/391]	Step 15249	Loss 1.9237	Prec@(1,5) (54.7%, 82.9%)
07/04 06:12:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 1.8970	Prec@(1,5) (55.1%, 83.1%)
07/04 06:12:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][250/391]	Step 15249	Loss 1.9050	Prec@(1,5) (55.1%, 83.0%)
07/04 06:12:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 1.9070	Prec@(1,5) (55.1%, 82.9%)
07/04 06:12:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][350/391]	Step 15249	Loss 1.9022	Prec@(1,5) (55.2%, 83.0%)
07/04 06:13:02午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 1.8949	Prec@(1,5) (55.4%, 83.1%)
07/04 06:13:02午後 searchStage_trainer.py:264 [INFO] Valid: [ 38/49] Final Prec@1 55.3800%
07/04 06:13:02午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 06:13:03午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.3800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2485, 0.2579, 0.2465],
        [0.2478, 0.2485, 0.2551, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2120, 0.2211, 0.3315, 0.2353],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2493, 0.2576, 0.2473],
        [0.2474, 0.2485, 0.2545, 0.2495],
        [0.2481, 0.2503, 0.2502, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2493, 0.2539, 0.2426],
        [0.2492, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2489],
        [0.2483, 0.2482, 0.2536, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2474, 0.2575, 0.2473],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2497, 0.2514, 0.2498],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2497, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2488, 0.2562, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2494, 0.2545, 0.2468],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2529, 0.2502],
        [0.2495, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2431, 0.2559, 0.2498],
        [0.2504, 0.2443, 0.2555, 0.2499],
        [0.2505, 0.2492, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2539, 0.2440, 0.2618, 0.2404],
        [0.2496, 0.2479, 0.2511, 0.2514],
        [0.2505, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2489, 0.2522, 0.2481],
        [0.2505, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2491, 0.2546, 0.2422],
        [0.2514, 0.2473, 0.2513, 0.2500],
        [0.2501, 0.2477, 0.2484, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2487, 0.2542, 0.2443],
        [0.2513, 0.2486, 0.2498, 0.2502],
        [0.2492, 0.2482, 0.2507, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 06:13:45午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][50/390]	Step 15299	lr 0.00375	Loss 0.4854 (0.4969)	Architecture Loss 2.4166 (2.0079)	Prec@(1,5) (84.9%, 98.8%)	
07/04 06:14:26午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.5555 (0.4974)	Architecture Loss 1.2881 (1.9453)	Prec@(1,5) (84.9%, 98.8%)	
07/04 06:15:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][150/390]	Step 15399	lr 0.00375	Loss 0.3762 (0.5034)	Architecture Loss 2.2738 (1.9425)	Prec@(1,5) (84.6%, 98.5%)	
07/04 06:15:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.4157 (0.5158)	Architecture Loss 2.0399 (1.9607)	Prec@(1,5) (84.2%, 98.3%)	
07/04 06:16:28午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][250/390]	Step 15499	lr 0.00375	Loss 0.7261 (0.5166)	Architecture Loss 1.6336 (1.9392)	Prec@(1,5) (84.1%, 98.3%)	
07/04 06:17:09午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.7055 (0.5250)	Architecture Loss 2.3306 (1.9304)	Prec@(1,5) (83.8%, 98.3%)	
07/04 06:17:51午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][350/390]	Step 15599	lr 0.00375	Loss 0.4390 (0.5250)	Architecture Loss 2.0857 (1.9348)	Prec@(1,5) (83.8%, 98.3%)	
07/04 06:18:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.6586 (0.5294)	Architecture Loss 2.6770 (1.9360)	Prec@(1,5) (83.6%, 98.3%)	
07/04 06:18:24午後 searchStage_trainer.py:225 [INFO] Train: [ 39/49] Final Prec@1 83.6000%
07/04 06:18:29午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][50/391]	Step 15640	Loss 1.9281	Prec@(1,5) (54.6%, 82.9%)
07/04 06:18:34午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 1.9339	Prec@(1,5) (54.6%, 83.0%)
07/04 06:18:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][150/391]	Step 15640	Loss 1.9266	Prec@(1,5) (54.9%, 82.8%)
07/04 06:18:45午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 1.9256	Prec@(1,5) (54.9%, 82.7%)
07/04 06:18:50午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][250/391]	Step 15640	Loss 1.9027	Prec@(1,5) (55.3%, 83.1%)
07/04 06:18:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 1.9029	Prec@(1,5) (55.1%, 83.3%)
07/04 06:19:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][350/391]	Step 15640	Loss 1.9059	Prec@(1,5) (55.0%, 83.3%)
07/04 06:19:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 1.9038	Prec@(1,5) (55.1%, 83.2%)
07/04 06:19:06午後 searchStage_trainer.py:264 [INFO] Valid: [ 39/49] Final Prec@1 55.0800%
07/04 06:19:06午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 06:19:06午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.3800%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2485, 0.2580, 0.2465],
        [0.2478, 0.2485, 0.2551, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2117, 0.2209, 0.3321, 0.2353],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2492, 0.2576, 0.2473],
        [0.2474, 0.2485, 0.2545, 0.2495],
        [0.2481, 0.2503, 0.2501, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2493, 0.2539, 0.2426],
        [0.2492, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2489],
        [0.2483, 0.2482, 0.2536, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2474, 0.2576, 0.2473],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2496, 0.2514, 0.2498],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2497, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2488, 0.2562, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2494, 0.2545, 0.2468],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2529, 0.2502],
        [0.2495, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2431, 0.2559, 0.2498],
        [0.2504, 0.2443, 0.2555, 0.2499],
        [0.2505, 0.2492, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2440, 0.2621, 0.2401],
        [0.2496, 0.2479, 0.2511, 0.2514],
        [0.2505, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2489, 0.2522, 0.2481],
        [0.2505, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2492, 0.2546, 0.2421],
        [0.2514, 0.2473, 0.2513, 0.2500],
        [0.2500, 0.2477, 0.2484, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2488, 0.2543, 0.2442],
        [0.2513, 0.2486, 0.2498, 0.2502],
        [0.2492, 0.2482, 0.2507, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 06:19:49午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][50/390]	Step 15690	lr 0.00329	Loss 0.6196 (0.4639)	Architecture Loss 1.4721 (1.9256)	Prec@(1,5) (86.2%, 98.6%)	
07/04 06:20:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.3900 (0.4723)	Architecture Loss 1.7850 (1.9082)	Prec@(1,5) (85.5%, 98.5%)	
07/04 06:21:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][150/390]	Step 15790	lr 0.00329	Loss 0.4738 (0.4677)	Architecture Loss 2.1309 (1.9260)	Prec@(1,5) (85.6%, 98.6%)	
07/04 06:21:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.4941 (0.4729)	Architecture Loss 1.7501 (1.9296)	Prec@(1,5) (85.4%, 98.6%)	
07/04 06:22:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][250/390]	Step 15890	lr 0.00329	Loss 0.6619 (0.4799)	Architecture Loss 1.8967 (1.9426)	Prec@(1,5) (85.2%, 98.4%)	
07/04 06:23:13午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.5357 (0.4808)	Architecture Loss 2.3969 (1.9531)	Prec@(1,5) (85.2%, 98.4%)	
07/04 06:23:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][350/390]	Step 15990	lr 0.00329	Loss 0.4508 (0.4861)	Architecture Loss 1.7228 (1.9515)	Prec@(1,5) (85.0%, 98.5%)	
07/04 06:24:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.4472 (0.4874)	Architecture Loss 1.7214 (1.9538)	Prec@(1,5) (85.0%, 98.4%)	
07/04 06:24:28午後 searchStage_trainer.py:225 [INFO] Train: [ 40/49] Final Prec@1 85.0560%
07/04 06:24:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][50/391]	Step 16031	Loss 1.8860	Prec@(1,5) (55.9%, 82.9%)
07/04 06:24:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 1.8799	Prec@(1,5) (56.2%, 84.0%)
07/04 06:24:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][150/391]	Step 16031	Loss 1.8981	Prec@(1,5) (56.0%, 83.9%)
07/04 06:24:50午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 1.9045	Prec@(1,5) (55.8%, 83.8%)
07/04 06:24:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][250/391]	Step 16031	Loss 1.9125	Prec@(1,5) (55.7%, 83.7%)
07/04 06:25:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 1.9021	Prec@(1,5) (55.8%, 83.7%)
07/04 06:25:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][350/391]	Step 16031	Loss 1.8932	Prec@(1,5) (55.9%, 83.8%)
07/04 06:25:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 1.9042	Prec@(1,5) (55.7%, 83.7%)
07/04 06:25:11午後 searchStage_trainer.py:264 [INFO] Valid: [ 40/49] Final Prec@1 55.7400%
07/04 06:25:11午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('max_pool_3x3', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 06:25:11午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.7400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2485, 0.2580, 0.2465],
        [0.2478, 0.2484, 0.2552, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2114, 0.2207, 0.3327, 0.2352],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2492, 0.2577, 0.2473],
        [0.2474, 0.2485, 0.2545, 0.2495],
        [0.2481, 0.2503, 0.2501, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2493, 0.2539, 0.2426],
        [0.2492, 0.2498, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2489],
        [0.2483, 0.2482, 0.2537, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2474, 0.2576, 0.2474],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2496, 0.2514, 0.2498],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2497, 0.2508, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2488, 0.2562, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2494, 0.2545, 0.2468],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2529, 0.2502],
        [0.2495, 0.2475, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2431, 0.2560, 0.2498],
        [0.2503, 0.2443, 0.2556, 0.2498],
        [0.2504, 0.2492, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2440, 0.2623, 0.2398],
        [0.2496, 0.2479, 0.2511, 0.2515],
        [0.2505, 0.2485, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2490, 0.2523, 0.2480],
        [0.2505, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2489, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2492, 0.2547, 0.2420],
        [0.2514, 0.2474, 0.2513, 0.2500],
        [0.2500, 0.2477, 0.2484, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2489, 0.2543, 0.2440],
        [0.2513, 0.2487, 0.2498, 0.2502],
        [0.2492, 0.2482, 0.2507, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 06:25:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][50/390]	Step 16081	lr 0.00287	Loss 0.3780 (0.4515)	Architecture Loss 2.3368 (2.0425)	Prec@(1,5) (85.8%, 98.9%)	
07/04 06:26:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.4359 (0.4404)	Architecture Loss 1.2716 (2.0192)	Prec@(1,5) (86.4%, 98.8%)	
07/04 06:27:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][150/390]	Step 16181	lr 0.00287	Loss 0.6682 (0.4360)	Architecture Loss 2.0704 (1.9553)	Prec@(1,5) (86.6%, 98.8%)	
07/04 06:27:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.3122 (0.4376)	Architecture Loss 1.6972 (1.9862)	Prec@(1,5) (86.6%, 98.8%)	
07/04 06:28:37午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][250/390]	Step 16281	lr 0.00287	Loss 0.5433 (0.4414)	Architecture Loss 1.6197 (1.9705)	Prec@(1,5) (86.4%, 98.8%)	
07/04 06:29:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.2855 (0.4421)	Architecture Loss 1.9539 (1.9660)	Prec@(1,5) (86.4%, 98.7%)	
07/04 06:29:59午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][350/390]	Step 16381	lr 0.00287	Loss 0.4402 (0.4423)	Architecture Loss 2.2056 (1.9705)	Prec@(1,5) (86.4%, 98.7%)	
07/04 06:30:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.4315 (0.4462)	Architecture Loss 2.3461 (1.9633)	Prec@(1,5) (86.3%, 98.7%)	
07/04 06:30:33午後 searchStage_trainer.py:225 [INFO] Train: [ 41/49] Final Prec@1 86.2960%
07/04 06:30:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][50/391]	Step 16422	Loss 1.9090	Prec@(1,5) (56.7%, 83.9%)
07/04 06:30:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 1.8917	Prec@(1,5) (56.5%, 83.9%)
07/04 06:30:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][150/391]	Step 16422	Loss 1.9080	Prec@(1,5) (56.4%, 83.5%)
07/04 06:30:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 1.9114	Prec@(1,5) (56.3%, 83.6%)
07/04 06:30:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][250/391]	Step 16422	Loss 1.9128	Prec@(1,5) (56.2%, 83.5%)
07/04 06:31:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 1.9055	Prec@(1,5) (56.1%, 83.6%)
07/04 06:31:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][350/391]	Step 16422	Loss 1.9189	Prec@(1,5) (56.0%, 83.4%)
07/04 06:31:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 1.9237	Prec@(1,5) (55.9%, 83.4%)
07/04 06:31:15午後 searchStage_trainer.py:264 [INFO] Valid: [ 41/49] Final Prec@1 55.9080%
07/04 06:31:15午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 06:31:15午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.9080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2485, 0.2580, 0.2465],
        [0.2478, 0.2484, 0.2552, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2111, 0.2204, 0.3334, 0.2352],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2492, 0.2577, 0.2473],
        [0.2474, 0.2485, 0.2546, 0.2495],
        [0.2481, 0.2503, 0.2501, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2493, 0.2539, 0.2425],
        [0.2492, 0.2497, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2489],
        [0.2483, 0.2482, 0.2537, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2474, 0.2577, 0.2473],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2496, 0.2514, 0.2498],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2497, 0.2508, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2489, 0.2563, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2494, 0.2545, 0.2468],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2529, 0.2502],
        [0.2495, 0.2476, 0.2528, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2431, 0.2560, 0.2497],
        [0.2503, 0.2442, 0.2556, 0.2498],
        [0.2504, 0.2492, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2441, 0.2627, 0.2395],
        [0.2496, 0.2479, 0.2511, 0.2515],
        [0.2505, 0.2485, 0.2503, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2490, 0.2523, 0.2480],
        [0.2504, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2490, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2493, 0.2547, 0.2419],
        [0.2513, 0.2474, 0.2513, 0.2500],
        [0.2500, 0.2477, 0.2485, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2489, 0.2544, 0.2439],
        [0.2513, 0.2487, 0.2499, 0.2502],
        [0.2492, 0.2482, 0.2507, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 06:31:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][50/390]	Step 16472	lr 0.00248	Loss 0.5249 (0.3809)	Architecture Loss 2.2252 (1.9955)	Prec@(1,5) (88.1%, 99.3%)	
07/04 06:32:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.5129 (0.3830)	Architecture Loss 1.9602 (1.9880)	Prec@(1,5) (88.1%, 99.4%)	
07/04 06:33:20午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][150/390]	Step 16572	lr 0.00248	Loss 0.2320 (0.3966)	Architecture Loss 2.5087 (1.9705)	Prec@(1,5) (87.6%, 99.3%)	
07/04 06:34:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.3779 (0.4084)	Architecture Loss 1.6213 (1.9587)	Prec@(1,5) (87.2%, 99.2%)	
07/04 06:34:42午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][250/390]	Step 16672	lr 0.00248	Loss 0.4934 (0.4089)	Architecture Loss 1.9682 (1.9673)	Prec@(1,5) (87.3%, 99.1%)	
07/04 06:35:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.1971 (0.4110)	Architecture Loss 1.5528 (1.9722)	Prec@(1,5) (87.4%, 99.0%)	
07/04 06:36:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][350/390]	Step 16772	lr 0.00248	Loss 0.4168 (0.4121)	Architecture Loss 1.5630 (1.9628)	Prec@(1,5) (87.5%, 99.0%)	
07/04 06:36:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.4454 (0.4167)	Architecture Loss 2.5926 (1.9612)	Prec@(1,5) (87.3%, 99.0%)	
07/04 06:36:37午後 searchStage_trainer.py:225 [INFO] Train: [ 42/49] Final Prec@1 87.2960%
07/04 06:36:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][50/391]	Step 16813	Loss 1.9345	Prec@(1,5) (55.7%, 83.9%)
07/04 06:36:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 1.9018	Prec@(1,5) (55.9%, 83.9%)
07/04 06:36:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][150/391]	Step 16813	Loss 1.9048	Prec@(1,5) (55.6%, 83.9%)
07/04 06:36:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 1.9216	Prec@(1,5) (55.6%, 83.6%)
07/04 06:37:04午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][250/391]	Step 16813	Loss 1.9297	Prec@(1,5) (55.5%, 83.6%)
07/04 06:37:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 1.9267	Prec@(1,5) (55.6%, 83.8%)
07/04 06:37:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][350/391]	Step 16813	Loss 1.9270	Prec@(1,5) (55.6%, 83.7%)
07/04 06:37:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 1.9321	Prec@(1,5) (55.5%, 83.5%)
07/04 06:37:19午後 searchStage_trainer.py:264 [INFO] Valid: [ 42/49] Final Prec@1 55.5280%
07/04 06:37:20午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 06:37:20午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.9080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2485, 0.2580, 0.2465],
        [0.2478, 0.2484, 0.2552, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2107, 0.2202, 0.3339, 0.2351],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2492, 0.2577, 0.2473],
        [0.2474, 0.2485, 0.2546, 0.2495],
        [0.2481, 0.2503, 0.2501, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2493, 0.2540, 0.2425],
        [0.2492, 0.2497, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2489],
        [0.2483, 0.2482, 0.2537, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2473, 0.2578, 0.2474],
        [0.2498, 0.2494, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2496, 0.2514, 0.2498],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2497, 0.2508, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2489, 0.2563, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2494, 0.2546, 0.2468],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2530, 0.2502],
        [0.2495, 0.2476, 0.2529, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2431, 0.2560, 0.2497],
        [0.2503, 0.2442, 0.2557, 0.2498],
        [0.2504, 0.2492, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2441, 0.2629, 0.2392],
        [0.2496, 0.2479, 0.2511, 0.2515],
        [0.2505, 0.2485, 0.2503, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2490, 0.2523, 0.2480],
        [0.2504, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2490, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2541, 0.2493, 0.2548, 0.2418],
        [0.2513, 0.2474, 0.2514, 0.2500],
        [0.2500, 0.2477, 0.2485, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2528, 0.2490, 0.2544, 0.2438],
        [0.2513, 0.2487, 0.2499, 0.2502],
        [0.2491, 0.2482, 0.2507, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 06:38:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][50/390]	Step 16863	lr 0.00214	Loss 0.4740 (0.3772)	Architecture Loss 1.9386 (2.0051)	Prec@(1,5) (88.6%, 99.2%)	
07/04 06:38:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.3604 (0.3771)	Architecture Loss 2.1553 (2.0171)	Prec@(1,5) (88.8%, 99.3%)	
07/04 06:39:24午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][150/390]	Step 16963	lr 0.00214	Loss 0.3273 (0.3757)	Architecture Loss 2.1205 (1.9947)	Prec@(1,5) (88.6%, 99.3%)	
07/04 06:40:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.4532 (0.3748)	Architecture Loss 1.8391 (2.0035)	Prec@(1,5) (88.6%, 99.2%)	
07/04 06:40:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][250/390]	Step 17063	lr 0.00214	Loss 0.4224 (0.3806)	Architecture Loss 2.0144 (2.0034)	Prec@(1,5) (88.3%, 99.2%)	
07/04 06:41:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.3648 (0.3838)	Architecture Loss 1.7011 (1.9972)	Prec@(1,5) (88.2%, 99.1%)	
07/04 06:42:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][350/390]	Step 17163	lr 0.00214	Loss 0.5014 (0.3847)	Architecture Loss 1.6267 (1.9945)	Prec@(1,5) (88.2%, 99.1%)	
07/04 06:42:41午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.5012 (0.3858)	Architecture Loss 1.9284 (1.9924)	Prec@(1,5) (88.1%, 99.1%)	
07/04 06:42:41午後 searchStage_trainer.py:225 [INFO] Train: [ 43/49] Final Prec@1 88.1360%
07/04 06:42:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][50/391]	Step 17204	Loss 1.9938	Prec@(1,5) (55.8%, 83.0%)
07/04 06:42:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 1.9715	Prec@(1,5) (55.6%, 83.2%)
07/04 06:42:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][150/391]	Step 17204	Loss 1.9659	Prec@(1,5) (55.4%, 83.4%)
07/04 06:43:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 1.9492	Prec@(1,5) (55.4%, 83.5%)
07/04 06:43:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][250/391]	Step 17204	Loss 1.9313	Prec@(1,5) (55.5%, 83.8%)
07/04 06:43:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 1.9305	Prec@(1,5) (55.6%, 83.8%)
07/04 06:43:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][350/391]	Step 17204	Loss 1.9331	Prec@(1,5) (55.5%, 83.8%)
07/04 06:43:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 1.9319	Prec@(1,5) (55.5%, 83.8%)
07/04 06:43:24午後 searchStage_trainer.py:264 [INFO] Valid: [ 43/49] Final Prec@1 55.5520%
07/04 06:43:24午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 06:43:24午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.9080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2484, 0.2580, 0.2465],
        [0.2478, 0.2484, 0.2552, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2105, 0.2200, 0.3344, 0.2351],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2492, 0.2577, 0.2474],
        [0.2474, 0.2485, 0.2546, 0.2495],
        [0.2481, 0.2503, 0.2501, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2493, 0.2540, 0.2425],
        [0.2492, 0.2497, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2488],
        [0.2483, 0.2482, 0.2537, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2473, 0.2578, 0.2474],
        [0.2498, 0.2495, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2496, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2509, 0.2504],
        [0.2494, 0.2497, 0.2508, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2489, 0.2563, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2494, 0.2546, 0.2468],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2530, 0.2502],
        [0.2495, 0.2476, 0.2529, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2431, 0.2561, 0.2497],
        [0.2503, 0.2442, 0.2557, 0.2498],
        [0.2504, 0.2492, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2538, 0.2441, 0.2632, 0.2389],
        [0.2495, 0.2479, 0.2511, 0.2515],
        [0.2505, 0.2485, 0.2503, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2491, 0.2523, 0.2480],
        [0.2504, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2490, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2494, 0.2548, 0.2417],
        [0.2513, 0.2474, 0.2514, 0.2500],
        [0.2500, 0.2477, 0.2485, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2490, 0.2545, 0.2437],
        [0.2513, 0.2487, 0.2499, 0.2502],
        [0.2491, 0.2482, 0.2507, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 06:44:07午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][50/390]	Step 17254	lr 0.00184	Loss 0.3095 (0.3460)	Architecture Loss 2.0951 (1.9864)	Prec@(1,5) (89.8%, 99.1%)	
07/04 06:44:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.4416 (0.3499)	Architecture Loss 2.0827 (2.0091)	Prec@(1,5) (89.8%, 99.1%)	
07/04 06:45:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][150/390]	Step 17354	lr 0.00184	Loss 0.3339 (0.3463)	Architecture Loss 1.6716 (1.9822)	Prec@(1,5) (89.9%, 99.2%)	
07/04 06:46:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.4133 (0.3475)	Architecture Loss 2.3371 (1.9853)	Prec@(1,5) (89.9%, 99.2%)	
07/04 06:46:51午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][250/390]	Step 17454	lr 0.00184	Loss 0.3287 (0.3459)	Architecture Loss 1.9405 (1.9796)	Prec@(1,5) (89.8%, 99.2%)	
07/04 06:47:31午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.4551 (0.3479)	Architecture Loss 1.4379 (1.9736)	Prec@(1,5) (89.7%, 99.2%)	
07/04 06:48:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][350/390]	Step 17554	lr 0.00184	Loss 0.4352 (0.3504)	Architecture Loss 2.2143 (1.9895)	Prec@(1,5) (89.6%, 99.2%)	
07/04 06:48:45午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.3840 (0.3536)	Architecture Loss 1.8823 (1.9960)	Prec@(1,5) (89.5%, 99.2%)	
07/04 06:48:45午後 searchStage_trainer.py:225 [INFO] Train: [ 44/49] Final Prec@1 89.5320%
07/04 06:48:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][50/391]	Step 17595	Loss 2.0037	Prec@(1,5) (55.4%, 82.9%)
07/04 06:48:57午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 1.9597	Prec@(1,5) (55.3%, 83.3%)
07/04 06:49:02午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][150/391]	Step 17595	Loss 1.9621	Prec@(1,5) (55.1%, 83.6%)
07/04 06:49:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 1.9617	Prec@(1,5) (55.4%, 83.5%)
07/04 06:49:13午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][250/391]	Step 17595	Loss 1.9509	Prec@(1,5) (55.6%, 83.7%)
07/04 06:49:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 1.9496	Prec@(1,5) (55.5%, 83.8%)
07/04 06:49:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][350/391]	Step 17595	Loss 1.9571	Prec@(1,5) (55.4%, 83.6%)
07/04 06:49:29午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 1.9612	Prec@(1,5) (55.5%, 83.6%)
07/04 06:49:29午後 searchStage_trainer.py:264 [INFO] Valid: [ 44/49] Final Prec@1 55.5040%
07/04 06:49:29午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 06:49:29午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.9080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2484, 0.2580, 0.2465],
        [0.2478, 0.2484, 0.2552, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2103, 0.2199, 0.3348, 0.2351],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2503, 0.2487],
        [0.2498, 0.2507, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2492, 0.2577, 0.2474],
        [0.2474, 0.2485, 0.2546, 0.2495],
        [0.2481, 0.2503, 0.2501, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2493, 0.2540, 0.2425],
        [0.2492, 0.2497, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2488],
        [0.2483, 0.2482, 0.2537, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2473, 0.2579, 0.2474],
        [0.2498, 0.2495, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2496, 0.2514, 0.2499],
        [0.2491, 0.2496, 0.2509, 0.2504],
        [0.2494, 0.2497, 0.2508, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2489, 0.2563, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2494, 0.2546, 0.2468],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2530, 0.2502],
        [0.2495, 0.2476, 0.2529, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2431, 0.2561, 0.2497],
        [0.2502, 0.2442, 0.2557, 0.2498],
        [0.2504, 0.2492, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2441, 0.2635, 0.2386],
        [0.2495, 0.2479, 0.2511, 0.2515],
        [0.2504, 0.2485, 0.2503, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2491, 0.2523, 0.2479],
        [0.2504, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2490, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2494, 0.2549, 0.2417],
        [0.2513, 0.2474, 0.2514, 0.2500],
        [0.2500, 0.2477, 0.2485, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2491, 0.2546, 0.2436],
        [0.2513, 0.2487, 0.2499, 0.2502],
        [0.2491, 0.2482, 0.2507, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 06:50:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][50/390]	Step 17645	lr 0.00159	Loss 0.2987 (0.3364)	Architecture Loss 1.8373 (2.0103)	Prec@(1,5) (90.2%, 99.3%)	
07/04 06:50:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.4384 (0.3245)	Architecture Loss 2.5060 (2.0126)	Prec@(1,5) (90.6%, 99.4%)	
07/04 06:51:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][150/390]	Step 17745	lr 0.00159	Loss 0.2240 (0.3222)	Architecture Loss 1.6252 (2.0285)	Prec@(1,5) (90.7%, 99.4%)	
07/04 06:52:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.3246 (0.3238)	Architecture Loss 1.9403 (2.0023)	Prec@(1,5) (90.4%, 99.4%)	
07/04 06:52:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][250/390]	Step 17845	lr 0.00159	Loss 0.1995 (0.3235)	Architecture Loss 2.8368 (2.0125)	Prec@(1,5) (90.5%, 99.4%)	
07/04 06:53:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.3374 (0.3275)	Architecture Loss 2.1552 (1.9994)	Prec@(1,5) (90.3%, 99.4%)	
07/04 06:54:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][350/390]	Step 17945	lr 0.00159	Loss 0.3830 (0.3292)	Architecture Loss 1.6118 (2.0086)	Prec@(1,5) (90.2%, 99.4%)	
07/04 06:54:49午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.3021 (0.3316)	Architecture Loss 2.1784 (2.0107)	Prec@(1,5) (90.2%, 99.4%)	
07/04 06:54:50午後 searchStage_trainer.py:225 [INFO] Train: [ 45/49] Final Prec@1 90.1600%
07/04 06:54:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][50/391]	Step 17986	Loss 2.0531	Prec@(1,5) (55.3%, 82.9%)
07/04 06:55:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 2.0044	Prec@(1,5) (55.7%, 83.1%)
07/04 06:55:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][150/391]	Step 17986	Loss 1.9818	Prec@(1,5) (55.9%, 83.2%)
07/04 06:55:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 1.9964	Prec@(1,5) (55.5%, 83.2%)
07/04 06:55:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][250/391]	Step 17986	Loss 1.9851	Prec@(1,5) (55.7%, 83.3%)
07/04 06:55:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 1.9706	Prec@(1,5) (55.7%, 83.5%)
07/04 06:55:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][350/391]	Step 17986	Loss 1.9726	Prec@(1,5) (55.8%, 83.6%)
07/04 06:55:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 1.9682	Prec@(1,5) (55.9%, 83.6%)
07/04 06:55:32午後 searchStage_trainer.py:264 [INFO] Valid: [ 45/49] Final Prec@1 55.9240%
07/04 06:55:32午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 06:55:33午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.9240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2484, 0.2580, 0.2465],
        [0.2478, 0.2484, 0.2552, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2100, 0.2197, 0.3352, 0.2350],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2502, 0.2487],
        [0.2499, 0.2506, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2491, 0.2578, 0.2474],
        [0.2474, 0.2485, 0.2546, 0.2495],
        [0.2481, 0.2503, 0.2501, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2493, 0.2540, 0.2425],
        [0.2492, 0.2497, 0.2499, 0.2511],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2488],
        [0.2483, 0.2482, 0.2537, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2472, 0.2580, 0.2474],
        [0.2498, 0.2495, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2496, 0.2514, 0.2499],
        [0.2491, 0.2496, 0.2509, 0.2504],
        [0.2494, 0.2497, 0.2508, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2489, 0.2563, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2494, 0.2547, 0.2468],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2530, 0.2502],
        [0.2495, 0.2476, 0.2529, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2431, 0.2562, 0.2497],
        [0.2502, 0.2442, 0.2558, 0.2498],
        [0.2504, 0.2492, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2442, 0.2638, 0.2384],
        [0.2495, 0.2479, 0.2511, 0.2515],
        [0.2504, 0.2485, 0.2503, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2491, 0.2524, 0.2479],
        [0.2504, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2490, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2495, 0.2549, 0.2416],
        [0.2513, 0.2474, 0.2514, 0.2499],
        [0.2500, 0.2477, 0.2485, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2491, 0.2546, 0.2436],
        [0.2512, 0.2487, 0.2499, 0.2502],
        [0.2491, 0.2482, 0.2507, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 06:56:15午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][50/390]	Step 18036	lr 0.00138	Loss 0.2756 (0.3120)	Architecture Loss 1.2965 (1.9136)	Prec@(1,5) (90.7%, 99.5%)	
07/04 06:56:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.2384 (0.3108)	Architecture Loss 2.0388 (1.9985)	Prec@(1,5) (90.9%, 99.4%)	
07/04 06:57:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][150/390]	Step 18136	lr 0.00138	Loss 0.2535 (0.3175)	Architecture Loss 2.4981 (1.9998)	Prec@(1,5) (90.6%, 99.4%)	
07/04 06:58:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.3232 (0.3128)	Architecture Loss 2.2214 (2.0098)	Prec@(1,5) (90.8%, 99.4%)	
07/04 06:59:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][250/390]	Step 18236	lr 0.00138	Loss 0.2216 (0.3132)	Architecture Loss 1.8991 (2.0192)	Prec@(1,5) (90.8%, 99.4%)	
07/04 06:59:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.3803 (0.3152)	Architecture Loss 1.8938 (2.0158)	Prec@(1,5) (90.7%, 99.4%)	
07/04 07:00:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][350/390]	Step 18336	lr 0.00138	Loss 0.3158 (0.3160)	Architecture Loss 1.5522 (2.0214)	Prec@(1,5) (90.7%, 99.4%)	
07/04 07:00:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.2411 (0.3182)	Architecture Loss 2.3404 (2.0188)	Prec@(1,5) (90.6%, 99.4%)	
07/04 07:00:55午後 searchStage_trainer.py:225 [INFO] Train: [ 46/49] Final Prec@1 90.5760%
07/04 07:01:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][50/391]	Step 18377	Loss 1.9118	Prec@(1,5) (57.1%, 83.8%)
07/04 07:01:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 1.9342	Prec@(1,5) (56.3%, 83.9%)
07/04 07:01:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][150/391]	Step 18377	Loss 1.9662	Prec@(1,5) (55.8%, 83.5%)
07/04 07:01:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 1.9796	Prec@(1,5) (55.6%, 83.4%)
07/04 07:01:23午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][250/391]	Step 18377	Loss 1.9606	Prec@(1,5) (55.9%, 83.7%)
07/04 07:01:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 1.9576	Prec@(1,5) (55.9%, 83.7%)
07/04 07:01:34午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][350/391]	Step 18377	Loss 1.9623	Prec@(1,5) (55.9%, 83.5%)
07/04 07:01:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 1.9564	Prec@(1,5) (56.0%, 83.6%)
07/04 07:01:39午後 searchStage_trainer.py:264 [INFO] Valid: [ 46/49] Final Prec@1 56.0160%
07/04 07:01:39午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 07:01:39午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 56.0160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2484, 0.2581, 0.2465],
        [0.2478, 0.2484, 0.2553, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2098, 0.2196, 0.3356, 0.2350],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2502, 0.2487],
        [0.2499, 0.2506, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2491, 0.2578, 0.2474],
        [0.2474, 0.2485, 0.2547, 0.2495],
        [0.2481, 0.2503, 0.2501, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2493, 0.2540, 0.2425],
        [0.2492, 0.2497, 0.2499, 0.2512],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2488],
        [0.2483, 0.2482, 0.2537, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2472, 0.2580, 0.2474],
        [0.2498, 0.2495, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2496, 0.2514, 0.2499],
        [0.2491, 0.2496, 0.2509, 0.2504],
        [0.2494, 0.2497, 0.2508, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2489, 0.2563, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2495, 0.2547, 0.2468],
        [0.2501, 0.2489, 0.2504, 0.2505],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2530, 0.2502],
        [0.2495, 0.2476, 0.2529, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2431, 0.2562, 0.2497],
        [0.2502, 0.2442, 0.2558, 0.2497],
        [0.2504, 0.2492, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2537, 0.2442, 0.2640, 0.2381],
        [0.2495, 0.2479, 0.2511, 0.2515],
        [0.2504, 0.2485, 0.2503, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2492, 0.2524, 0.2479],
        [0.2504, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2490, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2495, 0.2550, 0.2415],
        [0.2512, 0.2474, 0.2514, 0.2499],
        [0.2500, 0.2477, 0.2485, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2492, 0.2547, 0.2435],
        [0.2512, 0.2487, 0.2499, 0.2501],
        [0.2491, 0.2482, 0.2507, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 07:02:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][50/390]	Step 18427	lr 0.00121	Loss 0.2671 (0.2918)	Architecture Loss 1.6625 (2.1507)	Prec@(1,5) (91.8%, 99.7%)	
07/04 07:03:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.2560 (0.2910)	Architecture Loss 2.2144 (2.0575)	Prec@(1,5) (91.8%, 99.6%)	
07/04 07:03:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][150/390]	Step 18527	lr 0.00121	Loss 0.4844 (0.2902)	Architecture Loss 1.8976 (2.0223)	Prec@(1,5) (91.6%, 99.6%)	
07/04 07:04:24午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.3022 (0.2919)	Architecture Loss 2.9940 (2.0399)	Prec@(1,5) (91.6%, 99.6%)	
07/04 07:05:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][250/390]	Step 18627	lr 0.00121	Loss 0.3059 (0.2942)	Architecture Loss 1.5902 (2.0229)	Prec@(1,5) (91.4%, 99.6%)	
07/04 07:05:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.3030 (0.2928)	Architecture Loss 2.2014 (2.0268)	Prec@(1,5) (91.5%, 99.6%)	
07/04 07:06:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][350/390]	Step 18727	lr 0.00121	Loss 0.2375 (0.2952)	Architecture Loss 2.5583 (2.0316)	Prec@(1,5) (91.4%, 99.6%)	
07/04 07:06:59午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.2499 (0.2962)	Architecture Loss 2.4425 (2.0419)	Prec@(1,5) (91.4%, 99.6%)	
07/04 07:07:00午後 searchStage_trainer.py:225 [INFO] Train: [ 47/49] Final Prec@1 91.3520%
07/04 07:07:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][50/391]	Step 18768	Loss 2.0652	Prec@(1,5) (54.7%, 82.9%)
07/04 07:07:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 1.9752	Prec@(1,5) (55.7%, 84.0%)
07/04 07:07:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][150/391]	Step 18768	Loss 2.0096	Prec@(1,5) (55.5%, 83.6%)
07/04 07:07:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 2.0047	Prec@(1,5) (55.4%, 83.7%)
07/04 07:07:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][250/391]	Step 18768	Loss 1.9926	Prec@(1,5) (55.6%, 83.8%)
07/04 07:07:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 1.9950	Prec@(1,5) (55.6%, 83.8%)
07/04 07:07:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][350/391]	Step 18768	Loss 1.9901	Prec@(1,5) (55.6%, 83.7%)
07/04 07:07:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 1.9852	Prec@(1,5) (55.8%, 83.7%)
07/04 07:07:42午後 searchStage_trainer.py:264 [INFO] Valid: [ 47/49] Final Prec@1 55.7840%
07/04 07:07:42午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 07:07:42午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 56.0160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2484, 0.2581, 0.2465],
        [0.2478, 0.2484, 0.2553, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2096, 0.2194, 0.3360, 0.2350],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2502, 0.2487],
        [0.2499, 0.2506, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2491, 0.2578, 0.2474],
        [0.2474, 0.2485, 0.2547, 0.2495],
        [0.2481, 0.2503, 0.2501, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2493, 0.2540, 0.2425],
        [0.2492, 0.2497, 0.2499, 0.2512],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2488],
        [0.2483, 0.2481, 0.2537, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2472, 0.2581, 0.2474],
        [0.2498, 0.2495, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2496, 0.2514, 0.2499],
        [0.2491, 0.2496, 0.2509, 0.2504],
        [0.2494, 0.2497, 0.2508, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2489, 0.2564, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2495, 0.2547, 0.2467],
        [0.2501, 0.2489, 0.2504, 0.2506],
        [0.2500, 0.2494, 0.2495, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2530, 0.2502],
        [0.2495, 0.2476, 0.2529, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2431, 0.2562, 0.2497],
        [0.2502, 0.2442, 0.2559, 0.2497],
        [0.2504, 0.2492, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2442, 0.2643, 0.2379],
        [0.2495, 0.2479, 0.2511, 0.2515],
        [0.2504, 0.2485, 0.2503, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2492, 0.2524, 0.2478],
        [0.2504, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2490, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2495, 0.2550, 0.2414],
        [0.2512, 0.2474, 0.2514, 0.2499],
        [0.2500, 0.2477, 0.2485, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2492, 0.2547, 0.2434],
        [0.2512, 0.2487, 0.2499, 0.2501],
        [0.2491, 0.2482, 0.2507, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 07:08:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][50/390]	Step 18818	lr 0.00109	Loss 0.2562 (0.2870)	Architecture Loss 1.3775 (2.0782)	Prec@(1,5) (92.3%, 99.6%)	
07/04 07:09:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.5156 (0.2884)	Architecture Loss 2.2008 (1.9979)	Prec@(1,5) (92.1%, 99.7%)	
07/04 07:09:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][150/390]	Step 18918	lr 0.00109	Loss 0.2596 (0.2870)	Architecture Loss 1.9974 (1.9953)	Prec@(1,5) (91.9%, 99.6%)	
07/04 07:10:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.2871 (0.2863)	Architecture Loss 1.8188 (2.0006)	Prec@(1,5) (91.9%, 99.7%)	
07/04 07:11:09午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][250/390]	Step 19018	lr 0.00109	Loss 0.2699 (0.2857)	Architecture Loss 1.5536 (2.0087)	Prec@(1,5) (91.9%, 99.7%)	
07/04 07:11:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.2065 (0.2891)	Architecture Loss 1.8268 (2.0151)	Prec@(1,5) (91.8%, 99.6%)	
07/04 07:12:31午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][350/390]	Step 19118	lr 0.00109	Loss 0.1557 (0.2865)	Architecture Loss 2.0682 (2.0263)	Prec@(1,5) (91.9%, 99.6%)	
07/04 07:13:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.2905 (0.2865)	Architecture Loss 1.9032 (2.0314)	Prec@(1,5) (91.9%, 99.6%)	
07/04 07:13:04午後 searchStage_trainer.py:225 [INFO] Train: [ 48/49] Final Prec@1 91.8560%
07/04 07:13:09午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][50/391]	Step 19159	Loss 1.9969	Prec@(1,5) (55.5%, 83.3%)
07/04 07:13:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 1.9923	Prec@(1,5) (56.3%, 83.5%)
07/04 07:13:20午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][150/391]	Step 19159	Loss 1.9749	Prec@(1,5) (56.3%, 83.8%)
07/04 07:13:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 1.9756	Prec@(1,5) (56.2%, 83.6%)
07/04 07:13:31午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][250/391]	Step 19159	Loss 1.9713	Prec@(1,5) (56.3%, 83.8%)
07/04 07:13:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 1.9686	Prec@(1,5) (56.3%, 83.8%)
07/04 07:13:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][350/391]	Step 19159	Loss 1.9746	Prec@(1,5) (56.2%, 83.7%)
07/04 07:13:46午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 1.9739	Prec@(1,5) (56.3%, 83.8%)
07/04 07:13:46午後 searchStage_trainer.py:264 [INFO] Valid: [ 48/49] Final Prec@1 56.2600%
07/04 07:13:46午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 07:13:47午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 56.2600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2493, 0.2514, 0.2497],
        [0.2491, 0.2490, 0.2514, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2484, 0.2581, 0.2465],
        [0.2478, 0.2484, 0.2553, 0.2486],
        [0.2495, 0.2510, 0.2478, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2094, 0.2193, 0.3364, 0.2349],
        [0.2485, 0.2511, 0.2494, 0.2509],
        [0.2477, 0.2505, 0.2494, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2504, 0.2502, 0.2487],
        [0.2499, 0.2506, 0.2496, 0.2499],
        [0.2499, 0.2507, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2491, 0.2579, 0.2474],
        [0.2474, 0.2485, 0.2547, 0.2495],
        [0.2481, 0.2503, 0.2501, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2542, 0.2493, 0.2540, 0.2425],
        [0.2492, 0.2497, 0.2499, 0.2512],
        [0.2483, 0.2498, 0.2489, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2493, 0.2486, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2539, 0.2488],
        [0.2483, 0.2481, 0.2537, 0.2499],
        [0.2503, 0.2496, 0.2495, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2471, 0.2581, 0.2474],
        [0.2499, 0.2495, 0.2504, 0.2503],
        [0.2496, 0.2492, 0.2505, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2496, 0.2514, 0.2499],
        [0.2491, 0.2496, 0.2509, 0.2504],
        [0.2494, 0.2497, 0.2508, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2489, 0.2564, 0.2470],
        [0.2491, 0.2490, 0.2520, 0.2499],
        [0.2495, 0.2491, 0.2499, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2495, 0.2547, 0.2467],
        [0.2501, 0.2489, 0.2504, 0.2506],
        [0.2500, 0.2494, 0.2495, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2468, 0.2530, 0.2502],
        [0.2495, 0.2476, 0.2529, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2431, 0.2563, 0.2496],
        [0.2502, 0.2442, 0.2559, 0.2497],
        [0.2504, 0.2492, 0.2504, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2536, 0.2442, 0.2645, 0.2377],
        [0.2495, 0.2479, 0.2511, 0.2515],
        [0.2504, 0.2485, 0.2503, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2492, 0.2524, 0.2478],
        [0.2504, 0.2472, 0.2511, 0.2512],
        [0.2504, 0.2490, 0.2508, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2540, 0.2496, 0.2550, 0.2414],
        [0.2512, 0.2474, 0.2514, 0.2499],
        [0.2500, 0.2477, 0.2485, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2493, 0.2548, 0.2433],
        [0.2512, 0.2487, 0.2499, 0.2501],
        [0.2491, 0.2482, 0.2507, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/04 07:14:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][50/390]	Step 19209	lr 0.00102	Loss 0.1830 (0.2440)	Architecture Loss 2.3749 (2.1306)	Prec@(1,5) (93.5%, 99.7%)	
07/04 07:15:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.4231 (0.2604)	Architecture Loss 1.6514 (2.0861)	Prec@(1,5) (92.7%, 99.6%)	
07/04 07:15:51午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][150/390]	Step 19309	lr 0.00102	Loss 0.3107 (0.2674)	Architecture Loss 1.8226 (2.0681)	Prec@(1,5) (92.3%, 99.6%)	
07/04 07:16:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.2805 (0.2730)	Architecture Loss 2.3373 (2.0656)	Prec@(1,5) (92.3%, 99.5%)	
07/04 07:17:13午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][250/390]	Step 19409	lr 0.00102	Loss 0.2512 (0.2767)	Architecture Loss 2.2337 (2.0576)	Prec@(1,5) (92.1%, 99.6%)	
07/04 07:17:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.2216 (0.2771)	Architecture Loss 1.9527 (2.0534)	Prec@(1,5) (92.2%, 99.6%)	
07/04 07:18:34午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][350/390]	Step 19509	lr 0.00102	Loss 0.2325 (0.2789)	Architecture Loss 2.5720 (2.0514)	Prec@(1,5) (92.1%, 99.6%)	
07/04 07:19:07午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.2704 (0.2781)	Architecture Loss 2.1837 (2.0489)	Prec@(1,5) (92.1%, 99.6%)	
07/04 07:19:08午後 searchStage_trainer.py:225 [INFO] Train: [ 49/49] Final Prec@1 92.0720%
07/04 07:19:13午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][50/391]	Step 19550	Loss 2.0518	Prec@(1,5) (55.3%, 83.2%)
07/04 07:19:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 2.0370	Prec@(1,5) (55.5%, 83.3%)
07/04 07:19:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][150/391]	Step 19550	Loss 1.9922	Prec@(1,5) (56.0%, 83.7%)
07/04 07:19:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 1.9795	Prec@(1,5) (56.3%, 84.0%)
07/04 07:19:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][250/391]	Step 19550	Loss 1.9658	Prec@(1,5) (56.5%, 84.0%)
07/04 07:19:40午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 1.9902	Prec@(1,5) (56.3%, 83.8%)
07/04 07:19:46午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][350/391]	Step 19550	Loss 1.9878	Prec@(1,5) (56.3%, 83.7%)
07/04 07:19:50午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 1.9991	Prec@(1,5) (56.2%, 83.6%)
07/04 07:19:50午後 searchStage_trainer.py:264 [INFO] Valid: [ 49/49] Final Prec@1 56.1520%
07/04 07:19:51午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/04 07:19:51午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 56.2600%
07/04 07:19:51午後 searchStage_main.py:87 [INFO] Final best Prec@1 = 56.2600%
07/04 07:19:51午後 searchStage_main.py:88 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('max_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('max_pool_3x3', 5), ('skip_connect', 4)], [('skip_connect', 6), ('max_pool_3x3', 5)]], DAG3_concat=range(6, 8))
