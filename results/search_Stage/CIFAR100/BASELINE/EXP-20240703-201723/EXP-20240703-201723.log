07/03 08:17:23PM parser.py:28 [INFO] 
07/03 08:17:23PM parser.py:29 [INFO] Parameters:
07/03 08:17:23PM parser.py:31 [INFO] DAG_PATH=results/search_Stage/CIFAR100/BASELINE/EXP-20240703-201723/DAG
07/03 08:17:23PM parser.py:31 [INFO] ALPHA_LR=0.0003
07/03 08:17:23PM parser.py:31 [INFO] ALPHA_WEIGHT_DECAY=0.001
07/03 08:17:23PM parser.py:31 [INFO] BATCH_SIZE=64
07/03 08:17:23PM parser.py:31 [INFO] CHECKPOINT_RESET=False
07/03 08:17:23PM parser.py:31 [INFO] DATA_PATH=../data/
07/03 08:17:23PM parser.py:31 [INFO] DATASET=CIFAR100
07/03 08:17:23PM parser.py:31 [INFO] EPOCHS=50
07/03 08:17:23PM parser.py:31 [INFO] EXP_NAME=EXP-20240703-201723
07/03 08:17:23PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 3)]], normal1_concat=range(2, 6), reduce1=[[('max_pool_3x3', 0), ('skip_connect', 0)], [('max_pool_3x3', 0), ('max_pool_3x3', 2)], [('sep_conv_3x3', 0), ('skip_connect', 1)], [('sep_conv_3x3', 0), ('dil_conv_3x3', 4)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)]], normal2_concat=range(2, 6), reduce2=[[('dil_conv_3x3', 0), ('max_pool_3x3', 1)], [('max_pool_3x3', 1), ('dil_conv_5x5', 2)], [('max_pool_3x3', 1), ('dil_conv_5x5', 3)], [('max_pool_3x3', 1), ('skip_connect', 4)]], reduce2_concat=range(2, 6), normal3=[[('dil_conv_5x5', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 1), ('dil_conv_3x3', 2)], [('dil_conv_3x3', 0), ('sep_conv_5x5', 2)], [('sep_conv_5x5', 3), ('sep_conv_3x3', 4)]], normal3_concat=range(2, 6))
07/03 08:17:23PM parser.py:31 [INFO] GPUS=[0]
07/03 08:17:23PM parser.py:31 [INFO] INIT_CHANNELS=16
07/03 08:17:23PM parser.py:31 [INFO] LAYERS=20
07/03 08:17:23PM parser.py:31 [INFO] LOCAL_RANK=0
07/03 08:17:23PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
07/03 08:17:23PM parser.py:31 [INFO] NAME=BASELINE
07/03 08:17:23PM parser.py:31 [INFO] PATH=results/search_Stage/CIFAR100/BASELINE/EXP-20240703-201723
07/03 08:17:23PM parser.py:31 [INFO] PRINT_FREQ=50
07/03 08:17:23PM parser.py:31 [INFO] RESUME_PATH=None
07/03 08:17:23PM parser.py:31 [INFO] SAVE=EXP
07/03 08:17:23PM parser.py:31 [INFO] SEED=2
07/03 08:17:23PM parser.py:31 [INFO] SHARE_STAGE=False
07/03 08:17:23PM parser.py:31 [INFO] SPEC_CELL=True
07/03 08:17:23PM parser.py:31 [INFO] TRAIN_PORTION=0.5
07/03 08:17:23PM parser.py:31 [INFO] W_GRAD_CLIP=5.0
07/03 08:17:23PM parser.py:31 [INFO] W_LR=0.025
07/03 08:17:23PM parser.py:31 [INFO] W_LR_MIN=0.001
07/03 08:17:23PM parser.py:31 [INFO] W_MOMENTUM=0.9
07/03 08:17:23PM parser.py:31 [INFO] W_WEIGHT_DECAY=0.0003
07/03 08:17:23PM parser.py:31 [INFO] WORKERS=4
07/03 08:17:23PM parser.py:32 [INFO] 
07/03 08:17:25PM searchStage_trainer.py:103 [INFO] --> No loaded checkpoint!
####### ALPHA #######
# Alpha - DAG
tensor([[0.2502, 0.2500, 0.2500, 0.2498],
        [0.2502, 0.2497, 0.2500, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2497, 0.2503, 0.2501],
        [0.2499, 0.2499, 0.2502, 0.2500],
        [0.2502, 0.2499, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2498, 0.2503, 0.2498],
        [0.2499, 0.2501, 0.2501, 0.2500],
        [0.2500, 0.2501, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2501, 0.2501, 0.2499],
        [0.2497, 0.2504, 0.2499, 0.2501],
        [0.2501, 0.2501, 0.2501, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2501, 0.2499, 0.2502],
        [0.2502, 0.2499, 0.2499, 0.2500],
        [0.2498, 0.2501, 0.2502, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2502, 0.2496, 0.2502],
        [0.2504, 0.2499, 0.2499, 0.2498],
        [0.2501, 0.2501, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2500, 0.2501, 0.2503],
        [0.2501, 0.2498, 0.2503, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2497, 0.2503, 0.2502],
        [0.2497, 0.2501, 0.2500, 0.2502],
        [0.2504, 0.2500, 0.2498, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2500, 0.2502],
        [0.2500, 0.2503, 0.2497, 0.2499],
        [0.2497, 0.2498, 0.2502, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2503, 0.2498, 0.2500],
        [0.2497, 0.2501, 0.2501, 0.2501],
        [0.2498, 0.2499, 0.2500, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2499, 0.2502, 0.2501],
        [0.2503, 0.2500, 0.2499, 0.2498],
        [0.2502, 0.2501, 0.2497, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2499, 0.2505, 0.2499],
        [0.2497, 0.2500, 0.2501, 0.2501],
        [0.2501, 0.2503, 0.2496, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2497, 0.2502, 0.2501],
        [0.2497, 0.2500, 0.2502, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2498, 0.2502, 0.2502],
        [0.2500, 0.2500, 0.2498, 0.2502],
        [0.2496, 0.2503, 0.2503, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2500, 0.2502, 0.2498],
        [0.2497, 0.2498, 0.2504, 0.2501],
        [0.2501, 0.2500, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2503, 0.2497, 0.2498],
        [0.2501, 0.2495, 0.2503, 0.2500],
        [0.2499, 0.2502, 0.2501, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2501, 0.2499, 0.2498],
        [0.2502, 0.2500, 0.2496, 0.2502],
        [0.2498, 0.2502, 0.2500, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2499, 0.2499, 0.2502],
        [0.2499, 0.2504, 0.2497, 0.2500],
        [0.2497, 0.2500, 0.2501, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:17:44PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][50/390]	Step 50	lr 0.025	Loss 4.6014 (4.6456)	Architecture Loss 4.8997 (4.6258)	Prec@(1,5) (1.5%, 7.2%)	
07/03 08:18:01PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][100/390]	Step 100	lr 0.025	Loss 4.4645 (4.6158)	Architecture Loss 4.3008 (4.6156)	Prec@(1,5) (1.8%, 8.0%)	
07/03 08:18:19PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][150/390]	Step 150	lr 0.025	Loss 4.3612 (4.5533)	Architecture Loss 4.1845 (4.5519)	Prec@(1,5) (2.3%, 9.8%)	
07/03 08:18:36PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][200/390]	Step 200	lr 0.025	Loss 4.1915 (4.4861)	Architecture Loss 4.2789 (4.4880)	Prec@(1,5) (2.9%, 12.1%)	
07/03 08:18:54PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][250/390]	Step 250	lr 0.025	Loss 4.2692 (4.4229)	Architecture Loss 4.3364 (4.4284)	Prec@(1,5) (3.5%, 13.9%)	
07/03 08:19:11PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][300/390]	Step 300	lr 0.025	Loss 3.9089 (4.3740)	Architecture Loss 3.9192 (4.3759)	Prec@(1,5) (3.9%, 15.2%)	
07/03 08:19:29PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][350/390]	Step 350	lr 0.025	Loss 3.9655 (4.3243)	Architecture Loss 3.7374 (4.3246)	Prec@(1,5) (4.4%, 16.7%)	
07/03 08:19:43PM searchStage_trainer.py:214 [INFO] Train: Epoch: [0][390/390]	Step 390	lr 0.025	Loss 3.9460 (4.2930)	Architecture Loss 4.0260 (4.2893)	Prec@(1,5) (4.7%, 17.8%)	
07/03 08:19:44PM searchStage_trainer.py:225 [INFO] Train: [  0/49] Final Prec@1 4.6920%
07/03 08:19:47PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][50/391]	Step 391	Loss 3.9418	Prec@(1,5) (8.7%, 29.1%)
07/03 08:19:49PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][100/391]	Step 391	Loss 3.9633	Prec@(1,5) (8.5%, 28.0%)
07/03 08:19:52PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][150/391]	Step 391	Loss 3.9503	Prec@(1,5) (8.8%, 28.6%)
07/03 08:19:54PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][200/391]	Step 391	Loss 3.9477	Prec@(1,5) (8.8%, 29.0%)
07/03 08:19:56PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][250/391]	Step 391	Loss 3.9459	Prec@(1,5) (8.8%, 29.0%)
07/03 08:19:59PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][300/391]	Step 391	Loss 3.9424	Prec@(1,5) (9.0%, 29.1%)
07/03 08:20:01PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][350/391]	Step 391	Loss 3.9433	Prec@(1,5) (8.9%, 29.0%)
07/03 08:20:03PM searchStage_trainer.py:253 [INFO] Valid: Epoch: [0][390/391]	Step 391	Loss 3.9465	Prec@(1,5) (8.8%, 28.9%)
07/03 08:20:03PM searchStage_trainer.py:264 [INFO] Valid: [  0/49] Final Prec@1 8.7680%
07/03 08:20:03PM searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('avg_pool_3x3', 3)], [('skip_connect', 3), ('avg_pool_3x3', 5)], [('skip_connect', 4), ('avg_pool_3x3', 6)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 2), ('avg_pool_3x3', 3)], [('skip_connect', 3), ('avg_pool_3x3', 4)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/03 08:20:04PM searchStage_main.py:82 [INFO] Until now, best Prec@1 = 8.7680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2503, 0.2511, 0.2488, 0.2499],
        [0.2503, 0.2501, 0.2506, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2502, 0.2508, 0.2484],
        [0.2497, 0.2505, 0.2503, 0.2495],
        [0.2502, 0.2513, 0.2493, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2502, 0.2505, 0.2488],
        [0.2500, 0.2501, 0.2498, 0.2501],
        [0.2503, 0.2503, 0.2493, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2509, 0.2488, 0.2496],
        [0.2498, 0.2503, 0.2499, 0.2501],
        [0.2507, 0.2506, 0.2497, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2506, 0.2496, 0.2503],
        [0.2500, 0.2506, 0.2499, 0.2496],
        [0.2494, 0.2507, 0.2501, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2506, 0.2494, 0.2500],
        [0.2503, 0.2505, 0.2493, 0.2499],
        [0.2501, 0.2503, 0.2499, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2500, 0.2502, 0.2503],
        [0.2501, 0.2497, 0.2505, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2497, 0.2503, 0.2502],
        [0.2498, 0.2500, 0.2501, 0.2502],
        [0.2504, 0.2500, 0.2499, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2504, 0.2502, 0.2498],
        [0.2498, 0.2503, 0.2498, 0.2501],
        [0.2496, 0.2498, 0.2503, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2503, 0.2499, 0.2501],
        [0.2497, 0.2502, 0.2501, 0.2499],
        [0.2498, 0.2499, 0.2500, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2501, 0.2506, 0.2496],
        [0.2501, 0.2500, 0.2500, 0.2499],
        [0.2500, 0.2501, 0.2497, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2501, 0.2506, 0.2496],
        [0.2496, 0.2501, 0.2501, 0.2503],
        [0.2501, 0.2504, 0.2495, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2496, 0.2503, 0.2501],
        [0.2495, 0.2501, 0.2503, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2498, 0.2506, 0.2500],
        [0.2498, 0.2501, 0.2501, 0.2499],
        [0.2492, 0.2503, 0.2501, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2501, 0.2506, 0.2490],
        [0.2495, 0.2497, 0.2504, 0.2504],
        [0.2498, 0.2501, 0.2498, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2503, 0.2498, 0.2498],
        [0.2501, 0.2495, 0.2505, 0.2500],
        [0.2496, 0.2502, 0.2502, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2503, 0.2507, 0.2493],
        [0.2496, 0.2501, 0.2503, 0.2499],
        [0.2491, 0.2501, 0.2503, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2500, 0.2504, 0.2498],
        [0.2495, 0.2503, 0.2501, 0.2501],
        [0.2492, 0.2499, 0.2506, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:20:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][50/390]	Step 441	lr 0.02498	Loss 4.0888 (3.9193)	Architecture Loss 3.7680 (3.9414)	Prec@(1,5) (8.1%, 29.3%)	
07/03 08:20:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][100/390]	Step 491	lr 0.02498	Loss 3.7321 (3.8995)	Architecture Loss 3.9706 (3.8892)	Prec@(1,5) (8.6%, 29.7%)	
07/03 08:20:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][150/390]	Step 541	lr 0.02498	Loss 3.7342 (3.8770)	Architecture Loss 3.6895 (3.8699)	Prec@(1,5) (9.2%, 30.6%)	
07/03 08:21:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][200/390]	Step 591	lr 0.02498	Loss 4.0335 (3.8492)	Architecture Loss 3.7179 (3.8430)	Prec@(1,5) (9.7%, 31.3%)	
07/03 08:21:34午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][250/390]	Step 641	lr 0.02498	Loss 3.7266 (3.8245)	Architecture Loss 3.7886 (3.8116)	Prec@(1,5) (10.1%, 32.2%)	
07/03 08:21:51午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][300/390]	Step 691	lr 0.02498	Loss 3.4901 (3.8040)	Architecture Loss 3.5523 (3.7881)	Prec@(1,5) (10.5%, 32.9%)	
07/03 08:22:09午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][350/390]	Step 741	lr 0.02498	Loss 3.7422 (3.7812)	Architecture Loss 3.6247 (3.7674)	Prec@(1,5) (10.9%, 33.6%)	
07/03 08:22:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [1][390/390]	Step 781	lr 0.02498	Loss 3.5505 (3.7637)	Architecture Loss 3.4852 (3.7460)	Prec@(1,5) (11.3%, 34.2%)	
07/03 08:22:23午後 searchStage_trainer.py:225 [INFO] Train: [  1/49] Final Prec@1 11.2600%
07/03 08:22:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][50/391]	Step 782	Loss 3.6035	Prec@(1,5) (13.4%, 39.2%)
07/03 08:22:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][100/391]	Step 782	Loss 3.6093	Prec@(1,5) (13.5%, 38.6%)
07/03 08:22:31午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][150/391]	Step 782	Loss 3.5891	Prec@(1,5) (13.9%, 39.2%)
07/03 08:22:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][200/391]	Step 782	Loss 3.5949	Prec@(1,5) (13.9%, 38.9%)
07/03 08:22:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][250/391]	Step 782	Loss 3.5970	Prec@(1,5) (13.9%, 38.8%)
07/03 08:22:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][300/391]	Step 782	Loss 3.5912	Prec@(1,5) (13.9%, 38.9%)
07/03 08:22:40午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][350/391]	Step 782	Loss 3.5873	Prec@(1,5) (14.0%, 39.1%)
07/03 08:22:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [1][390/391]	Step 782	Loss 3.5870	Prec@(1,5) (14.1%, 39.2%)
07/03 08:22:42午後 searchStage_trainer.py:264 [INFO] Valid: [  1/49] Final Prec@1 14.1240%
07/03 08:22:42午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('max_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 6)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 4)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 3), ('skip_connect', 4)], [('skip_connect', 5), ('skip_connect', 3)], [('skip_connect', 6), ('skip_connect', 4)]], DAG3_concat=range(6, 8))
07/03 08:22:43午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 14.1240%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2501, 0.2512, 0.2488, 0.2498],
        [0.2502, 0.2500, 0.2508, 0.2489]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2501, 0.2510, 0.2483],
        [0.2496, 0.2506, 0.2505, 0.2493],
        [0.2502, 0.2514, 0.2493, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2503, 0.2508, 0.2482],
        [0.2500, 0.2501, 0.2498, 0.2501],
        [0.2502, 0.2502, 0.2491, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2509, 0.2488, 0.2494],
        [0.2498, 0.2503, 0.2500, 0.2499],
        [0.2506, 0.2506, 0.2495, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2506, 0.2498, 0.2502],
        [0.2500, 0.2505, 0.2500, 0.2495],
        [0.2493, 0.2507, 0.2502, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2507, 0.2496, 0.2497],
        [0.2502, 0.2506, 0.2494, 0.2498],
        [0.2500, 0.2503, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2499, 0.2505, 0.2503],
        [0.2500, 0.2497, 0.2507, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2496, 0.2507, 0.2500],
        [0.2497, 0.2499, 0.2504, 0.2500],
        [0.2502, 0.2499, 0.2499, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2504, 0.2506, 0.2494],
        [0.2497, 0.2502, 0.2499, 0.2503],
        [0.2495, 0.2498, 0.2505, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2502, 0.2501, 0.2499],
        [0.2496, 0.2502, 0.2502, 0.2500],
        [0.2496, 0.2499, 0.2501, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2501, 0.2511, 0.2494],
        [0.2499, 0.2500, 0.2504, 0.2498],
        [0.2498, 0.2500, 0.2498, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2501, 0.2510, 0.2494],
        [0.2494, 0.2500, 0.2503, 0.2503],
        [0.2500, 0.2503, 0.2496, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2495, 0.2505, 0.2501],
        [0.2494, 0.2499, 0.2505, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2496, 0.2511, 0.2496],
        [0.2498, 0.2499, 0.2507, 0.2496],
        [0.2491, 0.2502, 0.2499, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2503, 0.2514, 0.2477],
        [0.2491, 0.2495, 0.2503, 0.2511],
        [0.2496, 0.2500, 0.2498, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2503, 0.2501, 0.2497],
        [0.2500, 0.2494, 0.2505, 0.2500],
        [0.2495, 0.2502, 0.2503, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2505, 0.2513, 0.2485],
        [0.2497, 0.2501, 0.2505, 0.2497],
        [0.2487, 0.2499, 0.2499, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2503, 0.2510, 0.2488],
        [0.2494, 0.2502, 0.2498, 0.2506],
        [0.2490, 0.2497, 0.2503, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:23:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][50/390]	Step 832	lr 0.02491	Loss 3.5903 (3.5535)	Architecture Loss 3.5061 (3.5688)	Prec@(1,5) (14.5%, 40.8%)	
07/03 08:23:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][100/390]	Step 882	lr 0.02491	Loss 3.6122 (3.5204)	Architecture Loss 3.6587 (3.5564)	Prec@(1,5) (15.1%, 41.8%)	
07/03 08:23:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][150/390]	Step 932	lr 0.02491	Loss 3.6946 (3.5084)	Architecture Loss 3.3595 (3.5294)	Prec@(1,5) (15.6%, 41.8%)	
07/03 08:23:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][200/390]	Step 982	lr 0.02491	Loss 3.4382 (3.4945)	Architecture Loss 3.8673 (3.5161)	Prec@(1,5) (15.8%, 41.9%)	
07/03 08:24:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][250/390]	Step 1032	lr 0.02491	Loss 3.5815 (3.4881)	Architecture Loss 3.4731 (3.5087)	Prec@(1,5) (15.9%, 42.2%)	
07/03 08:24:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][300/390]	Step 1082	lr 0.02491	Loss 3.5968 (3.4764)	Architecture Loss 3.3948 (3.4868)	Prec@(1,5) (16.1%, 42.5%)	
07/03 08:24:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][350/390]	Step 1132	lr 0.02491	Loss 3.3616 (3.4572)	Architecture Loss 3.4389 (3.4724)	Prec@(1,5) (16.5%, 43.1%)	
07/03 08:25:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [2][390/390]	Step 1172	lr 0.02491	Loss 3.3997 (3.4442)	Architecture Loss 3.3868 (3.4555)	Prec@(1,5) (16.7%, 43.5%)	
07/03 08:25:01午後 searchStage_trainer.py:225 [INFO] Train: [  2/49] Final Prec@1 16.7160%
07/03 08:25:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][50/391]	Step 1173	Loss 3.3547	Prec@(1,5) (18.1%, 47.1%)
07/03 08:25:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][100/391]	Step 1173	Loss 3.3751	Prec@(1,5) (18.0%, 46.2%)
07/03 08:25:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][150/391]	Step 1173	Loss 3.3729	Prec@(1,5) (18.2%, 46.0%)
07/03 08:25:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][200/391]	Step 1173	Loss 3.3758	Prec@(1,5) (18.5%, 45.8%)
07/03 08:25:13午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][250/391]	Step 1173	Loss 3.3666	Prec@(1,5) (18.4%, 46.1%)
07/03 08:25:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][300/391]	Step 1173	Loss 3.3685	Prec@(1,5) (18.6%, 46.1%)
07/03 08:25:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][350/391]	Step 1173	Loss 3.3708	Prec@(1,5) (18.5%, 45.9%)
07/03 08:25:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [2][390/391]	Step 1173	Loss 3.3707	Prec@(1,5) (18.4%, 46.0%)
07/03 08:25:19午後 searchStage_trainer.py:264 [INFO] Valid: [  2/49] Final Prec@1 18.4200%
07/03 08:25:19午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('max_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('avg_pool_3x3', 3)], [('avg_pool_3x3', 5), ('avg_pool_3x3', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('max_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('avg_pool_3x3', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/03 08:25:20午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 18.4200%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2501, 0.2511, 0.2491, 0.2498],
        [0.2502, 0.2499, 0.2510, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2500, 0.2514, 0.2482],
        [0.2494, 0.2504, 0.2510, 0.2492],
        [0.2501, 0.2514, 0.2494, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2502, 0.2513, 0.2474],
        [0.2499, 0.2500, 0.2497, 0.2504],
        [0.2502, 0.2500, 0.2492, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2509, 0.2490, 0.2493],
        [0.2497, 0.2502, 0.2500, 0.2501],
        [0.2506, 0.2506, 0.2497, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2505, 0.2502, 0.2499],
        [0.2500, 0.2505, 0.2501, 0.2494],
        [0.2490, 0.2506, 0.2502, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2506, 0.2498, 0.2496],
        [0.2501, 0.2506, 0.2496, 0.2497],
        [0.2499, 0.2503, 0.2498, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2499, 0.2507, 0.2503],
        [0.2498, 0.2496, 0.2509, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2496, 0.2510, 0.2499],
        [0.2495, 0.2499, 0.2507, 0.2499],
        [0.2501, 0.2498, 0.2500, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2505, 0.2510, 0.2489],
        [0.2495, 0.2500, 0.2499, 0.2505],
        [0.2494, 0.2497, 0.2505, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2502, 0.2503, 0.2499],
        [0.2495, 0.2501, 0.2504, 0.2499],
        [0.2495, 0.2498, 0.2501, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2501, 0.2516, 0.2490],
        [0.2496, 0.2498, 0.2506, 0.2500],
        [0.2496, 0.2499, 0.2499, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2500, 0.2512, 0.2493],
        [0.2493, 0.2500, 0.2504, 0.2503],
        [0.2499, 0.2502, 0.2496, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2494, 0.2508, 0.2502],
        [0.2493, 0.2498, 0.2507, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2494, 0.2518, 0.2492],
        [0.2499, 0.2496, 0.2512, 0.2493],
        [0.2489, 0.2500, 0.2497, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2505, 0.2523, 0.2460],
        [0.2489, 0.2493, 0.2502, 0.2516],
        [0.2493, 0.2498, 0.2497, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2500, 0.2503, 0.2503, 0.2494],
        [0.2498, 0.2493, 0.2506, 0.2503],
        [0.2493, 0.2501, 0.2504, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2507, 0.2520, 0.2474],
        [0.2495, 0.2499, 0.2506, 0.2499],
        [0.2483, 0.2496, 0.2497, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2506, 0.2516, 0.2476],
        [0.2493, 0.2501, 0.2498, 0.2509],
        [0.2486, 0.2495, 0.2501, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:25:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][50/390]	Step 1223	lr 0.02479	Loss 3.2757 (3.2333)	Architecture Loss 3.6711 (3.3226)	Prec@(1,5) (20.1%, 49.1%)	
07/03 08:25:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][100/390]	Step 1273	lr 0.02479	Loss 3.2947 (3.2480)	Architecture Loss 3.1933 (3.3228)	Prec@(1,5) (20.2%, 48.6%)	
07/03 08:26:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][150/390]	Step 1323	lr 0.02479	Loss 3.6041 (3.2622)	Architecture Loss 3.4389 (3.3154)	Prec@(1,5) (19.7%, 48.0%)	
07/03 08:26:31午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][200/390]	Step 1373	lr 0.02479	Loss 3.2705 (3.2583)	Architecture Loss 3.1859 (3.2939)	Prec@(1,5) (19.8%, 48.4%)	
07/03 08:26:49午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][250/390]	Step 1423	lr 0.02479	Loss 3.0094 (3.2417)	Architecture Loss 3.3447 (3.2918)	Prec@(1,5) (20.2%, 48.7%)	
07/03 08:27:07午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][300/390]	Step 1473	lr 0.02479	Loss 2.9453 (3.2413)	Architecture Loss 3.3367 (3.2785)	Prec@(1,5) (20.2%, 48.8%)	
07/03 08:27:24午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][350/390]	Step 1523	lr 0.02479	Loss 3.3140 (3.2384)	Architecture Loss 3.0964 (3.2733)	Prec@(1,5) (20.2%, 48.7%)	
07/03 08:27:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [3][390/390]	Step 1563	lr 0.02479	Loss 3.0047 (3.2322)	Architecture Loss 3.0869 (3.2708)	Prec@(1,5) (20.4%, 49.0%)	
07/03 08:27:39午後 searchStage_trainer.py:225 [INFO] Train: [  3/49] Final Prec@1 20.3840%
07/03 08:27:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][50/391]	Step 1564	Loss 3.2176	Prec@(1,5) (19.9%, 50.3%)
07/03 08:27:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][100/391]	Step 1564	Loss 3.2316	Prec@(1,5) (19.7%, 49.5%)
07/03 08:27:46午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][150/391]	Step 1564	Loss 3.2262	Prec@(1,5) (20.2%, 49.3%)
07/03 08:27:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][200/391]	Step 1564	Loss 3.2376	Prec@(1,5) (20.0%, 49.2%)
07/03 08:27:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][250/391]	Step 1564	Loss 3.2429	Prec@(1,5) (20.1%, 49.1%)
07/03 08:27:53午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][300/391]	Step 1564	Loss 3.2340	Prec@(1,5) (20.2%, 49.5%)
07/03 08:27:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][350/391]	Step 1564	Loss 3.2345	Prec@(1,5) (20.3%, 49.5%)
07/03 08:27:57午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [3][390/391]	Step 1564	Loss 3.2385	Prec@(1,5) (20.2%, 49.4%)
07/03 08:27:57午後 searchStage_trainer.py:264 [INFO] Valid: [  3/49] Final Prec@1 20.2040%
07/03 08:27:57午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('avg_pool_3x3', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('avg_pool_3x3', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG3_concat=range(6, 8))
07/03 08:27:58午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 20.2040%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2498, 0.2510, 0.2494, 0.2498],
        [0.2499, 0.2498, 0.2513, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2499, 0.2519, 0.2481],
        [0.2489, 0.2504, 0.2516, 0.2491],
        [0.2500, 0.2514, 0.2494, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2502, 0.2520, 0.2470],
        [0.2498, 0.2499, 0.2498, 0.2505],
        [0.2499, 0.2500, 0.2494, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2510, 0.2494, 0.2490],
        [0.2494, 0.2502, 0.2502, 0.2502],
        [0.2503, 0.2505, 0.2499, 0.2493]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2505, 0.2507, 0.2497],
        [0.2499, 0.2503, 0.2504, 0.2493],
        [0.2488, 0.2505, 0.2504, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2506, 0.2501, 0.2494],
        [0.2500, 0.2506, 0.2496, 0.2498],
        [0.2498, 0.2503, 0.2499, 0.2500]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2499, 0.2509, 0.2503],
        [0.2496, 0.2495, 0.2512, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2497, 0.2515, 0.2498],
        [0.2491, 0.2499, 0.2511, 0.2498],
        [0.2499, 0.2497, 0.2501, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2507, 0.2516, 0.2483],
        [0.2493, 0.2499, 0.2501, 0.2507],
        [0.2492, 0.2496, 0.2506, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2502, 0.2505, 0.2498],
        [0.2494, 0.2501, 0.2506, 0.2499],
        [0.2494, 0.2497, 0.2502, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2521, 0.2487],
        [0.2494, 0.2497, 0.2509, 0.2500],
        [0.2494, 0.2497, 0.2500, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2500, 0.2515, 0.2491],
        [0.2492, 0.2499, 0.2507, 0.2502],
        [0.2497, 0.2501, 0.2497, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2493, 0.2511, 0.2502],
        [0.2491, 0.2497, 0.2510, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2492, 0.2523, 0.2488],
        [0.2499, 0.2494, 0.2517, 0.2491],
        [0.2487, 0.2498, 0.2496, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2507, 0.2533, 0.2444],
        [0.2486, 0.2491, 0.2501, 0.2522],
        [0.2491, 0.2496, 0.2496, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2503, 0.2507, 0.2491],
        [0.2497, 0.2491, 0.2507, 0.2505],
        [0.2492, 0.2499, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2509, 0.2527, 0.2462],
        [0.2495, 0.2497, 0.2507, 0.2501],
        [0.2480, 0.2492, 0.2495, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2507, 0.2521, 0.2468],
        [0.2491, 0.2499, 0.2499, 0.2511],
        [0.2484, 0.2493, 0.2501, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:28:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][50/390]	Step 1614	lr 0.02462	Loss 3.1096 (3.1367)	Architecture Loss 3.3716 (3.2056)	Prec@(1,5) (21.8%, 51.6%)	
07/03 08:28:34午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][100/390]	Step 1664	lr 0.02462	Loss 3.3781 (3.1007)	Architecture Loss 2.9849 (3.1887)	Prec@(1,5) (22.0%, 52.4%)	
07/03 08:28:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][150/390]	Step 1714	lr 0.02462	Loss 2.9065 (3.0812)	Architecture Loss 2.6438 (3.1649)	Prec@(1,5) (22.6%, 53.1%)	
07/03 08:29:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][200/390]	Step 1764	lr 0.02462	Loss 3.1450 (3.0835)	Architecture Loss 3.0069 (3.1614)	Prec@(1,5) (22.7%, 53.1%)	
07/03 08:29:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][250/390]	Step 1814	lr 0.02462	Loss 3.4041 (3.0699)	Architecture Loss 3.2209 (3.1509)	Prec@(1,5) (23.0%, 53.5%)	
07/03 08:29:45午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][300/390]	Step 1864	lr 0.02462	Loss 3.3756 (3.0658)	Architecture Loss 3.4725 (3.1410)	Prec@(1,5) (23.0%, 53.7%)	
07/03 08:30:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][350/390]	Step 1914	lr 0.02462	Loss 2.8370 (3.0692)	Architecture Loss 3.0055 (3.1292)	Prec@(1,5) (22.9%, 53.6%)	
07/03 08:30:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [4][390/390]	Step 1954	lr 0.02462	Loss 2.9702 (3.0634)	Architecture Loss 3.2695 (3.1238)	Prec@(1,5) (23.0%, 53.7%)	
07/03 08:30:18午後 searchStage_trainer.py:225 [INFO] Train: [  4/49] Final Prec@1 22.9720%
07/03 08:30:20午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][50/391]	Step 1955	Loss 3.0846	Prec@(1,5) (22.8%, 53.4%)
07/03 08:30:23午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][100/391]	Step 1955	Loss 3.1050	Prec@(1,5) (22.3%, 53.2%)
07/03 08:30:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][150/391]	Step 1955	Loss 3.0845	Prec@(1,5) (22.7%, 53.8%)
07/03 08:30:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][200/391]	Step 1955	Loss 3.0890	Prec@(1,5) (22.8%, 53.7%)
07/03 08:30:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][250/391]	Step 1955	Loss 3.0907	Prec@(1,5) (22.8%, 53.5%)
07/03 08:30:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][300/391]	Step 1955	Loss 3.1012	Prec@(1,5) (22.6%, 53.4%)
07/03 08:30:34午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][350/391]	Step 1955	Loss 3.0967	Prec@(1,5) (22.7%, 53.5%)
07/03 08:30:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [4][390/391]	Step 1955	Loss 3.0947	Prec@(1,5) (22.7%, 53.6%)
07/03 08:30:36午後 searchStage_trainer.py:264 [INFO] Valid: [  4/49] Final Prec@1 22.6600%
07/03 08:30:36午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:30:37午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 22.6600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2496, 0.2509, 0.2497, 0.2498],
        [0.2497, 0.2497, 0.2516, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2497, 0.2527, 0.2478],
        [0.2486, 0.2502, 0.2523, 0.2489],
        [0.2498, 0.2513, 0.2494, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2501, 0.2530, 0.2465],
        [0.2496, 0.2499, 0.2499, 0.2506],
        [0.2496, 0.2499, 0.2496, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2509, 0.2495, 0.2489],
        [0.2493, 0.2502, 0.2504, 0.2501],
        [0.2502, 0.2505, 0.2499, 0.2494]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2504, 0.2511, 0.2495],
        [0.2496, 0.2503, 0.2509, 0.2492],
        [0.2485, 0.2505, 0.2505, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2505, 0.2505, 0.2491],
        [0.2499, 0.2505, 0.2497, 0.2499],
        [0.2497, 0.2503, 0.2499, 0.2501]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2498, 0.2511, 0.2503],
        [0.2494, 0.2494, 0.2514, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2497, 0.2519, 0.2497],
        [0.2489, 0.2499, 0.2515, 0.2496],
        [0.2497, 0.2497, 0.2502, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2509, 0.2521, 0.2477],
        [0.2491, 0.2499, 0.2502, 0.2508],
        [0.2490, 0.2495, 0.2506, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2503, 0.2507, 0.2496],
        [0.2493, 0.2500, 0.2508, 0.2499],
        [0.2493, 0.2497, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2502, 0.2527, 0.2482],
        [0.2492, 0.2496, 0.2512, 0.2501],
        [0.2491, 0.2496, 0.2500, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2500, 0.2518, 0.2489],
        [0.2491, 0.2499, 0.2508, 0.2502],
        [0.2495, 0.2500, 0.2498, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2491, 0.2514, 0.2502],
        [0.2489, 0.2495, 0.2512, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2490, 0.2528, 0.2486],
        [0.2498, 0.2491, 0.2521, 0.2489],
        [0.2487, 0.2497, 0.2495, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2509, 0.2543, 0.2428],
        [0.2484, 0.2488, 0.2500, 0.2528],
        [0.2489, 0.2495, 0.2496, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2503, 0.2510, 0.2487],
        [0.2496, 0.2490, 0.2508, 0.2506],
        [0.2490, 0.2498, 0.2505, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2510, 0.2533, 0.2454],
        [0.2494, 0.2495, 0.2510, 0.2501],
        [0.2477, 0.2489, 0.2494, 0.2540]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2508, 0.2526, 0.2460],
        [0.2490, 0.2498, 0.2500, 0.2512],
        [0.2482, 0.2490, 0.2500, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:30:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][50/390]	Step 2005	lr 0.02441	Loss 3.0157 (2.8749)	Architecture Loss 2.6565 (3.0831)	Prec@(1,5) (26.1%, 58.7%)	
07/03 08:31:13午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][100/390]	Step 2055	lr 0.02441	Loss 3.1687 (2.9003)	Architecture Loss 3.0510 (3.0724)	Prec@(1,5) (26.4%, 58.1%)	
07/03 08:31:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][150/390]	Step 2105	lr 0.02441	Loss 3.1353 (2.8896)	Architecture Loss 2.7756 (3.0469)	Prec@(1,5) (26.5%, 58.3%)	
07/03 08:31:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][200/390]	Step 2155	lr 0.02441	Loss 2.8188 (2.8944)	Architecture Loss 2.9265 (3.0415)	Prec@(1,5) (26.6%, 58.0%)	
07/03 08:32:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][250/390]	Step 2205	lr 0.02441	Loss 3.2131 (2.9070)	Architecture Loss 2.8963 (3.0265)	Prec@(1,5) (26.3%, 57.6%)	
07/03 08:32:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][300/390]	Step 2255	lr 0.02441	Loss 2.8510 (2.9197)	Architecture Loss 2.8664 (3.0209)	Prec@(1,5) (26.1%, 57.4%)	
07/03 08:32:41午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][350/390]	Step 2305	lr 0.02441	Loss 2.8124 (2.9128)	Architecture Loss 3.1037 (3.0096)	Prec@(1,5) (26.3%, 57.5%)	
07/03 08:32:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [5][390/390]	Step 2345	lr 0.02441	Loss 2.9895 (2.9071)	Architecture Loss 3.0513 (2.9977)	Prec@(1,5) (26.3%, 57.8%)	
07/03 08:32:56午後 searchStage_trainer.py:225 [INFO] Train: [  5/49] Final Prec@1 26.3360%
07/03 08:32:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][50/391]	Step 2346	Loss 2.9627	Prec@(1,5) (25.2%, 56.6%)
07/03 08:33:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][100/391]	Step 2346	Loss 2.9480	Prec@(1,5) (26.0%, 57.2%)
07/03 08:33:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][150/391]	Step 2346	Loss 2.9294	Prec@(1,5) (26.5%, 57.7%)
07/03 08:33:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][200/391]	Step 2346	Loss 2.9164	Prec@(1,5) (26.6%, 57.8%)
07/03 08:33:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][250/391]	Step 2346	Loss 2.9046	Prec@(1,5) (26.8%, 58.0%)
07/03 08:33:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][300/391]	Step 2346	Loss 2.9044	Prec@(1,5) (26.8%, 58.0%)
07/03 08:33:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][350/391]	Step 2346	Loss 2.9083	Prec@(1,5) (26.7%, 57.8%)
07/03 08:33:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [5][390/391]	Step 2346	Loss 2.9036	Prec@(1,5) (26.8%, 57.9%)
07/03 08:33:14午後 searchStage_trainer.py:264 [INFO] Valid: [  5/49] Final Prec@1 26.8320%
07/03 08:33:14午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:33:15午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 26.8320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2508, 0.2499, 0.2498],
        [0.2496, 0.2496, 0.2518, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2495, 0.2534, 0.2476],
        [0.2483, 0.2500, 0.2529, 0.2489],
        [0.2498, 0.2512, 0.2494, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2501, 0.2499, 0.2541, 0.2458],
        [0.2494, 0.2498, 0.2501, 0.2508],
        [0.2495, 0.2498, 0.2496, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2509, 0.2498, 0.2488],
        [0.2493, 0.2501, 0.2504, 0.2502],
        [0.2501, 0.2504, 0.2500, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2502, 0.2515, 0.2493],
        [0.2494, 0.2501, 0.2513, 0.2492],
        [0.2483, 0.2504, 0.2507, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2504, 0.2509, 0.2488],
        [0.2498, 0.2504, 0.2497, 0.2500],
        [0.2495, 0.2503, 0.2501, 0.2502]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2498, 0.2513, 0.2503],
        [0.2493, 0.2494, 0.2515, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2497, 0.2522, 0.2496],
        [0.2488, 0.2499, 0.2518, 0.2495],
        [0.2496, 0.2496, 0.2502, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2511, 0.2526, 0.2472],
        [0.2490, 0.2498, 0.2502, 0.2510],
        [0.2489, 0.2495, 0.2507, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2502, 0.2509, 0.2495],
        [0.2492, 0.2500, 0.2509, 0.2500],
        [0.2492, 0.2496, 0.2503, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2503, 0.2531, 0.2480],
        [0.2489, 0.2496, 0.2514, 0.2500],
        [0.2489, 0.2495, 0.2500, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2501, 0.2520, 0.2487],
        [0.2491, 0.2498, 0.2508, 0.2503],
        [0.2494, 0.2500, 0.2499, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2490, 0.2516, 0.2502],
        [0.2488, 0.2494, 0.2514, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2488, 0.2534, 0.2484],
        [0.2497, 0.2488, 0.2526, 0.2489],
        [0.2485, 0.2496, 0.2495, 0.2523]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2512, 0.2556, 0.2412],
        [0.2482, 0.2486, 0.2500, 0.2531],
        [0.2487, 0.2493, 0.2495, 0.2525]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2504, 0.2513, 0.2484],
        [0.2495, 0.2489, 0.2509, 0.2508],
        [0.2489, 0.2496, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2510, 0.2539, 0.2447],
        [0.2493, 0.2493, 0.2512, 0.2502],
        [0.2475, 0.2487, 0.2494, 0.2545]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2509, 0.2532, 0.2453],
        [0.2488, 0.2497, 0.2501, 0.2515],
        [0.2480, 0.2488, 0.2500, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:33:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][50/390]	Step 2396	lr 0.02416	Loss 3.0726 (2.7798)	Architecture Loss 2.8377 (2.9169)	Prec@(1,5) (29.1%, 60.7%)	
07/03 08:33:51午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][100/390]	Step 2446	lr 0.02416	Loss 2.8240 (2.7808)	Architecture Loss 2.9112 (2.8980)	Prec@(1,5) (28.9%, 60.4%)	
07/03 08:34:09午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][150/390]	Step 2496	lr 0.02416	Loss 3.0128 (2.7727)	Architecture Loss 2.9122 (2.8876)	Prec@(1,5) (29.1%, 60.5%)	
07/03 08:34:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][200/390]	Step 2546	lr 0.02416	Loss 2.5410 (2.7780)	Architecture Loss 2.8844 (2.8972)	Prec@(1,5) (29.0%, 60.8%)	
07/03 08:34:44午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][250/390]	Step 2596	lr 0.02416	Loss 3.0419 (2.7619)	Architecture Loss 2.2425 (2.8899)	Prec@(1,5) (29.4%, 61.2%)	
07/03 08:35:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][300/390]	Step 2646	lr 0.02416	Loss 3.0399 (2.7610)	Architecture Loss 2.9500 (2.8816)	Prec@(1,5) (29.5%, 61.3%)	
07/03 08:35:19午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][350/390]	Step 2696	lr 0.02416	Loss 2.6712 (2.7544)	Architecture Loss 2.7612 (2.8728)	Prec@(1,5) (29.6%, 61.3%)	
07/03 08:35:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [6][390/390]	Step 2736	lr 0.02416	Loss 2.6818 (2.7550)	Architecture Loss 2.7944 (2.8674)	Prec@(1,5) (29.5%, 61.2%)	
07/03 08:35:33午後 searchStage_trainer.py:225 [INFO] Train: [  6/49] Final Prec@1 29.5720%
07/03 08:35:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][50/391]	Step 2737	Loss 2.8661	Prec@(1,5) (27.8%, 58.6%)
07/03 08:35:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][100/391]	Step 2737	Loss 2.8807	Prec@(1,5) (28.1%, 58.4%)
07/03 08:35:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][150/391]	Step 2737	Loss 2.8819	Prec@(1,5) (28.0%, 58.5%)
07/03 08:35:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][200/391]	Step 2737	Loss 2.8770	Prec@(1,5) (28.3%, 58.7%)
07/03 08:35:46午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][250/391]	Step 2737	Loss 2.8675	Prec@(1,5) (28.3%, 59.0%)
07/03 08:35:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][300/391]	Step 2737	Loss 2.8567	Prec@(1,5) (28.4%, 59.2%)
07/03 08:35:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][350/391]	Step 2737	Loss 2.8487	Prec@(1,5) (28.5%, 59.6%)
07/03 08:35:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [6][390/391]	Step 2737	Loss 2.8481	Prec@(1,5) (28.5%, 59.7%)
07/03 08:35:53午後 searchStage_trainer.py:264 [INFO] Valid: [  6/49] Final Prec@1 28.5280%
07/03 08:35:53午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('avg_pool_3x3', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('avg_pool_3x3', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:35:53午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 28.5280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2495, 0.2507, 0.2500, 0.2498],
        [0.2495, 0.2495, 0.2519, 0.2490]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2492, 0.2541, 0.2476],
        [0.2479, 0.2498, 0.2535, 0.2488],
        [0.2496, 0.2512, 0.2495, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2498, 0.2553, 0.2450],
        [0.2492, 0.2497, 0.2501, 0.2509],
        [0.2493, 0.2497, 0.2497, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2509, 0.2499, 0.2487],
        [0.2492, 0.2500, 0.2506, 0.2503],
        [0.2500, 0.2504, 0.2501, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2501, 0.2519, 0.2493],
        [0.2491, 0.2500, 0.2517, 0.2491],
        [0.2481, 0.2503, 0.2508, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2502, 0.2514, 0.2485],
        [0.2497, 0.2504, 0.2498, 0.2501],
        [0.2493, 0.2502, 0.2501, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2497, 0.2515, 0.2503],
        [0.2492, 0.2493, 0.2517, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2496, 0.2525, 0.2496],
        [0.2486, 0.2499, 0.2521, 0.2494],
        [0.2495, 0.2496, 0.2502, 0.2506]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2512, 0.2531, 0.2468],
        [0.2489, 0.2497, 0.2502, 0.2512],
        [0.2488, 0.2494, 0.2508, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2502, 0.2510, 0.2495],
        [0.2491, 0.2499, 0.2510, 0.2500],
        [0.2491, 0.2496, 0.2504, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2503, 0.2535, 0.2477],
        [0.2487, 0.2495, 0.2516, 0.2501],
        [0.2488, 0.2495, 0.2501, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2500, 0.2523, 0.2485],
        [0.2490, 0.2497, 0.2509, 0.2503],
        [0.2493, 0.2499, 0.2499, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2490, 0.2517, 0.2503],
        [0.2487, 0.2493, 0.2516, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2486, 0.2537, 0.2483],
        [0.2497, 0.2486, 0.2529, 0.2489],
        [0.2485, 0.2496, 0.2495, 0.2524]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2511, 0.2564, 0.2401],
        [0.2481, 0.2485, 0.2500, 0.2534],
        [0.2485, 0.2492, 0.2495, 0.2527]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2504, 0.2515, 0.2482],
        [0.2494, 0.2488, 0.2510, 0.2508],
        [0.2488, 0.2495, 0.2506, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2505, 0.2510, 0.2544, 0.2440],
        [0.2492, 0.2491, 0.2513, 0.2503],
        [0.2472, 0.2485, 0.2494, 0.2549]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2509, 0.2537, 0.2447],
        [0.2486, 0.2495, 0.2502, 0.2516],
        [0.2478, 0.2486, 0.2500, 0.2535]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:36:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][50/390]	Step 2787	lr 0.02386	Loss 2.8256 (2.6346)	Architecture Loss 2.6850 (2.8167)	Prec@(1,5) (31.1%, 64.0%)	
07/03 08:36:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][100/390]	Step 2837	lr 0.02386	Loss 2.6487 (2.6623)	Architecture Loss 2.8489 (2.8012)	Prec@(1,5) (30.9%, 63.6%)	
07/03 08:36:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][150/390]	Step 2887	lr 0.02386	Loss 2.8529 (2.6688)	Architecture Loss 2.5539 (2.7910)	Prec@(1,5) (30.7%, 63.4%)	
07/03 08:37:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][200/390]	Step 2937	lr 0.02386	Loss 2.6887 (2.6554)	Architecture Loss 2.9215 (2.7777)	Prec@(1,5) (31.1%, 63.5%)	
07/03 08:37:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][250/390]	Step 2987	lr 0.02386	Loss 2.4837 (2.6501)	Architecture Loss 2.8651 (2.7750)	Prec@(1,5) (31.1%, 63.5%)	
07/03 08:37:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][300/390]	Step 3037	lr 0.02386	Loss 2.4033 (2.6422)	Architecture Loss 2.7459 (2.7725)	Prec@(1,5) (31.3%, 63.7%)	
07/03 08:37:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][350/390]	Step 3087	lr 0.02386	Loss 2.2531 (2.6335)	Architecture Loss 2.6706 (2.7713)	Prec@(1,5) (31.5%, 63.8%)	
07/03 08:38:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [7][390/390]	Step 3127	lr 0.02386	Loss 2.3605 (2.6248)	Architecture Loss 2.7755 (2.7689)	Prec@(1,5) (31.8%, 64.1%)	
07/03 08:38:13午後 searchStage_trainer.py:225 [INFO] Train: [  7/49] Final Prec@1 31.8080%
07/03 08:38:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][50/391]	Step 3128	Loss 2.6454	Prec@(1,5) (31.2%, 64.0%)
07/03 08:38:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][100/391]	Step 3128	Loss 2.6727	Prec@(1,5) (31.2%, 63.4%)
07/03 08:38:20午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][150/391]	Step 3128	Loss 2.6799	Prec@(1,5) (31.2%, 63.2%)
07/03 08:38:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][200/391]	Step 3128	Loss 2.7027	Prec@(1,5) (30.8%, 62.8%)
07/03 08:38:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][250/391]	Step 3128	Loss 2.6982	Prec@(1,5) (30.9%, 63.0%)
07/03 08:38:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][300/391]	Step 3128	Loss 2.6949	Prec@(1,5) (31.0%, 63.1%)
07/03 08:38:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][350/391]	Step 3128	Loss 2.6986	Prec@(1,5) (30.9%, 62.9%)
07/03 08:38:31午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [7][390/391]	Step 3128	Loss 2.6966	Prec@(1,5) (30.8%, 63.0%)
07/03 08:38:31午後 searchStage_trainer.py:264 [INFO] Valid: [  7/49] Final Prec@1 30.8360%
07/03 08:38:31午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:38:32午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 30.8360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2494, 0.2507, 0.2501, 0.2498],
        [0.2494, 0.2494, 0.2521, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2490, 0.2549, 0.2475],
        [0.2474, 0.2495, 0.2542, 0.2489],
        [0.2496, 0.2512, 0.2496, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2494, 0.2567, 0.2447],
        [0.2490, 0.2497, 0.2503, 0.2510],
        [0.2490, 0.2497, 0.2499, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2508, 0.2500, 0.2488],
        [0.2491, 0.2499, 0.2507, 0.2502],
        [0.2499, 0.2503, 0.2502, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2499, 0.2525, 0.2492],
        [0.2488, 0.2499, 0.2521, 0.2491],
        [0.2480, 0.2503, 0.2509, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2501, 0.2519, 0.2485],
        [0.2495, 0.2504, 0.2499, 0.2502],
        [0.2492, 0.2503, 0.2503, 0.2503]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2497, 0.2516, 0.2503],
        [0.2491, 0.2492, 0.2519, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2496, 0.2528, 0.2495],
        [0.2484, 0.2499, 0.2524, 0.2494],
        [0.2495, 0.2496, 0.2502, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2513, 0.2535, 0.2464],
        [0.2488, 0.2496, 0.2503, 0.2513],
        [0.2488, 0.2494, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2502, 0.2511, 0.2494],
        [0.2490, 0.2499, 0.2511, 0.2500],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2503, 0.2539, 0.2476],
        [0.2486, 0.2495, 0.2518, 0.2500],
        [0.2487, 0.2494, 0.2501, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2524, 0.2484],
        [0.2490, 0.2497, 0.2510, 0.2504],
        [0.2492, 0.2499, 0.2500, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2489, 0.2518, 0.2503],
        [0.2486, 0.2493, 0.2517, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2484, 0.2541, 0.2482],
        [0.2496, 0.2484, 0.2532, 0.2488],
        [0.2484, 0.2496, 0.2495, 0.2525]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2512, 0.2574, 0.2389],
        [0.2480, 0.2484, 0.2500, 0.2537],
        [0.2484, 0.2491, 0.2495, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2499, 0.2504, 0.2517, 0.2480],
        [0.2493, 0.2488, 0.2511, 0.2509],
        [0.2487, 0.2495, 0.2506, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2511, 0.2549, 0.2434],
        [0.2491, 0.2490, 0.2515, 0.2505],
        [0.2470, 0.2484, 0.2495, 0.2551]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2509, 0.2541, 0.2442],
        [0.2485, 0.2495, 0.2504, 0.2517],
        [0.2477, 0.2485, 0.2500, 0.2539]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:38:51午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][50/390]	Step 3178	lr 0.02352	Loss 2.5535 (2.4975)	Architecture Loss 2.6572 (2.7253)	Prec@(1,5) (33.3%, 67.8%)	
07/03 08:39:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][100/390]	Step 3228	lr 0.02352	Loss 2.2619 (2.4969)	Architecture Loss 2.3289 (2.6979)	Prec@(1,5) (33.7%, 67.9%)	
07/03 08:39:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][150/390]	Step 3278	lr 0.02352	Loss 2.8740 (2.4968)	Architecture Loss 2.9547 (2.6947)	Prec@(1,5) (34.0%, 67.5%)	
07/03 08:39:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][200/390]	Step 3328	lr 0.02352	Loss 2.7543 (2.4927)	Architecture Loss 2.4564 (2.6766)	Prec@(1,5) (34.3%, 67.5%)	
07/03 08:40:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][250/390]	Step 3378	lr 0.02352	Loss 3.0432 (2.4889)	Architecture Loss 2.8259 (2.6731)	Prec@(1,5) (34.6%, 67.4%)	
07/03 08:40:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][300/390]	Step 3428	lr 0.02352	Loss 2.6639 (2.4869)	Architecture Loss 2.7666 (2.6653)	Prec@(1,5) (34.6%, 67.4%)	
07/03 08:40:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][350/390]	Step 3478	lr 0.02352	Loss 2.3240 (2.4881)	Architecture Loss 2.7243 (2.6686)	Prec@(1,5) (34.8%, 67.5%)	
07/03 08:40:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [8][390/390]	Step 3518	lr 0.02352	Loss 2.5017 (2.4873)	Architecture Loss 2.8869 (2.6619)	Prec@(1,5) (34.8%, 67.6%)	
07/03 08:40:51午後 searchStage_trainer.py:225 [INFO] Train: [  8/49] Final Prec@1 34.8560%
07/03 08:40:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][50/391]	Step 3519	Loss 2.7177	Prec@(1,5) (31.0%, 62.3%)
07/03 08:40:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][100/391]	Step 3519	Loss 2.7106	Prec@(1,5) (31.6%, 62.6%)
07/03 08:40:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][150/391]	Step 3519	Loss 2.7050	Prec@(1,5) (31.3%, 62.8%)
07/03 08:41:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][200/391]	Step 3519	Loss 2.6943	Prec@(1,5) (31.5%, 63.0%)
07/03 08:41:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][250/391]	Step 3519	Loss 2.6920	Prec@(1,5) (31.4%, 63.0%)
07/03 08:41:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][300/391]	Step 3519	Loss 2.6909	Prec@(1,5) (31.5%, 63.1%)
07/03 08:41:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][350/391]	Step 3519	Loss 2.6883	Prec@(1,5) (31.6%, 63.0%)
07/03 08:41:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [8][390/391]	Step 3519	Loss 2.6841	Prec@(1,5) (31.7%, 63.0%)
07/03 08:41:10午後 searchStage_trainer.py:264 [INFO] Valid: [  8/49] Final Prec@1 31.7440%
07/03 08:41:10午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('avg_pool_3x3', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:41:10午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 31.7440%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2493, 0.2506, 0.2503, 0.2498],
        [0.2493, 0.2494, 0.2523, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2487, 0.2555, 0.2475],
        [0.2471, 0.2494, 0.2547, 0.2488],
        [0.2495, 0.2512, 0.2496, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2490, 0.2584, 0.2442],
        [0.2489, 0.2497, 0.2504, 0.2511],
        [0.2489, 0.2497, 0.2500, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2504, 0.2508, 0.2500, 0.2488],
        [0.2490, 0.2499, 0.2508, 0.2503],
        [0.2499, 0.2503, 0.2503, 0.2495]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2498, 0.2531, 0.2492],
        [0.2485, 0.2499, 0.2525, 0.2491],
        [0.2478, 0.2503, 0.2510, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2500, 0.2525, 0.2482],
        [0.2494, 0.2504, 0.2499, 0.2503],
        [0.2490, 0.2502, 0.2503, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2496, 0.2517, 0.2503],
        [0.2490, 0.2492, 0.2520, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2495, 0.2530, 0.2495],
        [0.2482, 0.2498, 0.2527, 0.2494],
        [0.2495, 0.2495, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2514, 0.2540, 0.2461],
        [0.2488, 0.2496, 0.2503, 0.2513],
        [0.2487, 0.2494, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2502, 0.2512, 0.2494],
        [0.2490, 0.2498, 0.2512, 0.2500],
        [0.2490, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2503, 0.2541, 0.2475],
        [0.2485, 0.2495, 0.2520, 0.2500],
        [0.2487, 0.2493, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2526, 0.2482],
        [0.2490, 0.2496, 0.2510, 0.2504],
        [0.2492, 0.2499, 0.2500, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2488, 0.2520, 0.2503],
        [0.2485, 0.2492, 0.2518, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2483, 0.2544, 0.2481],
        [0.2495, 0.2482, 0.2535, 0.2488],
        [0.2484, 0.2495, 0.2495, 0.2526]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2513, 0.2583, 0.2379],
        [0.2478, 0.2483, 0.2500, 0.2539],
        [0.2483, 0.2491, 0.2495, 0.2532]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2504, 0.2519, 0.2479],
        [0.2492, 0.2487, 0.2511, 0.2509],
        [0.2486, 0.2494, 0.2507, 0.2513]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2506, 0.2511, 0.2553, 0.2430],
        [0.2489, 0.2488, 0.2516, 0.2506],
        [0.2469, 0.2483, 0.2495, 0.2553]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2510, 0.2545, 0.2437],
        [0.2484, 0.2494, 0.2505, 0.2517],
        [0.2476, 0.2483, 0.2500, 0.2541]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:41:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][50/390]	Step 3569	lr 0.02313	Loss 2.1183 (2.2956)	Architecture Loss 2.6335 (2.6118)	Prec@(1,5) (38.8%, 70.9%)	
07/03 08:41:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][100/390]	Step 3619	lr 0.02313	Loss 2.6369 (2.3174)	Architecture Loss 2.5652 (2.5951)	Prec@(1,5) (37.9%, 70.9%)	
07/03 08:42:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][150/390]	Step 3669	lr 0.02313	Loss 2.7530 (2.3339)	Architecture Loss 2.5750 (2.6010)	Prec@(1,5) (37.9%, 70.9%)	
07/03 08:42:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][200/390]	Step 3719	lr 0.02313	Loss 2.2256 (2.3456)	Architecture Loss 2.8077 (2.5913)	Prec@(1,5) (37.9%, 70.6%)	
07/03 08:42:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][250/390]	Step 3769	lr 0.02313	Loss 2.0358 (2.3572)	Architecture Loss 2.3782 (2.5870)	Prec@(1,5) (37.5%, 70.4%)	
07/03 08:42:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][300/390]	Step 3819	lr 0.02313	Loss 2.3217 (2.3593)	Architecture Loss 2.6048 (2.5755)	Prec@(1,5) (37.6%, 70.3%)	
07/03 08:43:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][350/390]	Step 3869	lr 0.02313	Loss 1.6640 (2.3612)	Architecture Loss 2.4428 (2.5602)	Prec@(1,5) (37.6%, 70.2%)	
07/03 08:43:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [9][390/390]	Step 3909	lr 0.02313	Loss 2.2934 (2.3598)	Architecture Loss 2.5775 (2.5560)	Prec@(1,5) (37.7%, 70.3%)	
07/03 08:43:30午後 searchStage_trainer.py:225 [INFO] Train: [  9/49] Final Prec@1 37.6240%
07/03 08:43:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][50/391]	Step 3910	Loss 2.5269	Prec@(1,5) (34.6%, 66.6%)
07/03 08:43:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][100/391]	Step 3910	Loss 2.5469	Prec@(1,5) (34.2%, 66.4%)
07/03 08:43:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][150/391]	Step 3910	Loss 2.5323	Prec@(1,5) (34.7%, 66.6%)
07/03 08:43:40午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][200/391]	Step 3910	Loss 2.5283	Prec@(1,5) (34.8%, 66.8%)
07/03 08:43:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][250/391]	Step 3910	Loss 2.5312	Prec@(1,5) (34.7%, 66.6%)
07/03 08:43:45午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][300/391]	Step 3910	Loss 2.5348	Prec@(1,5) (34.6%, 66.6%)
07/03 08:43:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][350/391]	Step 3910	Loss 2.5365	Prec@(1,5) (34.6%, 66.5%)
07/03 08:43:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [9][390/391]	Step 3910	Loss 2.5360	Prec@(1,5) (34.6%, 66.6%)
07/03 08:43:49午後 searchStage_trainer.py:264 [INFO] Valid: [  9/49] Final Prec@1 34.5600%
07/03 08:43:49午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:43:50午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 34.5600%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2506, 0.2504, 0.2498],
        [0.2492, 0.2493, 0.2524, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2486, 0.2561, 0.2474],
        [0.2468, 0.2492, 0.2552, 0.2488],
        [0.2494, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2479, 0.2485, 0.2600, 0.2436],
        [0.2488, 0.2496, 0.2504, 0.2512],
        [0.2488, 0.2496, 0.2500, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2508, 0.2501, 0.2487],
        [0.2490, 0.2499, 0.2509, 0.2503],
        [0.2498, 0.2503, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2497, 0.2536, 0.2492],
        [0.2482, 0.2499, 0.2529, 0.2490],
        [0.2477, 0.2503, 0.2511, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2498, 0.2531, 0.2480],
        [0.2493, 0.2504, 0.2499, 0.2503],
        [0.2490, 0.2502, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2496, 0.2518, 0.2503],
        [0.2489, 0.2492, 0.2520, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2495, 0.2532, 0.2495],
        [0.2480, 0.2498, 0.2529, 0.2493],
        [0.2494, 0.2495, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2515, 0.2544, 0.2458],
        [0.2487, 0.2495, 0.2503, 0.2514],
        [0.2487, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2502, 0.2512, 0.2494],
        [0.2490, 0.2498, 0.2512, 0.2500],
        [0.2490, 0.2495, 0.2505, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2503, 0.2544, 0.2473],
        [0.2484, 0.2495, 0.2522, 0.2500],
        [0.2486, 0.2493, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2527, 0.2482],
        [0.2490, 0.2496, 0.2510, 0.2504],
        [0.2492, 0.2498, 0.2500, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2488, 0.2521, 0.2503],
        [0.2485, 0.2492, 0.2519, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2481, 0.2547, 0.2481],
        [0.2494, 0.2481, 0.2537, 0.2488],
        [0.2484, 0.2495, 0.2495, 0.2527]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2513, 0.2592, 0.2369],
        [0.2477, 0.2482, 0.2500, 0.2542],
        [0.2482, 0.2490, 0.2495, 0.2533]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2505, 0.2520, 0.2477],
        [0.2492, 0.2487, 0.2512, 0.2510],
        [0.2486, 0.2494, 0.2507, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2511, 0.2556, 0.2426],
        [0.2488, 0.2488, 0.2518, 0.2507],
        [0.2468, 0.2482, 0.2496, 0.2555]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2510, 0.2548, 0.2434],
        [0.2484, 0.2493, 0.2506, 0.2517],
        [0.2475, 0.2482, 0.2500, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:44:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][50/390]	Step 3960	lr 0.02271	Loss 2.3807 (2.1827)	Architecture Loss 2.6625 (2.4853)	Prec@(1,5) (40.8%, 74.2%)	
07/03 08:44:26午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][100/390]	Step 4010	lr 0.02271	Loss 2.3100 (2.1873)	Architecture Loss 2.8194 (2.4917)	Prec@(1,5) (41.2%, 73.9%)	
07/03 08:44:44午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][150/390]	Step 4060	lr 0.02271	Loss 1.9568 (2.2133)	Architecture Loss 2.2051 (2.4907)	Prec@(1,5) (40.4%, 73.3%)	
07/03 08:45:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][200/390]	Step 4110	lr 0.02271	Loss 2.2045 (2.2302)	Architecture Loss 2.4323 (2.4910)	Prec@(1,5) (40.2%, 72.8%)	
07/03 08:45:19午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][250/390]	Step 4160	lr 0.02271	Loss 2.3184 (2.2359)	Architecture Loss 2.5150 (2.4974)	Prec@(1,5) (40.2%, 72.8%)	
07/03 08:45:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][300/390]	Step 4210	lr 0.02271	Loss 2.1917 (2.2358)	Architecture Loss 2.3177 (2.4811)	Prec@(1,5) (40.3%, 72.8%)	
07/03 08:45:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][350/390]	Step 4260	lr 0.02271	Loss 1.9657 (2.2393)	Architecture Loss 1.9051 (2.4688)	Prec@(1,5) (40.1%, 72.7%)	
07/03 08:46:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [10][390/390]	Step 4300	lr 0.02271	Loss 2.1205 (2.2426)	Architecture Loss 2.4635 (2.4654)	Prec@(1,5) (40.0%, 72.7%)	
07/03 08:46:08午後 searchStage_trainer.py:225 [INFO] Train: [ 10/49] Final Prec@1 39.9800%
07/03 08:46:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][50/391]	Step 4301	Loss 2.3799	Prec@(1,5) (38.0%, 69.9%)
07/03 08:46:13午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][100/391]	Step 4301	Loss 2.3996	Prec@(1,5) (37.7%, 69.5%)
07/03 08:46:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][150/391]	Step 4301	Loss 2.4061	Prec@(1,5) (37.7%, 69.1%)
07/03 08:46:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][200/391]	Step 4301	Loss 2.3994	Prec@(1,5) (37.9%, 69.1%)
07/03 08:46:20午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][250/391]	Step 4301	Loss 2.3926	Prec@(1,5) (37.9%, 69.3%)
07/03 08:46:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][300/391]	Step 4301	Loss 2.3925	Prec@(1,5) (38.0%, 69.3%)
07/03 08:46:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][350/391]	Step 4301	Loss 2.3987	Prec@(1,5) (37.9%, 69.2%)
07/03 08:46:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [10][390/391]	Step 4301	Loss 2.4059	Prec@(1,5) (37.7%, 69.1%)
07/03 08:46:27午後 searchStage_trainer.py:264 [INFO] Valid: [ 10/49] Final Prec@1 37.7280%
07/03 08:46:27午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:46:27午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 37.7280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2492, 0.2506, 0.2504, 0.2498],
        [0.2491, 0.2493, 0.2524, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2484, 0.2565, 0.2474],
        [0.2465, 0.2491, 0.2557, 0.2488],
        [0.2494, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2480, 0.2617, 0.2431],
        [0.2486, 0.2496, 0.2505, 0.2513],
        [0.2487, 0.2496, 0.2501, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2508, 0.2502, 0.2487],
        [0.2489, 0.2498, 0.2509, 0.2503],
        [0.2498, 0.2503, 0.2504, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2495, 0.2542, 0.2490],
        [0.2480, 0.2498, 0.2531, 0.2491],
        [0.2476, 0.2503, 0.2512, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2496, 0.2534, 0.2480],
        [0.2493, 0.2504, 0.2499, 0.2504],
        [0.2490, 0.2502, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2496, 0.2519, 0.2503],
        [0.2489, 0.2491, 0.2521, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2494, 0.2534, 0.2495],
        [0.2480, 0.2497, 0.2530, 0.2493],
        [0.2494, 0.2495, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2516, 0.2548, 0.2456],
        [0.2487, 0.2495, 0.2504, 0.2515],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2501, 0.2513, 0.2494],
        [0.2489, 0.2498, 0.2513, 0.2500],
        [0.2490, 0.2495, 0.2505, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2502, 0.2546, 0.2473],
        [0.2483, 0.2495, 0.2523, 0.2500],
        [0.2486, 0.2493, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2529, 0.2481],
        [0.2489, 0.2496, 0.2510, 0.2504],
        [0.2491, 0.2498, 0.2500, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2487, 0.2521, 0.2503],
        [0.2484, 0.2491, 0.2520, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2480, 0.2549, 0.2480],
        [0.2493, 0.2479, 0.2539, 0.2489],
        [0.2483, 0.2495, 0.2495, 0.2527]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2513, 0.2600, 0.2360],
        [0.2476, 0.2481, 0.2500, 0.2543],
        [0.2481, 0.2489, 0.2495, 0.2535]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2505, 0.2522, 0.2476],
        [0.2491, 0.2487, 0.2513, 0.2510],
        [0.2485, 0.2493, 0.2507, 0.2514]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2507, 0.2512, 0.2559, 0.2422],
        [0.2487, 0.2487, 0.2519, 0.2507],
        [0.2467, 0.2481, 0.2496, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2510, 0.2551, 0.2430],
        [0.2483, 0.2492, 0.2507, 0.2518],
        [0.2474, 0.2481, 0.2500, 0.2545]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:46:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][50/390]	Step 4351	lr 0.02225	Loss 2.2131 (2.0828)	Architecture Loss 2.1876 (2.4685)	Prec@(1,5) (44.5%, 76.2%)	
07/03 08:47:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][100/390]	Step 4401	lr 0.02225	Loss 2.0483 (2.0814)	Architecture Loss 2.4612 (2.4544)	Prec@(1,5) (44.6%, 76.1%)	
07/03 08:47:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][150/390]	Step 4451	lr 0.02225	Loss 2.2074 (2.1155)	Architecture Loss 2.1875 (2.4516)	Prec@(1,5) (43.6%, 75.4%)	
07/03 08:47:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][200/390]	Step 4501	lr 0.02225	Loss 1.7423 (2.1227)	Architecture Loss 2.7236 (2.4447)	Prec@(1,5) (43.1%, 75.3%)	
07/03 08:47:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][250/390]	Step 4551	lr 0.02225	Loss 2.0725 (2.1312)	Architecture Loss 2.2117 (2.4365)	Prec@(1,5) (42.9%, 75.1%)	
07/03 08:48:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][300/390]	Step 4601	lr 0.02225	Loss 2.4191 (2.1375)	Architecture Loss 2.6780 (2.4199)	Prec@(1,5) (42.8%, 74.9%)	
07/03 08:48:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][350/390]	Step 4651	lr 0.02225	Loss 2.3016 (2.1418)	Architecture Loss 2.0507 (2.4093)	Prec@(1,5) (42.7%, 74.8%)	
07/03 08:48:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [11][390/390]	Step 4691	lr 0.02225	Loss 2.3363 (2.1424)	Architecture Loss 2.4167 (2.4076)	Prec@(1,5) (42.7%, 74.8%)	
07/03 08:48:46午後 searchStage_trainer.py:225 [INFO] Train: [ 11/49] Final Prec@1 42.6840%
07/03 08:48:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][50/391]	Step 4692	Loss 2.4783	Prec@(1,5) (36.3%, 68.3%)
07/03 08:48:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][100/391]	Step 4692	Loss 2.5005	Prec@(1,5) (36.5%, 68.5%)
07/03 08:48:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][150/391]	Step 4692	Loss 2.5076	Prec@(1,5) (36.4%, 68.3%)
07/03 08:48:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][200/391]	Step 4692	Loss 2.4861	Prec@(1,5) (36.8%, 68.5%)
07/03 08:48:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][250/391]	Step 4692	Loss 2.4707	Prec@(1,5) (37.0%, 68.9%)
07/03 08:49:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][300/391]	Step 4692	Loss 2.4718	Prec@(1,5) (36.9%, 68.7%)
07/03 08:49:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][350/391]	Step 4692	Loss 2.4706	Prec@(1,5) (36.9%, 68.7%)
07/03 08:49:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [11][390/391]	Step 4692	Loss 2.4700	Prec@(1,5) (36.9%, 68.7%)
07/03 08:49:05午後 searchStage_trainer.py:264 [INFO] Valid: [ 11/49] Final Prec@1 36.8560%
07/03 08:49:05午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:49:05午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 37.7280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2491, 0.2506, 0.2505, 0.2498],
        [0.2491, 0.2493, 0.2525, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2482, 0.2570, 0.2474],
        [0.2463, 0.2489, 0.2561, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2473, 0.2633, 0.2428],
        [0.2486, 0.2496, 0.2505, 0.2513],
        [0.2486, 0.2495, 0.2501, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2508, 0.2502, 0.2487],
        [0.2489, 0.2498, 0.2509, 0.2503],
        [0.2498, 0.2503, 0.2504, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2493, 0.2547, 0.2490],
        [0.2478, 0.2496, 0.2535, 0.2491],
        [0.2476, 0.2503, 0.2512, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2494, 0.2536, 0.2479],
        [0.2492, 0.2504, 0.2499, 0.2505],
        [0.2489, 0.2502, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2496, 0.2519, 0.2503],
        [0.2489, 0.2491, 0.2522, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2493, 0.2536, 0.2495],
        [0.2478, 0.2496, 0.2532, 0.2493],
        [0.2494, 0.2495, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2478, 0.2516, 0.2552, 0.2454],
        [0.2486, 0.2494, 0.2504, 0.2515],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2492, 0.2501, 0.2514, 0.2494],
        [0.2489, 0.2498, 0.2513, 0.2500],
        [0.2490, 0.2495, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2477, 0.2502, 0.2548, 0.2473],
        [0.2482, 0.2494, 0.2524, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2530, 0.2480],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2491, 0.2498, 0.2501, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2487, 0.2522, 0.2504],
        [0.2484, 0.2491, 0.2521, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2478, 0.2551, 0.2480],
        [0.2492, 0.2478, 0.2541, 0.2489],
        [0.2483, 0.2495, 0.2495, 0.2527]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2513, 0.2608, 0.2352],
        [0.2476, 0.2480, 0.2499, 0.2545],
        [0.2481, 0.2489, 0.2494, 0.2536]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2505, 0.2523, 0.2475],
        [0.2491, 0.2486, 0.2513, 0.2510],
        [0.2485, 0.2493, 0.2507, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2512, 0.2562, 0.2418],
        [0.2486, 0.2486, 0.2519, 0.2508],
        [0.2466, 0.2481, 0.2496, 0.2557]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2511, 0.2554, 0.2427],
        [0.2482, 0.2492, 0.2508, 0.2518],
        [0.2473, 0.2481, 0.2500, 0.2546]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:49:24午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][50/390]	Step 4742	lr 0.02175	Loss 1.6020 (1.9939)	Architecture Loss 2.3189 (2.3577)	Prec@(1,5) (44.8%, 77.8%)	
07/03 08:49:42午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][100/390]	Step 4792	lr 0.02175	Loss 1.7616 (1.9738)	Architecture Loss 2.3047 (2.3497)	Prec@(1,5) (46.0%, 78.4%)	
07/03 08:50:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][150/390]	Step 4842	lr 0.02175	Loss 2.3247 (1.9856)	Architecture Loss 2.3028 (2.3311)	Prec@(1,5) (45.8%, 77.9%)	
07/03 08:50:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][200/390]	Step 4892	lr 0.02175	Loss 1.8433 (1.9867)	Architecture Loss 2.3896 (2.3269)	Prec@(1,5) (46.0%, 77.9%)	
07/03 08:50:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][250/390]	Step 4942	lr 0.02175	Loss 2.0998 (1.9999)	Architecture Loss 2.5659 (2.3341)	Prec@(1,5) (45.7%, 77.6%)	
07/03 08:50:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][300/390]	Step 4992	lr 0.02175	Loss 2.0101 (2.0172)	Architecture Loss 2.0535 (2.3341)	Prec@(1,5) (45.3%, 77.3%)	
07/03 08:51:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][350/390]	Step 5042	lr 0.02175	Loss 2.2163 (2.0237)	Architecture Loss 2.1510 (2.3336)	Prec@(1,5) (45.2%, 77.2%)	
07/03 08:51:24午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [12][390/390]	Step 5082	lr 0.02175	Loss 1.9037 (2.0327)	Architecture Loss 2.2955 (2.3302)	Prec@(1,5) (45.0%, 77.0%)	
07/03 08:51:24午後 searchStage_trainer.py:225 [INFO] Train: [ 12/49] Final Prec@1 45.0920%
07/03 08:51:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][50/391]	Step 5083	Loss 2.3626	Prec@(1,5) (38.7%, 71.0%)
07/03 08:51:29午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][100/391]	Step 5083	Loss 2.3557	Prec@(1,5) (38.9%, 70.9%)
07/03 08:51:31午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][150/391]	Step 5083	Loss 2.3292	Prec@(1,5) (39.3%, 71.5%)
07/03 08:51:34午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][200/391]	Step 5083	Loss 2.3333	Prec@(1,5) (39.1%, 71.3%)
07/03 08:51:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][250/391]	Step 5083	Loss 2.3351	Prec@(1,5) (39.2%, 71.3%)
07/03 08:51:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][300/391]	Step 5083	Loss 2.3275	Prec@(1,5) (39.2%, 71.4%)
07/03 08:51:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][350/391]	Step 5083	Loss 2.3238	Prec@(1,5) (39.4%, 71.5%)
07/03 08:51:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [12][390/391]	Step 5083	Loss 2.3210	Prec@(1,5) (39.4%, 71.6%)
07/03 08:51:43午後 searchStage_trainer.py:264 [INFO] Valid: [ 12/49] Final Prec@1 39.4480%
07/03 08:51:43午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('avg_pool_3x3', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:51:43午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 39.4480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2491, 0.2506, 0.2505, 0.2498],
        [0.2491, 0.2493, 0.2525, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2481, 0.2574, 0.2474],
        [0.2460, 0.2488, 0.2565, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2466, 0.2650, 0.2426],
        [0.2485, 0.2496, 0.2505, 0.2514],
        [0.2486, 0.2495, 0.2502, 0.2517]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2508, 0.2502, 0.2487],
        [0.2489, 0.2498, 0.2510, 0.2503],
        [0.2498, 0.2502, 0.2504, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2492, 0.2551, 0.2490],
        [0.2476, 0.2496, 0.2538, 0.2491],
        [0.2475, 0.2503, 0.2512, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2492, 0.2540, 0.2478],
        [0.2492, 0.2504, 0.2499, 0.2505],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2520, 0.2503],
        [0.2488, 0.2491, 0.2522, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2493, 0.2537, 0.2494],
        [0.2477, 0.2496, 0.2533, 0.2493],
        [0.2494, 0.2495, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2517, 0.2555, 0.2453],
        [0.2486, 0.2494, 0.2504, 0.2515],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2514, 0.2494],
        [0.2489, 0.2498, 0.2513, 0.2500],
        [0.2490, 0.2495, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2476, 0.2502, 0.2549, 0.2472],
        [0.2482, 0.2494, 0.2525, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2531, 0.2480],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2491, 0.2498, 0.2501, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2487, 0.2522, 0.2504],
        [0.2484, 0.2491, 0.2521, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2478, 0.2553, 0.2479],
        [0.2492, 0.2477, 0.2542, 0.2489],
        [0.2483, 0.2495, 0.2495, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2527, 0.2515, 0.2615, 0.2343],
        [0.2475, 0.2479, 0.2499, 0.2546],
        [0.2480, 0.2489, 0.2494, 0.2537]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2505, 0.2523, 0.2474],
        [0.2491, 0.2486, 0.2513, 0.2510],
        [0.2485, 0.2493, 0.2507, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2513, 0.2564, 0.2415],
        [0.2485, 0.2486, 0.2520, 0.2509],
        [0.2465, 0.2480, 0.2496, 0.2558]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2511, 0.2556, 0.2424],
        [0.2481, 0.2492, 0.2508, 0.2518],
        [0.2473, 0.2480, 0.2499, 0.2547]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:52:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][50/390]	Step 5133	lr 0.02121	Loss 1.8232 (1.8354)	Architecture Loss 2.3853 (2.2411)	Prec@(1,5) (48.9%, 80.8%)	
07/03 08:52:19午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][100/390]	Step 5183	lr 0.02121	Loss 1.5250 (1.8615)	Architecture Loss 2.4389 (2.2583)	Prec@(1,5) (47.9%, 80.8%)	
07/03 08:52:37午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][150/390]	Step 5233	lr 0.02121	Loss 1.9626 (1.8980)	Architecture Loss 2.2081 (2.2658)	Prec@(1,5) (47.4%, 79.9%)	
07/03 08:52:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][200/390]	Step 5283	lr 0.02121	Loss 1.8395 (1.9008)	Architecture Loss 2.4588 (2.2746)	Prec@(1,5) (47.4%, 79.7%)	
07/03 08:53:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][250/390]	Step 5333	lr 0.02121	Loss 1.8726 (1.9159)	Architecture Loss 2.2507 (2.2794)	Prec@(1,5) (47.1%, 79.5%)	
07/03 08:53:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][300/390]	Step 5383	lr 0.02121	Loss 1.8701 (1.9229)	Architecture Loss 2.8341 (2.2819)	Prec@(1,5) (47.0%, 79.3%)	
07/03 08:53:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][350/390]	Step 5433	lr 0.02121	Loss 1.9957 (1.9274)	Architecture Loss 2.0401 (2.2823)	Prec@(1,5) (47.1%, 79.0%)	
07/03 08:54:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [13][390/390]	Step 5473	lr 0.02121	Loss 2.0038 (1.9368)	Architecture Loss 2.4958 (2.2826)	Prec@(1,5) (46.9%, 78.8%)	
07/03 08:54:01午後 searchStage_trainer.py:225 [INFO] Train: [ 13/49] Final Prec@1 46.8560%
07/03 08:54:04午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][50/391]	Step 5474	Loss 2.3009	Prec@(1,5) (40.3%, 71.9%)
07/03 08:54:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][100/391]	Step 5474	Loss 2.2924	Prec@(1,5) (40.4%, 72.4%)
07/03 08:54:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][150/391]	Step 5474	Loss 2.2944	Prec@(1,5) (40.5%, 72.4%)
07/03 08:54:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][200/391]	Step 5474	Loss 2.2882	Prec@(1,5) (40.6%, 72.4%)
07/03 08:54:13午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][250/391]	Step 5474	Loss 2.2907	Prec@(1,5) (40.5%, 72.4%)
07/03 08:54:16午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][300/391]	Step 5474	Loss 2.2963	Prec@(1,5) (40.5%, 72.3%)
07/03 08:54:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][350/391]	Step 5474	Loss 2.3039	Prec@(1,5) (40.5%, 72.1%)
07/03 08:54:20午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [13][390/391]	Step 5474	Loss 2.3050	Prec@(1,5) (40.5%, 72.0%)
07/03 08:54:20午後 searchStage_trainer.py:264 [INFO] Valid: [ 13/49] Final Prec@1 40.4880%
07/03 08:54:20午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:54:20午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 40.4880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2491, 0.2506, 0.2506, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2480, 0.2578, 0.2473],
        [0.2458, 0.2486, 0.2568, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2459, 0.2668, 0.2425],
        [0.2485, 0.2496, 0.2506, 0.2514],
        [0.2485, 0.2495, 0.2502, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2508, 0.2502, 0.2487],
        [0.2489, 0.2498, 0.2510, 0.2503],
        [0.2497, 0.2502, 0.2504, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2490, 0.2556, 0.2490],
        [0.2474, 0.2495, 0.2541, 0.2490],
        [0.2475, 0.2503, 0.2512, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2491, 0.2543, 0.2477],
        [0.2491, 0.2503, 0.2499, 0.2506],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2520, 0.2503],
        [0.2488, 0.2490, 0.2522, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2493, 0.2538, 0.2494],
        [0.2476, 0.2496, 0.2534, 0.2493],
        [0.2494, 0.2495, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2517, 0.2557, 0.2452],
        [0.2486, 0.2494, 0.2504, 0.2515],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2514, 0.2494],
        [0.2489, 0.2498, 0.2513, 0.2500],
        [0.2490, 0.2495, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2475, 0.2502, 0.2551, 0.2471],
        [0.2481, 0.2494, 0.2525, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2500, 0.2532, 0.2479],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2491, 0.2498, 0.2501, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2487, 0.2523, 0.2504],
        [0.2483, 0.2491, 0.2521, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2477, 0.2554, 0.2478],
        [0.2491, 0.2476, 0.2544, 0.2488],
        [0.2483, 0.2495, 0.2495, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2526, 0.2516, 0.2622, 0.2336],
        [0.2474, 0.2479, 0.2500, 0.2548],
        [0.2480, 0.2488, 0.2494, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2498, 0.2505, 0.2524, 0.2473],
        [0.2490, 0.2486, 0.2514, 0.2510],
        [0.2484, 0.2493, 0.2507, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2508, 0.2514, 0.2567, 0.2412],
        [0.2484, 0.2485, 0.2521, 0.2510],
        [0.2464, 0.2480, 0.2497, 0.2559]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2512, 0.2558, 0.2421],
        [0.2481, 0.2491, 0.2509, 0.2519],
        [0.2473, 0.2480, 0.2499, 0.2548]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:54:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][50/390]	Step 5524	lr 0.02065	Loss 1.6279 (1.7814)	Architecture Loss 1.8514 (2.2699)	Prec@(1,5) (51.3%, 81.5%)	
07/03 08:54:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][100/390]	Step 5574	lr 0.02065	Loss 1.5950 (1.7949)	Architecture Loss 2.3300 (2.2620)	Prec@(1,5) (50.7%, 80.8%)	
07/03 08:55:13午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][150/390]	Step 5624	lr 0.02065	Loss 1.8636 (1.8161)	Architecture Loss 2.1550 (2.2674)	Prec@(1,5) (49.8%, 80.7%)	
07/03 08:55:31午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][200/390]	Step 5674	lr 0.02065	Loss 1.8957 (1.8249)	Architecture Loss 2.1025 (2.2535)	Prec@(1,5) (49.4%, 80.5%)	
07/03 08:55:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][250/390]	Step 5724	lr 0.02065	Loss 2.0992 (1.8442)	Architecture Loss 2.5654 (2.2450)	Prec@(1,5) (49.0%, 80.3%)	
07/03 08:56:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][300/390]	Step 5774	lr 0.02065	Loss 1.8712 (1.8560)	Architecture Loss 2.4361 (2.2404)	Prec@(1,5) (48.6%, 80.1%)	
07/03 08:56:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][350/390]	Step 5824	lr 0.02065	Loss 1.7718 (1.8597)	Architecture Loss 2.0870 (2.2348)	Prec@(1,5) (48.6%, 80.1%)	
07/03 08:56:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [14][390/390]	Step 5864	lr 0.02065	Loss 1.6539 (1.8634)	Architecture Loss 2.1409 (2.2270)	Prec@(1,5) (48.5%, 80.1%)	
07/03 08:56:39午後 searchStage_trainer.py:225 [INFO] Train: [ 14/49] Final Prec@1 48.5600%
07/03 08:56:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][50/391]	Step 5865	Loss 2.2085	Prec@(1,5) (40.9%, 74.0%)
07/03 08:56:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][100/391]	Step 5865	Loss 2.2068	Prec@(1,5) (41.3%, 73.9%)
07/03 08:56:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][150/391]	Step 5865	Loss 2.2031	Prec@(1,5) (41.9%, 73.9%)
07/03 08:56:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][200/391]	Step 5865	Loss 2.2029	Prec@(1,5) (42.0%, 73.9%)
07/03 08:56:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][250/391]	Step 5865	Loss 2.2018	Prec@(1,5) (41.9%, 74.0%)
07/03 08:56:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][300/391]	Step 5865	Loss 2.2079	Prec@(1,5) (41.8%, 73.7%)
07/03 08:56:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][350/391]	Step 5865	Loss 2.2121	Prec@(1,5) (41.7%, 73.7%)
07/03 08:56:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [14][390/391]	Step 5865	Loss 2.2060	Prec@(1,5) (42.0%, 73.8%)
07/03 08:56:58午後 searchStage_trainer.py:264 [INFO] Valid: [ 14/49] Final Prec@1 41.9680%
07/03 08:56:58午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:56:59午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 41.9680%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2506, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2491]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2478, 0.2581, 0.2474],
        [0.2456, 0.2485, 0.2571, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2451, 0.2685, 0.2422],
        [0.2484, 0.2496, 0.2506, 0.2514],
        [0.2485, 0.2495, 0.2502, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2508, 0.2503, 0.2487],
        [0.2489, 0.2498, 0.2511, 0.2503],
        [0.2497, 0.2502, 0.2504, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2489, 0.2560, 0.2490],
        [0.2473, 0.2494, 0.2543, 0.2490],
        [0.2475, 0.2503, 0.2512, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2489, 0.2546, 0.2476],
        [0.2491, 0.2503, 0.2499, 0.2506],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2520, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2493, 0.2540, 0.2494],
        [0.2475, 0.2496, 0.2535, 0.2494],
        [0.2494, 0.2495, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2517, 0.2560, 0.2451],
        [0.2486, 0.2494, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2514, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2500],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2474, 0.2503, 0.2552, 0.2471],
        [0.2480, 0.2494, 0.2526, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2500, 0.2532, 0.2479],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2491, 0.2497, 0.2501, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2486, 0.2523, 0.2504],
        [0.2483, 0.2490, 0.2522, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2476, 0.2556, 0.2478],
        [0.2491, 0.2476, 0.2545, 0.2488],
        [0.2483, 0.2495, 0.2494, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2525, 0.2517, 0.2628, 0.2330],
        [0.2474, 0.2478, 0.2500, 0.2549],
        [0.2479, 0.2488, 0.2494, 0.2538]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2505, 0.2525, 0.2472],
        [0.2490, 0.2486, 0.2514, 0.2510],
        [0.2484, 0.2493, 0.2507, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2514, 0.2569, 0.2409],
        [0.2484, 0.2485, 0.2522, 0.2510],
        [0.2464, 0.2480, 0.2497, 0.2560]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2512, 0.2560, 0.2419],
        [0.2480, 0.2491, 0.2510, 0.2519],
        [0.2472, 0.2480, 0.2499, 0.2549]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:57:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][50/390]	Step 5915	lr 0.02005	Loss 1.6256 (1.7306)	Architecture Loss 1.8862 (2.1782)	Prec@(1,5) (51.4%, 83.2%)	
07/03 08:57:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][100/390]	Step 5965	lr 0.02005	Loss 1.7361 (1.7333)	Architecture Loss 2.5035 (2.1857)	Prec@(1,5) (51.4%, 82.6%)	
07/03 08:57:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][150/390]	Step 6015	lr 0.02005	Loss 1.7153 (1.7458)	Architecture Loss 2.0693 (2.1757)	Prec@(1,5) (51.3%, 82.3%)	
07/03 08:58:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][200/390]	Step 6065	lr 0.02005	Loss 2.3789 (1.7698)	Architecture Loss 2.3113 (2.1827)	Prec@(1,5) (50.5%, 81.7%)	
07/03 08:58:28午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][250/390]	Step 6115	lr 0.02005	Loss 1.9526 (1.7766)	Architecture Loss 2.0218 (2.1948)	Prec@(1,5) (50.2%, 81.7%)	
07/03 08:58:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][300/390]	Step 6165	lr 0.02005	Loss 1.9834 (1.7800)	Architecture Loss 2.0682 (2.1981)	Prec@(1,5) (50.2%, 81.6%)	
07/03 08:59:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][350/390]	Step 6215	lr 0.02005	Loss 1.8336 (1.7857)	Architecture Loss 1.8025 (2.1920)	Prec@(1,5) (50.2%, 81.5%)	
07/03 08:59:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [15][390/390]	Step 6255	lr 0.02005	Loss 1.9193 (1.7945)	Architecture Loss 2.2562 (2.1854)	Prec@(1,5) (50.2%, 81.4%)	
07/03 08:59:18午後 searchStage_trainer.py:225 [INFO] Train: [ 15/49] Final Prec@1 50.1560%
07/03 08:59:21午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][50/391]	Step 6256	Loss 2.1499	Prec@(1,5) (43.2%, 74.7%)
07/03 08:59:23午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][100/391]	Step 6256	Loss 2.1848	Prec@(1,5) (42.8%, 73.9%)
07/03 08:59:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][150/391]	Step 6256	Loss 2.1881	Prec@(1,5) (42.7%, 73.6%)
07/03 08:59:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][200/391]	Step 6256	Loss 2.1769	Prec@(1,5) (42.7%, 74.1%)
07/03 08:59:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][250/391]	Step 6256	Loss 2.1873	Prec@(1,5) (42.7%, 73.8%)
07/03 08:59:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][300/391]	Step 6256	Loss 2.1775	Prec@(1,5) (43.1%, 74.0%)
07/03 08:59:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][350/391]	Step 6256	Loss 2.1823	Prec@(1,5) (42.9%, 73.9%)
07/03 08:59:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [15][390/391]	Step 6256	Loss 2.1827	Prec@(1,5) (42.8%, 73.9%)
07/03 08:59:37午後 searchStage_trainer.py:264 [INFO] Valid: [ 15/49] Final Prec@1 42.7880%
07/03 08:59:37午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 08:59:38午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 42.7880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2506, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2477, 0.2584, 0.2473],
        [0.2454, 0.2484, 0.2574, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2433, 0.2445, 0.2702, 0.2420],
        [0.2484, 0.2496, 0.2506, 0.2514],
        [0.2485, 0.2495, 0.2502, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2503, 0.2508, 0.2503, 0.2487],
        [0.2489, 0.2498, 0.2511, 0.2503],
        [0.2497, 0.2502, 0.2504, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2487, 0.2563, 0.2490],
        [0.2471, 0.2493, 0.2545, 0.2490],
        [0.2475, 0.2503, 0.2513, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2488, 0.2548, 0.2476],
        [0.2491, 0.2503, 0.2499, 0.2507],
        [0.2489, 0.2501, 0.2505, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2520, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2492, 0.2541, 0.2494],
        [0.2475, 0.2495, 0.2536, 0.2494],
        [0.2495, 0.2495, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2517, 0.2563, 0.2450],
        [0.2486, 0.2494, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2514, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2500],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2502, 0.2554, 0.2471],
        [0.2480, 0.2494, 0.2526, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2533, 0.2479],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2491, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2486, 0.2523, 0.2504],
        [0.2483, 0.2490, 0.2522, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2476, 0.2557, 0.2477],
        [0.2491, 0.2475, 0.2546, 0.2488],
        [0.2483, 0.2495, 0.2494, 0.2528]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2518, 0.2633, 0.2324],
        [0.2473, 0.2478, 0.2500, 0.2549],
        [0.2479, 0.2488, 0.2494, 0.2539]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2505, 0.2526, 0.2472],
        [0.2490, 0.2486, 0.2514, 0.2510],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2515, 0.2571, 0.2405],
        [0.2483, 0.2484, 0.2522, 0.2510],
        [0.2463, 0.2479, 0.2497, 0.2560]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2512, 0.2562, 0.2416],
        [0.2480, 0.2491, 0.2510, 0.2519],
        [0.2472, 0.2479, 0.2499, 0.2550]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 08:59:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][50/390]	Step 6306	lr 0.01943	Loss 1.8144 (1.6544)	Architecture Loss 2.2454 (2.1424)	Prec@(1,5) (53.9%, 83.9%)	
07/03 09:00:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][100/390]	Step 6356	lr 0.01943	Loss 2.0680 (1.6499)	Architecture Loss 2.5426 (2.1651)	Prec@(1,5) (53.8%, 83.9%)	
07/03 09:00:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][150/390]	Step 6406	lr 0.01943	Loss 1.4196 (1.6653)	Architecture Loss 2.2279 (2.1645)	Prec@(1,5) (53.2%, 83.9%)	
07/03 09:00:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][200/390]	Step 6456	lr 0.01943	Loss 1.5102 (1.6789)	Architecture Loss 2.0764 (2.1592)	Prec@(1,5) (52.7%, 83.6%)	
07/03 09:01:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][250/390]	Step 6506	lr 0.01943	Loss 1.8843 (1.6941)	Architecture Loss 2.2159 (2.1599)	Prec@(1,5) (52.3%, 83.3%)	
07/03 09:01:26午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][300/390]	Step 6556	lr 0.01943	Loss 1.7201 (1.6997)	Architecture Loss 2.1763 (2.1476)	Prec@(1,5) (52.2%, 83.2%)	
07/03 09:01:44午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][350/390]	Step 6606	lr 0.01943	Loss 1.6488 (1.6989)	Architecture Loss 1.9308 (2.1394)	Prec@(1,5) (52.3%, 83.1%)	
07/03 09:01:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [16][390/390]	Step 6646	lr 0.01943	Loss 1.9113 (1.6983)	Architecture Loss 1.9967 (2.1413)	Prec@(1,5) (52.4%, 83.1%)	
07/03 09:01:58午後 searchStage_trainer.py:225 [INFO] Train: [ 16/49] Final Prec@1 52.4000%
07/03 09:02:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][50/391]	Step 6647	Loss 2.1015	Prec@(1,5) (43.7%, 75.8%)
07/03 09:02:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][100/391]	Step 6647	Loss 2.1099	Prec@(1,5) (43.6%, 75.7%)
07/03 09:02:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][150/391]	Step 6647	Loss 2.0969	Prec@(1,5) (44.1%, 76.1%)
07/03 09:02:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][200/391]	Step 6647	Loss 2.1164	Prec@(1,5) (43.5%, 75.8%)
07/03 09:02:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][250/391]	Step 6647	Loss 2.1207	Prec@(1,5) (43.6%, 75.7%)
07/03 09:02:13午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][300/391]	Step 6647	Loss 2.1246	Prec@(1,5) (43.6%, 75.6%)
07/03 09:02:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][350/391]	Step 6647	Loss 2.1309	Prec@(1,5) (43.6%, 75.4%)
07/03 09:02:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [16][390/391]	Step 6647	Loss 2.1305	Prec@(1,5) (43.7%, 75.3%)
07/03 09:02:17午後 searchStage_trainer.py:264 [INFO] Valid: [ 16/49] Final Prec@1 43.6960%
07/03 09:02:17午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:02:18午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 43.6960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2506, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2476, 0.2587, 0.2473],
        [0.2452, 0.2484, 0.2577, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2425, 0.2440, 0.2716, 0.2419],
        [0.2484, 0.2496, 0.2506, 0.2515],
        [0.2485, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2508, 0.2503, 0.2487],
        [0.2489, 0.2498, 0.2511, 0.2503],
        [0.2497, 0.2502, 0.2504, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2486, 0.2567, 0.2490],
        [0.2470, 0.2492, 0.2547, 0.2490],
        [0.2474, 0.2503, 0.2513, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2488, 0.2549, 0.2476],
        [0.2491, 0.2503, 0.2499, 0.2507],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2492, 0.2542, 0.2494],
        [0.2474, 0.2495, 0.2537, 0.2494],
        [0.2495, 0.2495, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2517, 0.2565, 0.2449],
        [0.2486, 0.2494, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2514, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2473, 0.2502, 0.2554, 0.2470],
        [0.2480, 0.2494, 0.2526, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2534, 0.2478],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2491, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2524, 0.2504],
        [0.2483, 0.2490, 0.2522, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2475, 0.2558, 0.2477],
        [0.2490, 0.2475, 0.2547, 0.2488],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2519, 0.2638, 0.2319],
        [0.2473, 0.2477, 0.2500, 0.2550],
        [0.2479, 0.2488, 0.2494, 0.2539]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2505, 0.2526, 0.2472],
        [0.2490, 0.2486, 0.2515, 0.2510],
        [0.2484, 0.2493, 0.2508, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2515, 0.2572, 0.2403],
        [0.2483, 0.2484, 0.2523, 0.2511],
        [0.2463, 0.2479, 0.2497, 0.2561]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2509, 0.2513, 0.2564, 0.2414],
        [0.2480, 0.2491, 0.2510, 0.2519],
        [0.2472, 0.2479, 0.2499, 0.2550]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:02:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][50/390]	Step 6697	lr 0.01878	Loss 1.7134 (1.6025)	Architecture Loss 2.1490 (2.1215)	Prec@(1,5) (55.3%, 83.5%)	
07/03 09:02:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][100/390]	Step 6747	lr 0.01878	Loss 1.7703 (1.5701)	Architecture Loss 1.9141 (2.0954)	Prec@(1,5) (55.9%, 84.4%)	
07/03 09:03:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][150/390]	Step 6797	lr 0.01878	Loss 1.4680 (1.5833)	Architecture Loss 2.0297 (2.0882)	Prec@(1,5) (55.2%, 84.7%)	
07/03 09:03:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][200/390]	Step 6847	lr 0.01878	Loss 1.3807 (1.5892)	Architecture Loss 2.2284 (2.0934)	Prec@(1,5) (55.0%, 84.6%)	
07/03 09:03:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][250/390]	Step 6897	lr 0.01878	Loss 1.4109 (1.5975)	Architecture Loss 1.9455 (2.0860)	Prec@(1,5) (54.6%, 84.6%)	
07/03 09:04:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][300/390]	Step 6947	lr 0.01878	Loss 1.5621 (1.6095)	Architecture Loss 2.1779 (2.0944)	Prec@(1,5) (54.3%, 84.4%)	
07/03 09:04:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][350/390]	Step 6997	lr 0.01878	Loss 1.5076 (1.6228)	Architecture Loss 2.5384 (2.0951)	Prec@(1,5) (54.1%, 84.2%)	
07/03 09:04:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [17][390/390]	Step 7037	lr 0.01878	Loss 1.9284 (1.6252)	Architecture Loss 1.9647 (2.0924)	Prec@(1,5) (53.9%, 84.2%)	
07/03 09:04:37午後 searchStage_trainer.py:225 [INFO] Train: [ 17/49] Final Prec@1 53.8840%
07/03 09:04:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][50/391]	Step 7038	Loss 2.0356	Prec@(1,5) (45.5%, 77.3%)
07/03 09:04:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][100/391]	Step 7038	Loss 2.0542	Prec@(1,5) (45.8%, 76.5%)
07/03 09:04:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][150/391]	Step 7038	Loss 2.0598	Prec@(1,5) (45.6%, 76.4%)
07/03 09:04:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][200/391]	Step 7038	Loss 2.0671	Prec@(1,5) (45.7%, 76.3%)
07/03 09:04:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][250/391]	Step 7038	Loss 2.0655	Prec@(1,5) (45.8%, 76.3%)
07/03 09:04:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][300/391]	Step 7038	Loss 2.0738	Prec@(1,5) (45.6%, 76.2%)
07/03 09:04:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][350/391]	Step 7038	Loss 2.0728	Prec@(1,5) (45.7%, 76.3%)
07/03 09:04:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [17][390/391]	Step 7038	Loss 2.0713	Prec@(1,5) (45.7%, 76.3%)
07/03 09:04:56午後 searchStage_trainer.py:264 [INFO] Valid: [ 17/49] Final Prec@1 45.7080%
07/03 09:04:56午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:04:56午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 45.7080%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2506, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2475, 0.2589, 0.2473],
        [0.2451, 0.2483, 0.2579, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2419, 0.2434, 0.2728, 0.2419],
        [0.2484, 0.2496, 0.2506, 0.2515],
        [0.2485, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2508, 0.2503, 0.2487],
        [0.2488, 0.2497, 0.2511, 0.2503],
        [0.2497, 0.2502, 0.2504, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2485, 0.2570, 0.2490],
        [0.2469, 0.2492, 0.2549, 0.2490],
        [0.2474, 0.2503, 0.2513, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2487, 0.2550, 0.2476],
        [0.2491, 0.2503, 0.2499, 0.2507],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2492, 0.2543, 0.2494],
        [0.2473, 0.2495, 0.2538, 0.2494],
        [0.2495, 0.2495, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2517, 0.2567, 0.2448],
        [0.2486, 0.2494, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2514, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2502, 0.2556, 0.2470],
        [0.2479, 0.2494, 0.2527, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2534, 0.2478],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2491, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2524, 0.2504],
        [0.2483, 0.2490, 0.2522, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2475, 0.2559, 0.2476],
        [0.2490, 0.2474, 0.2548, 0.2488],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2520, 0.2643, 0.2313],
        [0.2472, 0.2477, 0.2500, 0.2551],
        [0.2479, 0.2488, 0.2494, 0.2540]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2506, 0.2527, 0.2471],
        [0.2489, 0.2486, 0.2515, 0.2510],
        [0.2484, 0.2493, 0.2508, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2516, 0.2574, 0.2401],
        [0.2482, 0.2484, 0.2523, 0.2511],
        [0.2463, 0.2479, 0.2497, 0.2561]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2513, 0.2565, 0.2412],
        [0.2479, 0.2491, 0.2511, 0.2519],
        [0.2472, 0.2479, 0.2499, 0.2551]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:05:15午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][50/390]	Step 7088	lr 0.01811	Loss 1.2474 (1.4880)	Architecture Loss 2.4975 (2.0770)	Prec@(1,5) (56.8%, 87.8%)	
07/03 09:05:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][100/390]	Step 7138	lr 0.01811	Loss 1.1824 (1.4808)	Architecture Loss 2.3461 (2.0734)	Prec@(1,5) (57.3%, 87.3%)	
07/03 09:05:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][150/390]	Step 7188	lr 0.01811	Loss 1.5379 (1.4914)	Architecture Loss 1.8704 (2.0502)	Prec@(1,5) (56.9%, 87.0%)	
07/03 09:06:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][200/390]	Step 7238	lr 0.01811	Loss 1.5356 (1.5019)	Architecture Loss 1.8323 (2.0557)	Prec@(1,5) (56.9%, 86.7%)	
07/03 09:06:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][250/390]	Step 7288	lr 0.01811	Loss 1.6620 (1.5245)	Architecture Loss 2.8025 (2.0603)	Prec@(1,5) (56.2%, 86.2%)	
07/03 09:06:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][300/390]	Step 7338	lr 0.01811	Loss 1.4191 (1.5510)	Architecture Loss 1.7757 (2.0653)	Prec@(1,5) (55.9%, 85.8%)	
07/03 09:07:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][350/390]	Step 7388	lr 0.01811	Loss 1.5329 (1.5589)	Architecture Loss 2.7137 (2.0717)	Prec@(1,5) (55.7%, 85.7%)	
07/03 09:07:15午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [18][390/390]	Step 7428	lr 0.01811	Loss 1.8762 (1.5603)	Architecture Loss 1.9365 (2.0676)	Prec@(1,5) (55.7%, 85.7%)	
07/03 09:07:15午後 searchStage_trainer.py:225 [INFO] Train: [ 18/49] Final Prec@1 55.6960%
07/03 09:07:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][50/391]	Step 7429	Loss 2.1070	Prec@(1,5) (44.7%, 76.2%)
07/03 09:07:20午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][100/391]	Step 7429	Loss 2.0711	Prec@(1,5) (45.5%, 76.5%)
07/03 09:07:23午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][150/391]	Step 7429	Loss 2.0658	Prec@(1,5) (45.8%, 76.8%)
07/03 09:07:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][200/391]	Step 7429	Loss 2.0735	Prec@(1,5) (45.5%, 76.7%)
07/03 09:07:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][250/391]	Step 7429	Loss 2.0605	Prec@(1,5) (46.0%, 76.8%)
07/03 09:07:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][300/391]	Step 7429	Loss 2.0609	Prec@(1,5) (46.0%, 76.8%)
07/03 09:07:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][350/391]	Step 7429	Loss 2.0523	Prec@(1,5) (46.1%, 76.9%)
07/03 09:07:34午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [18][390/391]	Step 7429	Loss 2.0605	Prec@(1,5) (45.9%, 76.8%)
07/03 09:07:34午後 searchStage_trainer.py:264 [INFO] Valid: [ 18/49] Final Prec@1 45.9360%
07/03 09:07:34午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:07:35午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 45.9360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2475, 0.2591, 0.2473],
        [0.2450, 0.2482, 0.2581, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2414, 0.2429, 0.2738, 0.2419],
        [0.2484, 0.2496, 0.2506, 0.2515],
        [0.2485, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2508, 0.2503, 0.2487],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2497, 0.2502, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2483, 0.2573, 0.2490],
        [0.2468, 0.2491, 0.2550, 0.2490],
        [0.2474, 0.2503, 0.2513, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2486, 0.2552, 0.2476],
        [0.2491, 0.2503, 0.2499, 0.2507],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2492, 0.2543, 0.2494],
        [0.2473, 0.2494, 0.2539, 0.2494],
        [0.2495, 0.2495, 0.2503, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2516, 0.2570, 0.2448],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2472, 0.2502, 0.2557, 0.2469],
        [0.2479, 0.2494, 0.2527, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2535, 0.2477],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2491, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2524, 0.2504],
        [0.2483, 0.2490, 0.2523, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2474, 0.2560, 0.2476],
        [0.2490, 0.2474, 0.2549, 0.2488],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2522, 0.2648, 0.2307],
        [0.2472, 0.2477, 0.2500, 0.2551],
        [0.2478, 0.2487, 0.2494, 0.2540]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2497, 0.2506, 0.2527, 0.2471],
        [0.2489, 0.2486, 0.2515, 0.2510],
        [0.2484, 0.2493, 0.2508, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2516, 0.2575, 0.2398],
        [0.2482, 0.2484, 0.2524, 0.2511],
        [0.2462, 0.2479, 0.2497, 0.2562]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2514, 0.2566, 0.2410],
        [0.2479, 0.2491, 0.2511, 0.2518],
        [0.2471, 0.2479, 0.2499, 0.2551]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:07:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][50/390]	Step 7479	lr 0.01742	Loss 1.4576 (1.4228)	Architecture Loss 1.6621 (2.0476)	Prec@(1,5) (58.8%, 87.5%)	
07/03 09:08:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][100/390]	Step 7529	lr 0.01742	Loss 1.2068 (1.4302)	Architecture Loss 2.1302 (2.0182)	Prec@(1,5) (58.5%, 87.8%)	
07/03 09:08:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][150/390]	Step 7579	lr 0.01742	Loss 2.0060 (1.4456)	Architecture Loss 1.9452 (2.0304)	Prec@(1,5) (58.5%, 87.5%)	
07/03 09:08:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][200/390]	Step 7629	lr 0.01742	Loss 1.0103 (1.4383)	Architecture Loss 2.3194 (2.0376)	Prec@(1,5) (58.6%, 87.8%)	
07/03 09:09:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][250/390]	Step 7679	lr 0.01742	Loss 1.6362 (1.4671)	Architecture Loss 1.6867 (2.0463)	Prec@(1,5) (57.6%, 87.4%)	
07/03 09:09:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][300/390]	Step 7729	lr 0.01742	Loss 1.3821 (1.4746)	Architecture Loss 2.0890 (2.0432)	Prec@(1,5) (57.4%, 87.1%)	
07/03 09:09:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][350/390]	Step 7779	lr 0.01742	Loss 1.2832 (1.4829)	Architecture Loss 2.2605 (2.0300)	Prec@(1,5) (57.2%, 86.9%)	
07/03 09:09:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [19][390/390]	Step 7819	lr 0.01742	Loss 1.3930 (1.4921)	Architecture Loss 2.0509 (2.0317)	Prec@(1,5) (57.0%, 86.8%)	
07/03 09:09:53午後 searchStage_trainer.py:225 [INFO] Train: [ 19/49] Final Prec@1 57.0480%
07/03 09:09:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][50/391]	Step 7820	Loss 2.0805	Prec@(1,5) (46.3%, 76.4%)
07/03 09:09:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][100/391]	Step 7820	Loss 2.0638	Prec@(1,5) (46.2%, 76.7%)
07/03 09:10:00午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][150/391]	Step 7820	Loss 2.0710	Prec@(1,5) (46.2%, 76.4%)
07/03 09:10:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][200/391]	Step 7820	Loss 2.0497	Prec@(1,5) (46.6%, 76.8%)
07/03 09:10:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][250/391]	Step 7820	Loss 2.0347	Prec@(1,5) (46.8%, 77.0%)
07/03 09:10:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][300/391]	Step 7820	Loss 2.0242	Prec@(1,5) (46.9%, 77.4%)
07/03 09:10:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][350/391]	Step 7820	Loss 2.0311	Prec@(1,5) (46.7%, 77.3%)
07/03 09:10:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [19][390/391]	Step 7820	Loss 2.0295	Prec@(1,5) (46.8%, 77.4%)
07/03 09:10:12午後 searchStage_trainer.py:264 [INFO] Valid: [ 19/49] Final Prec@1 46.7640%
07/03 09:10:12午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:10:13午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 46.7640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2474, 0.2593, 0.2473],
        [0.2449, 0.2481, 0.2583, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2406, 0.2423, 0.2752, 0.2419],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2508, 0.2503, 0.2487],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2483, 0.2575, 0.2490],
        [0.2467, 0.2491, 0.2552, 0.2490],
        [0.2474, 0.2503, 0.2513, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2552, 0.2476],
        [0.2491, 0.2503, 0.2499, 0.2507],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2492, 0.2544, 0.2494],
        [0.2472, 0.2494, 0.2540, 0.2494],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2516, 0.2572, 0.2448],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2502, 0.2558, 0.2469],
        [0.2479, 0.2494, 0.2527, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2535, 0.2477],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2491, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2524, 0.2504],
        [0.2483, 0.2490, 0.2523, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2474, 0.2561, 0.2476],
        [0.2489, 0.2473, 0.2549, 0.2488],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2524, 0.2523, 0.2652, 0.2302],
        [0.2472, 0.2476, 0.2500, 0.2552],
        [0.2478, 0.2487, 0.2494, 0.2541]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2506, 0.2527, 0.2471],
        [0.2489, 0.2486, 0.2515, 0.2510],
        [0.2484, 0.2493, 0.2508, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2517, 0.2576, 0.2397],
        [0.2481, 0.2484, 0.2524, 0.2511],
        [0.2462, 0.2479, 0.2497, 0.2562]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2514, 0.2567, 0.2409],
        [0.2479, 0.2491, 0.2512, 0.2518],
        [0.2471, 0.2478, 0.2499, 0.2552]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:10:31午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][50/390]	Step 7870	lr 0.01671	Loss 1.3965 (1.3000)	Architecture Loss 2.4081 (2.0441)	Prec@(1,5) (62.0%, 90.2%)	
07/03 09:10:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][100/390]	Step 7920	lr 0.01671	Loss 1.2436 (1.3368)	Architecture Loss 2.0187 (2.0288)	Prec@(1,5) (61.0%, 89.2%)	
07/03 09:11:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][150/390]	Step 7970	lr 0.01671	Loss 1.6398 (1.3639)	Architecture Loss 1.7688 (2.0445)	Prec@(1,5) (60.6%, 88.8%)	
07/03 09:11:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][200/390]	Step 8020	lr 0.01671	Loss 1.5151 (1.3812)	Architecture Loss 1.8697 (2.0163)	Prec@(1,5) (60.2%, 88.5%)	
07/03 09:11:41午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][250/390]	Step 8070	lr 0.01671	Loss 1.5884 (1.3992)	Architecture Loss 1.9164 (2.0293)	Prec@(1,5) (59.7%, 88.2%)	
07/03 09:11:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][300/390]	Step 8120	lr 0.01671	Loss 1.6409 (1.4039)	Architecture Loss 2.1141 (2.0290)	Prec@(1,5) (59.5%, 88.1%)	
07/03 09:12:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][350/390]	Step 8170	lr 0.01671	Loss 1.6119 (1.4130)	Architecture Loss 1.9507 (2.0238)	Prec@(1,5) (59.3%, 87.9%)	
07/03 09:12:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [20][390/390]	Step 8210	lr 0.01671	Loss 1.6519 (1.4190)	Architecture Loss 1.8257 (2.0235)	Prec@(1,5) (59.2%, 87.9%)	
07/03 09:12:30午後 searchStage_trainer.py:225 [INFO] Train: [ 20/49] Final Prec@1 59.1880%
07/03 09:12:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][50/391]	Step 8211	Loss 1.8943	Prec@(1,5) (48.9%, 79.9%)
07/03 09:12:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][100/391]	Step 8211	Loss 1.9802	Prec@(1,5) (48.0%, 78.7%)
07/03 09:12:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][150/391]	Step 8211	Loss 1.9888	Prec@(1,5) (47.7%, 78.7%)
07/03 09:12:40午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][200/391]	Step 8211	Loss 1.9870	Prec@(1,5) (47.9%, 78.5%)
07/03 09:12:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][250/391]	Step 8211	Loss 1.9876	Prec@(1,5) (47.8%, 78.6%)
07/03 09:12:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][300/391]	Step 8211	Loss 1.9793	Prec@(1,5) (48.1%, 78.7%)
07/03 09:12:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][350/391]	Step 8211	Loss 1.9753	Prec@(1,5) (48.2%, 78.8%)
07/03 09:12:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [20][390/391]	Step 8211	Loss 1.9726	Prec@(1,5) (48.4%, 78.8%)
07/03 09:12:49午後 searchStage_trainer.py:264 [INFO] Valid: [ 20/49] Final Prec@1 48.3960%
07/03 09:12:49午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:12:49午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 48.3960%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2473, 0.2594, 0.2474],
        [0.2448, 0.2481, 0.2584, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2400, 0.2418, 0.2764, 0.2418],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2508, 0.2503, 0.2487],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2482, 0.2578, 0.2490],
        [0.2467, 0.2490, 0.2553, 0.2490],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2485, 0.2554, 0.2476],
        [0.2490, 0.2503, 0.2499, 0.2507],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2491, 0.2545, 0.2494],
        [0.2472, 0.2494, 0.2540, 0.2494],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2463, 0.2515, 0.2574, 0.2448],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2471, 0.2501, 0.2559, 0.2469],
        [0.2479, 0.2494, 0.2528, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2535, 0.2477],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2524, 0.2504],
        [0.2483, 0.2490, 0.2523, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2474, 0.2562, 0.2476],
        [0.2489, 0.2473, 0.2550, 0.2488],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2523, 0.2524, 0.2656, 0.2298],
        [0.2472, 0.2476, 0.2500, 0.2553],
        [0.2478, 0.2487, 0.2494, 0.2541]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2506, 0.2528, 0.2470],
        [0.2489, 0.2486, 0.2515, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2517, 0.2577, 0.2395],
        [0.2481, 0.2483, 0.2524, 0.2511],
        [0.2462, 0.2479, 0.2497, 0.2563]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2515, 0.2568, 0.2407],
        [0.2479, 0.2491, 0.2512, 0.2518],
        [0.2471, 0.2478, 0.2499, 0.2552]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:13:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][50/390]	Step 8261	lr 0.01598	Loss 1.7463 (1.2845)	Architecture Loss 2.2797 (2.0439)	Prec@(1,5) (62.7%, 90.3%)	
07/03 09:13:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][100/390]	Step 8311	lr 0.01598	Loss 1.3469 (1.3054)	Architecture Loss 1.8294 (2.0296)	Prec@(1,5) (62.0%, 90.3%)	
07/03 09:13:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][150/390]	Step 8361	lr 0.01598	Loss 1.2046 (1.3132)	Architecture Loss 1.9881 (2.0193)	Prec@(1,5) (61.6%, 90.0%)	
07/03 09:14:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][200/390]	Step 8411	lr 0.01598	Loss 1.2960 (1.3321)	Architecture Loss 2.3298 (2.0132)	Prec@(1,5) (61.0%, 89.6%)	
07/03 09:14:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][250/390]	Step 8461	lr 0.01598	Loss 1.2757 (1.3417)	Architecture Loss 2.2213 (2.0088)	Prec@(1,5) (60.8%, 89.4%)	
07/03 09:14:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][300/390]	Step 8511	lr 0.01598	Loss 1.3421 (1.3514)	Architecture Loss 1.6755 (2.0165)	Prec@(1,5) (60.6%, 89.1%)	
07/03 09:14:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][350/390]	Step 8561	lr 0.01598	Loss 1.3381 (1.3586)	Architecture Loss 1.8000 (2.0084)	Prec@(1,5) (60.4%, 88.9%)	
07/03 09:15:07午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [21][390/390]	Step 8601	lr 0.01598	Loss 1.4568 (1.3671)	Architecture Loss 1.9248 (2.0087)	Prec@(1,5) (60.2%, 88.9%)	
07/03 09:15:08午後 searchStage_trainer.py:225 [INFO] Train: [ 21/49] Final Prec@1 60.2440%
07/03 09:15:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][50/391]	Step 8602	Loss 1.9736	Prec@(1,5) (48.1%, 79.2%)
07/03 09:15:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][100/391]	Step 8602	Loss 1.9840	Prec@(1,5) (48.1%, 78.9%)
07/03 09:15:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][150/391]	Step 8602	Loss 1.9907	Prec@(1,5) (48.2%, 78.6%)
07/03 09:15:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][200/391]	Step 8602	Loss 1.9824	Prec@(1,5) (48.4%, 78.8%)
07/03 09:15:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][250/391]	Step 8602	Loss 1.9835	Prec@(1,5) (48.3%, 78.9%)
07/03 09:15:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][300/391]	Step 8602	Loss 1.9867	Prec@(1,5) (48.1%, 78.8%)
07/03 09:15:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][350/391]	Step 8602	Loss 1.9812	Prec@(1,5) (48.4%, 78.7%)
07/03 09:15:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [21][390/391]	Step 8602	Loss 1.9766	Prec@(1,5) (48.5%, 78.9%)
07/03 09:15:26午後 searchStage_trainer.py:264 [INFO] Valid: [ 21/49] Final Prec@1 48.5160%
07/03 09:15:26午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:15:27午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 48.5160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2473, 0.2596, 0.2474],
        [0.2447, 0.2481, 0.2586, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2394, 0.2413, 0.2776, 0.2417],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2487],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2481, 0.2580, 0.2490],
        [0.2466, 0.2490, 0.2554, 0.2490],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2485, 0.2554, 0.2475],
        [0.2490, 0.2503, 0.2499, 0.2508],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2491, 0.2545, 0.2494],
        [0.2472, 0.2493, 0.2541, 0.2494],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2462, 0.2515, 0.2575, 0.2448],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2501, 0.2560, 0.2469],
        [0.2478, 0.2494, 0.2528, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2536, 0.2477],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2524, 0.2503],
        [0.2483, 0.2490, 0.2523, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2489, 0.2473, 0.2563, 0.2475],
        [0.2489, 0.2473, 0.2551, 0.2488],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2525, 0.2659, 0.2293],
        [0.2471, 0.2476, 0.2500, 0.2553],
        [0.2478, 0.2487, 0.2494, 0.2541]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2506, 0.2528, 0.2470],
        [0.2489, 0.2486, 0.2516, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2518, 0.2578, 0.2393],
        [0.2481, 0.2483, 0.2525, 0.2511],
        [0.2461, 0.2479, 0.2497, 0.2563]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2516, 0.2569, 0.2405],
        [0.2479, 0.2491, 0.2512, 0.2518],
        [0.2471, 0.2478, 0.2498, 0.2553]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:15:45午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][50/390]	Step 8652	lr 0.01525	Loss 1.5366 (1.1977)	Architecture Loss 2.1029 (1.9257)	Prec@(1,5) (65.3%, 90.7%)	
07/03 09:16:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][100/390]	Step 8702	lr 0.01525	Loss 1.3161 (1.2224)	Architecture Loss 1.8847 (1.9414)	Prec@(1,5) (64.2%, 90.7%)	
07/03 09:16:20午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][150/390]	Step 8752	lr 0.01525	Loss 1.2841 (1.2378)	Architecture Loss 1.4055 (1.9651)	Prec@(1,5) (63.6%, 90.6%)	
07/03 09:16:37午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][200/390]	Step 8802	lr 0.01525	Loss 1.4780 (1.2500)	Architecture Loss 1.6864 (1.9746)	Prec@(1,5) (63.5%, 90.3%)	
07/03 09:16:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][250/390]	Step 8852	lr 0.01525	Loss 1.2131 (1.2662)	Architecture Loss 2.2774 (1.9627)	Prec@(1,5) (63.1%, 90.1%)	
07/03 09:17:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][300/390]	Step 8902	lr 0.01525	Loss 1.5479 (1.2813)	Architecture Loss 1.8609 (1.9712)	Prec@(1,5) (62.7%, 89.9%)	
07/03 09:17:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][350/390]	Step 8952	lr 0.01525	Loss 1.2501 (1.2970)	Architecture Loss 2.1349 (1.9749)	Prec@(1,5) (62.3%, 89.6%)	
07/03 09:17:44午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [22][390/390]	Step 8992	lr 0.01525	Loss 1.2512 (1.3013)	Architecture Loss 1.9537 (1.9726)	Prec@(1,5) (62.2%, 89.6%)	
07/03 09:17:45午後 searchStage_trainer.py:225 [INFO] Train: [ 22/49] Final Prec@1 62.1880%
07/03 09:17:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][50/391]	Step 8993	Loss 1.9959	Prec@(1,5) (48.3%, 78.1%)
07/03 09:17:50午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][100/391]	Step 8993	Loss 1.9831	Prec@(1,5) (49.0%, 78.5%)
07/03 09:17:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][150/391]	Step 8993	Loss 1.9714	Prec@(1,5) (48.9%, 78.9%)
07/03 09:17:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][200/391]	Step 8993	Loss 1.9622	Prec@(1,5) (49.0%, 79.3%)
07/03 09:17:57午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][250/391]	Step 8993	Loss 1.9667	Prec@(1,5) (49.0%, 79.3%)
07/03 09:17:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][300/391]	Step 8993	Loss 1.9580	Prec@(1,5) (49.1%, 79.4%)
07/03 09:18:02午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][350/391]	Step 8993	Loss 1.9685	Prec@(1,5) (48.9%, 79.3%)
07/03 09:18:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [22][390/391]	Step 8993	Loss 1.9733	Prec@(1,5) (48.8%, 79.2%)
07/03 09:18:03午後 searchStage_trainer.py:264 [INFO] Valid: [ 22/49] Final Prec@1 48.8360%
07/03 09:18:03午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:18:04午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 48.8360%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2472, 0.2597, 0.2474],
        [0.2446, 0.2480, 0.2587, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2387, 0.2409, 0.2786, 0.2417],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2480, 0.2582, 0.2491],
        [0.2465, 0.2490, 0.2555, 0.2490],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2484, 0.2556, 0.2475],
        [0.2490, 0.2503, 0.2499, 0.2508],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2491, 0.2546, 0.2494],
        [0.2471, 0.2493, 0.2541, 0.2494],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2461, 0.2514, 0.2577, 0.2448],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2470, 0.2501, 0.2560, 0.2469],
        [0.2478, 0.2494, 0.2528, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2536, 0.2477],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2524, 0.2503],
        [0.2483, 0.2490, 0.2523, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2473, 0.2563, 0.2475],
        [0.2489, 0.2473, 0.2551, 0.2488],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2522, 0.2525, 0.2664, 0.2289],
        [0.2471, 0.2476, 0.2500, 0.2554],
        [0.2478, 0.2487, 0.2494, 0.2541]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2506, 0.2528, 0.2470],
        [0.2489, 0.2486, 0.2516, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2518, 0.2579, 0.2391],
        [0.2481, 0.2483, 0.2525, 0.2511],
        [0.2461, 0.2478, 0.2497, 0.2563]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2516, 0.2570, 0.2404],
        [0.2479, 0.2491, 0.2512, 0.2518],
        [0.2471, 0.2478, 0.2498, 0.2553]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:18:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][50/390]	Step 9043	lr 0.0145	Loss 0.9926 (1.1225)	Architecture Loss 2.0893 (1.9421)	Prec@(1,5) (66.5%, 92.3%)	
07/03 09:18:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][100/390]	Step 9093	lr 0.0145	Loss 1.1467 (1.1311)	Architecture Loss 2.1786 (1.9430)	Prec@(1,5) (66.7%, 92.1%)	
07/03 09:18:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][150/390]	Step 9143	lr 0.0145	Loss 0.9824 (1.1680)	Architecture Loss 2.2586 (1.9586)	Prec@(1,5) (65.5%, 91.5%)	
07/03 09:19:15午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][200/390]	Step 9193	lr 0.0145	Loss 1.0847 (1.1815)	Architecture Loss 1.5492 (1.9491)	Prec@(1,5) (65.2%, 91.3%)	
07/03 09:19:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][250/390]	Step 9243	lr 0.0145	Loss 1.3671 (1.1917)	Architecture Loss 1.5686 (1.9535)	Prec@(1,5) (64.9%, 91.1%)	
07/03 09:19:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][300/390]	Step 9293	lr 0.0145	Loss 0.9864 (1.2042)	Architecture Loss 2.1689 (1.9610)	Prec@(1,5) (64.5%, 90.9%)	
07/03 09:20:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][350/390]	Step 9343	lr 0.0145	Loss 1.0326 (1.2119)	Architecture Loss 2.1184 (1.9650)	Prec@(1,5) (64.4%, 90.8%)	
07/03 09:20:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [23][390/390]	Step 9383	lr 0.0145	Loss 1.1369 (1.2234)	Architecture Loss 2.4535 (1.9571)	Prec@(1,5) (63.9%, 90.7%)	
07/03 09:20:22午後 searchStage_trainer.py:225 [INFO] Train: [ 23/49] Final Prec@1 63.9520%
07/03 09:20:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][50/391]	Step 9384	Loss 1.9236	Prec@(1,5) (50.1%, 80.5%)
07/03 09:20:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][100/391]	Step 9384	Loss 1.9377	Prec@(1,5) (49.4%, 80.0%)
07/03 09:20:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][150/391]	Step 9384	Loss 1.9250	Prec@(1,5) (49.8%, 80.3%)
07/03 09:20:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][200/391]	Step 9384	Loss 1.9450	Prec@(1,5) (49.5%, 79.8%)
07/03 09:20:34午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][250/391]	Step 9384	Loss 1.9580	Prec@(1,5) (49.3%, 79.6%)
07/03 09:20:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][300/391]	Step 9384	Loss 1.9596	Prec@(1,5) (49.4%, 79.4%)
07/03 09:20:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][350/391]	Step 9384	Loss 1.9581	Prec@(1,5) (49.5%, 79.4%)
07/03 09:20:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [23][390/391]	Step 9384	Loss 1.9606	Prec@(1,5) (49.5%, 79.4%)
07/03 09:20:41午後 searchStage_trainer.py:264 [INFO] Valid: [ 23/49] Final Prec@1 49.4760%
07/03 09:20:41午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:20:42午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 49.4760%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2472, 0.2598, 0.2474],
        [0.2445, 0.2480, 0.2588, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2382, 0.2405, 0.2796, 0.2417],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2479, 0.2584, 0.2491],
        [0.2465, 0.2489, 0.2556, 0.2490],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2485, 0.2484, 0.2556, 0.2475],
        [0.2490, 0.2503, 0.2499, 0.2508],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2491, 0.2546, 0.2494],
        [0.2471, 0.2493, 0.2542, 0.2494],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2460, 0.2514, 0.2578, 0.2449],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2501, 0.2561, 0.2469],
        [0.2478, 0.2494, 0.2528, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2536, 0.2476],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2523, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2473, 0.2564, 0.2475],
        [0.2488, 0.2472, 0.2552, 0.2487],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2526, 0.2668, 0.2285],
        [0.2471, 0.2475, 0.2500, 0.2554],
        [0.2478, 0.2487, 0.2494, 0.2541]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2506, 0.2529, 0.2469],
        [0.2489, 0.2486, 0.2516, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2519, 0.2580, 0.2390],
        [0.2480, 0.2483, 0.2525, 0.2511],
        [0.2461, 0.2478, 0.2497, 0.2563]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2517, 0.2571, 0.2402],
        [0.2479, 0.2491, 0.2513, 0.2518],
        [0.2471, 0.2478, 0.2498, 0.2553]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:21:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][50/390]	Step 9434	lr 0.01375	Loss 0.9901 (1.0876)	Architecture Loss 2.0550 (2.0009)	Prec@(1,5) (68.3%, 92.3%)	
07/03 09:21:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][100/390]	Step 9484	lr 0.01375	Loss 1.3421 (1.1093)	Architecture Loss 1.9297 (1.9584)	Prec@(1,5) (67.4%, 92.1%)	
07/03 09:21:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][150/390]	Step 9534	lr 0.01375	Loss 1.4250 (1.1202)	Architecture Loss 2.2537 (1.9545)	Prec@(1,5) (67.0%, 92.1%)	
07/03 09:21:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][200/390]	Step 9584	lr 0.01375	Loss 1.1851 (1.1299)	Architecture Loss 1.6297 (1.9458)	Prec@(1,5) (66.6%, 92.0%)	
07/03 09:22:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][250/390]	Step 9634	lr 0.01375	Loss 1.1997 (1.1330)	Architecture Loss 1.8361 (1.9511)	Prec@(1,5) (66.5%, 91.9%)	
07/03 09:22:28午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][300/390]	Step 9684	lr 0.01375	Loss 1.2201 (1.1505)	Architecture Loss 2.1369 (1.9430)	Prec@(1,5) (66.0%, 91.7%)	
07/03 09:22:45午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][350/390]	Step 9734	lr 0.01375	Loss 1.4101 (1.1616)	Architecture Loss 1.9333 (1.9389)	Prec@(1,5) (65.6%, 91.6%)	
07/03 09:22:59午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [24][390/390]	Step 9774	lr 0.01375	Loss 1.1216 (1.1649)	Architecture Loss 2.3774 (1.9476)	Prec@(1,5) (65.6%, 91.6%)	
07/03 09:23:00午後 searchStage_trainer.py:225 [INFO] Train: [ 24/49] Final Prec@1 65.6080%
07/03 09:23:02午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][50/391]	Step 9775	Loss 1.9348	Prec@(1,5) (48.8%, 80.3%)
07/03 09:23:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][100/391]	Step 9775	Loss 1.9493	Prec@(1,5) (49.2%, 80.3%)
07/03 09:23:07午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][150/391]	Step 9775	Loss 1.9328	Prec@(1,5) (49.9%, 80.0%)
07/03 09:23:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][200/391]	Step 9775	Loss 1.9416	Prec@(1,5) (49.6%, 79.9%)
07/03 09:23:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][250/391]	Step 9775	Loss 1.9451	Prec@(1,5) (49.8%, 79.7%)
07/03 09:23:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][300/391]	Step 9775	Loss 1.9406	Prec@(1,5) (49.8%, 79.8%)
07/03 09:23:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][350/391]	Step 9775	Loss 1.9350	Prec@(1,5) (50.1%, 79.9%)
07/03 09:23:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [24][390/391]	Step 9775	Loss 1.9353	Prec@(1,5) (49.9%, 79.9%)
07/03 09:23:19午後 searchStage_trainer.py:264 [INFO] Valid: [ 24/49] Final Prec@1 49.8720%
07/03 09:23:19午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:23:19午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 49.8720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2471, 0.2599, 0.2474],
        [0.2445, 0.2479, 0.2589, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2377, 0.2402, 0.2805, 0.2416],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2479, 0.2586, 0.2491],
        [0.2464, 0.2489, 0.2557, 0.2490],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2484, 0.2557, 0.2475],
        [0.2490, 0.2503, 0.2499, 0.2508],
        [0.2489, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2491, 0.2546, 0.2494],
        [0.2471, 0.2493, 0.2542, 0.2495],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2459, 0.2513, 0.2580, 0.2449],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2501, 0.2561, 0.2469],
        [0.2478, 0.2494, 0.2528, 0.2500],
        [0.2485, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2536, 0.2476],
        [0.2489, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2523, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2473, 0.2564, 0.2474],
        [0.2488, 0.2472, 0.2552, 0.2487],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2527, 0.2671, 0.2281],
        [0.2470, 0.2475, 0.2500, 0.2555],
        [0.2478, 0.2487, 0.2494, 0.2541]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2506, 0.2529, 0.2469],
        [0.2489, 0.2486, 0.2516, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2516]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2520, 0.2581, 0.2388],
        [0.2480, 0.2483, 0.2526, 0.2511],
        [0.2461, 0.2478, 0.2497, 0.2564]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2517, 0.2572, 0.2401],
        [0.2479, 0.2491, 0.2513, 0.2518],
        [0.2471, 0.2478, 0.2498, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:23:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][50/390]	Step 9825	lr 0.013	Loss 0.8493 (1.0091)	Architecture Loss 2.2529 (1.9748)	Prec@(1,5) (71.0%, 93.9%)	
07/03 09:23:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][100/390]	Step 9875	lr 0.013	Loss 1.1160 (1.0108)	Architecture Loss 1.8899 (1.9541)	Prec@(1,5) (70.6%, 93.6%)	
07/03 09:24:13午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][150/390]	Step 9925	lr 0.013	Loss 0.9802 (1.0350)	Architecture Loss 1.8872 (1.9687)	Prec@(1,5) (69.4%, 93.4%)	
07/03 09:24:31午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][200/390]	Step 9975	lr 0.013	Loss 0.9958 (1.0521)	Architecture Loss 1.8026 (1.9503)	Prec@(1,5) (68.8%, 93.2%)	
07/03 09:24:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][250/390]	Step 10025	lr 0.013	Loss 1.1453 (1.0563)	Architecture Loss 2.1216 (1.9615)	Prec@(1,5) (68.6%, 93.2%)	
07/03 09:25:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][300/390]	Step 10075	lr 0.013	Loss 1.0580 (1.0739)	Architecture Loss 1.9363 (1.9574)	Prec@(1,5) (67.9%, 92.9%)	
07/03 09:25:24午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][350/390]	Step 10125	lr 0.013	Loss 1.2425 (1.0795)	Architecture Loss 2.0905 (1.9569)	Prec@(1,5) (67.7%, 92.8%)	
07/03 09:25:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [25][390/390]	Step 10165	lr 0.013	Loss 1.3012 (1.0914)	Architecture Loss 1.8069 (1.9497)	Prec@(1,5) (67.4%, 92.6%)	
07/03 09:25:39午後 searchStage_trainer.py:225 [INFO] Train: [ 25/49] Final Prec@1 67.3480%
07/03 09:25:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][50/391]	Step 10166	Loss 1.8550	Prec@(1,5) (51.2%, 81.4%)
07/03 09:25:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][100/391]	Step 10166	Loss 1.8718	Prec@(1,5) (51.3%, 81.2%)
07/03 09:25:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][150/391]	Step 10166	Loss 1.8634	Prec@(1,5) (51.5%, 81.1%)
07/03 09:25:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][200/391]	Step 10166	Loss 1.8733	Prec@(1,5) (51.2%, 80.9%)
07/03 09:25:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][250/391]	Step 10166	Loss 1.8886	Prec@(1,5) (51.0%, 80.8%)
07/03 09:25:53午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][300/391]	Step 10166	Loss 1.8906	Prec@(1,5) (51.0%, 80.7%)
07/03 09:25:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][350/391]	Step 10166	Loss 1.8958	Prec@(1,5) (50.8%, 80.6%)
07/03 09:25:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [25][390/391]	Step 10166	Loss 1.9065	Prec@(1,5) (50.8%, 80.4%)
07/03 09:25:58午後 searchStage_trainer.py:264 [INFO] Valid: [ 25/49] Final Prec@1 50.7720%
07/03 09:25:58午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:25:58午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 50.7720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2471, 0.2599, 0.2474],
        [0.2444, 0.2479, 0.2590, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2373, 0.2398, 0.2812, 0.2416],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2444, 0.2478, 0.2588, 0.2491],
        [0.2464, 0.2489, 0.2558, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2483, 0.2558, 0.2475],
        [0.2490, 0.2503, 0.2499, 0.2508],
        [0.2490, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2490, 0.2547, 0.2494],
        [0.2470, 0.2493, 0.2542, 0.2495],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2458, 0.2513, 0.2581, 0.2448],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2490, 0.2494, 0.2506, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2501, 0.2562, 0.2468],
        [0.2478, 0.2494, 0.2528, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2536, 0.2476],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2523, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2473, 0.2565, 0.2474],
        [0.2488, 0.2472, 0.2553, 0.2487],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2521, 0.2528, 0.2674, 0.2277],
        [0.2470, 0.2475, 0.2500, 0.2555],
        [0.2478, 0.2487, 0.2494, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2506, 0.2529, 0.2469],
        [0.2489, 0.2486, 0.2516, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2521, 0.2581, 0.2386],
        [0.2480, 0.2483, 0.2526, 0.2511],
        [0.2460, 0.2478, 0.2497, 0.2564]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2518, 0.2573, 0.2399],
        [0.2478, 0.2491, 0.2513, 0.2518],
        [0.2471, 0.2478, 0.2498, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:26:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][50/390]	Step 10216	lr 0.01225	Loss 0.8688 (0.9364)	Architecture Loss 1.4673 (1.9013)	Prec@(1,5) (72.3%, 94.3%)	
07/03 09:26:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][100/390]	Step 10266	lr 0.01225	Loss 0.8511 (0.9491)	Architecture Loss 1.9365 (1.9183)	Prec@(1,5) (71.8%, 94.1%)	
07/03 09:26:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][150/390]	Step 10316	lr 0.01225	Loss 1.0209 (0.9768)	Architecture Loss 2.4079 (1.9251)	Prec@(1,5) (70.8%, 93.8%)	
07/03 09:27:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][200/390]	Step 10366	lr 0.01225	Loss 1.0204 (0.9972)	Architecture Loss 1.8279 (1.9248)	Prec@(1,5) (70.3%, 93.4%)	
07/03 09:27:28午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][250/390]	Step 10416	lr 0.01225	Loss 1.1375 (1.0013)	Architecture Loss 1.5899 (1.9250)	Prec@(1,5) (70.2%, 93.4%)	
07/03 09:27:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][300/390]	Step 10466	lr 0.01225	Loss 1.1537 (1.0176)	Architecture Loss 2.3890 (1.9318)	Prec@(1,5) (69.7%, 93.3%)	
07/03 09:28:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][350/390]	Step 10516	lr 0.01225	Loss 0.9763 (1.0250)	Architecture Loss 1.9897 (1.9357)	Prec@(1,5) (69.3%, 93.2%)	
07/03 09:28:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [26][390/390]	Step 10556	lr 0.01225	Loss 1.0111 (1.0329)	Architecture Loss 2.1863 (1.9415)	Prec@(1,5) (69.1%, 93.2%)	
07/03 09:28:18午後 searchStage_trainer.py:225 [INFO] Train: [ 26/49] Final Prec@1 69.1120%
07/03 09:28:21午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][50/391]	Step 10557	Loss 1.9989	Prec@(1,5) (48.7%, 79.9%)
07/03 09:28:23午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][100/391]	Step 10557	Loss 1.9881	Prec@(1,5) (49.7%, 79.7%)
07/03 09:28:25午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][150/391]	Step 10557	Loss 1.9834	Prec@(1,5) (50.3%, 79.9%)
07/03 09:28:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][200/391]	Step 10557	Loss 1.9738	Prec@(1,5) (50.4%, 80.0%)
07/03 09:28:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][250/391]	Step 10557	Loss 1.9796	Prec@(1,5) (50.2%, 79.9%)
07/03 09:28:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][300/391]	Step 10557	Loss 1.9842	Prec@(1,5) (50.1%, 79.7%)
07/03 09:28:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][350/391]	Step 10557	Loss 1.9859	Prec@(1,5) (49.8%, 79.6%)
07/03 09:28:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [26][390/391]	Step 10557	Loss 1.9826	Prec@(1,5) (49.9%, 79.7%)
07/03 09:28:37午後 searchStage_trainer.py:264 [INFO] Valid: [ 26/49] Final Prec@1 49.9080%
07/03 09:28:37午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:28:37午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 50.7720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2527, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2455, 0.2471, 0.2600, 0.2474],
        [0.2444, 0.2479, 0.2591, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2369, 0.2395, 0.2821, 0.2415],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2477, 0.2589, 0.2491],
        [0.2464, 0.2488, 0.2559, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2482, 0.2559, 0.2475],
        [0.2490, 0.2503, 0.2499, 0.2508],
        [0.2490, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2490, 0.2547, 0.2494],
        [0.2470, 0.2492, 0.2543, 0.2495],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2457, 0.2512, 0.2582, 0.2449],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2494, 0.2505, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2501, 0.2563, 0.2468],
        [0.2478, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2537, 0.2476],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2523, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2473, 0.2565, 0.2474],
        [0.2488, 0.2472, 0.2553, 0.2487],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2520, 0.2530, 0.2678, 0.2273],
        [0.2470, 0.2475, 0.2500, 0.2555],
        [0.2478, 0.2487, 0.2494, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2496, 0.2506, 0.2529, 0.2469],
        [0.2489, 0.2486, 0.2516, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2521, 0.2582, 0.2385],
        [0.2479, 0.2483, 0.2526, 0.2511],
        [0.2460, 0.2478, 0.2497, 0.2564]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2519, 0.2574, 0.2397],
        [0.2478, 0.2491, 0.2513, 0.2518],
        [0.2470, 0.2478, 0.2498, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:28:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][50/390]	Step 10607	lr 0.0115	Loss 0.9571 (0.8662)	Architecture Loss 1.8653 (1.9657)	Prec@(1,5) (74.4%, 95.3%)	
07/03 09:29:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][100/390]	Step 10657	lr 0.0115	Loss 0.7555 (0.8805)	Architecture Loss 1.6683 (1.9291)	Prec@(1,5) (73.3%, 95.3%)	
07/03 09:29:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][150/390]	Step 10707	lr 0.0115	Loss 0.9168 (0.8952)	Architecture Loss 2.1247 (1.9322)	Prec@(1,5) (72.8%, 95.2%)	
07/03 09:29:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][200/390]	Step 10757	lr 0.0115	Loss 0.9152 (0.9131)	Architecture Loss 2.1866 (1.9437)	Prec@(1,5) (72.3%, 94.9%)	
07/03 09:30:07午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][250/390]	Step 10807	lr 0.0115	Loss 0.9230 (0.9293)	Architecture Loss 1.8925 (1.9427)	Prec@(1,5) (72.0%, 94.7%)	
07/03 09:30:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][300/390]	Step 10857	lr 0.0115	Loss 0.8486 (0.9482)	Architecture Loss 1.9829 (1.9350)	Prec@(1,5) (71.5%, 94.4%)	
07/03 09:30:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][350/390]	Step 10907	lr 0.0115	Loss 1.3909 (0.9628)	Architecture Loss 2.4545 (1.9344)	Prec@(1,5) (71.2%, 94.1%)	
07/03 09:30:57午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [27][390/390]	Step 10947	lr 0.0115	Loss 0.8919 (0.9702)	Architecture Loss 1.7830 (1.9355)	Prec@(1,5) (71.0%, 94.0%)	
07/03 09:30:58午後 searchStage_trainer.py:225 [INFO] Train: [ 27/49] Final Prec@1 70.9440%
07/03 09:31:00午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][50/391]	Step 10948	Loss 1.9710	Prec@(1,5) (51.0%, 80.3%)
07/03 09:31:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][100/391]	Step 10948	Loss 1.9217	Prec@(1,5) (51.8%, 80.7%)
07/03 09:31:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][150/391]	Step 10948	Loss 1.9216	Prec@(1,5) (51.9%, 80.7%)
07/03 09:31:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][200/391]	Step 10948	Loss 1.9264	Prec@(1,5) (51.7%, 80.7%)
07/03 09:31:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][250/391]	Step 10948	Loss 1.9337	Prec@(1,5) (51.2%, 80.6%)
07/03 09:31:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][300/391]	Step 10948	Loss 1.9194	Prec@(1,5) (51.7%, 80.7%)
07/03 09:31:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][350/391]	Step 10948	Loss 1.9143	Prec@(1,5) (51.6%, 80.8%)
07/03 09:31:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [27][390/391]	Step 10948	Loss 1.9242	Prec@(1,5) (51.3%, 80.7%)
07/03 09:31:17午後 searchStage_trainer.py:264 [INFO] Valid: [ 27/49] Final Prec@1 51.3280%
07/03 09:31:17午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:31:17午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 51.3280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2454, 0.2470, 0.2601, 0.2474],
        [0.2443, 0.2478, 0.2592, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2365, 0.2392, 0.2828, 0.2415],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2476, 0.2591, 0.2491],
        [0.2463, 0.2488, 0.2559, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2482, 0.2560, 0.2475],
        [0.2490, 0.2503, 0.2499, 0.2508],
        [0.2490, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2469, 0.2490, 0.2547, 0.2494],
        [0.2470, 0.2492, 0.2543, 0.2495],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2456, 0.2512, 0.2584, 0.2449],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2494, 0.2505, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2500, 0.2563, 0.2468],
        [0.2478, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2537, 0.2476],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2523, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2473, 0.2566, 0.2474],
        [0.2487, 0.2472, 0.2554, 0.2487],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2530, 0.2681, 0.2270],
        [0.2470, 0.2475, 0.2500, 0.2556],
        [0.2478, 0.2487, 0.2494, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2506, 0.2530, 0.2469],
        [0.2489, 0.2486, 0.2516, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2522, 0.2583, 0.2383],
        [0.2479, 0.2483, 0.2526, 0.2511],
        [0.2460, 0.2478, 0.2497, 0.2564]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2520, 0.2574, 0.2396],
        [0.2478, 0.2491, 0.2513, 0.2517],
        [0.2470, 0.2477, 0.2498, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:31:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][50/390]	Step 10998	lr 0.01075	Loss 0.8512 (0.8306)	Architecture Loss 2.1554 (1.9413)	Prec@(1,5) (76.2%, 95.6%)	
07/03 09:31:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][100/390]	Step 11048	lr 0.01075	Loss 0.9759 (0.8396)	Architecture Loss 1.6887 (1.9324)	Prec@(1,5) (75.1%, 95.6%)	
07/03 09:32:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][150/390]	Step 11098	lr 0.01075	Loss 0.6196 (0.8469)	Architecture Loss 2.0618 (1.9371)	Prec@(1,5) (74.6%, 95.6%)	
07/03 09:32:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][200/390]	Step 11148	lr 0.01075	Loss 0.8072 (0.8546)	Architecture Loss 1.9923 (1.9271)	Prec@(1,5) (74.5%, 95.5%)	
07/03 09:32:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][250/390]	Step 11198	lr 0.01075	Loss 0.6561 (0.8668)	Architecture Loss 1.9231 (1.9276)	Prec@(1,5) (74.2%, 95.3%)	
07/03 09:33:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][300/390]	Step 11248	lr 0.01075	Loss 0.8914 (0.8802)	Architecture Loss 2.2634 (1.9177)	Prec@(1,5) (73.6%, 95.2%)	
07/03 09:33:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][350/390]	Step 11298	lr 0.01075	Loss 1.0979 (0.8870)	Architecture Loss 1.7507 (1.9279)	Prec@(1,5) (73.3%, 95.2%)	
07/03 09:33:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [28][390/390]	Step 11338	lr 0.01075	Loss 0.9833 (0.8975)	Architecture Loss 2.0576 (1.9298)	Prec@(1,5) (73.0%, 95.0%)	
07/03 09:33:36午後 searchStage_trainer.py:225 [INFO] Train: [ 28/49] Final Prec@1 72.9560%
07/03 09:33:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][50/391]	Step 11339	Loss 1.8826	Prec@(1,5) (52.3%, 81.2%)
07/03 09:33:42午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][100/391]	Step 11339	Loss 1.9092	Prec@(1,5) (51.8%, 81.2%)
07/03 09:33:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][150/391]	Step 11339	Loss 1.9177	Prec@(1,5) (51.7%, 80.9%)
07/03 09:33:46午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][200/391]	Step 11339	Loss 1.9182	Prec@(1,5) (51.7%, 80.9%)
07/03 09:33:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][250/391]	Step 11339	Loss 1.9206	Prec@(1,5) (51.6%, 80.8%)
07/03 09:33:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][300/391]	Step 11339	Loss 1.9218	Prec@(1,5) (51.7%, 80.8%)
07/03 09:33:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][350/391]	Step 11339	Loss 1.9271	Prec@(1,5) (51.6%, 80.7%)
07/03 09:33:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [28][390/391]	Step 11339	Loss 1.9256	Prec@(1,5) (51.6%, 80.8%)
07/03 09:33:56午後 searchStage_trainer.py:264 [INFO] Valid: [ 28/49] Final Prec@1 51.5560%
07/03 09:33:56午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:33:56午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 51.5560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2454, 0.2470, 0.2602, 0.2474],
        [0.2443, 0.2478, 0.2592, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2361, 0.2389, 0.2836, 0.2415],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2518]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2504, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2440, 0.2476, 0.2593, 0.2491],
        [0.2463, 0.2488, 0.2560, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2484, 0.2481, 0.2561, 0.2474],
        [0.2490, 0.2503, 0.2499, 0.2508],
        [0.2490, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2490, 0.2548, 0.2494],
        [0.2470, 0.2492, 0.2543, 0.2495],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2454, 0.2512, 0.2585, 0.2450],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2494, 0.2505, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2500, 0.2564, 0.2468],
        [0.2478, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2537, 0.2476],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2523, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2473, 0.2566, 0.2473],
        [0.2487, 0.2472, 0.2554, 0.2487],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2519, 0.2531, 0.2684, 0.2266],
        [0.2470, 0.2475, 0.2500, 0.2556],
        [0.2477, 0.2487, 0.2494, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2506, 0.2530, 0.2468],
        [0.2489, 0.2486, 0.2516, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2523, 0.2583, 0.2382],
        [0.2479, 0.2483, 0.2526, 0.2511],
        [0.2460, 0.2478, 0.2498, 0.2564]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2520, 0.2575, 0.2395],
        [0.2478, 0.2491, 0.2513, 0.2517],
        [0.2470, 0.2477, 0.2498, 0.2554]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:34:15午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][50/390]	Step 11389	lr 0.01002	Loss 0.5876 (0.7294)	Architecture Loss 1.7652 (1.9366)	Prec@(1,5) (77.4%, 96.7%)	
07/03 09:34:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][100/390]	Step 11439	lr 0.01002	Loss 0.5575 (0.7618)	Architecture Loss 2.3345 (1.9352)	Prec@(1,5) (76.7%, 96.4%)	
07/03 09:34:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][150/390]	Step 11489	lr 0.01002	Loss 0.7024 (0.7751)	Architecture Loss 2.1256 (1.9485)	Prec@(1,5) (76.4%, 96.2%)	
07/03 09:35:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][200/390]	Step 11539	lr 0.01002	Loss 0.8943 (0.7844)	Architecture Loss 1.4574 (1.9369)	Prec@(1,5) (76.2%, 96.1%)	
07/03 09:35:26午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][250/390]	Step 11589	lr 0.01002	Loss 0.6754 (0.7995)	Architecture Loss 2.0040 (1.9566)	Prec@(1,5) (75.7%, 95.9%)	
07/03 09:35:44午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][300/390]	Step 11639	lr 0.01002	Loss 0.8197 (0.8113)	Architecture Loss 2.2564 (1.9575)	Prec@(1,5) (75.5%, 95.7%)	
07/03 09:36:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][350/390]	Step 11689	lr 0.01002	Loss 0.8003 (0.8256)	Architecture Loss 2.2221 (1.9510)	Prec@(1,5) (75.0%, 95.5%)	
07/03 09:36:15午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [29][390/390]	Step 11729	lr 0.01002	Loss 0.9405 (0.8311)	Architecture Loss 2.0420 (1.9510)	Prec@(1,5) (74.8%, 95.5%)	
07/03 09:36:16午後 searchStage_trainer.py:225 [INFO] Train: [ 29/49] Final Prec@1 74.7720%
07/03 09:36:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][50/391]	Step 11730	Loss 1.8836	Prec@(1,5) (53.4%, 80.4%)
07/03 09:36:21午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][100/391]	Step 11730	Loss 1.9012	Prec@(1,5) (52.9%, 80.6%)
07/03 09:36:23午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][150/391]	Step 11730	Loss 1.9071	Prec@(1,5) (53.0%, 80.7%)
07/03 09:36:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][200/391]	Step 11730	Loss 1.9097	Prec@(1,5) (52.8%, 80.9%)
07/03 09:36:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][250/391]	Step 11730	Loss 1.9177	Prec@(1,5) (52.5%, 80.9%)
07/03 09:36:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][300/391]	Step 11730	Loss 1.9105	Prec@(1,5) (52.5%, 81.1%)
07/03 09:36:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][350/391]	Step 11730	Loss 1.9028	Prec@(1,5) (52.5%, 81.2%)
07/03 09:36:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [29][390/391]	Step 11730	Loss 1.8983	Prec@(1,5) (52.6%, 81.2%)
07/03 09:36:35午後 searchStage_trainer.py:264 [INFO] Valid: [ 29/49] Final Prec@1 52.6480%
07/03 09:36:35午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:36:35午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 52.6480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2470, 0.2602, 0.2474],
        [0.2442, 0.2478, 0.2593, 0.2487],
        [0.2493, 0.2512, 0.2498, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2357, 0.2386, 0.2844, 0.2414],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2487],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2475, 0.2594, 0.2491],
        [0.2462, 0.2487, 0.2561, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2481, 0.2562, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2508],
        [0.2490, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2490, 0.2548, 0.2494],
        [0.2470, 0.2492, 0.2543, 0.2495],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2511, 0.2586, 0.2450],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2494, 0.2505, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2500, 0.2564, 0.2468],
        [0.2478, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2537, 0.2476],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2482, 0.2490, 0.2523, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2473, 0.2567, 0.2473],
        [0.2487, 0.2472, 0.2554, 0.2487],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2532, 0.2687, 0.2262],
        [0.2469, 0.2475, 0.2500, 0.2556],
        [0.2477, 0.2487, 0.2494, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2507, 0.2530, 0.2468],
        [0.2489, 0.2486, 0.2516, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2523, 0.2584, 0.2380],
        [0.2479, 0.2483, 0.2527, 0.2511],
        [0.2460, 0.2478, 0.2498, 0.2564]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2521, 0.2575, 0.2393],
        [0.2478, 0.2491, 0.2513, 0.2517],
        [0.2470, 0.2477, 0.2498, 0.2555]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:36:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][50/390]	Step 11780	lr 0.00929	Loss 0.9290 (0.7391)	Architecture Loss 1.7882 (1.9252)	Prec@(1,5) (78.1%, 96.9%)	
07/03 09:37:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][100/390]	Step 11830	lr 0.00929	Loss 0.7711 (0.7459)	Architecture Loss 2.3571 (1.8977)	Prec@(1,5) (78.1%, 96.5%)	
07/03 09:37:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][150/390]	Step 11880	lr 0.00929	Loss 0.4931 (0.7368)	Architecture Loss 2.3689 (1.9069)	Prec@(1,5) (78.0%, 96.6%)	
07/03 09:37:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][200/390]	Step 11930	lr 0.00929	Loss 0.7931 (0.7513)	Architecture Loss 1.9854 (1.9298)	Prec@(1,5) (77.4%, 96.4%)	
07/03 09:38:05午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][250/390]	Step 11980	lr 0.00929	Loss 0.8250 (0.7531)	Architecture Loss 2.2068 (1.9394)	Prec@(1,5) (77.3%, 96.4%)	
07/03 09:38:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][300/390]	Step 12030	lr 0.00929	Loss 0.6374 (0.7602)	Architecture Loss 2.1259 (1.9371)	Prec@(1,5) (77.0%, 96.3%)	
07/03 09:38:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][350/390]	Step 12080	lr 0.00929	Loss 0.8117 (0.7671)	Architecture Loss 2.1098 (1.9368)	Prec@(1,5) (76.8%, 96.3%)	
07/03 09:38:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [30][390/390]	Step 12120	lr 0.00929	Loss 0.9345 (0.7675)	Architecture Loss 2.0985 (1.9414)	Prec@(1,5) (76.7%, 96.3%)	
07/03 09:38:54午後 searchStage_trainer.py:225 [INFO] Train: [ 30/49] Final Prec@1 76.6560%
07/03 09:38:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][50/391]	Step 12121	Loss 2.0031	Prec@(1,5) (51.3%, 80.7%)
07/03 09:38:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][100/391]	Step 12121	Loss 1.9601	Prec@(1,5) (51.6%, 81.0%)
07/03 09:39:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][150/391]	Step 12121	Loss 1.9426	Prec@(1,5) (52.2%, 81.1%)
07/03 09:39:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][200/391]	Step 12121	Loss 1.9258	Prec@(1,5) (52.7%, 81.4%)
07/03 09:39:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][250/391]	Step 12121	Loss 1.9315	Prec@(1,5) (52.5%, 81.3%)
07/03 09:39:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][300/391]	Step 12121	Loss 1.9284	Prec@(1,5) (52.7%, 81.3%)
07/03 09:39:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][350/391]	Step 12121	Loss 1.9167	Prec@(1,5) (52.8%, 81.4%)
07/03 09:39:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [30][390/391]	Step 12121	Loss 1.9272	Prec@(1,5) (52.7%, 81.2%)
07/03 09:39:12午後 searchStage_trainer.py:264 [INFO] Valid: [ 30/49] Final Prec@1 52.6720%
07/03 09:39:12午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:39:13午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 52.6720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2470, 0.2603, 0.2474],
        [0.2442, 0.2478, 0.2594, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2353, 0.2383, 0.2851, 0.2413],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2487],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2503, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2475, 0.2595, 0.2492],
        [0.2462, 0.2487, 0.2562, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2480, 0.2562, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2508],
        [0.2490, 0.2501, 0.2504, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2490, 0.2548, 0.2494],
        [0.2469, 0.2492, 0.2544, 0.2495],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2511, 0.2587, 0.2450],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2494, 0.2505, 0.2510]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2500, 0.2564, 0.2468],
        [0.2477, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2537, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2482, 0.2490, 0.2523, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2473, 0.2567, 0.2473],
        [0.2487, 0.2472, 0.2555, 0.2487],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2533, 0.2690, 0.2259],
        [0.2469, 0.2474, 0.2500, 0.2557],
        [0.2477, 0.2487, 0.2494, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2507, 0.2530, 0.2468],
        [0.2489, 0.2486, 0.2516, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2524, 0.2584, 0.2379],
        [0.2479, 0.2483, 0.2527, 0.2511],
        [0.2460, 0.2478, 0.2498, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2522, 0.2576, 0.2392],
        [0.2478, 0.2491, 0.2513, 0.2517],
        [0.2470, 0.2477, 0.2498, 0.2555]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:39:31午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][50/390]	Step 12171	lr 0.00858	Loss 0.6101 (0.6750)	Architecture Loss 1.7788 (1.9799)	Prec@(1,5) (79.1%, 97.2%)	
07/03 09:39:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][100/390]	Step 12221	lr 0.00858	Loss 0.7490 (0.6582)	Architecture Loss 2.4423 (1.9573)	Prec@(1,5) (79.8%, 97.3%)	
07/03 09:40:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][150/390]	Step 12271	lr 0.00858	Loss 0.6663 (0.6688)	Architecture Loss 1.5052 (1.9579)	Prec@(1,5) (79.5%, 97.1%)	
07/03 09:40:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][200/390]	Step 12321	lr 0.00858	Loss 0.6598 (0.6802)	Architecture Loss 2.8607 (1.9803)	Prec@(1,5) (79.0%, 97.1%)	
07/03 09:40:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][250/390]	Step 12371	lr 0.00858	Loss 0.6991 (0.6886)	Architecture Loss 1.8475 (1.9775)	Prec@(1,5) (78.7%, 97.0%)	
07/03 09:40:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][300/390]	Step 12421	lr 0.00858	Loss 0.4911 (0.6919)	Architecture Loss 1.9962 (1.9653)	Prec@(1,5) (78.7%, 96.9%)	
07/03 09:41:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][350/390]	Step 12471	lr 0.00858	Loss 0.6833 (0.6946)	Architecture Loss 1.9701 (1.9516)	Prec@(1,5) (78.6%, 96.9%)	
07/03 09:41:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [31][390/390]	Step 12511	lr 0.00858	Loss 0.7805 (0.6983)	Architecture Loss 1.9409 (1.9489)	Prec@(1,5) (78.4%, 96.9%)	
07/03 09:41:17午後 searchStage_trainer.py:225 [INFO] Train: [ 31/49] Final Prec@1 78.4440%
07/03 09:41:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][50/391]	Step 12512	Loss 1.8267	Prec@(1,5) (54.7%, 83.0%)
07/03 09:41:21午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][100/391]	Step 12512	Loss 1.8431	Prec@(1,5) (54.8%, 82.8%)
07/03 09:41:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][150/391]	Step 12512	Loss 1.8776	Prec@(1,5) (54.1%, 82.5%)
07/03 09:41:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][200/391]	Step 12512	Loss 1.8953	Prec@(1,5) (54.1%, 82.1%)
07/03 09:41:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][250/391]	Step 12512	Loss 1.8919	Prec@(1,5) (54.0%, 82.0%)
07/03 09:41:31午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][300/391]	Step 12512	Loss 1.8965	Prec@(1,5) (53.8%, 82.1%)
07/03 09:41:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][350/391]	Step 12512	Loss 1.9086	Prec@(1,5) (53.6%, 81.9%)
07/03 09:41:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [31][390/391]	Step 12512	Loss 1.9062	Prec@(1,5) (53.7%, 82.0%)
07/03 09:41:35午後 searchStage_trainer.py:264 [INFO] Valid: [ 31/49] Final Prec@1 53.6640%
07/03 09:41:35午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:41:35午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 53.6640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2453, 0.2469, 0.2604, 0.2474],
        [0.2442, 0.2477, 0.2594, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2350, 0.2380, 0.2857, 0.2413],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2437, 0.2474, 0.2597, 0.2492],
        [0.2462, 0.2487, 0.2562, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2480, 0.2563, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2508],
        [0.2490, 0.2501, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2490, 0.2548, 0.2494],
        [0.2469, 0.2492, 0.2544, 0.2495],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2511, 0.2588, 0.2450],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2494, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2500, 0.2565, 0.2468],
        [0.2477, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2537, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2482, 0.2490, 0.2523, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2473, 0.2567, 0.2473],
        [0.2486, 0.2472, 0.2555, 0.2487],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2534, 0.2693, 0.2255],
        [0.2469, 0.2474, 0.2500, 0.2557],
        [0.2477, 0.2487, 0.2494, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2507, 0.2530, 0.2468],
        [0.2488, 0.2486, 0.2516, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2525, 0.2585, 0.2377],
        [0.2478, 0.2483, 0.2527, 0.2511],
        [0.2460, 0.2478, 0.2498, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2522, 0.2576, 0.2390],
        [0.2478, 0.2491, 0.2514, 0.2517],
        [0.2470, 0.2477, 0.2498, 0.2555]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:41:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][50/390]	Step 12562	lr 0.00789	Loss 0.5002 (0.5640)	Architecture Loss 2.1677 (1.9096)	Prec@(1,5) (82.6%, 97.8%)	
07/03 09:42:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][100/390]	Step 12612	lr 0.00789	Loss 0.4812 (0.5831)	Architecture Loss 2.0874 (1.9324)	Prec@(1,5) (82.4%, 97.7%)	
07/03 09:42:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][150/390]	Step 12662	lr 0.00789	Loss 0.3376 (0.5910)	Architecture Loss 2.9278 (1.9573)	Prec@(1,5) (81.9%, 97.6%)	
07/03 09:42:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][200/390]	Step 12712	lr 0.00789	Loss 0.9313 (0.6059)	Architecture Loss 2.2604 (1.9449)	Prec@(1,5) (81.6%, 97.4%)	
07/03 09:42:55午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][250/390]	Step 12762	lr 0.00789	Loss 0.5687 (0.6064)	Architecture Loss 1.9897 (1.9682)	Prec@(1,5) (81.7%, 97.4%)	
07/03 09:43:11午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][300/390]	Step 12812	lr 0.00789	Loss 0.7410 (0.6195)	Architecture Loss 1.5877 (1.9611)	Prec@(1,5) (81.2%, 97.3%)	
07/03 09:43:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][350/390]	Step 12862	lr 0.00789	Loss 0.9163 (0.6243)	Architecture Loss 1.6356 (1.9615)	Prec@(1,5) (81.1%, 97.3%)	
07/03 09:43:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [32][390/390]	Step 12902	lr 0.00789	Loss 0.8504 (0.6323)	Architecture Loss 2.0906 (1.9695)	Prec@(1,5) (80.8%, 97.3%)	
07/03 09:43:40午後 searchStage_trainer.py:225 [INFO] Train: [ 32/49] Final Prec@1 80.7680%
07/03 09:43:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][50/391]	Step 12903	Loss 1.9396	Prec@(1,5) (54.4%, 81.8%)
07/03 09:43:45午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][100/391]	Step 12903	Loss 1.9335	Prec@(1,5) (53.9%, 81.8%)
07/03 09:43:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][150/391]	Step 12903	Loss 1.9308	Prec@(1,5) (53.5%, 81.9%)
07/03 09:43:50午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][200/391]	Step 12903	Loss 1.9408	Prec@(1,5) (53.3%, 81.9%)
07/03 09:43:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][250/391]	Step 12903	Loss 1.9413	Prec@(1,5) (53.3%, 81.9%)
07/03 09:43:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][300/391]	Step 12903	Loss 1.9478	Prec@(1,5) (53.2%, 81.8%)
07/03 09:43:57午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][350/391]	Step 12903	Loss 1.9566	Prec@(1,5) (53.0%, 81.6%)
07/03 09:43:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [32][390/391]	Step 12903	Loss 1.9601	Prec@(1,5) (53.0%, 81.5%)
07/03 09:43:59午後 searchStage_trainer.py:264 [INFO] Valid: [ 32/49] Final Prec@1 53.0520%
07/03 09:43:59午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:43:59午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 53.6640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2469, 0.2604, 0.2474],
        [0.2441, 0.2477, 0.2595, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2346, 0.2378, 0.2863, 0.2413],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2437, 0.2474, 0.2598, 0.2492],
        [0.2461, 0.2487, 0.2563, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2479, 0.2564, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2508],
        [0.2490, 0.2501, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2490, 0.2548, 0.2494],
        [0.2469, 0.2492, 0.2544, 0.2495],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2510, 0.2588, 0.2451],
        [0.2486, 0.2493, 0.2505, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2501, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2494, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2500, 0.2565, 0.2468],
        [0.2477, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2537, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2473, 0.2568, 0.2472],
        [0.2486, 0.2472, 0.2555, 0.2487],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2518, 0.2535, 0.2696, 0.2252],
        [0.2469, 0.2474, 0.2500, 0.2557],
        [0.2477, 0.2487, 0.2494, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2507, 0.2531, 0.2467],
        [0.2488, 0.2486, 0.2517, 0.2509],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2525, 0.2585, 0.2376],
        [0.2478, 0.2483, 0.2527, 0.2511],
        [0.2460, 0.2478, 0.2498, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2523, 0.2577, 0.2389],
        [0.2478, 0.2492, 0.2514, 0.2517],
        [0.2470, 0.2477, 0.2497, 0.2555]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:44:15午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][50/390]	Step 12953	lr 0.00722	Loss 0.4353 (0.5568)	Architecture Loss 2.4259 (1.9468)	Prec@(1,5) (83.3%, 97.7%)	
07/03 09:44:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][100/390]	Step 13003	lr 0.00722	Loss 0.6711 (0.5505)	Architecture Loss 2.0111 (1.9638)	Prec@(1,5) (83.6%, 97.8%)	
07/03 09:44:46午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][150/390]	Step 13053	lr 0.00722	Loss 0.6118 (0.5587)	Architecture Loss 2.3454 (1.9832)	Prec@(1,5) (83.1%, 97.9%)	
07/03 09:45:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][200/390]	Step 13103	lr 0.00722	Loss 0.8949 (0.5708)	Architecture Loss 1.9368 (1.9696)	Prec@(1,5) (82.5%, 97.8%)	
07/03 09:45:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][250/390]	Step 13153	lr 0.00722	Loss 0.6467 (0.5753)	Architecture Loss 2.2013 (1.9792)	Prec@(1,5) (82.3%, 97.8%)	
07/03 09:45:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][300/390]	Step 13203	lr 0.00722	Loss 0.7478 (0.5806)	Architecture Loss 2.0649 (1.9749)	Prec@(1,5) (82.2%, 97.7%)	
07/03 09:45:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][350/390]	Step 13253	lr 0.00722	Loss 0.5371 (0.5847)	Architecture Loss 2.1015 (1.9729)	Prec@(1,5) (82.1%, 97.7%)	
07/03 09:46:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [33][390/390]	Step 13293	lr 0.00722	Loss 0.8948 (0.5903)	Architecture Loss 1.9259 (1.9713)	Prec@(1,5) (81.8%, 97.7%)	
07/03 09:46:00午後 searchStage_trainer.py:225 [INFO] Train: [ 33/49] Final Prec@1 81.8000%
07/03 09:46:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][50/391]	Step 13294	Loss 1.9900	Prec@(1,5) (51.7%, 82.3%)
07/03 09:46:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][100/391]	Step 13294	Loss 1.9818	Prec@(1,5) (52.4%, 82.0%)
07/03 09:46:08午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][150/391]	Step 13294	Loss 1.9649	Prec@(1,5) (52.6%, 82.4%)
07/03 09:46:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][200/391]	Step 13294	Loss 1.9777	Prec@(1,5) (52.4%, 82.0%)
07/03 09:46:13午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][250/391]	Step 13294	Loss 1.9780	Prec@(1,5) (52.7%, 81.9%)
07/03 09:46:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][300/391]	Step 13294	Loss 1.9875	Prec@(1,5) (52.8%, 81.7%)
07/03 09:46:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][350/391]	Step 13294	Loss 1.9925	Prec@(1,5) (52.9%, 81.7%)
07/03 09:46:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [33][390/391]	Step 13294	Loss 1.9966	Prec@(1,5) (52.9%, 81.6%)
07/03 09:46:19午後 searchStage_trainer.py:264 [INFO] Valid: [ 33/49] Final Prec@1 52.9360%
07/03 09:46:19午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:46:19午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 53.6640%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2469, 0.2605, 0.2474],
        [0.2441, 0.2477, 0.2595, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2343, 0.2375, 0.2868, 0.2413],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2436, 0.2473, 0.2599, 0.2492],
        [0.2461, 0.2487, 0.2563, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2483, 0.2479, 0.2564, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2508],
        [0.2490, 0.2501, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2490, 0.2549, 0.2494],
        [0.2469, 0.2492, 0.2544, 0.2495],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2509, 0.2590, 0.2452],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2494, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2500, 0.2565, 0.2468],
        [0.2477, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2537, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2500, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2473, 0.2568, 0.2472],
        [0.2486, 0.2472, 0.2556, 0.2487],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2535, 0.2699, 0.2249],
        [0.2469, 0.2474, 0.2500, 0.2557],
        [0.2477, 0.2487, 0.2494, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2507, 0.2531, 0.2467],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2526, 0.2585, 0.2375],
        [0.2478, 0.2483, 0.2527, 0.2511],
        [0.2460, 0.2478, 0.2498, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2524, 0.2577, 0.2388],
        [0.2478, 0.2492, 0.2514, 0.2517],
        [0.2470, 0.2477, 0.2497, 0.2555]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:46:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][50/390]	Step 13344	lr 0.00657	Loss 0.3818 (0.4955)	Architecture Loss 2.0151 (1.9904)	Prec@(1,5) (85.2%, 98.3%)	
07/03 09:46:51午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][100/390]	Step 13394	lr 0.00657	Loss 0.3625 (0.4984)	Architecture Loss 1.4088 (1.9295)	Prec@(1,5) (85.3%, 98.3%)	
07/03 09:47:07午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][150/390]	Step 13444	lr 0.00657	Loss 0.5401 (0.5042)	Architecture Loss 1.8741 (1.9483)	Prec@(1,5) (84.9%, 98.3%)	
07/03 09:47:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][200/390]	Step 13494	lr 0.00657	Loss 0.5065 (0.5081)	Architecture Loss 1.7797 (1.9891)	Prec@(1,5) (84.7%, 98.3%)	
07/03 09:47:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][250/390]	Step 13544	lr 0.00657	Loss 0.6338 (0.5111)	Architecture Loss 1.2304 (1.9899)	Prec@(1,5) (84.5%, 98.3%)	
07/03 09:47:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][300/390]	Step 13594	lr 0.00657	Loss 0.4390 (0.5191)	Architecture Loss 2.2360 (1.9939)	Prec@(1,5) (84.3%, 98.1%)	
07/03 09:48:09午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][350/390]	Step 13644	lr 0.00657	Loss 0.4033 (0.5269)	Architecture Loss 1.4203 (1.9917)	Prec@(1,5) (83.9%, 98.1%)	
07/03 09:48:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [34][390/390]	Step 13684	lr 0.00657	Loss 0.7164 (0.5331)	Architecture Loss 2.4562 (1.9871)	Prec@(1,5) (83.7%, 98.1%)	
07/03 09:48:21午後 searchStage_trainer.py:225 [INFO] Train: [ 34/49] Final Prec@1 83.6960%
07/03 09:48:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][50/391]	Step 13685	Loss 1.8832	Prec@(1,5) (54.0%, 83.1%)
07/03 09:48:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][100/391]	Step 13685	Loss 1.9055	Prec@(1,5) (54.4%, 82.5%)
07/03 09:48:29午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][150/391]	Step 13685	Loss 1.9267	Prec@(1,5) (54.4%, 82.3%)
07/03 09:48:31午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][200/391]	Step 13685	Loss 1.9486	Prec@(1,5) (54.3%, 82.2%)
07/03 09:48:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][250/391]	Step 13685	Loss 1.9483	Prec@(1,5) (54.2%, 82.1%)
07/03 09:48:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][300/391]	Step 13685	Loss 1.9435	Prec@(1,5) (54.2%, 82.2%)
07/03 09:48:38午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][350/391]	Step 13685	Loss 1.9615	Prec@(1,5) (54.0%, 81.9%)
07/03 09:48:40午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [34][390/391]	Step 13685	Loss 1.9554	Prec@(1,5) (54.1%, 82.0%)
07/03 09:48:40午後 searchStage_trainer.py:264 [INFO] Valid: [ 34/49] Final Prec@1 54.0920%
07/03 09:48:40午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:48:40午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 54.0920%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2469, 0.2605, 0.2474],
        [0.2441, 0.2477, 0.2596, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2340, 0.2373, 0.2873, 0.2413],
        [0.2482, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2435, 0.2473, 0.2600, 0.2492],
        [0.2461, 0.2486, 0.2564, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2479, 0.2565, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2508],
        [0.2490, 0.2501, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2468, 0.2490, 0.2549, 0.2494],
        [0.2469, 0.2492, 0.2544, 0.2495],
        [0.2495, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2508, 0.2591, 0.2452],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2500, 0.2566, 0.2468],
        [0.2477, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2521]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2537, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2473, 0.2569, 0.2472],
        [0.2486, 0.2472, 0.2556, 0.2486],
        [0.2482, 0.2495, 0.2494, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2537, 0.2701, 0.2246],
        [0.2469, 0.2474, 0.2500, 0.2558],
        [0.2477, 0.2487, 0.2494, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2495, 0.2507, 0.2531, 0.2467],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2527, 0.2586, 0.2373],
        [0.2478, 0.2483, 0.2527, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2524, 0.2578, 0.2387],
        [0.2478, 0.2492, 0.2514, 0.2516],
        [0.2470, 0.2477, 0.2497, 0.2555]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:48:57午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][50/390]	Step 13735	lr 0.00595	Loss 0.5184 (0.4537)	Architecture Loss 1.8579 (2.0020)	Prec@(1,5) (86.5%, 98.3%)	
07/03 09:49:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][100/390]	Step 13785	lr 0.00595	Loss 0.4094 (0.4454)	Architecture Loss 1.8612 (2.0452)	Prec@(1,5) (86.7%, 98.5%)	
07/03 09:49:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][150/390]	Step 13835	lr 0.00595	Loss 0.4417 (0.4544)	Architecture Loss 2.0765 (2.0287)	Prec@(1,5) (86.4%, 98.4%)	
07/03 09:49:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][200/390]	Step 13885	lr 0.00595	Loss 0.5864 (0.4559)	Architecture Loss 1.6670 (2.0182)	Prec@(1,5) (86.3%, 98.5%)	
07/03 09:49:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][250/390]	Step 13935	lr 0.00595	Loss 0.4911 (0.4641)	Architecture Loss 2.0446 (2.0209)	Prec@(1,5) (86.0%, 98.5%)	
07/03 09:50:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][300/390]	Step 13985	lr 0.00595	Loss 0.4005 (0.4692)	Architecture Loss 2.6093 (2.0193)	Prec@(1,5) (85.9%, 98.5%)	
07/03 09:50:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][350/390]	Step 14035	lr 0.00595	Loss 0.6914 (0.4715)	Architecture Loss 1.6825 (2.0153)	Prec@(1,5) (85.7%, 98.5%)	
07/03 09:50:42午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [35][390/390]	Step 14075	lr 0.00595	Loss 0.3690 (0.4761)	Architecture Loss 2.1179 (2.0148)	Prec@(1,5) (85.6%, 98.4%)	
07/03 09:50:42午後 searchStage_trainer.py:225 [INFO] Train: [ 35/49] Final Prec@1 85.6240%
07/03 09:50:45午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][50/391]	Step 14076	Loss 1.9263	Prec@(1,5) (55.7%, 82.7%)
07/03 09:50:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][100/391]	Step 14076	Loss 1.9189	Prec@(1,5) (55.6%, 82.6%)
07/03 09:50:50午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][150/391]	Step 14076	Loss 1.9350	Prec@(1,5) (55.3%, 82.5%)
07/03 09:50:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][200/391]	Step 14076	Loss 1.9340	Prec@(1,5) (54.9%, 82.6%)
07/03 09:50:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][250/391]	Step 14076	Loss 1.9380	Prec@(1,5) (54.6%, 82.5%)
07/03 09:50:57午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][300/391]	Step 14076	Loss 1.9347	Prec@(1,5) (54.6%, 82.4%)
07/03 09:50:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][350/391]	Step 14076	Loss 1.9331	Prec@(1,5) (54.7%, 82.4%)
07/03 09:51:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [35][390/391]	Step 14076	Loss 1.9309	Prec@(1,5) (54.7%, 82.4%)
07/03 09:51:01午後 searchStage_trainer.py:264 [INFO] Valid: [ 35/49] Final Prec@1 54.7400%
07/03 09:51:01午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:51:01午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 54.7400%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2452, 0.2469, 0.2605, 0.2474],
        [0.2440, 0.2477, 0.2596, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2337, 0.2371, 0.2879, 0.2413],
        [0.2482, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2435, 0.2472, 0.2601, 0.2493],
        [0.2461, 0.2486, 0.2564, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2478, 0.2566, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2508],
        [0.2490, 0.2501, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2490, 0.2549, 0.2494],
        [0.2469, 0.2491, 0.2544, 0.2495],
        [0.2496, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2448, 0.2508, 0.2591, 0.2453],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2500, 0.2566, 0.2468],
        [0.2477, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2473, 0.2569, 0.2472],
        [0.2486, 0.2472, 0.2556, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2538, 0.2704, 0.2242],
        [0.2468, 0.2474, 0.2500, 0.2558],
        [0.2477, 0.2487, 0.2494, 0.2542]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2507, 0.2531, 0.2467],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2527, 0.2586, 0.2372],
        [0.2478, 0.2484, 0.2528, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2525, 0.2578, 0.2386],
        [0.2478, 0.2492, 0.2514, 0.2516],
        [0.2470, 0.2477, 0.2497, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:51:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][50/390]	Step 14126	lr 0.00535	Loss 0.5738 (0.4058)	Architecture Loss 2.4763 (2.0389)	Prec@(1,5) (87.2%, 99.1%)	
07/03 09:51:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][100/390]	Step 14176	lr 0.00535	Loss 0.6360 (0.4065)	Architecture Loss 1.7834 (1.9863)	Prec@(1,5) (87.8%, 98.8%)	
07/03 09:51:49午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][150/390]	Step 14226	lr 0.00535	Loss 0.3530 (0.3982)	Architecture Loss 1.9157 (2.0095)	Prec@(1,5) (87.9%, 98.9%)	
07/03 09:52:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][200/390]	Step 14276	lr 0.00535	Loss 0.4865 (0.4027)	Architecture Loss 1.7377 (1.9861)	Prec@(1,5) (87.8%, 98.8%)	
07/03 09:52:19午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][250/390]	Step 14326	lr 0.00535	Loss 0.4348 (0.4112)	Architecture Loss 2.2863 (1.9898)	Prec@(1,5) (87.5%, 98.8%)	
07/03 09:52:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][300/390]	Step 14376	lr 0.00535	Loss 0.6017 (0.4121)	Architecture Loss 2.3704 (1.9945)	Prec@(1,5) (87.6%, 98.8%)	
07/03 09:52:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][350/390]	Step 14426	lr 0.00535	Loss 0.4789 (0.4145)	Architecture Loss 2.4737 (2.0028)	Prec@(1,5) (87.5%, 98.8%)	
07/03 09:53:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [36][390/390]	Step 14466	lr 0.00535	Loss 0.3656 (0.4162)	Architecture Loss 2.0656 (2.0074)	Prec@(1,5) (87.5%, 98.8%)	
07/03 09:53:02午後 searchStage_trainer.py:225 [INFO] Train: [ 36/49] Final Prec@1 87.4480%
07/03 09:53:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][50/391]	Step 14467	Loss 1.9662	Prec@(1,5) (55.3%, 82.6%)
07/03 09:53:07午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][100/391]	Step 14467	Loss 1.9842	Prec@(1,5) (54.9%, 82.6%)
07/03 09:53:09午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][150/391]	Step 14467	Loss 1.9680	Prec@(1,5) (54.8%, 82.7%)
07/03 09:53:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][200/391]	Step 14467	Loss 1.9698	Prec@(1,5) (54.9%, 82.8%)
07/03 09:53:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][250/391]	Step 14467	Loss 1.9530	Prec@(1,5) (55.2%, 83.0%)
07/03 09:53:16午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][300/391]	Step 14467	Loss 1.9646	Prec@(1,5) (55.0%, 82.8%)
07/03 09:53:18午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][350/391]	Step 14467	Loss 1.9717	Prec@(1,5) (54.9%, 82.7%)
07/03 09:53:20午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [36][390/391]	Step 14467	Loss 1.9658	Prec@(1,5) (54.9%, 82.8%)
07/03 09:53:20午後 searchStage_trainer.py:264 [INFO] Valid: [ 36/49] Final Prec@1 54.9480%
07/03 09:53:20午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:53:21午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 54.9480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2468, 0.2606, 0.2474],
        [0.2440, 0.2477, 0.2597, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2335, 0.2369, 0.2883, 0.2413],
        [0.2482, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2503],
        [0.2498, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2434, 0.2472, 0.2602, 0.2493],
        [0.2460, 0.2486, 0.2565, 0.2489],
        [0.2474, 0.2503, 0.2513, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2478, 0.2566, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2490, 0.2501, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2490, 0.2549, 0.2494],
        [0.2469, 0.2491, 0.2545, 0.2495],
        [0.2496, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2447, 0.2508, 0.2592, 0.2453],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2500, 0.2566, 0.2468],
        [0.2477, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2473, 0.2569, 0.2472],
        [0.2486, 0.2472, 0.2556, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2539, 0.2706, 0.2239],
        [0.2468, 0.2474, 0.2500, 0.2558],
        [0.2477, 0.2487, 0.2494, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2507, 0.2531, 0.2467],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2528, 0.2586, 0.2371],
        [0.2477, 0.2484, 0.2528, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2526, 0.2578, 0.2384],
        [0.2478, 0.2492, 0.2514, 0.2516],
        [0.2470, 0.2477, 0.2497, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:53:37午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][50/390]	Step 14517	lr 0.00479	Loss 0.3934 (0.3694)	Architecture Loss 1.9880 (1.9798)	Prec@(1,5) (88.4%, 99.4%)	
07/03 09:53:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][100/390]	Step 14567	lr 0.00479	Loss 0.4314 (0.3739)	Architecture Loss 1.8159 (1.9892)	Prec@(1,5) (88.5%, 99.2%)	
07/03 09:54:07午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][150/390]	Step 14617	lr 0.00479	Loss 0.2543 (0.3733)	Architecture Loss 1.8866 (1.9873)	Prec@(1,5) (88.7%, 99.2%)	
07/03 09:54:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][200/390]	Step 14667	lr 0.00479	Loss 0.4494 (0.3795)	Architecture Loss 2.8043 (1.9943)	Prec@(1,5) (88.5%, 99.2%)	
07/03 09:54:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][250/390]	Step 14717	lr 0.00479	Loss 0.3676 (0.3833)	Architecture Loss 1.6362 (2.0251)	Prec@(1,5) (88.4%, 99.2%)	
07/03 09:54:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][300/390]	Step 14767	lr 0.00479	Loss 0.2990 (0.3836)	Architecture Loss 2.4838 (2.0259)	Prec@(1,5) (88.5%, 99.1%)	
07/03 09:55:09午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][350/390]	Step 14817	lr 0.00479	Loss 0.3282 (0.3839)	Architecture Loss 1.7705 (2.0312)	Prec@(1,5) (88.4%, 99.1%)	
07/03 09:55:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [37][390/390]	Step 14857	lr 0.00479	Loss 0.2330 (0.3861)	Architecture Loss 1.8380 (2.0192)	Prec@(1,5) (88.4%, 99.1%)	
07/03 09:55:21午後 searchStage_trainer.py:225 [INFO] Train: [ 37/49] Final Prec@1 88.3440%
07/03 09:55:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][50/391]	Step 14858	Loss 1.9829	Prec@(1,5) (53.9%, 82.8%)
07/03 09:55:27午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][100/391]	Step 14858	Loss 1.9600	Prec@(1,5) (55.0%, 82.9%)
07/03 09:55:29午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][150/391]	Step 14858	Loss 1.9556	Prec@(1,5) (55.3%, 82.8%)
07/03 09:55:31午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][200/391]	Step 14858	Loss 1.9430	Prec@(1,5) (55.6%, 83.0%)
07/03 09:55:34午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][250/391]	Step 14858	Loss 1.9601	Prec@(1,5) (55.4%, 82.8%)
07/03 09:55:36午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][300/391]	Step 14858	Loss 1.9563	Prec@(1,5) (55.6%, 82.8%)
07/03 09:55:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][350/391]	Step 14858	Loss 1.9675	Prec@(1,5) (55.4%, 82.7%)
07/03 09:55:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [37][390/391]	Step 14858	Loss 1.9734	Prec@(1,5) (55.3%, 82.6%)
07/03 09:55:41午後 searchStage_trainer.py:264 [INFO] Valid: [ 37/49] Final Prec@1 55.3160%
07/03 09:55:41午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:55:41午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.3160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2468, 0.2606, 0.2475],
        [0.2440, 0.2476, 0.2597, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2332, 0.2367, 0.2888, 0.2413],
        [0.2482, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2503, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2502],
        [0.2498, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2433, 0.2471, 0.2602, 0.2493],
        [0.2460, 0.2486, 0.2565, 0.2489],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2477, 0.2567, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2490, 0.2501, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2489, 0.2549, 0.2494],
        [0.2468, 0.2491, 0.2545, 0.2496],
        [0.2496, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2508, 0.2593, 0.2453],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2499, 0.2567, 0.2468],
        [0.2477, 0.2494, 0.2529, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2569, 0.2472],
        [0.2485, 0.2472, 0.2557, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2540, 0.2708, 0.2236],
        [0.2468, 0.2474, 0.2500, 0.2558],
        [0.2477, 0.2487, 0.2494, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2508, 0.2532, 0.2467],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2529, 0.2586, 0.2370],
        [0.2477, 0.2484, 0.2528, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2526, 0.2579, 0.2383],
        [0.2478, 0.2492, 0.2514, 0.2516],
        [0.2470, 0.2477, 0.2497, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:55:57午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][50/390]	Step 14908	lr 0.00425	Loss 0.3261 (0.3231)	Architecture Loss 2.5239 (1.9534)	Prec@(1,5) (91.2%, 99.4%)	
07/03 09:56:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][100/390]	Step 14958	lr 0.00425	Loss 0.1892 (0.3258)	Architecture Loss 2.2519 (1.9784)	Prec@(1,5) (90.7%, 99.4%)	
07/03 09:56:28午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][150/390]	Step 15008	lr 0.00425	Loss 0.3730 (0.3215)	Architecture Loss 2.1100 (2.0176)	Prec@(1,5) (90.9%, 99.3%)	
07/03 09:56:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][200/390]	Step 15058	lr 0.00425	Loss 0.4244 (0.3228)	Architecture Loss 2.0308 (2.0175)	Prec@(1,5) (90.6%, 99.3%)	
07/03 09:56:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][250/390]	Step 15108	lr 0.00425	Loss 0.2865 (0.3235)	Architecture Loss 2.0697 (2.0209)	Prec@(1,5) (90.7%, 99.3%)	
07/03 09:57:13午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][300/390]	Step 15158	lr 0.00425	Loss 0.2556 (0.3280)	Architecture Loss 2.3380 (2.0240)	Prec@(1,5) (90.4%, 99.3%)	
07/03 09:57:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][350/390]	Step 15208	lr 0.00425	Loss 0.4088 (0.3287)	Architecture Loss 1.9044 (2.0213)	Prec@(1,5) (90.3%, 99.3%)	
07/03 09:57:41午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [38][390/390]	Step 15248	lr 0.00425	Loss 0.3405 (0.3293)	Architecture Loss 1.7293 (2.0212)	Prec@(1,5) (90.2%, 99.3%)	
07/03 09:57:41午後 searchStage_trainer.py:225 [INFO] Train: [ 38/49] Final Prec@1 90.2320%
07/03 09:57:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][50/391]	Step 15249	Loss 1.9106	Prec@(1,5) (56.7%, 83.5%)
07/03 09:57:46午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][100/391]	Step 15249	Loss 1.9659	Prec@(1,5) (55.2%, 82.9%)
07/03 09:57:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][150/391]	Step 15249	Loss 1.9790	Prec@(1,5) (55.3%, 82.5%)
07/03 09:57:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][200/391]	Step 15249	Loss 1.9658	Prec@(1,5) (55.5%, 82.6%)
07/03 09:57:53午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][250/391]	Step 15249	Loss 1.9793	Prec@(1,5) (55.4%, 82.5%)
07/03 09:57:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][300/391]	Step 15249	Loss 1.9732	Prec@(1,5) (55.4%, 82.6%)
07/03 09:57:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][350/391]	Step 15249	Loss 1.9911	Prec@(1,5) (55.1%, 82.4%)
07/03 09:58:00午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [38][390/391]	Step 15249	Loss 1.9873	Prec@(1,5) (55.2%, 82.5%)
07/03 09:58:00午後 searchStage_trainer.py:264 [INFO] Valid: [ 38/49] Final Prec@1 55.1600%
07/03 09:58:00午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 09:58:00午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.3160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2451, 0.2468, 0.2607, 0.2475],
        [0.2440, 0.2476, 0.2597, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2498]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2329, 0.2365, 0.2893, 0.2413],
        [0.2482, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2502],
        [0.2499, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2433, 0.2471, 0.2603, 0.2493],
        [0.2460, 0.2486, 0.2565, 0.2489],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2477, 0.2567, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2490, 0.2501, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2489, 0.2549, 0.2494],
        [0.2468, 0.2491, 0.2545, 0.2496],
        [0.2496, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2446, 0.2507, 0.2594, 0.2454],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2499, 0.2567, 0.2468],
        [0.2477, 0.2494, 0.2530, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2488, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2570, 0.2471],
        [0.2485, 0.2472, 0.2557, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2541, 0.2711, 0.2232],
        [0.2468, 0.2474, 0.2500, 0.2558],
        [0.2477, 0.2487, 0.2494, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2508, 0.2532, 0.2467],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2529, 0.2587, 0.2368],
        [0.2477, 0.2484, 0.2528, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2527, 0.2579, 0.2382],
        [0.2478, 0.2492, 0.2514, 0.2516],
        [0.2470, 0.2477, 0.2497, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 09:58:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][50/390]	Step 15299	lr 0.00375	Loss 0.3359 (0.2693)	Architecture Loss 1.8974 (1.9848)	Prec@(1,5) (92.1%, 99.7%)	
07/03 09:58:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][100/390]	Step 15349	lr 0.00375	Loss 0.2113 (0.2636)	Architecture Loss 1.8399 (2.0220)	Prec@(1,5) (92.5%, 99.6%)	
07/03 09:58:47午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][150/390]	Step 15399	lr 0.00375	Loss 0.3612 (0.2685)	Architecture Loss 2.3097 (2.0162)	Prec@(1,5) (92.3%, 99.5%)	
07/03 09:59:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][200/390]	Step 15449	lr 0.00375	Loss 0.5531 (0.2756)	Architecture Loss 1.9234 (2.0367)	Prec@(1,5) (92.2%, 99.5%)	
07/03 09:59:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][250/390]	Step 15499	lr 0.00375	Loss 0.2239 (0.2793)	Architecture Loss 1.9597 (2.0404)	Prec@(1,5) (92.2%, 99.4%)	
07/03 09:59:34午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][300/390]	Step 15549	lr 0.00375	Loss 0.1901 (0.2816)	Architecture Loss 2.1560 (2.0483)	Prec@(1,5) (92.1%, 99.4%)	
07/03 09:59:49午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][350/390]	Step 15599	lr 0.00375	Loss 0.5258 (0.2857)	Architecture Loss 1.8982 (2.0532)	Prec@(1,5) (91.9%, 99.4%)	
07/03 10:00:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [39][390/390]	Step 15639	lr 0.00375	Loss 0.2499 (0.2854)	Architecture Loss 2.1236 (2.0473)	Prec@(1,5) (91.9%, 99.5%)	
07/03 10:00:02午後 searchStage_trainer.py:225 [INFO] Train: [ 39/49] Final Prec@1 91.8640%
07/03 10:00:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][50/391]	Step 15640	Loss 2.0106	Prec@(1,5) (55.0%, 83.0%)
07/03 10:00:07午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][100/391]	Step 15640	Loss 2.0472	Prec@(1,5) (54.9%, 82.5%)
07/03 10:00:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][150/391]	Step 15640	Loss 2.0174	Prec@(1,5) (55.4%, 82.7%)
07/03 10:00:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][200/391]	Step 15640	Loss 1.9974	Prec@(1,5) (55.6%, 83.0%)
07/03 10:00:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][250/391]	Step 15640	Loss 2.0020	Prec@(1,5) (55.4%, 83.1%)
07/03 10:00:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][300/391]	Step 15640	Loss 1.9932	Prec@(1,5) (55.5%, 83.1%)
07/03 10:00:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][350/391]	Step 15640	Loss 2.0001	Prec@(1,5) (55.3%, 83.1%)
07/03 10:00:21午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [39][390/391]	Step 15640	Loss 2.0007	Prec@(1,5) (55.3%, 83.0%)
07/03 10:00:21午後 searchStage_trainer.py:264 [INFO] Valid: [ 39/49] Final Prec@1 55.2640%
07/03 10:00:21午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 10:00:21午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.3160%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2468, 0.2607, 0.2475],
        [0.2439, 0.2476, 0.2598, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2327, 0.2363, 0.2897, 0.2414],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2502],
        [0.2499, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2432, 0.2471, 0.2604, 0.2494],
        [0.2460, 0.2486, 0.2566, 0.2489],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2477, 0.2568, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2490, 0.2501, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2489, 0.2549, 0.2494],
        [0.2468, 0.2491, 0.2545, 0.2496],
        [0.2496, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2506, 0.2594, 0.2454],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2499, 0.2567, 0.2468],
        [0.2477, 0.2494, 0.2530, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2570, 0.2471],
        [0.2485, 0.2472, 0.2557, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2515, 0.2542, 0.2714, 0.2229],
        [0.2468, 0.2474, 0.2500, 0.2559],
        [0.2477, 0.2487, 0.2494, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2508, 0.2532, 0.2466],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2530, 0.2587, 0.2367],
        [0.2477, 0.2484, 0.2528, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2528, 0.2579, 0.2381],
        [0.2478, 0.2492, 0.2514, 0.2516],
        [0.2470, 0.2477, 0.2497, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 10:00:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][50/390]	Step 15690	lr 0.00329	Loss 0.4142 (0.2583)	Architecture Loss 2.2835 (2.0298)	Prec@(1,5) (92.9%, 99.6%)	
07/03 10:00:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][100/390]	Step 15740	lr 0.00329	Loss 0.2405 (0.2510)	Architecture Loss 2.6165 (2.0245)	Prec@(1,5) (93.0%, 99.7%)	
07/03 10:01:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][150/390]	Step 15790	lr 0.00329	Loss 0.2767 (0.2467)	Architecture Loss 1.3443 (2.0111)	Prec@(1,5) (93.2%, 99.7%)	
07/03 10:01:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][200/390]	Step 15840	lr 0.00329	Loss 0.3129 (0.2485)	Architecture Loss 1.4720 (2.0280)	Prec@(1,5) (93.0%, 99.7%)	
07/03 10:01:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][250/390]	Step 15890	lr 0.00329	Loss 0.1851 (0.2480)	Architecture Loss 1.7941 (2.0304)	Prec@(1,5) (93.0%, 99.7%)	
07/03 10:01:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][300/390]	Step 15940	lr 0.00329	Loss 0.1453 (0.2513)	Architecture Loss 2.5735 (2.0437)	Prec@(1,5) (92.9%, 99.7%)	
07/03 10:02:09午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][350/390]	Step 15990	lr 0.00329	Loss 0.2914 (0.2539)	Architecture Loss 2.1041 (2.0529)	Prec@(1,5) (92.8%, 99.7%)	
07/03 10:02:21午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [40][390/390]	Step 16030	lr 0.00329	Loss 0.3954 (0.2551)	Architecture Loss 2.1430 (2.0529)	Prec@(1,5) (92.8%, 99.7%)	
07/03 10:02:21午後 searchStage_trainer.py:225 [INFO] Train: [ 40/49] Final Prec@1 92.7720%
07/03 10:02:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][50/391]	Step 16031	Loss 1.9966	Prec@(1,5) (55.9%, 83.6%)
07/03 10:02:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][100/391]	Step 16031	Loss 2.0278	Prec@(1,5) (55.3%, 83.5%)
07/03 10:02:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][150/391]	Step 16031	Loss 2.0389	Prec@(1,5) (55.2%, 82.8%)
07/03 10:02:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][200/391]	Step 16031	Loss 2.0432	Prec@(1,5) (55.5%, 82.7%)
07/03 10:02:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][250/391]	Step 16031	Loss 2.0371	Prec@(1,5) (55.5%, 82.8%)
07/03 10:02:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][300/391]	Step 16031	Loss 2.0193	Prec@(1,5) (55.6%, 82.8%)
07/03 10:02:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][350/391]	Step 16031	Loss 2.0186	Prec@(1,5) (55.8%, 82.8%)
07/03 10:02:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [40][390/391]	Step 16031	Loss 2.0183	Prec@(1,5) (55.8%, 82.8%)
07/03 10:02:39午後 searchStage_trainer.py:264 [INFO] Valid: [ 40/49] Final Prec@1 55.7480%
07/03 10:02:39午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 10:02:40午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.7480%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2468, 0.2607, 0.2475],
        [0.2439, 0.2476, 0.2598, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2325, 0.2361, 0.2900, 0.2414],
        [0.2482, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2502],
        [0.2499, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2431, 0.2470, 0.2605, 0.2494],
        [0.2460, 0.2486, 0.2566, 0.2489],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2477, 0.2568, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2490, 0.2502, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2489, 0.2549, 0.2494],
        [0.2468, 0.2491, 0.2545, 0.2496],
        [0.2496, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2445, 0.2506, 0.2595, 0.2455],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2499, 0.2567, 0.2468],
        [0.2477, 0.2494, 0.2530, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2570, 0.2471],
        [0.2485, 0.2472, 0.2557, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2530]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2544, 0.2716, 0.2226],
        [0.2468, 0.2474, 0.2500, 0.2559],
        [0.2477, 0.2487, 0.2494, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2508, 0.2532, 0.2466],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2531, 0.2587, 0.2366],
        [0.2477, 0.2484, 0.2528, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2565]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2529, 0.2580, 0.2380],
        [0.2478, 0.2492, 0.2514, 0.2516],
        [0.2470, 0.2477, 0.2497, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 10:02:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][50/390]	Step 16081	lr 0.00287	Loss 0.1584 (0.2204)	Architecture Loss 1.9619 (1.9490)	Prec@(1,5) (94.0%, 99.8%)	
07/03 10:03:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][100/390]	Step 16131	lr 0.00287	Loss 0.2292 (0.2163)	Architecture Loss 1.8821 (2.0112)	Prec@(1,5) (94.3%, 99.8%)	
07/03 10:03:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][150/390]	Step 16181	lr 0.00287	Loss 0.1961 (0.2192)	Architecture Loss 2.2360 (2.0341)	Prec@(1,5) (94.1%, 99.8%)	
07/03 10:03:42午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][200/390]	Step 16231	lr 0.00287	Loss 0.1321 (0.2234)	Architecture Loss 3.2349 (2.0279)	Prec@(1,5) (94.0%, 99.7%)	
07/03 10:03:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][250/390]	Step 16281	lr 0.00287	Loss 0.2860 (0.2240)	Architecture Loss 2.1736 (2.0534)	Prec@(1,5) (93.9%, 99.7%)	
07/03 10:04:13午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][300/390]	Step 16331	lr 0.00287	Loss 0.2310 (0.2237)	Architecture Loss 2.4092 (2.0696)	Prec@(1,5) (93.9%, 99.7%)	
07/03 10:04:29午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][350/390]	Step 16381	lr 0.00287	Loss 0.1795 (0.2250)	Architecture Loss 2.3443 (2.0661)	Prec@(1,5) (93.8%, 99.7%)	
07/03 10:04:41午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [41][390/390]	Step 16421	lr 0.00287	Loss 0.2587 (0.2253)	Architecture Loss 2.2153 (2.0693)	Prec@(1,5) (93.8%, 99.7%)	
07/03 10:04:42午後 searchStage_trainer.py:225 [INFO] Train: [ 41/49] Final Prec@1 93.8280%
07/03 10:04:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][50/391]	Step 16422	Loss 2.0751	Prec@(1,5) (55.3%, 82.9%)
07/03 10:04:47午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][100/391]	Step 16422	Loss 2.0342	Prec@(1,5) (56.5%, 83.1%)
07/03 10:04:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][150/391]	Step 16422	Loss 2.0382	Prec@(1,5) (56.1%, 82.9%)
07/03 10:04:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][200/391]	Step 16422	Loss 2.0248	Prec@(1,5) (56.0%, 82.9%)
07/03 10:04:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][250/391]	Step 16422	Loss 1.9978	Prec@(1,5) (56.4%, 83.2%)
07/03 10:04:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][300/391]	Step 16422	Loss 2.0011	Prec@(1,5) (56.2%, 83.1%)
07/03 10:04:58午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][350/391]	Step 16422	Loss 2.0213	Prec@(1,5) (55.9%, 82.9%)
07/03 10:05:00午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [41][390/391]	Step 16422	Loss 2.0269	Prec@(1,5) (55.8%, 82.8%)
07/03 10:05:00午後 searchStage_trainer.py:264 [INFO] Valid: [ 41/49] Final Prec@1 55.7720%
07/03 10:05:00午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 10:05:01午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.7720%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2468, 0.2608, 0.2475],
        [0.2439, 0.2476, 0.2598, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2323, 0.2360, 0.2904, 0.2413],
        [0.2482, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2502],
        [0.2499, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2431, 0.2470, 0.2605, 0.2494],
        [0.2459, 0.2486, 0.2566, 0.2489],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2476, 0.2569, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2490, 0.2501, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2488, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2489, 0.2550, 0.2494],
        [0.2468, 0.2491, 0.2545, 0.2496],
        [0.2496, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2444, 0.2505, 0.2596, 0.2455],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2499, 0.2568, 0.2468],
        [0.2477, 0.2494, 0.2530, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2570, 0.2471],
        [0.2485, 0.2472, 0.2557, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2514, 0.2545, 0.2718, 0.2223],
        [0.2468, 0.2473, 0.2500, 0.2559],
        [0.2477, 0.2487, 0.2494, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2508, 0.2532, 0.2466],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2531, 0.2588, 0.2365],
        [0.2477, 0.2484, 0.2528, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2566]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2529, 0.2580, 0.2379],
        [0.2478, 0.2492, 0.2514, 0.2516],
        [0.2470, 0.2477, 0.2497, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 10:05:17午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][50/390]	Step 16472	lr 0.00248	Loss 0.1216 (0.1772)	Architecture Loss 2.5762 (2.1735)	Prec@(1,5) (95.6%, 99.8%)	
07/03 10:05:33午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][100/390]	Step 16522	lr 0.00248	Loss 0.2444 (0.1926)	Architecture Loss 2.2430 (2.1138)	Prec@(1,5) (95.0%, 99.8%)	
07/03 10:05:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][150/390]	Step 16572	lr 0.00248	Loss 0.2351 (0.1996)	Architecture Loss 1.7608 (2.0943)	Prec@(1,5) (94.8%, 99.8%)	
07/03 10:06:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][200/390]	Step 16622	lr 0.00248	Loss 0.2647 (0.2002)	Architecture Loss 2.0206 (2.1125)	Prec@(1,5) (94.7%, 99.8%)	
07/03 10:06:19午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][250/390]	Step 16672	lr 0.00248	Loss 0.1876 (0.1978)	Architecture Loss 2.5327 (2.1043)	Prec@(1,5) (94.8%, 99.8%)	
07/03 10:06:34午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][300/390]	Step 16722	lr 0.00248	Loss 0.1383 (0.1978)	Architecture Loss 1.6271 (2.1045)	Prec@(1,5) (94.7%, 99.8%)	
07/03 10:06:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][350/390]	Step 16772	lr 0.00248	Loss 0.1744 (0.2002)	Architecture Loss 2.1308 (2.0988)	Prec@(1,5) (94.6%, 99.8%)	
07/03 10:07:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [42][390/390]	Step 16812	lr 0.00248	Loss 0.2233 (0.1998)	Architecture Loss 1.8555 (2.1000)	Prec@(1,5) (94.7%, 99.8%)	
07/03 10:07:03午後 searchStage_trainer.py:225 [INFO] Train: [ 42/49] Final Prec@1 94.6480%
07/03 10:07:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][50/391]	Step 16813	Loss 2.0944	Prec@(1,5) (54.9%, 82.3%)
07/03 10:07:07午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][100/391]	Step 16813	Loss 2.0743	Prec@(1,5) (55.8%, 82.8%)
07/03 10:07:10午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][150/391]	Step 16813	Loss 2.0311	Prec@(1,5) (56.0%, 83.1%)
07/03 10:07:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][200/391]	Step 16813	Loss 2.0366	Prec@(1,5) (55.9%, 83.3%)
07/03 10:07:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][250/391]	Step 16813	Loss 2.0517	Prec@(1,5) (55.6%, 83.1%)
07/03 10:07:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][300/391]	Step 16813	Loss 2.0367	Prec@(1,5) (55.7%, 83.2%)
07/03 10:07:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][350/391]	Step 16813	Loss 2.0375	Prec@(1,5) (56.0%, 83.0%)
07/03 10:07:21午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [42][390/391]	Step 16813	Loss 2.0416	Prec@(1,5) (55.8%, 83.0%)
07/03 10:07:21午後 searchStage_trainer.py:264 [INFO] Valid: [ 42/49] Final Prec@1 55.8280%
07/03 10:07:21午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 10:07:21午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.8280%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2468, 0.2608, 0.2475],
        [0.2439, 0.2476, 0.2599, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2321, 0.2358, 0.2907, 0.2413],
        [0.2482, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2502],
        [0.2499, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2430, 0.2470, 0.2606, 0.2494],
        [0.2459, 0.2485, 0.2567, 0.2489],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2476, 0.2569, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2490, 0.2502, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2489, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2489, 0.2550, 0.2494],
        [0.2468, 0.2491, 0.2545, 0.2496],
        [0.2496, 0.2495, 0.2502, 0.2508]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2443, 0.2505, 0.2597, 0.2456],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2486, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2515, 0.2494],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2499, 0.2568, 0.2468],
        [0.2477, 0.2494, 0.2530, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2571, 0.2471],
        [0.2485, 0.2472, 0.2558, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2546, 0.2720, 0.2221],
        [0.2467, 0.2473, 0.2500, 0.2559],
        [0.2477, 0.2487, 0.2494, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2494, 0.2508, 0.2532, 0.2466],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2493, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2532, 0.2588, 0.2364],
        [0.2477, 0.2484, 0.2528, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2566]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2530, 0.2580, 0.2378],
        [0.2478, 0.2492, 0.2514, 0.2516],
        [0.2470, 0.2477, 0.2497, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 10:07:38午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][50/390]	Step 16863	lr 0.00214	Loss 0.2003 (0.1743)	Architecture Loss 2.4897 (2.0350)	Prec@(1,5) (95.7%, 99.9%)	
07/03 10:07:54午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][100/390]	Step 16913	lr 0.00214	Loss 0.1596 (0.1770)	Architecture Loss 2.5689 (2.0684)	Prec@(1,5) (95.5%, 99.8%)	
07/03 10:08:10午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][150/390]	Step 16963	lr 0.00214	Loss 0.1221 (0.1758)	Architecture Loss 1.8910 (2.0657)	Prec@(1,5) (95.5%, 99.7%)	
07/03 10:08:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][200/390]	Step 17013	lr 0.00214	Loss 0.1211 (0.1745)	Architecture Loss 2.0973 (2.0756)	Prec@(1,5) (95.6%, 99.8%)	
07/03 10:08:42午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][250/390]	Step 17063	lr 0.00214	Loss 0.1840 (0.1755)	Architecture Loss 2.3333 (2.0695)	Prec@(1,5) (95.6%, 99.8%)	
07/03 10:08:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][300/390]	Step 17113	lr 0.00214	Loss 0.1962 (0.1737)	Architecture Loss 2.1899 (2.0768)	Prec@(1,5) (95.7%, 99.8%)	
07/03 10:09:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][350/390]	Step 17163	lr 0.00214	Loss 0.2037 (0.1783)	Architecture Loss 1.9874 (2.0827)	Prec@(1,5) (95.5%, 99.8%)	
07/03 10:09:27午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [43][390/390]	Step 17203	lr 0.00214	Loss 0.1750 (0.1810)	Architecture Loss 2.1605 (2.0875)	Prec@(1,5) (95.4%, 99.8%)	
07/03 10:09:27午後 searchStage_trainer.py:225 [INFO] Train: [ 43/49] Final Prec@1 95.4440%
07/03 10:09:30午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][50/391]	Step 17204	Loss 1.9556	Prec@(1,5) (58.6%, 84.0%)
07/03 10:09:32午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][100/391]	Step 17204	Loss 1.9438	Prec@(1,5) (57.8%, 83.8%)
07/03 10:09:34午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][150/391]	Step 17204	Loss 2.0005	Prec@(1,5) (56.5%, 83.0%)
07/03 10:09:37午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][200/391]	Step 17204	Loss 2.0106	Prec@(1,5) (56.5%, 83.0%)
07/03 10:09:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][250/391]	Step 17204	Loss 2.0285	Prec@(1,5) (56.1%, 82.8%)
07/03 10:09:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][300/391]	Step 17204	Loss 2.0345	Prec@(1,5) (56.0%, 82.8%)
07/03 10:09:44午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][350/391]	Step 17204	Loss 2.0307	Prec@(1,5) (56.1%, 83.0%)
07/03 10:09:45午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [43][390/391]	Step 17204	Loss 2.0372	Prec@(1,5) (56.0%, 82.9%)
07/03 10:09:45午後 searchStage_trainer.py:264 [INFO] Valid: [ 43/49] Final Prec@1 55.9560%
07/03 10:09:45午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 10:09:46午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.9560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2467, 0.2608, 0.2475],
        [0.2439, 0.2476, 0.2599, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2319, 0.2357, 0.2911, 0.2414],
        [0.2482, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2502],
        [0.2499, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2430, 0.2469, 0.2606, 0.2494],
        [0.2459, 0.2485, 0.2567, 0.2488],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2476, 0.2570, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2490, 0.2502, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2489, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2489, 0.2550, 0.2494],
        [0.2468, 0.2491, 0.2545, 0.2496],
        [0.2496, 0.2495, 0.2502, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2504, 0.2597, 0.2456],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2487, 0.2493, 0.2509, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2515, 0.2495],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2499, 0.2568, 0.2468],
        [0.2477, 0.2494, 0.2530, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2571, 0.2471],
        [0.2485, 0.2472, 0.2558, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2547, 0.2722, 0.2219],
        [0.2467, 0.2473, 0.2500, 0.2559],
        [0.2477, 0.2487, 0.2494, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2508, 0.2533, 0.2466],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2494, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2516, 0.2532, 0.2588, 0.2363],
        [0.2477, 0.2484, 0.2528, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2566]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2530, 0.2580, 0.2377],
        [0.2478, 0.2492, 0.2514, 0.2516],
        [0.2470, 0.2477, 0.2497, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 10:10:03午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][50/390]	Step 17254	lr 0.00184	Loss 0.1166 (0.1557)	Architecture Loss 1.8100 (2.0080)	Prec@(1,5) (96.2%, 99.9%)	
07/03 10:10:19午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][100/390]	Step 17304	lr 0.00184	Loss 0.1342 (0.1576)	Architecture Loss 2.2255 (2.0759)	Prec@(1,5) (96.2%, 99.9%)	
07/03 10:10:35午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][150/390]	Step 17354	lr 0.00184	Loss 0.1092 (0.1638)	Architecture Loss 2.0556 (2.0804)	Prec@(1,5) (95.9%, 99.9%)	
07/03 10:10:51午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][200/390]	Step 17404	lr 0.00184	Loss 0.2415 (0.1624)	Architecture Loss 1.9288 (2.0866)	Prec@(1,5) (95.9%, 99.9%)	
07/03 10:11:07午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][250/390]	Step 17454	lr 0.00184	Loss 0.2126 (0.1614)	Architecture Loss 1.8247 (2.0798)	Prec@(1,5) (96.0%, 99.9%)	
07/03 10:11:23午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][300/390]	Step 17504	lr 0.00184	Loss 0.2704 (0.1630)	Architecture Loss 1.9352 (2.0839)	Prec@(1,5) (95.9%, 99.9%)	
07/03 10:11:39午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][350/390]	Step 17554	lr 0.00184	Loss 0.1228 (0.1634)	Architecture Loss 2.3821 (2.0878)	Prec@(1,5) (95.9%, 99.9%)	
07/03 10:11:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [44][390/390]	Step 17594	lr 0.00184	Loss 0.1786 (0.1647)	Architecture Loss 2.5058 (2.0858)	Prec@(1,5) (95.9%, 99.9%)	
07/03 10:11:52午後 searchStage_trainer.py:225 [INFO] Train: [ 44/49] Final Prec@1 95.8520%
07/03 10:11:55午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][50/391]	Step 17595	Loss 1.9874	Prec@(1,5) (56.8%, 83.7%)
07/03 10:11:57午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][100/391]	Step 17595	Loss 2.0412	Prec@(1,5) (55.7%, 83.2%)
07/03 10:12:00午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][150/391]	Step 17595	Loss 2.0355	Prec@(1,5) (55.7%, 83.3%)
07/03 10:12:02午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][200/391]	Step 17595	Loss 2.0638	Prec@(1,5) (55.6%, 83.0%)
07/03 10:12:04午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][250/391]	Step 17595	Loss 2.0557	Prec@(1,5) (55.9%, 83.1%)
07/03 10:12:06午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][300/391]	Step 17595	Loss 2.0491	Prec@(1,5) (55.8%, 83.1%)
07/03 10:12:09午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][350/391]	Step 17595	Loss 2.0600	Prec@(1,5) (55.7%, 82.9%)
07/03 10:12:11午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [44][390/391]	Step 17595	Loss 2.0589	Prec@(1,5) (55.7%, 82.9%)
07/03 10:12:11午後 searchStage_trainer.py:264 [INFO] Valid: [ 44/49] Final Prec@1 55.6800%
07/03 10:12:11午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 10:12:11午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 55.9560%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2450, 0.2467, 0.2608, 0.2475],
        [0.2438, 0.2476, 0.2599, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2317, 0.2355, 0.2914, 0.2414],
        [0.2482, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2502],
        [0.2499, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2429, 0.2469, 0.2607, 0.2495],
        [0.2459, 0.2485, 0.2567, 0.2488],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2476, 0.2570, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2490, 0.2502, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2489, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2489, 0.2550, 0.2494],
        [0.2468, 0.2491, 0.2545, 0.2496],
        [0.2496, 0.2495, 0.2502, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2442, 0.2504, 0.2598, 0.2457],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2487, 0.2493, 0.2508, 0.2512]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2515, 0.2495],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2465, 0.2499, 0.2569, 0.2468],
        [0.2477, 0.2494, 0.2530, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2492, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2571, 0.2471],
        [0.2485, 0.2472, 0.2558, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2548, 0.2724, 0.2216],
        [0.2467, 0.2473, 0.2500, 0.2559],
        [0.2477, 0.2487, 0.2493, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2508, 0.2533, 0.2466],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2494, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2533, 0.2588, 0.2362],
        [0.2476, 0.2484, 0.2528, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2566]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2512, 0.2531, 0.2581, 0.2376],
        [0.2478, 0.2492, 0.2514, 0.2516],
        [0.2470, 0.2477, 0.2497, 0.2556]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 10:12:28午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][50/390]	Step 17645	lr 0.00159	Loss 0.1753 (0.1458)	Architecture Loss 2.2960 (2.0919)	Prec@(1,5) (96.1%, 99.9%)	
07/03 10:12:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][100/390]	Step 17695	lr 0.00159	Loss 0.2742 (0.1426)	Architecture Loss 1.5761 (2.1296)	Prec@(1,5) (96.4%, 99.9%)	
07/03 10:12:59午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][150/390]	Step 17745	lr 0.00159	Loss 0.1800 (0.1437)	Architecture Loss 1.6621 (2.0713)	Prec@(1,5) (96.4%, 99.9%)	
07/03 10:13:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][200/390]	Step 17795	lr 0.00159	Loss 0.1819 (0.1474)	Architecture Loss 2.6191 (2.0737)	Prec@(1,5) (96.3%, 99.9%)	
07/03 10:13:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][250/390]	Step 17845	lr 0.00159	Loss 0.1616 (0.1478)	Architecture Loss 2.5882 (2.0855)	Prec@(1,5) (96.3%, 99.9%)	
07/03 10:13:48午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][300/390]	Step 17895	lr 0.00159	Loss 0.2128 (0.1505)	Architecture Loss 1.9676 (2.0951)	Prec@(1,5) (96.2%, 99.9%)	
07/03 10:14:04午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][350/390]	Step 17945	lr 0.00159	Loss 0.1876 (0.1520)	Architecture Loss 1.8768 (2.0903)	Prec@(1,5) (96.1%, 99.9%)	
07/03 10:14:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [45][390/390]	Step 17985	lr 0.00159	Loss 0.0941 (0.1525)	Architecture Loss 2.6085 (2.0986)	Prec@(1,5) (96.1%, 99.9%)	
07/03 10:14:17午後 searchStage_trainer.py:225 [INFO] Train: [ 45/49] Final Prec@1 96.1120%
07/03 10:14:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][50/391]	Step 17986	Loss 1.9892	Prec@(1,5) (57.9%, 83.7%)
07/03 10:14:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][100/391]	Step 17986	Loss 1.9813	Prec@(1,5) (57.8%, 83.4%)
07/03 10:14:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][150/391]	Step 17986	Loss 2.0379	Prec@(1,5) (56.6%, 83.1%)
07/03 10:14:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][200/391]	Step 17986	Loss 2.0592	Prec@(1,5) (56.1%, 82.9%)
07/03 10:14:29午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][250/391]	Step 17986	Loss 2.0571	Prec@(1,5) (56.4%, 82.9%)
07/03 10:14:31午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][300/391]	Step 17986	Loss 2.0431	Prec@(1,5) (56.7%, 83.1%)
07/03 10:14:33午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][350/391]	Step 17986	Loss 2.0434	Prec@(1,5) (56.5%, 83.2%)
07/03 10:14:35午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [45][390/391]	Step 17986	Loss 2.0513	Prec@(1,5) (56.3%, 83.1%)
07/03 10:14:35午後 searchStage_trainer.py:264 [INFO] Valid: [ 45/49] Final Prec@1 56.3320%
07/03 10:14:35午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 10:14:36午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 56.3320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2467, 0.2609, 0.2475],
        [0.2438, 0.2476, 0.2599, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2316, 0.2354, 0.2917, 0.2414],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2489, 0.2497, 0.2511, 0.2502],
        [0.2499, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2429, 0.2469, 0.2608, 0.2495],
        [0.2459, 0.2485, 0.2567, 0.2488],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2475, 0.2571, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2490, 0.2502, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2503],
        [0.2489, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2489, 0.2550, 0.2494],
        [0.2468, 0.2491, 0.2546, 0.2496],
        [0.2496, 0.2495, 0.2502, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2503, 0.2598, 0.2457],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2487, 0.2493, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2491, 0.2500, 0.2515, 0.2495],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2498, 0.2569, 0.2468],
        [0.2477, 0.2494, 0.2530, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2493, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2503],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2571, 0.2471],
        [0.2484, 0.2472, 0.2558, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2549, 0.2726, 0.2214],
        [0.2467, 0.2473, 0.2500, 0.2559],
        [0.2477, 0.2487, 0.2493, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2508, 0.2533, 0.2466],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2494, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2534, 0.2588, 0.2361],
        [0.2476, 0.2484, 0.2529, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2566]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2531, 0.2581, 0.2375],
        [0.2478, 0.2492, 0.2514, 0.2515],
        [0.2470, 0.2477, 0.2497, 0.2557]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 10:14:53午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][50/390]	Step 18036	lr 0.00138	Loss 0.0969 (0.1305)	Architecture Loss 2.3254 (2.1434)	Prec@(1,5) (97.2%, 99.9%)	
07/03 10:15:09午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][100/390]	Step 18086	lr 0.00138	Loss 0.1022 (0.1285)	Architecture Loss 2.3055 (2.1466)	Prec@(1,5) (97.2%, 99.9%)	
07/03 10:15:25午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][150/390]	Step 18136	lr 0.00138	Loss 0.0917 (0.1329)	Architecture Loss 2.0745 (2.1345)	Prec@(1,5) (97.2%, 99.9%)	
07/03 10:15:42午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][200/390]	Step 18186	lr 0.00138	Loss 0.0793 (0.1355)	Architecture Loss 2.1015 (2.1316)	Prec@(1,5) (97.0%, 99.9%)	
07/03 10:15:58午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][250/390]	Step 18236	lr 0.00138	Loss 0.0999 (0.1346)	Architecture Loss 1.9192 (2.1123)	Prec@(1,5) (97.0%, 99.9%)	
07/03 10:16:14午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][300/390]	Step 18286	lr 0.00138	Loss 0.0707 (0.1367)	Architecture Loss 1.4715 (2.1059)	Prec@(1,5) (96.9%, 99.9%)	
07/03 10:16:30午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][350/390]	Step 18336	lr 0.00138	Loss 0.0902 (0.1370)	Architecture Loss 2.9217 (2.1093)	Prec@(1,5) (96.9%, 99.9%)	
07/03 10:16:43午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [46][390/390]	Step 18376	lr 0.00138	Loss 0.1386 (0.1386)	Architecture Loss 1.8582 (2.1192)	Prec@(1,5) (96.8%, 99.9%)	
07/03 10:16:44午後 searchStage_trainer.py:225 [INFO] Train: [ 46/49] Final Prec@1 96.7720%
07/03 10:16:46午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][50/391]	Step 18377	Loss 2.0340	Prec@(1,5) (57.3%, 83.6%)
07/03 10:16:49午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][100/391]	Step 18377	Loss 2.0490	Prec@(1,5) (56.3%, 83.4%)
07/03 10:16:51午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][150/391]	Step 18377	Loss 2.0355	Prec@(1,5) (56.4%, 83.3%)
07/03 10:16:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][200/391]	Step 18377	Loss 2.0678	Prec@(1,5) (55.9%, 83.0%)
07/03 10:16:56午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][250/391]	Step 18377	Loss 2.0648	Prec@(1,5) (56.2%, 83.1%)
07/03 10:16:59午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][300/391]	Step 18377	Loss 2.0630	Prec@(1,5) (56.3%, 83.0%)
07/03 10:17:01午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][350/391]	Step 18377	Loss 2.0660	Prec@(1,5) (56.2%, 83.0%)
07/03 10:17:03午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [46][390/391]	Step 18377	Loss 2.0671	Prec@(1,5) (56.1%, 83.1%)
07/03 10:17:03午後 searchStage_trainer.py:264 [INFO] Valid: [ 46/49] Final Prec@1 56.1360%
07/03 10:17:03午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 10:17:03午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 56.3320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2467, 0.2609, 0.2475],
        [0.2438, 0.2476, 0.2600, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2314, 0.2352, 0.2920, 0.2414],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2490, 0.2497, 0.2511, 0.2502],
        [0.2499, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2429, 0.2468, 0.2608, 0.2495],
        [0.2459, 0.2485, 0.2567, 0.2488],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2481, 0.2475, 0.2571, 0.2474],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2491, 0.2502, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2502],
        [0.2489, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2489, 0.2550, 0.2494],
        [0.2468, 0.2491, 0.2546, 0.2496],
        [0.2496, 0.2495, 0.2501, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2441, 0.2503, 0.2599, 0.2458],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2487, 0.2493, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2515, 0.2495],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2498, 0.2569, 0.2468],
        [0.2477, 0.2494, 0.2530, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2511, 0.2505],
        [0.2493, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2502],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2571, 0.2470],
        [0.2484, 0.2472, 0.2558, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2549, 0.2728, 0.2211],
        [0.2467, 0.2473, 0.2500, 0.2560],
        [0.2477, 0.2487, 0.2493, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2508, 0.2533, 0.2466],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2494, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2534, 0.2588, 0.2360],
        [0.2476, 0.2484, 0.2529, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2566]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2532, 0.2581, 0.2374],
        [0.2478, 0.2492, 0.2514, 0.2515],
        [0.2470, 0.2477, 0.2497, 0.2557]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 10:17:20午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][50/390]	Step 18427	lr 0.00121	Loss 0.1333 (0.1316)	Architecture Loss 2.0486 (2.1420)	Prec@(1,5) (97.1%, 99.9%)	
07/03 10:17:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][100/390]	Step 18477	lr 0.00121	Loss 0.1997 (0.1305)	Architecture Loss 1.4775 (2.1264)	Prec@(1,5) (97.1%, 99.9%)	
07/03 10:17:52午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][150/390]	Step 18527	lr 0.00121	Loss 0.0784 (0.1310)	Architecture Loss 1.7900 (2.1338)	Prec@(1,5) (97.1%, 99.9%)	
07/03 10:18:08午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][200/390]	Step 18577	lr 0.00121	Loss 0.1302 (0.1300)	Architecture Loss 1.8090 (2.1288)	Prec@(1,5) (97.1%, 99.9%)	
07/03 10:18:24午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][250/390]	Step 18627	lr 0.00121	Loss 0.0816 (0.1309)	Architecture Loss 1.7534 (2.1214)	Prec@(1,5) (97.0%, 99.9%)	
07/03 10:18:40午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][300/390]	Step 18677	lr 0.00121	Loss 0.1546 (0.1324)	Architecture Loss 2.5697 (2.1291)	Prec@(1,5) (97.0%, 99.9%)	
07/03 10:18:56午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][350/390]	Step 18727	lr 0.00121	Loss 0.1673 (0.1333)	Architecture Loss 1.7289 (2.1324)	Prec@(1,5) (97.0%, 99.9%)	
07/03 10:19:09午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [47][390/390]	Step 18767	lr 0.00121	Loss 0.1246 (0.1337)	Architecture Loss 1.8579 (2.1163)	Prec@(1,5) (96.9%, 99.9%)	
07/03 10:19:10午後 searchStage_trainer.py:225 [INFO] Train: [ 47/49] Final Prec@1 96.9320%
07/03 10:19:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][50/391]	Step 18768	Loss 1.9690	Prec@(1,5) (57.4%, 83.4%)
07/03 10:19:15午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][100/391]	Step 18768	Loss 2.0143	Prec@(1,5) (56.6%, 83.3%)
07/03 10:19:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][150/391]	Step 18768	Loss 2.0397	Prec@(1,5) (56.2%, 83.0%)
07/03 10:19:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][200/391]	Step 18768	Loss 2.0235	Prec@(1,5) (56.2%, 83.2%)
07/03 10:19:22午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][250/391]	Step 18768	Loss 2.0340	Prec@(1,5) (56.2%, 83.2%)
07/03 10:19:24午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][300/391]	Step 18768	Loss 2.0507	Prec@(1,5) (56.2%, 83.1%)
07/03 10:19:26午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][350/391]	Step 18768	Loss 2.0513	Prec@(1,5) (56.2%, 83.0%)
07/03 10:19:28午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [47][390/391]	Step 18768	Loss 2.0555	Prec@(1,5) (56.2%, 83.0%)
07/03 10:19:28午後 searchStage_trainer.py:264 [INFO] Valid: [ 47/49] Final Prec@1 56.2720%
07/03 10:19:28午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 10:19:29午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 56.3320%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2467, 0.2609, 0.2475],
        [0.2438, 0.2476, 0.2600, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2313, 0.2351, 0.2922, 0.2414],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2490, 0.2497, 0.2511, 0.2502],
        [0.2499, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2428, 0.2468, 0.2609, 0.2495],
        [0.2459, 0.2485, 0.2568, 0.2488],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2475, 0.2571, 0.2473],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2491, 0.2502, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2502],
        [0.2489, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2467, 0.2489, 0.2550, 0.2494],
        [0.2468, 0.2491, 0.2546, 0.2496],
        [0.2496, 0.2495, 0.2501, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2440, 0.2503, 0.2599, 0.2458],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2487, 0.2494, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2515, 0.2495],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2498, 0.2569, 0.2468],
        [0.2477, 0.2494, 0.2530, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2510, 0.2505],
        [0.2493, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2502],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2572, 0.2470],
        [0.2484, 0.2472, 0.2558, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2511, 0.2550, 0.2730, 0.2209],
        [0.2467, 0.2473, 0.2500, 0.2560],
        [0.2477, 0.2487, 0.2493, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2508, 0.2533, 0.2466],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2494, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2535, 0.2589, 0.2360],
        [0.2476, 0.2484, 0.2529, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2566]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2533, 0.2581, 0.2373],
        [0.2478, 0.2492, 0.2514, 0.2515],
        [0.2470, 0.2477, 0.2497, 0.2557]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 10:19:45午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][50/390]	Step 18818	lr 0.00109	Loss 0.2039 (0.1200)	Architecture Loss 1.8281 (1.9906)	Prec@(1,5) (97.4%, 99.8%)	
07/03 10:20:02午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][100/390]	Step 18868	lr 0.00109	Loss 0.0891 (0.1238)	Architecture Loss 2.3247 (2.0594)	Prec@(1,5) (97.4%, 99.8%)	
07/03 10:20:18午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][150/390]	Step 18918	lr 0.00109	Loss 0.1022 (0.1203)	Architecture Loss 2.1032 (2.0783)	Prec@(1,5) (97.5%, 99.9%)	
07/03 10:20:34午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][200/390]	Step 18968	lr 0.00109	Loss 0.1993 (0.1217)	Architecture Loss 1.9833 (2.1065)	Prec@(1,5) (97.5%, 99.9%)	
07/03 10:20:50午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][250/390]	Step 19018	lr 0.00109	Loss 0.1102 (0.1229)	Architecture Loss 2.3123 (2.1107)	Prec@(1,5) (97.4%, 99.9%)	
07/03 10:21:06午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][300/390]	Step 19068	lr 0.00109	Loss 0.2397 (0.1242)	Architecture Loss 2.2710 (2.1219)	Prec@(1,5) (97.3%, 99.9%)	
07/03 10:21:22午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][350/390]	Step 19118	lr 0.00109	Loss 0.0662 (0.1257)	Architecture Loss 1.7604 (2.1205)	Prec@(1,5) (97.2%, 99.9%)	
07/03 10:21:36午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [48][390/390]	Step 19158	lr 0.00109	Loss 0.1483 (0.1269)	Architecture Loss 1.9553 (2.1141)	Prec@(1,5) (97.2%, 99.9%)	
07/03 10:21:36午後 searchStage_trainer.py:225 [INFO] Train: [ 48/49] Final Prec@1 97.1560%
07/03 10:21:39午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][50/391]	Step 19159	Loss 1.9794	Prec@(1,5) (57.3%, 83.8%)
07/03 10:21:41午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][100/391]	Step 19159	Loss 2.0504	Prec@(1,5) (56.6%, 83.2%)
07/03 10:21:43午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][150/391]	Step 19159	Loss 2.0693	Prec@(1,5) (56.2%, 83.0%)
07/03 10:21:46午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][200/391]	Step 19159	Loss 2.0688	Prec@(1,5) (56.3%, 83.0%)
07/03 10:21:48午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][250/391]	Step 19159	Loss 2.0704	Prec@(1,5) (56.4%, 82.9%)
07/03 10:21:50午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][300/391]	Step 19159	Loss 2.0825	Prec@(1,5) (56.1%, 82.9%)
07/03 10:21:52午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][350/391]	Step 19159	Loss 2.0711	Prec@(1,5) (56.3%, 83.0%)
07/03 10:21:54午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [48][390/391]	Step 19159	Loss 2.0645	Prec@(1,5) (56.4%, 83.0%)
07/03 10:21:54午後 searchStage_trainer.py:264 [INFO] Valid: [ 48/49] Final Prec@1 56.3880%
07/03 10:21:54午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 10:21:55午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 56.3880%
####### ALPHA #######
# Alpha - DAG
tensor([[0.2490, 0.2505, 0.2507, 0.2498],
        [0.2490, 0.2492, 0.2526, 0.2492]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2449, 0.2467, 0.2609, 0.2475],
        [0.2438, 0.2475, 0.2600, 0.2487],
        [0.2493, 0.2512, 0.2497, 0.2497]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2311, 0.2350, 0.2924, 0.2414],
        [0.2483, 0.2496, 0.2506, 0.2515],
        [0.2484, 0.2495, 0.2502, 0.2519]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2502, 0.2507, 0.2503, 0.2488],
        [0.2490, 0.2497, 0.2511, 0.2502],
        [0.2499, 0.2502, 0.2503, 0.2496]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2428, 0.2468, 0.2609, 0.2495],
        [0.2459, 0.2485, 0.2568, 0.2488],
        [0.2474, 0.2503, 0.2512, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2480, 0.2475, 0.2571, 0.2473],
        [0.2490, 0.2502, 0.2499, 0.2509],
        [0.2491, 0.2502, 0.2503, 0.2505]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2482, 0.2495, 0.2521, 0.2502],
        [0.2489, 0.2490, 0.2523, 0.2499]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2466, 0.2489, 0.2550, 0.2494],
        [0.2468, 0.2491, 0.2546, 0.2496],
        [0.2496, 0.2495, 0.2501, 0.2507]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2439, 0.2502, 0.2600, 0.2459],
        [0.2486, 0.2493, 0.2504, 0.2516],
        [0.2487, 0.2494, 0.2508, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2490, 0.2500, 0.2515, 0.2495],
        [0.2489, 0.2498, 0.2514, 0.2499],
        [0.2491, 0.2495, 0.2505, 0.2509]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2464, 0.2498, 0.2569, 0.2468],
        [0.2477, 0.2494, 0.2530, 0.2500],
        [0.2486, 0.2492, 0.2502, 0.2520]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2487, 0.2500, 0.2538, 0.2475],
        [0.2490, 0.2495, 0.2510, 0.2505],
        [0.2493, 0.2497, 0.2499, 0.2511]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2486, 0.2525, 0.2502],
        [0.2483, 0.2490, 0.2524, 0.2504]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2486, 0.2473, 0.2572, 0.2470],
        [0.2484, 0.2472, 0.2558, 0.2486],
        [0.2482, 0.2495, 0.2493, 0.2529]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2510, 0.2551, 0.2732, 0.2207],
        [0.2467, 0.2473, 0.2500, 0.2560],
        [0.2477, 0.2487, 0.2493, 0.2543]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2493, 0.2508, 0.2533, 0.2466],
        [0.2488, 0.2487, 0.2517, 0.2508],
        [0.2484, 0.2494, 0.2508, 0.2515]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2517, 0.2535, 0.2589, 0.2359],
        [0.2476, 0.2484, 0.2529, 0.2511],
        [0.2459, 0.2478, 0.2498, 0.2566]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
tensor([[0.2513, 0.2533, 0.2581, 0.2372],
        [0.2478, 0.2492, 0.2514, 0.2515],
        [0.2470, 0.2477, 0.2497, 0.2557]], device='cuda:0',
       grad_fn=<SoftmaxBackward0>)
#####################
07/03 10:22:12午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][50/390]	Step 19209	lr 0.00102	Loss 0.0778 (0.1150)	Architecture Loss 2.0990 (2.1135)	Prec@(1,5) (97.3%, 99.9%)	
07/03 10:22:28午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][100/390]	Step 19259	lr 0.00102	Loss 0.1136 (0.1152)	Architecture Loss 2.5249 (2.1367)	Prec@(1,5) (97.4%, 99.9%)	
07/03 10:22:44午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][150/390]	Step 19309	lr 0.00102	Loss 0.2734 (0.1194)	Architecture Loss 1.7930 (2.1203)	Prec@(1,5) (97.3%, 99.9%)	
07/03 10:23:00午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][200/390]	Step 19359	lr 0.00102	Loss 0.0717 (0.1188)	Architecture Loss 1.9689 (2.1265)	Prec@(1,5) (97.4%, 99.9%)	
07/03 10:23:16午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][250/390]	Step 19409	lr 0.00102	Loss 0.1981 (0.1185)	Architecture Loss 1.9921 (2.1178)	Prec@(1,5) (97.4%, 99.9%)	
07/03 10:23:32午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][300/390]	Step 19459	lr 0.00102	Loss 0.1237 (0.1188)	Architecture Loss 1.4443 (2.1172)	Prec@(1,5) (97.3%, 99.9%)	
07/03 10:23:49午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][350/390]	Step 19509	lr 0.00102	Loss 0.1322 (0.1194)	Architecture Loss 2.1253 (2.1214)	Prec@(1,5) (97.3%, 99.9%)	
07/03 10:24:01午後 searchStage_trainer.py:214 [INFO] Train: Epoch: [49][390/390]	Step 19549	lr 0.00102	Loss 0.2757 (0.1211)	Architecture Loss 2.1042 (2.1141)	Prec@(1,5) (97.2%, 99.9%)	
07/03 10:24:02午後 searchStage_trainer.py:225 [INFO] Train: [ 49/49] Final Prec@1 97.2320%
07/03 10:24:05午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][50/391]	Step 19550	Loss 2.1259	Prec@(1,5) (56.6%, 82.9%)
07/03 10:24:07午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][100/391]	Step 19550	Loss 2.0912	Prec@(1,5) (56.5%, 82.9%)
07/03 10:24:09午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][150/391]	Step 19550	Loss 2.0683	Prec@(1,5) (56.6%, 83.0%)
07/03 10:24:12午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][200/391]	Step 19550	Loss 2.0828	Prec@(1,5) (56.6%, 82.8%)
07/03 10:24:14午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][250/391]	Step 19550	Loss 2.0732	Prec@(1,5) (56.5%, 83.0%)
07/03 10:24:17午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][300/391]	Step 19550	Loss 2.0658	Prec@(1,5) (56.5%, 83.2%)
07/03 10:24:19午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][350/391]	Step 19550	Loss 2.0575	Prec@(1,5) (56.8%, 83.2%)
07/03 10:24:21午後 searchStage_trainer.py:253 [INFO] Valid: Epoch: [49][390/391]	Step 19550	Loss 2.0674	Prec@(1,5) (56.7%, 83.2%)
07/03 10:24:21午後 searchStage_trainer.py:264 [INFO] Valid: [ 49/49] Final Prec@1 56.7080%
07/03 10:24:21午後 searchStage_main.py:60 [INFO] DAG = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
07/03 10:24:22午後 searchStage_main.py:82 [INFO] Until now, best Prec@1 = 56.7080%
07/03 10:24:22午後 searchStage_main.py:87 [INFO] Final best Prec@1 = 56.7080%
07/03 10:24:22午後 searchStage_main.py:88 [INFO] Final Best Genotype = Genotype2(DAG1=[[('skip_connect', 1), ('skip_connect', 0)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('avg_pool_3x3', 5)]], DAG1_concat=range(6, 8), DAG2=[[('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG2_concat=range(6, 8), DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('avg_pool_3x3', 2), ('skip_connect', 0)], [('skip_connect', 3), ('skip_connect', 2)], [('skip_connect', 4), ('skip_connect', 3)], [('skip_connect', 5), ('skip_connect', 4)], [('skip_connect', 6), ('skip_connect', 5)]], DAG3_concat=range(6, 8))
