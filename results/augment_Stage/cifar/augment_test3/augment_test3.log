05/06 08:03:36PM parser.py:28 [INFO] 
05/06 08:03:36PM parser.py:29 [INFO] Parameters:
05/06 08:03:36PM parser.py:31 [INFO] DAG=Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('skip_connect', 4)], [('max_pool_3x3', 3), ('skip_connect', 5)], [('max_pool_3x3', 4), ('skip_connect', 6)]], DAG1_concat=[6, 7], DAG2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('max_pool_3x3', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('avg_pool_3x3', 4), ('skip_connect', 6)]], DAG2_concat=[6, 7], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 2), ('skip_connect', 3)], [('max_pool_3x3', 3), ('skip_connect', 5)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)]], DAG3_concat=[6, 7])
05/06 08:03:36PM parser.py:31 [INFO] AMP_OPT_LEVEL=O0
05/06 08:03:36PM parser.py:31 [INFO] AMP_SYNC_BN=True
05/06 08:03:36PM parser.py:31 [INFO] AUX_WEIGHT=0.4
05/06 08:03:36PM parser.py:31 [INFO] BATCH_SIZE=128
05/06 08:03:36PM parser.py:31 [INFO] CUTOUT_LENGTH=16
05/06 08:03:36PM parser.py:31 [INFO] DATA_PATH=../data/
05/06 08:03:36PM parser.py:31 [INFO] DATASET=cifar10
05/06 08:03:36PM parser.py:31 [INFO] DIST=False
05/06 08:03:36PM parser.py:31 [INFO] DROP_PATH_PROB=0.2
05/06 08:03:36PM parser.py:31 [INFO] EPOCHS=2
05/06 08:03:36PM parser.py:31 [INFO] EXCLUDE_BIAS_AND_BN=True
05/06 08:03:36PM parser.py:31 [INFO] GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('skip_connect', 2)]], normal_concat=[2, 3, 4, 5], reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce_concat=[2, 3, 4, 5])
05/06 08:03:36PM parser.py:31 [INFO] GPUS=[0]
05/06 08:03:36PM parser.py:31 [INFO] GRAD_CLIP=5.0
05/06 08:03:36PM parser.py:31 [INFO] INIT_CHANNELS=36
05/06 08:03:36PM parser.py:31 [INFO] LAYERS=20
05/06 08:03:36PM parser.py:31 [INFO] LOCAL_RANK=0
05/06 08:03:36PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/06 08:03:36PM parser.py:31 [INFO] LR=0.025
05/06 08:03:36PM parser.py:31 [INFO] MOMENTUM=0.9
05/06 08:03:36PM parser.py:31 [INFO] NAME=augment_test3
05/06 08:03:36PM parser.py:31 [INFO] PATH=results/augment_Stage/cifar/augment_test3
05/06 08:03:36PM parser.py:31 [INFO] PRINT_FREQ=50
05/06 08:03:36PM parser.py:31 [INFO] RESUME_PATH=None
05/06 08:03:36PM parser.py:31 [INFO] SEED=0
05/06 08:03:36PM parser.py:31 [INFO] TRAIN_PORTION=1.0
05/06 08:03:36PM parser.py:31 [INFO] WEIGHT_DECAY=0.0003
05/06 08:03:36PM parser.py:31 [INFO] WORKERS=4
05/06 08:03:36PM parser.py:32 [INFO] 
05/06 08:03:36PM augmentStage_main.py:41 [INFO] Logger is set - training start
05/06 08:03:37PM augmentStage_main.py:131 [INFO] Epoch 0 LR 0.0125
05/06 08:03:39PM augmentStage_main.py:156 [INFO] Train: [  1/2] Step 000/390 Loss 3.291 Prec@(1,5) (6.2%, 59.4%)
05/06 08:03:53PM augmentStage_main.py:156 [INFO] Train: [  1/2] Step 050/390 Loss 3.192 Prec@(1,5) (14.2%, 60.8%)
05/06 08:04:07PM augmentStage_main.py:156 [INFO] Train: [  1/2] Step 100/390 Loss 3.120 Prec@(1,5) (16.1%, 64.8%)
05/06 08:04:22PM augmentStage_main.py:156 [INFO] Train: [  1/2] Step 150/390 Loss 3.009 Prec@(1,5) (19.3%, 69.5%)
05/06 08:04:36PM augmentStage_main.py:156 [INFO] Train: [  1/2] Step 200/390 Loss 2.908 Prec@(1,5) (22.0%, 73.4%)
05/06 08:04:50PM augmentStage_main.py:156 [INFO] Train: [  1/2] Step 250/390 Loss 2.814 Prec@(1,5) (24.5%, 76.2%)
05/06 08:05:05PM augmentStage_main.py:156 [INFO] Train: [  1/2] Step 300/390 Loss 2.736 Prec@(1,5) (26.6%, 78.2%)
05/06 08:05:19PM augmentStage_main.py:156 [INFO] Train: [  1/2] Step 350/390 Loss 2.664 Prec@(1,5) (28.8%, 79.9%)
05/06 08:05:32PM augmentStage_main.py:156 [INFO] Train: [  1/2] Step 390/390 Loss 2.614 Prec@(1,5) (30.2%, 81.0%)
05/06 08:05:32PM augmentStage_main.py:167 [INFO] Train: [  1/2] Final Prec@1 30.1620%
05/06 08:05:32PM augmentStage_main.py:193 [INFO] Valid: [  1/2] Step 000/078 Loss 1.607 Prec@(1,5) (45.3%, 89.8%)
05/06 08:05:37PM augmentStage_main.py:193 [INFO] Valid: [  1/2] Step 050/078 Loss 1.617 Prec@(1,5) (42.8%, 92.0%)
05/06 08:05:39PM augmentStage_main.py:193 [INFO] Valid: [  1/2] Step 078/078 Loss 1.624 Prec@(1,5) (42.6%, 92.2%)
05/06 08:05:39PM augmentStage_main.py:203 [INFO] Valid: [  1/2] Final Prec@1 42.6100%
05/06 08:05:40PM augmentStage_main.py:116 [INFO] until now best Prec@1 = 42.6100%
05/06 08:05:40午後 augmentStage_main.py:131 [INFO] Epoch 1 LR 0.0
05/06 08:05:40午後 augmentStage_main.py:156 [INFO] Train: [  2/2] Step 000/390 Loss 2.583 Prec@(1,5) (29.7%, 86.7%)
05/06 08:05:56午後 augmentStage_main.py:156 [INFO] Train: [  2/2] Step 050/390 Loss 2.660 Prec@(1,5) (31.4%, 83.5%)
05/06 08:06:12午後 augmentStage_main.py:156 [INFO] Train: [  2/2] Step 100/390 Loss 2.659 Prec@(1,5) (31.4%, 83.7%)
05/06 08:06:27午後 augmentStage_main.py:156 [INFO] Train: [  2/2] Step 150/390 Loss 2.664 Prec@(1,5) (31.4%, 83.7%)
05/06 08:06:43午後 augmentStage_main.py:156 [INFO] Train: [  2/2] Step 200/390 Loss 2.661 Prec@(1,5) (31.5%, 83.6%)
05/06 08:06:58午後 augmentStage_main.py:156 [INFO] Train: [  2/2] Step 250/390 Loss 2.662 Prec@(1,5) (31.4%, 83.6%)
05/06 08:07:14午後 augmentStage_main.py:156 [INFO] Train: [  2/2] Step 300/390 Loss 2.664 Prec@(1,5) (31.3%, 83.5%)
05/06 08:07:30午後 augmentStage_main.py:156 [INFO] Train: [  2/2] Step 350/390 Loss 2.665 Prec@(1,5) (31.4%, 83.5%)
05/06 08:07:42午後 augmentStage_main.py:156 [INFO] Train: [  2/2] Step 390/390 Loss 2.666 Prec@(1,5) (31.4%, 83.4%)
05/06 08:07:42午後 augmentStage_main.py:167 [INFO] Train: [  2/2] Final Prec@1 31.3920%
05/06 08:07:43午後 augmentStage_main.py:193 [INFO] Valid: [  2/2] Step 000/078 Loss 1.441 Prec@(1,5) (48.4%, 93.7%)
05/06 08:07:47午後 augmentStage_main.py:193 [INFO] Valid: [  2/2] Step 050/078 Loss 1.417 Prec@(1,5) (45.8%, 93.0%)
05/06 08:07:50午後 augmentStage_main.py:193 [INFO] Valid: [  2/2] Step 078/078 Loss 1.416 Prec@(1,5) (45.9%, 93.3%)
05/06 08:07:50午後 augmentStage_main.py:203 [INFO] Valid: [  2/2] Final Prec@1 45.9000%
05/06 08:07:50午後 augmentStage_main.py:116 [INFO] until now best Prec@1 = 45.9000%
05/06 08:07:50午後 augmentStage_main.py:121 [INFO] Final best Prec@1 = 45.9000%
05/06 08:09:56PM parser.py:28 [INFO] 
05/06 08:09:56PM parser.py:29 [INFO] Parameters:
05/06 08:09:56PM parser.py:31 [INFO] DAG=Genotype2(DAG1=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('max_pool_3x3', 2)], [('max_pool_3x3', 2), ('skip_connect', 4)], [('max_pool_3x3', 3), ('skip_connect', 5)], [('max_pool_3x3', 4), ('skip_connect', 6)]], DAG1_concat=[6, 7], DAG2=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('max_pool_3x3', 2), ('skip_connect', 3)], [('skip_connect', 3), ('max_pool_3x3', 5)], [('avg_pool_3x3', 4), ('skip_connect', 6)]], DAG2_concat=[6, 7], DAG3=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 2)], [('skip_connect', 2), ('skip_connect', 3)], [('max_pool_3x3', 2), ('skip_connect', 3)], [('max_pool_3x3', 3), ('skip_connect', 5)], [('max_pool_3x3', 4), ('max_pool_3x3', 5)]], DAG3_concat=[6, 7])
05/06 08:09:56PM parser.py:31 [INFO] AMP_OPT_LEVEL=O0
05/06 08:09:56PM parser.py:31 [INFO] AMP_SYNC_BN=True
05/06 08:09:56PM parser.py:31 [INFO] AUX_WEIGHT=0.4
05/06 08:09:56PM parser.py:31 [INFO] BATCH_SIZE=128
05/06 08:09:56PM parser.py:31 [INFO] CUTOUT_LENGTH=16
05/06 08:09:56PM parser.py:31 [INFO] DATA_PATH=../data/
05/06 08:09:56PM parser.py:31 [INFO] DATASET=cifar10
05/06 08:09:56PM parser.py:31 [INFO] DIST=False
05/06 08:09:56PM parser.py:31 [INFO] DROP_PATH_PROB=0.2
05/06 08:09:56PM parser.py:31 [INFO] EPOCHS=2
05/06 08:09:56PM parser.py:31 [INFO] EXCLUDE_BIAS_AND_BN=True
05/06 08:09:56PM parser.py:31 [INFO] GENOTYPE=Genotype(normal=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('skip_connect', 0), ('sep_conv_3x3', 1)], [('sep_conv_3x3', 0), ('skip_connect', 2)]], normal_concat=[2, 3, 4, 5], reduce=[[('max_pool_3x3', 0), ('max_pool_3x3', 1)], [('skip_connect', 2), ('max_pool_3x3', 0)], [('max_pool_3x3', 0), ('skip_connect', 2)], [('skip_connect', 2), ('avg_pool_3x3', 0)]], reduce_concat=[2, 3, 4, 5])
05/06 08:09:56PM parser.py:31 [INFO] GPUS=[0]
05/06 08:09:56PM parser.py:31 [INFO] GRAD_CLIP=5.0
05/06 08:09:56PM parser.py:31 [INFO] INIT_CHANNELS=36
05/06 08:09:56PM parser.py:31 [INFO] LAYERS=20
05/06 08:09:56PM parser.py:31 [INFO] LOCAL_RANK=0
05/06 08:09:56PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
05/06 08:09:56PM parser.py:31 [INFO] LR=0.025
05/06 08:09:56PM parser.py:31 [INFO] MOMENTUM=0.9
05/06 08:09:56PM parser.py:31 [INFO] NAME=augment_test3
05/06 08:09:56PM parser.py:31 [INFO] PATH=results/augment_Stage/cifar/augment_test3
05/06 08:09:56PM parser.py:31 [INFO] PRINT_FREQ=50
05/06 08:09:56PM parser.py:31 [INFO] RESUME_PATH=None
05/06 08:09:56PM parser.py:31 [INFO] SEED=0
05/06 08:09:56PM parser.py:31 [INFO] TRAIN_PORTION=1.0
05/06 08:09:56PM parser.py:31 [INFO] WEIGHT_DECAY=0.0003
05/06 08:09:56PM parser.py:31 [INFO] WORKERS=4
05/06 08:09:56PM parser.py:32 [INFO] 
05/06 08:09:56PM augmentStage_main.py:41 [INFO] Logger is set - training start
05/06 08:09:57PM augmentStage_main.py:133 [INFO] Epoch 0 LR 0.0125
05/06 08:09:59PM augmentStage_main.py:158 [INFO] Train: [  1/2] Step 000/390 Loss 3.291 Prec@(1,5) (6.2%, 59.4%)
05/06 08:10:13PM augmentStage_main.py:158 [INFO] Train: [  1/2] Step 050/390 Loss 3.190 Prec@(1,5) (14.3%, 60.0%)
05/06 08:10:27PM augmentStage_main.py:158 [INFO] Train: [  1/2] Step 100/390 Loss 3.107 Prec@(1,5) (16.4%, 64.6%)
05/06 08:10:42PM augmentStage_main.py:158 [INFO] Train: [  1/2] Step 150/390 Loss 2.984 Prec@(1,5) (19.5%, 70.2%)
05/06 08:10:57PM augmentStage_main.py:158 [INFO] Train: [  1/2] Step 200/390 Loss 2.878 Prec@(1,5) (22.4%, 73.8%)
05/06 08:11:11PM augmentStage_main.py:158 [INFO] Train: [  1/2] Step 250/390 Loss 2.789 Prec@(1,5) (24.9%, 76.4%)
05/06 08:11:26PM augmentStage_main.py:158 [INFO] Train: [  1/2] Step 300/390 Loss 2.714 Prec@(1,5) (27.1%, 78.4%)
05/06 08:11:40PM augmentStage_main.py:158 [INFO] Train: [  1/2] Step 350/390 Loss 2.651 Prec@(1,5) (29.1%, 79.9%)
05/06 08:11:53PM augmentStage_main.py:158 [INFO] Train: [  1/2] Step 390/390 Loss 2.605 Prec@(1,5) (30.4%, 81.1%)
05/06 08:11:53PM augmentStage_main.py:169 [INFO] Train: [  1/2] Final Prec@1 30.4460%
05/06 08:11:53PM augmentStage_main.py:195 [INFO] Valid: [  1/2] Step 000/078 Loss 1.624 Prec@(1,5) (42.2%, 88.3%)
05/06 08:11:58PM augmentStage_main.py:195 [INFO] Valid: [  1/2] Step 050/078 Loss 1.685 Prec@(1,5) (42.8%, 91.7%)
05/06 08:12:00PM augmentStage_main.py:195 [INFO] Valid: [  1/2] Step 078/078 Loss 1.684 Prec@(1,5) (42.4%, 92.0%)
05/06 08:12:00PM augmentStage_main.py:205 [INFO] Valid: [  1/2] Final Prec@1 42.4200%
05/06 08:12:01PM augmentStage_main.py:118 [INFO] until now best Prec@1 = 42.4200%
05/06 08:12:01午後 augmentStage_main.py:133 [INFO] Epoch 1 LR 0.0
05/06 08:12:01午後 augmentStage_main.py:158 [INFO] Train: [  2/2] Step 000/390 Loss 2.526 Prec@(1,5) (33.6%, 82.8%)
05/06 08:12:17午後 augmentStage_main.py:158 [INFO] Train: [  2/2] Step 050/390 Loss 2.605 Prec@(1,5) (33.1%, 84.9%)
05/06 08:12:33午後 augmentStage_main.py:158 [INFO] Train: [  2/2] Step 100/390 Loss 2.615 Prec@(1,5) (32.8%, 84.9%)
05/06 08:12:49午後 augmentStage_main.py:158 [INFO] Train: [  2/2] Step 150/390 Loss 2.616 Prec@(1,5) (32.8%, 84.7%)
05/06 08:13:04午後 augmentStage_main.py:158 [INFO] Train: [  2/2] Step 200/390 Loss 2.618 Prec@(1,5) (32.7%, 84.7%)
05/06 08:13:20午後 augmentStage_main.py:158 [INFO] Train: [  2/2] Step 250/390 Loss 2.618 Prec@(1,5) (32.5%, 84.6%)
05/06 08:13:36午後 augmentStage_main.py:158 [INFO] Train: [  2/2] Step 300/390 Loss 2.619 Prec@(1,5) (32.4%, 84.7%)
05/06 08:13:52午後 augmentStage_main.py:158 [INFO] Train: [  2/2] Step 350/390 Loss 2.617 Prec@(1,5) (32.5%, 84.6%)
05/06 08:14:04午後 augmentStage_main.py:158 [INFO] Train: [  2/2] Step 390/390 Loss 2.619 Prec@(1,5) (32.5%, 84.6%)
05/06 08:14:04午後 augmentStage_main.py:169 [INFO] Train: [  2/2] Final Prec@1 32.5100%
05/06 08:14:04午後 augmentStage_main.py:195 [INFO] Valid: [  2/2] Step 000/078 Loss 1.459 Prec@(1,5) (43.0%, 93.0%)
05/06 08:14:09午後 augmentStage_main.py:195 [INFO] Valid: [  2/2] Step 050/078 Loss 1.461 Prec@(1,5) (44.9%, 92.1%)
05/06 08:14:12午後 augmentStage_main.py:195 [INFO] Valid: [  2/2] Step 078/078 Loss 1.458 Prec@(1,5) (45.1%, 92.5%)
05/06 08:14:12午後 augmentStage_main.py:205 [INFO] Valid: [  2/2] Final Prec@1 45.0900%
05/06 08:14:12午後 augmentStage_main.py:118 [INFO] until now best Prec@1 = 45.0900%
05/06 08:14:12午後 augmentStage_main.py:123 [INFO] Final best Prec@1 = 45.0900%
