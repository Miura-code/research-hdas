07/03 07:31:11PM parser.py:28 [INFO] 
07/03 07:31:11PM parser.py:29 [INFO] Parameters:
07/03 07:31:11PM parser.py:31 [INFO] AUX_WEIGHT=0.4
07/03 07:31:11PM parser.py:31 [INFO] BATCH_SIZE=64
07/03 07:31:11PM parser.py:31 [INFO] CUTOUT_LENGTH=16
07/03 07:31:11PM parser.py:31 [INFO] DATA_PATH=../data/
07/03 07:31:11PM parser.py:31 [INFO] DATASET=cifar100
07/03 07:31:11PM parser.py:31 [INFO] DIST=False
07/03 07:31:11PM parser.py:31 [INFO] DROP_PATH_PROB=0.2
07/03 07:31:11PM parser.py:31 [INFO] EPOCHS=100
07/03 07:31:11PM parser.py:31 [INFO] EXCLUDE_BIAS_AND_BN=True
07/03 07:31:11PM parser.py:31 [INFO] EXP_NAME=eval-20240703-193111
07/03 07:31:11PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('dil_conv_5x5', 0), ('dil_conv_5x5', 1)], [('sep_conv_5x5', 0), ('dil_conv_3x3', 2)], [('sep_conv_3x3', 0), ('max_pool_3x3', 1)], [('sep_conv_5x5', 1), ('dil_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('dil_conv_5x5', 1), ('dil_conv_3x3', 0)], [('dil_conv_3x3', 1), ('max_pool_3x3', 0)], [('dil_conv_5x5', 1), ('dil_conv_5x5', 3)], [('dil_conv_5x5', 3), ('dil_conv_3x3', 2)]], reduce1_concat=range(2, 6), normal2=[[('dil_conv_3x3', 1), ('sep_conv_5x5', 0)], [('sep_conv_3x3', 0), ('dil_conv_5x5', 1)], [('sep_conv_5x5', 0), ('avg_pool_3x3', 1)], [('sep_conv_3x3', 2), ('sep_conv_5x5', 1)]], normal2_concat=range(2, 6), reduce2=[[('sep_conv_3x3', 0), ('dil_conv_5x5', 1)], [('sep_conv_3x3', 2), ('sep_conv_5x5', 1)], [('skip_connect', 0), ('sep_conv_5x5', 1)], [('sep_conv_3x3', 3), ('dil_conv_5x5', 1)]], reduce2_concat=range(2, 6), normal3=[[('sep_conv_5x5', 0), ('dil_conv_5x5', 1)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 0)], [('dil_conv_5x5', 2), ('sep_conv_3x3', 1)], [('sep_conv_5x5', 4), ('sep_conv_5x5', 2)]], normal3_concat=range(2, 6))
07/03 07:31:11PM parser.py:31 [INFO] GPUS=[0]
07/03 07:31:11PM parser.py:31 [INFO] GRAD_CLIP=5.0
07/03 07:31:11PM parser.py:31 [INFO] INIT_CHANNELS=32
07/03 07:31:11PM parser.py:31 [INFO] LAYERS=20
07/03 07:31:11PM parser.py:31 [INFO] LOCAL_RANK=0
07/03 07:31:11PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
07/03 07:31:11PM parser.py:31 [INFO] LR=0.025
07/03 07:31:11PM parser.py:31 [INFO] LR_MIN=0.001
07/03 07:31:11PM parser.py:31 [INFO] MOMENTUM=0.9
07/03 07:31:11PM parser.py:31 [INFO] NAME=BASELINE
07/03 07:31:11PM parser.py:31 [INFO] PATH=results/evaluate_Cell/cifar100/BASELINE/eval-20240703-193111
07/03 07:31:11PM parser.py:31 [INFO] PRINT_FREQ=50
07/03 07:31:11PM parser.py:31 [INFO] RESUME_PATH=None
07/03 07:31:11PM parser.py:31 [INFO] SAVE=eval
07/03 07:31:11PM parser.py:31 [INFO] SEED=0
07/03 07:31:11PM parser.py:31 [INFO] TRAIN_PORTION=1.0
07/03 07:31:11PM parser.py:31 [INFO] WEIGHT_DECAY=0.0003
07/03 07:31:11PM parser.py:31 [INFO] WORKERS=4
07/03 07:31:11PM parser.py:32 [INFO] 
07/03 07:31:14PM evaluateCell_trainer.py:103 [INFO] --> No loaded checkpoint!
07/03 07:31:32PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][50/781]	Step 50	lr 0.025	Loss 6.5572 (6.4682)	Prec@(1,5) (1.5%, 5.7%)	
07/03 07:31:46PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][100/781]	Step 100	lr 0.025	Loss 6.5748 (6.4540)	Prec@(1,5) (1.3%, 6.0%)	
07/03 07:32:01PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][150/781]	Step 150	lr 0.025	Loss 6.4202 (6.4531)	Prec@(1,5) (1.4%, 6.5%)	
07/03 07:32:15PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][200/781]	Step 200	lr 0.025	Loss 6.1892 (6.4151)	Prec@(1,5) (1.7%, 7.6%)	
07/03 07:32:30PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][250/781]	Step 250	lr 0.025	Loss 6.0837 (6.3695)	Prec@(1,5) (2.0%, 9.0%)	
07/03 07:32:44PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][300/781]	Step 300	lr 0.025	Loss 5.9770 (6.3220)	Prec@(1,5) (2.2%, 10.0%)	
07/03 07:32:59PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][350/781]	Step 350	lr 0.025	Loss 5.9666 (6.2723)	Prec@(1,5) (2.5%, 11.0%)	
07/03 07:33:13PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][400/781]	Step 400	lr 0.025	Loss 5.9000 (6.2248)	Prec@(1,5) (2.8%, 12.1%)	
07/03 07:33:27PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][450/781]	Step 450	lr 0.025	Loss 5.5515 (6.1868)	Prec@(1,5) (3.0%, 12.8%)	
07/03 07:33:40PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][500/781]	Step 500	lr 0.025	Loss 5.6729 (6.1476)	Prec@(1,5) (3.1%, 13.7%)	
07/03 07:33:54PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][550/781]	Step 550	lr 0.025	Loss 5.7625 (6.1153)	Prec@(1,5) (3.4%, 14.4%)	
07/03 07:34:08PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][600/781]	Step 600	lr 0.025	Loss 5.4007 (6.0850)	Prec@(1,5) (3.6%, 15.0%)	
07/03 07:34:23PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][650/781]	Step 650	lr 0.025	Loss 5.6257 (6.0553)	Prec@(1,5) (3.8%, 15.6%)	
07/03 07:34:38PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][700/781]	Step 700	lr 0.025	Loss 5.8154 (6.0256)	Prec@(1,5) (4.0%, 16.2%)	
07/03 07:34:53PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][750/781]	Step 750	lr 0.025	Loss 5.6527 (6.0011)	Prec@(1,5) (4.2%, 16.7%)	
07/03 07:35:02PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][781/781]	Step 781	lr 0.025	Loss 5.5664 (5.9848)	Prec@(1,5) (4.3%, 17.0%)	
07/03 07:35:04PM evaluateCell_trainer.py:172 [INFO] Train: [  0/99] Final Prec@1 4.3460%
07/03 07:35:08PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][50/391]	Step 782	Loss 3.9895	Prec@(1,5) (8.2%, 27.8%)
07/03 07:35:13PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][100/391]	Step 782	Loss 3.9740	Prec@(1,5) (7.7%, 27.2%)
07/03 07:35:17PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][150/391]	Step 782	Loss 3.9726	Prec@(1,5) (7.6%, 27.1%)
07/03 07:35:21PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][200/391]	Step 782	Loss 3.9864	Prec@(1,5) (7.6%, 27.0%)
07/03 07:35:25PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][250/391]	Step 782	Loss 3.9944	Prec@(1,5) (7.5%, 26.7%)
07/03 07:35:29PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][300/391]	Step 782	Loss 3.9934	Prec@(1,5) (7.5%, 26.7%)
07/03 07:35:33PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][350/391]	Step 782	Loss 3.9881	Prec@(1,5) (7.5%, 26.8%)
07/03 07:35:37PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][390/391]	Step 782	Loss 3.9896	Prec@(1,5) (7.4%, 26.8%)
07/03 07:35:37PM evaluateCell_trainer.py:208 [INFO] Valid: [  0/99] Final Prec@1 7.3880%
07/03 07:35:37PM evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 7.3880%
07/03 07:35:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][50/781]	Step 832	lr 0.02499	Loss 5.6449 (5.6085)	Prec@(1,5) (6.9%, 24.5%)	
07/03 07:36:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][100/781]	Step 882	lr 0.02499	Loss 5.7890 (5.5780)	Prec@(1,5) (7.5%, 26.0%)	
07/03 07:36:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][150/781]	Step 932	lr 0.02499	Loss 5.4466 (5.5593)	Prec@(1,5) (7.9%, 26.5%)	
07/03 07:36:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][200/781]	Step 982	lr 0.02499	Loss 5.3768 (5.5428)	Prec@(1,5) (8.0%, 26.8%)	
07/03 07:36:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][250/781]	Step 1032	lr 0.02499	Loss 5.6524 (5.5186)	Prec@(1,5) (8.2%, 27.4%)	
07/03 07:37:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][300/781]	Step 1082	lr 0.02499	Loss 5.5948 (5.5070)	Prec@(1,5) (8.3%, 27.8%)	
07/03 07:37:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][350/781]	Step 1132	lr 0.02499	Loss 5.4674 (5.4899)	Prec@(1,5) (8.4%, 28.1%)	
07/03 07:37:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][400/781]	Step 1182	lr 0.02499	Loss 5.1079 (5.4706)	Prec@(1,5) (8.6%, 28.6%)	
07/03 07:37:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][450/781]	Step 1232	lr 0.02499	Loss 5.3386 (5.4519)	Prec@(1,5) (8.9%, 29.1%)	
07/03 07:38:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][500/781]	Step 1282	lr 0.02499	Loss 4.9262 (5.4361)	Prec@(1,5) (9.1%, 29.5%)	
07/03 07:38:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][550/781]	Step 1332	lr 0.02499	Loss 5.3053 (5.4160)	Prec@(1,5) (9.3%, 29.9%)	
07/03 07:38:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][600/781]	Step 1382	lr 0.02499	Loss 5.3496 (5.4057)	Prec@(1,5) (9.5%, 30.2%)	
07/03 07:38:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][650/781]	Step 1432	lr 0.02499	Loss 5.0012 (5.3916)	Prec@(1,5) (9.7%, 30.6%)	
07/03 07:39:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][700/781]	Step 1482	lr 0.02499	Loss 4.8519 (5.3760)	Prec@(1,5) (9.8%, 30.9%)	
07/03 07:39:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][750/781]	Step 1532	lr 0.02499	Loss 5.0552 (5.3620)	Prec@(1,5) (10.0%, 31.3%)	
07/03 07:39:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][781/781]	Step 1563	lr 0.02499	Loss 4.8386 (5.3544)	Prec@(1,5) (10.1%, 31.4%)	
07/03 07:39:30午後 evaluateCell_trainer.py:172 [INFO] Train: [  1/99] Final Prec@1 10.0560%
07/03 07:39:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][50/391]	Step 1564	Loss 3.6847	Prec@(1,5) (12.9%, 36.9%)
07/03 07:39:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][100/391]	Step 1564	Loss 3.6958	Prec@(1,5) (13.1%, 36.5%)
07/03 07:39:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][150/391]	Step 1564	Loss 3.6969	Prec@(1,5) (13.0%, 36.2%)
07/03 07:39:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][200/391]	Step 1564	Loss 3.6988	Prec@(1,5) (13.0%, 36.3%)
07/03 07:39:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][250/391]	Step 1564	Loss 3.6999	Prec@(1,5) (12.9%, 36.2%)
07/03 07:39:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][300/391]	Step 1564	Loss 3.7022	Prec@(1,5) (12.8%, 36.2%)
07/03 07:39:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][350/391]	Step 1564	Loss 3.7028	Prec@(1,5) (12.6%, 36.1%)
07/03 07:40:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][390/391]	Step 1564	Loss 3.7029	Prec@(1,5) (12.7%, 36.0%)
07/03 07:40:02午後 evaluateCell_trainer.py:208 [INFO] Valid: [  1/99] Final Prec@1 12.6640%
07/03 07:40:03午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 12.6640%
07/03 07:40:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][50/781]	Step 1614	lr 0.02498	Loss 5.2489 (5.1846)	Prec@(1,5) (12.5%, 36.3%)	
07/03 07:40:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][100/781]	Step 1664	lr 0.02498	Loss 4.4176 (5.1677)	Prec@(1,5) (12.4%, 36.2%)	
07/03 07:40:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][150/781]	Step 1714	lr 0.02498	Loss 5.2575 (5.1340)	Prec@(1,5) (12.5%, 36.7%)	
07/03 07:41:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][200/781]	Step 1764	lr 0.02498	Loss 5.1772 (5.1240)	Prec@(1,5) (12.8%, 37.0%)	
07/03 07:41:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][250/781]	Step 1814	lr 0.02498	Loss 5.1432 (5.1046)	Prec@(1,5) (12.7%, 37.3%)	
07/03 07:41:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][300/781]	Step 1864	lr 0.02498	Loss 5.0819 (5.0907)	Prec@(1,5) (13.0%, 37.5%)	
07/03 07:41:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][350/781]	Step 1914	lr 0.02498	Loss 4.7084 (5.0658)	Prec@(1,5) (13.2%, 38.1%)	
07/03 07:42:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][400/781]	Step 1964	lr 0.02498	Loss 4.9329 (5.0448)	Prec@(1,5) (13.4%, 38.6%)	
07/03 07:42:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][450/781]	Step 2014	lr 0.02498	Loss 4.8331 (5.0325)	Prec@(1,5) (13.6%, 39.0%)	
07/03 07:42:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][500/781]	Step 2064	lr 0.02498	Loss 4.7007 (5.0172)	Prec@(1,5) (13.8%, 39.2%)	
07/03 07:42:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][550/781]	Step 2114	lr 0.02498	Loss 4.4626 (5.0048)	Prec@(1,5) (13.9%, 39.5%)	
07/03 07:43:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][600/781]	Step 2164	lr 0.02498	Loss 4.3911 (4.9951)	Prec@(1,5) (14.2%, 39.6%)	
07/03 07:43:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][650/781]	Step 2214	lr 0.02498	Loss 5.0998 (4.9841)	Prec@(1,5) (14.3%, 39.8%)	
07/03 07:43:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][700/781]	Step 2264	lr 0.02498	Loss 4.7034 (4.9686)	Prec@(1,5) (14.5%, 40.2%)	
07/03 07:43:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][750/781]	Step 2314	lr 0.02498	Loss 4.5483 (4.9550)	Prec@(1,5) (14.6%, 40.4%)	
07/03 07:43:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][781/781]	Step 2345	lr 0.02498	Loss 4.5627 (4.9498)	Prec@(1,5) (14.7%, 40.6%)	
07/03 07:43:54午後 evaluateCell_trainer.py:172 [INFO] Train: [  2/99] Final Prec@1 14.6780%
07/03 07:43:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][50/391]	Step 2346	Loss 3.3855	Prec@(1,5) (16.9%, 45.7%)
07/03 07:44:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][100/391]	Step 2346	Loss 3.3858	Prec@(1,5) (16.7%, 45.6%)
07/03 07:44:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][150/391]	Step 2346	Loss 3.3946	Prec@(1,5) (16.8%, 45.1%)
07/03 07:44:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][200/391]	Step 2346	Loss 3.3805	Prec@(1,5) (17.0%, 45.4%)
07/03 07:44:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][250/391]	Step 2346	Loss 3.3789	Prec@(1,5) (17.0%, 45.4%)
07/03 07:44:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][300/391]	Step 2346	Loss 3.3826	Prec@(1,5) (16.9%, 45.3%)
07/03 07:44:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][350/391]	Step 2346	Loss 3.3837	Prec@(1,5) (17.0%, 45.2%)
07/03 07:44:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][390/391]	Step 2346	Loss 3.3811	Prec@(1,5) (17.1%, 45.4%)
07/03 07:44:26午後 evaluateCell_trainer.py:208 [INFO] Valid: [  2/99] Final Prec@1 17.1160%
07/03 07:44:27午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 17.1160%
07/03 07:44:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][50/781]	Step 2396	lr 0.02495	Loss 4.6154 (4.7991)	Prec@(1,5) (17.2%, 42.9%)	
07/03 07:44:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][100/781]	Step 2446	lr 0.02495	Loss 4.7780 (4.7861)	Prec@(1,5) (17.2%, 43.4%)	
07/03 07:45:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][150/781]	Step 2496	lr 0.02495	Loss 4.6028 (4.7459)	Prec@(1,5) (17.6%, 44.7%)	
07/03 07:45:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][200/781]	Step 2546	lr 0.02495	Loss 4.5996 (4.7281)	Prec@(1,5) (17.7%, 44.9%)	
07/03 07:45:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][250/781]	Step 2596	lr 0.02495	Loss 4.5844 (4.7177)	Prec@(1,5) (17.8%, 45.0%)	
07/03 07:45:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][300/781]	Step 2646	lr 0.02495	Loss 4.6559 (4.6980)	Prec@(1,5) (18.0%, 45.2%)	
07/03 07:46:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][350/781]	Step 2696	lr 0.02495	Loss 4.2968 (4.6811)	Prec@(1,5) (18.1%, 45.6%)	
07/03 07:46:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][400/781]	Step 2746	lr 0.02495	Loss 4.2675 (4.6722)	Prec@(1,5) (18.2%, 45.6%)	
07/03 07:46:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][450/781]	Step 2796	lr 0.02495	Loss 4.3579 (4.6564)	Prec@(1,5) (18.3%, 45.9%)	
07/03 07:46:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][500/781]	Step 2846	lr 0.02495	Loss 4.5266 (4.6453)	Prec@(1,5) (18.4%, 46.2%)	
07/03 07:47:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][550/781]	Step 2896	lr 0.02495	Loss 4.1570 (4.6345)	Prec@(1,5) (18.5%, 46.4%)	
07/03 07:47:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][600/781]	Step 2946	lr 0.02495	Loss 4.5707 (4.6264)	Prec@(1,5) (18.6%, 46.6%)	
07/03 07:47:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][650/781]	Step 2996	lr 0.02495	Loss 4.5931 (4.6179)	Prec@(1,5) (18.7%, 46.7%)	
07/03 07:47:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][700/781]	Step 3046	lr 0.02495	Loss 4.7368 (4.6069)	Prec@(1,5) (18.9%, 47.0%)	
07/03 07:48:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][750/781]	Step 3096	lr 0.02495	Loss 4.1103 (4.5971)	Prec@(1,5) (19.0%, 47.2%)	
07/03 07:48:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][781/781]	Step 3127	lr 0.02495	Loss 4.3054 (4.5897)	Prec@(1,5) (19.1%, 47.4%)	
07/03 07:48:19午後 evaluateCell_trainer.py:172 [INFO] Train: [  3/99] Final Prec@1 19.0640%
07/03 07:48:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][50/391]	Step 3128	Loss 3.2135	Prec@(1,5) (20.9%, 49.1%)
07/03 07:48:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][100/391]	Step 3128	Loss 3.2476	Prec@(1,5) (20.3%, 48.4%)
07/03 07:48:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][150/391]	Step 3128	Loss 3.2384	Prec@(1,5) (20.3%, 48.6%)
07/03 07:48:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][200/391]	Step 3128	Loss 3.2473	Prec@(1,5) (19.9%, 48.6%)
07/03 07:48:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][250/391]	Step 3128	Loss 3.2379	Prec@(1,5) (19.9%, 48.9%)
07/03 07:48:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][300/391]	Step 3128	Loss 3.2339	Prec@(1,5) (20.1%, 49.1%)
07/03 07:48:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][350/391]	Step 3128	Loss 3.2303	Prec@(1,5) (20.2%, 49.2%)
07/03 07:48:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][390/391]	Step 3128	Loss 3.2215	Prec@(1,5) (20.2%, 49.5%)
07/03 07:48:52午後 evaluateCell_trainer.py:208 [INFO] Valid: [  3/99] Final Prec@1 20.2000%
07/03 07:48:52午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 20.2000%
07/03 07:49:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][50/781]	Step 3178	lr 0.02491	Loss 4.2338 (4.3794)	Prec@(1,5) (22.2%, 50.6%)	
07/03 07:49:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][100/781]	Step 3228	lr 0.02491	Loss 5.1184 (4.3897)	Prec@(1,5) (22.2%, 51.0%)	
07/03 07:49:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][150/781]	Step 3278	lr 0.02491	Loss 4.5540 (4.3557)	Prec@(1,5) (22.6%, 51.6%)	
07/03 07:49:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][200/781]	Step 3328	lr 0.02491	Loss 4.7601 (4.3572)	Prec@(1,5) (22.3%, 51.5%)	
07/03 07:50:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][250/781]	Step 3378	lr 0.02491	Loss 3.9629 (4.3469)	Prec@(1,5) (22.2%, 51.9%)	
07/03 07:50:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][300/781]	Step 3428	lr 0.02491	Loss 4.3998 (4.3391)	Prec@(1,5) (22.1%, 52.0%)	
07/03 07:50:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][350/781]	Step 3478	lr 0.02491	Loss 4.4395 (4.3313)	Prec@(1,5) (22.4%, 52.2%)	
07/03 07:50:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][400/781]	Step 3528	lr 0.02491	Loss 4.5555 (4.3179)	Prec@(1,5) (22.5%, 52.6%)	
07/03 07:51:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][450/781]	Step 3578	lr 0.02491	Loss 4.2321 (4.3173)	Prec@(1,5) (22.5%, 52.6%)	
07/03 07:51:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][500/781]	Step 3628	lr 0.02491	Loss 4.2971 (4.3078)	Prec@(1,5) (22.6%, 52.8%)	
07/03 07:51:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][550/781]	Step 3678	lr 0.02491	Loss 5.1038 (4.2947)	Prec@(1,5) (22.8%, 53.0%)	
07/03 07:51:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][600/781]	Step 3728	lr 0.02491	Loss 4.1417 (4.2874)	Prec@(1,5) (22.9%, 53.1%)	
07/03 07:52:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][650/781]	Step 3778	lr 0.02491	Loss 4.2541 (4.2821)	Prec@(1,5) (23.0%, 53.2%)	
07/03 07:52:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][700/781]	Step 3828	lr 0.02491	Loss 4.1917 (4.2707)	Prec@(1,5) (23.1%, 53.5%)	
07/03 07:52:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][750/781]	Step 3878	lr 0.02491	Loss 4.1901 (4.2607)	Prec@(1,5) (23.2%, 53.7%)	
07/03 07:52:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][781/781]	Step 3909	lr 0.02491	Loss 4.6137 (4.2565)	Prec@(1,5) (23.2%, 53.7%)	
07/03 07:52:44午後 evaluateCell_trainer.py:172 [INFO] Train: [  4/99] Final Prec@1 23.2240%
07/03 07:52:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][50/391]	Step 3910	Loss 3.0669	Prec@(1,5) (23.6%, 54.0%)
07/03 07:52:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][100/391]	Step 3910	Loss 3.0409	Prec@(1,5) (23.5%, 54.8%)
07/03 07:52:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][150/391]	Step 3910	Loss 3.0476	Prec@(1,5) (23.2%, 54.8%)
07/03 07:53:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][200/391]	Step 3910	Loss 3.0617	Prec@(1,5) (23.2%, 54.3%)
07/03 07:53:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][250/391]	Step 3910	Loss 3.0520	Prec@(1,5) (23.5%, 54.6%)
07/03 07:53:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][300/391]	Step 3910	Loss 3.0507	Prec@(1,5) (23.5%, 54.7%)
07/03 07:53:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][350/391]	Step 3910	Loss 3.0494	Prec@(1,5) (23.6%, 54.5%)
07/03 07:53:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][390/391]	Step 3910	Loss 3.0478	Prec@(1,5) (23.6%, 54.6%)
07/03 07:53:17午後 evaluateCell_trainer.py:208 [INFO] Valid: [  4/99] Final Prec@1 23.6000%
07/03 07:53:17午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 23.6000%
07/03 07:53:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][50/781]	Step 3960	lr 0.02485	Loss 4.6258 (4.1258)	Prec@(1,5) (25.0%, 56.3%)	
07/03 07:53:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][100/781]	Step 4010	lr 0.02485	Loss 4.3303 (4.0838)	Prec@(1,5) (25.6%, 56.7%)	
07/03 07:54:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][150/781]	Step 4060	lr 0.02485	Loss 4.1765 (4.0607)	Prec@(1,5) (25.9%, 57.0%)	
07/03 07:54:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][200/781]	Step 4110	lr 0.02485	Loss 4.4260 (4.0516)	Prec@(1,5) (26.1%, 57.0%)	
07/03 07:54:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][250/781]	Step 4160	lr 0.02485	Loss 3.7042 (4.0404)	Prec@(1,5) (26.4%, 57.2%)	
07/03 07:54:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][300/781]	Step 4210	lr 0.02485	Loss 3.8073 (4.0435)	Prec@(1,5) (26.2%, 57.2%)	
07/03 07:55:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][350/781]	Step 4260	lr 0.02485	Loss 3.5705 (4.0361)	Prec@(1,5) (26.3%, 57.3%)	
07/03 07:55:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][400/781]	Step 4310	lr 0.02485	Loss 3.7300 (4.0184)	Prec@(1,5) (26.4%, 57.7%)	
07/03 07:55:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][450/781]	Step 4360	lr 0.02485	Loss 4.1494 (4.0161)	Prec@(1,5) (26.6%, 57.9%)	
07/03 07:55:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][500/781]	Step 4410	lr 0.02485	Loss 3.8212 (4.0042)	Prec@(1,5) (26.7%, 58.0%)	
07/03 07:56:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][550/781]	Step 4460	lr 0.02485	Loss 4.2779 (3.9954)	Prec@(1,5) (26.9%, 58.2%)	
07/03 07:56:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][600/781]	Step 4510	lr 0.02485	Loss 3.8844 (3.9890)	Prec@(1,5) (26.9%, 58.3%)	
07/03 07:56:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][650/781]	Step 4560	lr 0.02485	Loss 3.7625 (3.9835)	Prec@(1,5) (26.9%, 58.3%)	
07/03 07:56:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][700/781]	Step 4610	lr 0.02485	Loss 3.7627 (3.9786)	Prec@(1,5) (27.0%, 58.4%)	
07/03 07:57:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][750/781]	Step 4660	lr 0.02485	Loss 3.8897 (3.9706)	Prec@(1,5) (27.1%, 58.6%)	
07/03 07:57:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][781/781]	Step 4691	lr 0.02485	Loss 3.6445 (3.9638)	Prec@(1,5) (27.2%, 58.7%)	
07/03 07:57:09午後 evaluateCell_trainer.py:172 [INFO] Train: [  5/99] Final Prec@1 27.1660%
07/03 07:57:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][50/391]	Step 4692	Loss 3.0121	Prec@(1,5) (25.8%, 55.2%)
07/03 07:57:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][100/391]	Step 4692	Loss 3.0152	Prec@(1,5) (25.4%, 55.2%)
07/03 07:57:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][150/391]	Step 4692	Loss 3.0017	Prec@(1,5) (25.7%, 55.6%)
07/03 07:57:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][200/391]	Step 4692	Loss 2.9884	Prec@(1,5) (26.0%, 55.9%)
07/03 07:57:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][250/391]	Step 4692	Loss 2.9851	Prec@(1,5) (26.3%, 55.9%)
07/03 07:57:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][300/391]	Step 4692	Loss 2.9919	Prec@(1,5) (26.1%, 55.8%)
07/03 07:57:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][350/391]	Step 4692	Loss 3.0014	Prec@(1,5) (25.8%, 55.6%)
07/03 07:57:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][390/391]	Step 4692	Loss 3.0022	Prec@(1,5) (25.7%, 55.7%)
07/03 07:57:41午後 evaluateCell_trainer.py:208 [INFO] Valid: [  5/99] Final Prec@1 25.6720%
07/03 07:57:41午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 25.6720%
07/03 07:57:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][50/781]	Step 4742	lr 0.02479	Loss 3.9612 (3.7839)	Prec@(1,5) (29.5%, 61.7%)	
07/03 07:58:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][100/781]	Step 4792	lr 0.02479	Loss 4.1868 (3.8041)	Prec@(1,5) (29.2%, 61.9%)	
07/03 07:58:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][150/781]	Step 4842	lr 0.02479	Loss 3.4796 (3.7904)	Prec@(1,5) (29.5%, 61.6%)	
07/03 07:58:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][200/781]	Step 4892	lr 0.02479	Loss 3.8423 (3.7736)	Prec@(1,5) (29.9%, 62.2%)	
07/03 07:58:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][250/781]	Step 4942	lr 0.02479	Loss 3.5284 (3.7630)	Prec@(1,5) (30.0%, 62.4%)	
07/03 07:59:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][300/781]	Step 4992	lr 0.02479	Loss 3.3834 (3.7556)	Prec@(1,5) (30.0%, 62.5%)	
07/03 07:59:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][350/781]	Step 5042	lr 0.02479	Loss 3.9021 (3.7429)	Prec@(1,5) (30.2%, 62.7%)	
07/03 07:59:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][400/781]	Step 5092	lr 0.02479	Loss 3.9071 (3.7329)	Prec@(1,5) (30.3%, 63.0%)	
07/03 07:59:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][450/781]	Step 5142	lr 0.02479	Loss 3.9868 (3.7278)	Prec@(1,5) (30.6%, 63.1%)	
07/03 08:00:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][500/781]	Step 5192	lr 0.02479	Loss 3.4490 (3.7217)	Prec@(1,5) (30.7%, 63.1%)	
07/03 08:00:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][550/781]	Step 5242	lr 0.02479	Loss 3.2710 (3.7152)	Prec@(1,5) (30.9%, 63.2%)	
07/03 08:00:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][600/781]	Step 5292	lr 0.02479	Loss 3.7488 (3.7103)	Prec@(1,5) (30.9%, 63.3%)	
07/03 08:00:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][650/781]	Step 5342	lr 0.02479	Loss 3.6163 (3.7083)	Prec@(1,5) (30.9%, 63.3%)	
07/03 08:01:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][700/781]	Step 5392	lr 0.02479	Loss 4.4098 (3.7070)	Prec@(1,5) (31.0%, 63.3%)	
07/03 08:01:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][750/781]	Step 5442	lr 0.02479	Loss 2.7511 (3.6991)	Prec@(1,5) (31.1%, 63.4%)	
07/03 08:01:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][781/781]	Step 5473	lr 0.02479	Loss 3.6395 (3.6947)	Prec@(1,5) (31.2%, 63.5%)	
07/03 08:01:33午後 evaluateCell_trainer.py:172 [INFO] Train: [  6/99] Final Prec@1 31.1860%
07/03 08:01:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][50/391]	Step 5474	Loss 2.8055	Prec@(1,5) (28.8%, 61.9%)
07/03 08:01:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][100/391]	Step 5474	Loss 2.8135	Prec@(1,5) (28.3%, 60.9%)
07/03 08:01:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][150/391]	Step 5474	Loss 2.8019	Prec@(1,5) (28.7%, 61.0%)
07/03 08:01:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][200/391]	Step 5474	Loss 2.8057	Prec@(1,5) (28.7%, 61.2%)
07/03 08:01:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][250/391]	Step 5474	Loss 2.8051	Prec@(1,5) (28.6%, 61.1%)
07/03 08:01:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][300/391]	Step 5474	Loss 2.8079	Prec@(1,5) (28.4%, 61.1%)
07/03 08:02:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][350/391]	Step 5474	Loss 2.8097	Prec@(1,5) (28.4%, 60.9%)
07/03 08:02:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][390/391]	Step 5474	Loss 2.8152	Prec@(1,5) (28.4%, 60.8%)
07/03 08:02:06午後 evaluateCell_trainer.py:208 [INFO] Valid: [  6/99] Final Prec@1 28.3880%
07/03 08:02:06午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 28.3880%
07/03 08:02:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][50/781]	Step 5524	lr 0.02471	Loss 3.4066 (3.5496)	Prec@(1,5) (33.8%, 66.0%)	
07/03 08:02:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][100/781]	Step 5574	lr 0.02471	Loss 3.7278 (3.5335)	Prec@(1,5) (33.5%, 66.4%)	
07/03 08:02:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][150/781]	Step 5624	lr 0.02471	Loss 3.9016 (3.5540)	Prec@(1,5) (33.1%, 65.8%)	
07/03 08:03:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][200/781]	Step 5674	lr 0.02471	Loss 3.3180 (3.5215)	Prec@(1,5) (33.5%, 66.2%)	
07/03 08:03:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][250/781]	Step 5724	lr 0.02471	Loss 3.5102 (3.5096)	Prec@(1,5) (33.8%, 66.4%)	
07/03 08:03:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][300/781]	Step 5774	lr 0.02471	Loss 3.0063 (3.5008)	Prec@(1,5) (33.9%, 66.7%)	
07/03 08:03:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][350/781]	Step 5824	lr 0.02471	Loss 3.0535 (3.4997)	Prec@(1,5) (34.0%, 66.6%)	
07/03 08:04:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][400/781]	Step 5874	lr 0.02471	Loss 3.7746 (3.5082)	Prec@(1,5) (33.9%, 66.5%)	
07/03 08:04:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][450/781]	Step 5924	lr 0.02471	Loss 3.2594 (3.5073)	Prec@(1,5) (33.9%, 66.6%)	
07/03 08:04:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][500/781]	Step 5974	lr 0.02471	Loss 3.6204 (3.4972)	Prec@(1,5) (34.0%, 66.8%)	
07/03 08:04:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][550/781]	Step 6024	lr 0.02471	Loss 3.5628 (3.5027)	Prec@(1,5) (34.0%, 66.7%)	
07/03 08:05:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][600/781]	Step 6074	lr 0.02471	Loss 3.1557 (3.4990)	Prec@(1,5) (34.0%, 66.8%)	
07/03 08:05:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][650/781]	Step 6124	lr 0.02471	Loss 3.7322 (3.4958)	Prec@(1,5) (34.1%, 66.8%)	
07/03 08:05:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][700/781]	Step 6174	lr 0.02471	Loss 3.3114 (3.4947)	Prec@(1,5) (34.1%, 66.8%)	
07/03 08:05:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][750/781]	Step 6224	lr 0.02471	Loss 3.5934 (3.4876)	Prec@(1,5) (34.2%, 66.9%)	
07/03 08:05:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][781/781]	Step 6255	lr 0.02471	Loss 3.4619 (3.4857)	Prec@(1,5) (34.2%, 67.0%)	
07/03 08:05:59午後 evaluateCell_trainer.py:172 [INFO] Train: [  7/99] Final Prec@1 34.2160%
07/03 08:06:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][50/391]	Step 6256	Loss 2.4467	Prec@(1,5) (35.9%, 68.3%)
07/03 08:06:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][100/391]	Step 6256	Loss 2.4337	Prec@(1,5) (36.5%, 69.0%)
07/03 08:06:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][150/391]	Step 6256	Loss 2.4324	Prec@(1,5) (36.4%, 68.9%)
07/03 08:06:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][200/391]	Step 6256	Loss 2.4289	Prec@(1,5) (36.2%, 69.0%)
07/03 08:06:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][250/391]	Step 6256	Loss 2.4215	Prec@(1,5) (36.2%, 68.9%)
07/03 08:06:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][300/391]	Step 6256	Loss 2.4223	Prec@(1,5) (36.1%, 69.0%)
07/03 08:06:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][350/391]	Step 6256	Loss 2.4234	Prec@(1,5) (36.0%, 69.0%)
07/03 08:06:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][390/391]	Step 6256	Loss 2.4292	Prec@(1,5) (35.8%, 68.9%)
07/03 08:06:31午後 evaluateCell_trainer.py:208 [INFO] Valid: [  7/99] Final Prec@1 35.7760%
07/03 08:06:31午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 35.7760%
07/03 08:06:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][50/781]	Step 6306	lr 0.02462	Loss 3.3488 (3.3209)	Prec@(1,5) (35.8%, 69.4%)	
07/03 08:07:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][100/781]	Step 6356	lr 0.02462	Loss 3.1387 (3.2750)	Prec@(1,5) (37.4%, 70.2%)	
07/03 08:07:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][150/781]	Step 6406	lr 0.02462	Loss 3.3569 (3.2980)	Prec@(1,5) (36.6%, 70.2%)	
07/03 08:07:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][200/781]	Step 6456	lr 0.02462	Loss 3.4403 (3.3001)	Prec@(1,5) (36.7%, 69.8%)	
07/03 08:07:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][250/781]	Step 6506	lr 0.02462	Loss 3.5707 (3.3120)	Prec@(1,5) (36.6%, 69.6%)	
07/03 08:08:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][300/781]	Step 6556	lr 0.02462	Loss 3.0672 (3.3088)	Prec@(1,5) (36.8%, 69.7%)	
07/03 08:08:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][350/781]	Step 6606	lr 0.02462	Loss 3.5314 (3.3107)	Prec@(1,5) (36.8%, 69.6%)	
07/03 08:08:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][400/781]	Step 6656	lr 0.02462	Loss 3.1933 (3.3145)	Prec@(1,5) (36.8%, 69.6%)	
07/03 08:08:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][450/781]	Step 6706	lr 0.02462	Loss 2.7103 (3.3232)	Prec@(1,5) (36.6%, 69.5%)	
07/03 08:09:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][500/781]	Step 6756	lr 0.02462	Loss 3.5447 (3.3171)	Prec@(1,5) (36.8%, 69.6%)	
07/03 08:09:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][550/781]	Step 6806	lr 0.02462	Loss 2.9303 (3.3185)	Prec@(1,5) (36.7%, 69.6%)	
07/03 08:09:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][600/781]	Step 6856	lr 0.02462	Loss 3.2386 (3.3193)	Prec@(1,5) (36.7%, 69.7%)	
07/03 08:09:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][650/781]	Step 6906	lr 0.02462	Loss 3.4750 (3.3168)	Prec@(1,5) (36.6%, 69.7%)	
07/03 08:09:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][700/781]	Step 6956	lr 0.02462	Loss 3.3283 (3.3169)	Prec@(1,5) (36.6%, 69.8%)	
07/03 08:10:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][750/781]	Step 7006	lr 0.02462	Loss 3.0873 (3.3138)	Prec@(1,5) (36.7%, 69.8%)	
07/03 08:10:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][781/781]	Step 7037	lr 0.02462	Loss 3.0282 (3.3128)	Prec@(1,5) (36.7%, 69.8%)	
07/03 08:10:24午後 evaluateCell_trainer.py:172 [INFO] Train: [  8/99] Final Prec@1 36.6500%
07/03 08:10:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][50/391]	Step 7038	Loss 2.4250	Prec@(1,5) (36.0%, 68.7%)
07/03 08:10:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][100/391]	Step 7038	Loss 2.4022	Prec@(1,5) (36.7%, 69.9%)
07/03 08:10:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][150/391]	Step 7038	Loss 2.4077	Prec@(1,5) (36.3%, 69.7%)
07/03 08:10:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][200/391]	Step 7038	Loss 2.4132	Prec@(1,5) (36.2%, 69.6%)
07/03 08:10:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][250/391]	Step 7038	Loss 2.4125	Prec@(1,5) (36.3%, 69.6%)
07/03 08:10:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][300/391]	Step 7038	Loss 2.4128	Prec@(1,5) (36.3%, 69.5%)
07/03 08:10:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][350/391]	Step 7038	Loss 2.4073	Prec@(1,5) (36.5%, 69.5%)
07/03 08:10:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][390/391]	Step 7038	Loss 2.4048	Prec@(1,5) (36.5%, 69.6%)
07/03 08:10:56午後 evaluateCell_trainer.py:208 [INFO] Valid: [  8/99] Final Prec@1 36.5040%
07/03 08:10:56午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 36.5040%
07/03 08:11:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][50/781]	Step 7088	lr 0.02452	Loss 3.3566 (3.2246)	Prec@(1,5) (37.3%, 71.2%)	
07/03 08:11:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][100/781]	Step 7138	lr 0.02452	Loss 3.2081 (3.1647)	Prec@(1,5) (38.7%, 71.9%)	
07/03 08:11:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][150/781]	Step 7188	lr 0.02452	Loss 3.2428 (3.2049)	Prec@(1,5) (38.1%, 71.3%)	
07/03 08:11:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][200/781]	Step 7238	lr 0.02452	Loss 2.8212 (3.1814)	Prec@(1,5) (38.4%, 71.6%)	
07/03 08:12:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][250/781]	Step 7288	lr 0.02452	Loss 3.1092 (3.1666)	Prec@(1,5) (38.8%, 71.9%)	
07/03 08:12:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][300/781]	Step 7338	lr 0.02452	Loss 3.0061 (3.1538)	Prec@(1,5) (39.0%, 72.0%)	
07/03 08:12:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][350/781]	Step 7388	lr 0.02452	Loss 3.7783 (3.1636)	Prec@(1,5) (38.8%, 71.8%)	
07/03 08:12:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][400/781]	Step 7438	lr 0.02452	Loss 3.3909 (3.1687)	Prec@(1,5) (38.7%, 71.8%)	
07/03 08:13:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][450/781]	Step 7488	lr 0.02452	Loss 2.8577 (3.1714)	Prec@(1,5) (38.7%, 71.8%)	
07/03 08:13:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][500/781]	Step 7538	lr 0.02452	Loss 3.2477 (3.1700)	Prec@(1,5) (38.7%, 71.9%)	
07/03 08:13:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][550/781]	Step 7588	lr 0.02452	Loss 2.7796 (3.1644)	Prec@(1,5) (38.9%, 72.0%)	
07/03 08:13:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][600/781]	Step 7638	lr 0.02452	Loss 3.2538 (3.1607)	Prec@(1,5) (38.9%, 72.0%)	
07/03 08:14:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][650/781]	Step 7688	lr 0.02452	Loss 3.5096 (3.1599)	Prec@(1,5) (38.9%, 72.0%)	
07/03 08:14:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][700/781]	Step 7738	lr 0.02452	Loss 3.5447 (3.1616)	Prec@(1,5) (38.9%, 71.9%)	
07/03 08:14:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][750/781]	Step 7788	lr 0.02452	Loss 3.4704 (3.1609)	Prec@(1,5) (38.9%, 71.9%)	
07/03 08:14:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][781/781]	Step 7819	lr 0.02452	Loss 3.0432 (3.1634)	Prec@(1,5) (38.9%, 71.9%)	
07/03 08:14:48午後 evaluateCell_trainer.py:172 [INFO] Train: [  9/99] Final Prec@1 38.8720%
07/03 08:14:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][50/391]	Step 7820	Loss 2.2275	Prec@(1,5) (39.6%, 72.8%)
07/03 08:14:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][100/391]	Step 7820	Loss 2.2475	Prec@(1,5) (39.7%, 72.4%)
07/03 08:15:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][150/391]	Step 7820	Loss 2.2368	Prec@(1,5) (40.2%, 72.4%)
07/03 08:15:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][200/391]	Step 7820	Loss 2.2492	Prec@(1,5) (40.0%, 72.2%)
07/03 08:15:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][250/391]	Step 7820	Loss 2.2679	Prec@(1,5) (39.7%, 71.8%)
07/03 08:15:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][300/391]	Step 7820	Loss 2.2654	Prec@(1,5) (39.9%, 72.0%)
07/03 08:15:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][350/391]	Step 7820	Loss 2.2644	Prec@(1,5) (39.8%, 72.1%)
07/03 08:15:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][390/391]	Step 7820	Loss 2.2638	Prec@(1,5) (39.9%, 72.1%)
07/03 08:15:20午後 evaluateCell_trainer.py:208 [INFO] Valid: [  9/99] Final Prec@1 39.8400%
07/03 08:15:21午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 39.8400%
07/03 08:15:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][50/781]	Step 7870	lr 0.02441	Loss 3.0831 (3.0434)	Prec@(1,5) (40.8%, 74.8%)	
07/03 08:15:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][100/781]	Step 7920	lr 0.02441	Loss 3.9088 (3.0643)	Prec@(1,5) (40.8%, 73.9%)	
07/03 08:16:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][150/781]	Step 7970	lr 0.02441	Loss 3.0958 (3.0491)	Prec@(1,5) (41.1%, 74.1%)	
07/03 08:16:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][200/781]	Step 8020	lr 0.02441	Loss 2.4544 (3.0395)	Prec@(1,5) (41.3%, 74.2%)	
07/03 08:16:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][250/781]	Step 8070	lr 0.02441	Loss 2.9436 (3.0376)	Prec@(1,5) (41.2%, 74.3%)	
07/03 08:16:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][300/781]	Step 8120	lr 0.02441	Loss 2.8023 (3.0320)	Prec@(1,5) (41.2%, 74.4%)	
07/03 08:17:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][350/781]	Step 8170	lr 0.02441	Loss 2.7629 (3.0309)	Prec@(1,5) (41.1%, 74.3%)	
07/03 08:17:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][400/781]	Step 8220	lr 0.02441	Loss 3.0616 (3.0348)	Prec@(1,5) (41.0%, 74.2%)	
07/03 08:17:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][450/781]	Step 8270	lr 0.02441	Loss 2.5845 (3.0385)	Prec@(1,5) (41.0%, 74.1%)	
07/03 08:17:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][500/781]	Step 8320	lr 0.02441	Loss 3.5190 (3.0361)	Prec@(1,5) (41.1%, 74.0%)	
07/03 08:18:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][550/781]	Step 8370	lr 0.02441	Loss 3.0741 (3.0351)	Prec@(1,5) (41.1%, 74.0%)	
07/03 08:18:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][600/781]	Step 8420	lr 0.02441	Loss 2.4659 (3.0332)	Prec@(1,5) (41.1%, 74.0%)	
07/03 08:18:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][650/781]	Step 8470	lr 0.02441	Loss 3.0649 (3.0365)	Prec@(1,5) (41.0%, 73.9%)	
07/03 08:18:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][700/781]	Step 8520	lr 0.02441	Loss 2.4452 (3.0358)	Prec@(1,5) (41.1%, 73.9%)	
07/03 08:19:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][750/781]	Step 8570	lr 0.02441	Loss 3.0075 (3.0335)	Prec@(1,5) (41.1%, 73.9%)	
07/03 08:19:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][781/781]	Step 8601	lr 0.02441	Loss 3.3545 (3.0310)	Prec@(1,5) (41.1%, 73.9%)	
07/03 08:19:13午後 evaluateCell_trainer.py:172 [INFO] Train: [ 10/99] Final Prec@1 41.0840%
07/03 08:19:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][50/391]	Step 8602	Loss 2.3890	Prec@(1,5) (37.7%, 70.4%)
07/03 08:19:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][100/391]	Step 8602	Loss 2.3628	Prec@(1,5) (38.3%, 70.5%)
07/03 08:19:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][150/391]	Step 8602	Loss 2.3671	Prec@(1,5) (38.1%, 70.3%)
07/03 08:19:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][200/391]	Step 8602	Loss 2.3740	Prec@(1,5) (38.0%, 70.2%)
07/03 08:19:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][250/391]	Step 8602	Loss 2.3850	Prec@(1,5) (37.7%, 70.1%)
07/03 08:19:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][300/391]	Step 8602	Loss 2.3825	Prec@(1,5) (37.9%, 70.2%)
07/03 08:19:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][350/391]	Step 8602	Loss 2.3840	Prec@(1,5) (37.8%, 70.1%)
07/03 08:19:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][390/391]	Step 8602	Loss 2.3776	Prec@(1,5) (37.9%, 70.1%)
07/03 08:19:46午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 10/99] Final Prec@1 37.9200%
07/03 08:19:46午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 39.8400%
07/03 08:20:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][50/781]	Step 8652	lr 0.02429	Loss 2.9402 (2.9652)	Prec@(1,5) (43.9%, 74.9%)	
07/03 08:20:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][100/781]	Step 8702	lr 0.02429	Loss 3.3489 (2.9308)	Prec@(1,5) (43.6%, 75.5%)	
07/03 08:20:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][150/781]	Step 8752	lr 0.02429	Loss 2.4992 (2.9143)	Prec@(1,5) (43.6%, 75.7%)	
07/03 08:20:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][200/781]	Step 8802	lr 0.02429	Loss 2.9201 (2.8895)	Prec@(1,5) (43.8%, 75.7%)	
07/03 08:21:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][250/781]	Step 8852	lr 0.02429	Loss 2.8149 (2.9016)	Prec@(1,5) (43.5%, 75.5%)	
07/03 08:21:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][300/781]	Step 8902	lr 0.02429	Loss 2.8473 (2.9185)	Prec@(1,5) (43.1%, 75.3%)	
07/03 08:21:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][350/781]	Step 8952	lr 0.02429	Loss 3.0619 (2.9154)	Prec@(1,5) (43.1%, 75.4%)	
07/03 08:21:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][400/781]	Step 9002	lr 0.02429	Loss 2.8037 (2.9148)	Prec@(1,5) (43.0%, 75.4%)	
07/03 08:21:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][450/781]	Step 9052	lr 0.02429	Loss 2.5573 (2.9115)	Prec@(1,5) (43.0%, 75.4%)	
07/03 08:22:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][500/781]	Step 9102	lr 0.02429	Loss 2.6847 (2.9127)	Prec@(1,5) (42.9%, 75.4%)	
07/03 08:22:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][550/781]	Step 9152	lr 0.02429	Loss 2.9603 (2.9177)	Prec@(1,5) (42.9%, 75.3%)	
07/03 08:22:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][600/781]	Step 9202	lr 0.02429	Loss 2.6505 (2.9149)	Prec@(1,5) (42.9%, 75.3%)	
07/03 08:22:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][650/781]	Step 9252	lr 0.02429	Loss 2.6477 (2.9121)	Prec@(1,5) (43.0%, 75.3%)	
07/03 08:23:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][700/781]	Step 9302	lr 0.02429	Loss 3.0852 (2.9113)	Prec@(1,5) (43.0%, 75.4%)	
07/03 08:23:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][750/781]	Step 9352	lr 0.02429	Loss 2.4694 (2.9084)	Prec@(1,5) (43.1%, 75.4%)	
07/03 08:23:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][781/781]	Step 9383	lr 0.02429	Loss 3.0690 (2.9107)	Prec@(1,5) (43.0%, 75.4%)	
07/03 08:23:38午後 evaluateCell_trainer.py:172 [INFO] Train: [ 11/99] Final Prec@1 43.0440%
07/03 08:23:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][50/391]	Step 9384	Loss 2.1266	Prec@(1,5) (42.6%, 75.3%)
07/03 08:23:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][100/391]	Step 9384	Loss 2.0941	Prec@(1,5) (43.9%, 75.9%)
07/03 08:23:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][150/391]	Step 9384	Loss 2.0857	Prec@(1,5) (43.6%, 76.2%)
07/03 08:23:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][200/391]	Step 9384	Loss 2.0898	Prec@(1,5) (43.7%, 75.9%)
07/03 08:23:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][250/391]	Step 9384	Loss 2.0947	Prec@(1,5) (43.5%, 76.0%)
07/03 08:24:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][300/391]	Step 9384	Loss 2.0945	Prec@(1,5) (43.3%, 76.1%)
07/03 08:24:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][350/391]	Step 9384	Loss 2.0966	Prec@(1,5) (43.3%, 76.0%)
07/03 08:24:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][390/391]	Step 9384	Loss 2.0927	Prec@(1,5) (43.3%, 76.1%)
07/03 08:24:10午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 11/99] Final Prec@1 43.3400%
07/03 08:24:10午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 43.3400%
07/03 08:24:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][50/781]	Step 9434	lr 0.02416	Loss 2.6332 (2.7698)	Prec@(1,5) (46.8%, 76.7%)	
07/03 08:24:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][100/781]	Step 9484	lr 0.02416	Loss 2.8841 (2.8323)	Prec@(1,5) (45.0%, 76.5%)	
07/03 08:24:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][150/781]	Step 9534	lr 0.02416	Loss 2.9130 (2.8079)	Prec@(1,5) (45.0%, 77.1%)	
07/03 08:25:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][200/781]	Step 9584	lr 0.02416	Loss 3.4120 (2.8179)	Prec@(1,5) (45.0%, 77.0%)	
07/03 08:25:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][250/781]	Step 9634	lr 0.02416	Loss 2.5198 (2.8162)	Prec@(1,5) (45.0%, 76.9%)	
07/03 08:25:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][300/781]	Step 9684	lr 0.02416	Loss 2.8430 (2.8134)	Prec@(1,5) (45.0%, 76.9%)	
07/03 08:25:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][350/781]	Step 9734	lr 0.02416	Loss 2.9312 (2.8161)	Prec@(1,5) (44.9%, 76.8%)	
07/03 08:26:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][400/781]	Step 9784	lr 0.02416	Loss 2.1570 (2.8272)	Prec@(1,5) (44.6%, 76.6%)	
07/03 08:26:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][450/781]	Step 9834	lr 0.02416	Loss 2.6548 (2.8268)	Prec@(1,5) (44.7%, 76.6%)	
07/03 08:26:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][500/781]	Step 9884	lr 0.02416	Loss 2.4941 (2.8244)	Prec@(1,5) (44.7%, 76.6%)	
07/03 08:26:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][550/781]	Step 9934	lr 0.02416	Loss 2.9419 (2.8243)	Prec@(1,5) (44.7%, 76.7%)	
07/03 08:27:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][600/781]	Step 9984	lr 0.02416	Loss 3.1285 (2.8208)	Prec@(1,5) (44.7%, 76.8%)	
07/03 08:27:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][650/781]	Step 10034	lr 0.02416	Loss 2.7889 (2.8180)	Prec@(1,5) (44.7%, 76.9%)	
07/03 08:27:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][700/781]	Step 10084	lr 0.02416	Loss 3.3989 (2.8203)	Prec@(1,5) (44.7%, 76.8%)	
07/03 08:27:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][750/781]	Step 10134	lr 0.02416	Loss 2.7010 (2.8176)	Prec@(1,5) (44.7%, 76.8%)	
07/03 08:28:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][781/781]	Step 10165	lr 0.02416	Loss 2.4935 (2.8179)	Prec@(1,5) (44.7%, 76.8%)	
07/03 08:28:02午後 evaluateCell_trainer.py:172 [INFO] Train: [ 12/99] Final Prec@1 44.6940%
07/03 08:28:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][50/391]	Step 10166	Loss 2.0685	Prec@(1,5) (43.7%, 76.9%)
07/03 08:28:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][100/391]	Step 10166	Loss 2.0603	Prec@(1,5) (44.1%, 77.0%)
07/03 08:28:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][150/391]	Step 10166	Loss 2.0426	Prec@(1,5) (44.6%, 77.0%)
07/03 08:28:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][200/391]	Step 10166	Loss 2.0516	Prec@(1,5) (44.6%, 76.6%)
07/03 08:28:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][250/391]	Step 10166	Loss 2.0480	Prec@(1,5) (44.7%, 76.8%)
07/03 08:28:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][300/391]	Step 10166	Loss 2.0480	Prec@(1,5) (44.8%, 76.8%)
07/03 08:28:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][350/391]	Step 10166	Loss 2.0455	Prec@(1,5) (44.8%, 76.8%)
07/03 08:28:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][390/391]	Step 10166	Loss 2.0476	Prec@(1,5) (44.7%, 76.9%)
07/03 08:28:34午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 12/99] Final Prec@1 44.7160%
07/03 08:28:35午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 44.7160%
07/03 08:28:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][50/781]	Step 10216	lr 0.02401	Loss 2.7779 (2.7508)	Prec@(1,5) (46.6%, 77.7%)	
07/03 08:29:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][100/781]	Step 10266	lr 0.02401	Loss 2.5254 (2.6896)	Prec@(1,5) (47.0%, 78.4%)	
07/03 08:29:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][150/781]	Step 10316	lr 0.02401	Loss 3.0742 (2.7118)	Prec@(1,5) (46.6%, 78.1%)	
07/03 08:29:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][200/781]	Step 10366	lr 0.02401	Loss 2.9780 (2.7230)	Prec@(1,5) (46.3%, 78.0%)	
07/03 08:29:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][250/781]	Step 10416	lr 0.02401	Loss 2.9120 (2.7170)	Prec@(1,5) (46.5%, 78.0%)	
07/03 08:30:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][300/781]	Step 10466	lr 0.02401	Loss 2.6410 (2.7245)	Prec@(1,5) (46.2%, 77.8%)	
07/03 08:30:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][350/781]	Step 10516	lr 0.02401	Loss 2.8546 (2.7263)	Prec@(1,5) (46.0%, 77.9%)	
07/03 08:30:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][400/781]	Step 10566	lr 0.02401	Loss 2.2925 (2.7306)	Prec@(1,5) (45.9%, 77.9%)	
07/03 08:30:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][450/781]	Step 10616	lr 0.02401	Loss 3.0841 (2.7247)	Prec@(1,5) (46.1%, 78.0%)	
07/03 08:31:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][500/781]	Step 10666	lr 0.02401	Loss 2.8218 (2.7234)	Prec@(1,5) (46.1%, 78.0%)	
07/03 08:31:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][550/781]	Step 10716	lr 0.02401	Loss 2.8134 (2.7297)	Prec@(1,5) (46.0%, 77.9%)	
07/03 08:31:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][600/781]	Step 10766	lr 0.02401	Loss 3.0692 (2.7304)	Prec@(1,5) (46.0%, 78.0%)	
07/03 08:31:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][650/781]	Step 10816	lr 0.02401	Loss 2.4097 (2.7314)	Prec@(1,5) (46.0%, 77.9%)	
07/03 08:32:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][700/781]	Step 10866	lr 0.02401	Loss 2.5372 (2.7326)	Prec@(1,5) (46.0%, 78.0%)	
07/03 08:32:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][750/781]	Step 10916	lr 0.02401	Loss 2.1895 (2.7337)	Prec@(1,5) (46.1%, 77.9%)	
07/03 08:32:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][781/781]	Step 10947	lr 0.02401	Loss 3.4804 (2.7315)	Prec@(1,5) (46.2%, 77.9%)	
07/03 08:32:27午後 evaluateCell_trainer.py:172 [INFO] Train: [ 13/99] Final Prec@1 46.1600%
07/03 08:32:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][50/391]	Step 10948	Loss 2.0096	Prec@(1,5) (44.7%, 77.7%)
07/03 08:32:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][100/391]	Step 10948	Loss 1.9950	Prec@(1,5) (45.8%, 77.8%)
07/03 08:32:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][150/391]	Step 10948	Loss 1.9704	Prec@(1,5) (46.0%, 78.5%)
07/03 08:32:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][200/391]	Step 10948	Loss 1.9584	Prec@(1,5) (46.3%, 78.7%)
07/03 08:32:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][250/391]	Step 10948	Loss 1.9345	Prec@(1,5) (46.8%, 79.0%)
07/03 08:32:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][300/391]	Step 10948	Loss 1.9330	Prec@(1,5) (46.9%, 79.2%)
07/03 08:32:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][350/391]	Step 10948	Loss 1.9311	Prec@(1,5) (47.2%, 79.2%)
07/03 08:32:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][390/391]	Step 10948	Loss 1.9308	Prec@(1,5) (47.1%, 79.2%)
07/03 08:32:59午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 13/99] Final Prec@1 47.1480%
07/03 08:33:00午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 47.1480%
07/03 08:33:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][50/781]	Step 10998	lr 0.02386	Loss 2.8298 (2.6857)	Prec@(1,5) (47.3%, 78.8%)	
07/03 08:33:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][100/781]	Step 11048	lr 0.02386	Loss 2.9307 (2.6612)	Prec@(1,5) (47.4%, 79.4%)	
07/03 08:33:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][150/781]	Step 11098	lr 0.02386	Loss 2.4428 (2.6250)	Prec@(1,5) (48.2%, 79.6%)	
07/03 08:34:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][200/781]	Step 11148	lr 0.02386	Loss 2.7071 (2.6295)	Prec@(1,5) (48.0%, 79.5%)	
07/03 08:34:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][250/781]	Step 11198	lr 0.02386	Loss 2.6614 (2.6382)	Prec@(1,5) (47.8%, 79.3%)	
07/03 08:34:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][300/781]	Step 11248	lr 0.02386	Loss 2.7024 (2.6531)	Prec@(1,5) (47.6%, 79.1%)	
07/03 08:34:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][350/781]	Step 11298	lr 0.02386	Loss 2.5450 (2.6649)	Prec@(1,5) (47.3%, 78.9%)	
07/03 08:34:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][400/781]	Step 11348	lr 0.02386	Loss 2.6073 (2.6540)	Prec@(1,5) (47.5%, 79.1%)	
07/03 08:35:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][450/781]	Step 11398	lr 0.02386	Loss 3.1635 (2.6606)	Prec@(1,5) (47.5%, 78.9%)	
07/03 08:35:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][500/781]	Step 11448	lr 0.02386	Loss 2.7516 (2.6604)	Prec@(1,5) (47.5%, 79.0%)	
07/03 08:35:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][550/781]	Step 11498	lr 0.02386	Loss 2.6879 (2.6640)	Prec@(1,5) (47.4%, 78.9%)	
07/03 08:35:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][600/781]	Step 11548	lr 0.02386	Loss 2.3191 (2.6580)	Prec@(1,5) (47.5%, 79.0%)	
07/03 08:36:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][650/781]	Step 11598	lr 0.02386	Loss 2.6474 (2.6563)	Prec@(1,5) (47.4%, 79.0%)	
07/03 08:36:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][700/781]	Step 11648	lr 0.02386	Loss 2.5202 (2.6553)	Prec@(1,5) (47.5%, 79.1%)	
07/03 08:36:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][750/781]	Step 11698	lr 0.02386	Loss 2.7404 (2.6540)	Prec@(1,5) (47.5%, 79.1%)	
07/03 08:36:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][781/781]	Step 11729	lr 0.02386	Loss 2.3755 (2.6537)	Prec@(1,5) (47.5%, 79.0%)	
07/03 08:36:52午後 evaluateCell_trainer.py:172 [INFO] Train: [ 14/99] Final Prec@1 47.4900%
07/03 08:36:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][50/391]	Step 11730	Loss 2.2074	Prec@(1,5) (42.7%, 75.5%)
07/03 08:37:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][100/391]	Step 11730	Loss 2.2343	Prec@(1,5) (41.8%, 75.0%)
07/03 08:37:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][150/391]	Step 11730	Loss 2.2021	Prec@(1,5) (42.3%, 75.4%)
07/03 08:37:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][200/391]	Step 11730	Loss 2.2125	Prec@(1,5) (42.0%, 75.1%)
07/03 08:37:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][250/391]	Step 11730	Loss 2.2211	Prec@(1,5) (41.8%, 75.0%)
07/03 08:37:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][300/391]	Step 11730	Loss 2.2124	Prec@(1,5) (42.0%, 75.2%)
07/03 08:37:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][350/391]	Step 11730	Loss 2.2050	Prec@(1,5) (42.2%, 75.2%)
07/03 08:37:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][390/391]	Step 11730	Loss 2.2016	Prec@(1,5) (42.3%, 75.2%)
07/03 08:37:25午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 14/99] Final Prec@1 42.2560%
07/03 08:37:25午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 47.1480%
07/03 08:37:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][50/781]	Step 11780	lr 0.02369	Loss 2.5607 (2.5903)	Prec@(1,5) (47.9%, 80.4%)	
07/03 08:37:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][100/781]	Step 11830	lr 0.02369	Loss 2.2912 (2.5308)	Prec@(1,5) (49.0%, 80.8%)	
07/03 08:38:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][150/781]	Step 11880	lr 0.02369	Loss 2.2742 (2.5502)	Prec@(1,5) (48.7%, 80.4%)	
07/03 08:38:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][200/781]	Step 11930	lr 0.02369	Loss 2.5691 (2.5587)	Prec@(1,5) (48.9%, 80.0%)	
07/03 08:38:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][250/781]	Step 11980	lr 0.02369	Loss 3.2163 (2.5498)	Prec@(1,5) (48.9%, 80.1%)	
07/03 08:38:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][300/781]	Step 12030	lr 0.02369	Loss 1.9196 (2.5542)	Prec@(1,5) (48.8%, 80.0%)	
07/03 08:39:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][350/781]	Step 12080	lr 0.02369	Loss 2.8043 (2.5535)	Prec@(1,5) (48.8%, 80.2%)	
07/03 08:39:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][400/781]	Step 12130	lr 0.02369	Loss 2.2190 (2.5605)	Prec@(1,5) (48.5%, 80.2%)	
07/03 08:39:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][450/781]	Step 12180	lr 0.02369	Loss 2.2789 (2.5691)	Prec@(1,5) (48.4%, 80.1%)	
07/03 08:39:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][500/781]	Step 12230	lr 0.02369	Loss 2.5646 (2.5718)	Prec@(1,5) (48.3%, 80.1%)	
07/03 08:40:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][550/781]	Step 12280	lr 0.02369	Loss 2.5735 (2.5745)	Prec@(1,5) (48.2%, 80.0%)	
07/03 08:40:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][600/781]	Step 12330	lr 0.02369	Loss 2.4960 (2.5734)	Prec@(1,5) (48.4%, 80.0%)	
07/03 08:40:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][650/781]	Step 12380	lr 0.02369	Loss 2.5508 (2.5745)	Prec@(1,5) (48.5%, 80.0%)	
07/03 08:40:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][700/781]	Step 12430	lr 0.02369	Loss 2.3445 (2.5693)	Prec@(1,5) (48.6%, 80.1%)	
07/03 08:41:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][750/781]	Step 12480	lr 0.02369	Loss 2.9639 (2.5709)	Prec@(1,5) (48.5%, 80.1%)	
07/03 08:41:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][781/781]	Step 12511	lr 0.02369	Loss 3.4602 (2.5721)	Prec@(1,5) (48.6%, 80.2%)	
07/03 08:41:17午後 evaluateCell_trainer.py:172 [INFO] Train: [ 15/99] Final Prec@1 48.6240%
07/03 08:41:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][50/391]	Step 12512	Loss 2.3551	Prec@(1,5) (40.6%, 69.7%)
07/03 08:41:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][100/391]	Step 12512	Loss 2.3235	Prec@(1,5) (40.5%, 70.6%)
07/03 08:41:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][150/391]	Step 12512	Loss 2.3000	Prec@(1,5) (40.8%, 71.0%)
07/03 08:41:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][200/391]	Step 12512	Loss 2.2934	Prec@(1,5) (40.8%, 70.9%)
07/03 08:41:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][250/391]	Step 12512	Loss 2.2988	Prec@(1,5) (40.7%, 71.0%)
07/03 08:41:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][300/391]	Step 12512	Loss 2.2965	Prec@(1,5) (40.5%, 71.1%)
07/03 08:41:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][350/391]	Step 12512	Loss 2.3027	Prec@(1,5) (40.2%, 71.1%)
07/03 08:41:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][390/391]	Step 12512	Loss 2.3032	Prec@(1,5) (40.2%, 71.2%)
07/03 08:41:48午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 15/99] Final Prec@1 40.1840%
07/03 08:41:48午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 47.1480%
07/03 08:42:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][50/781]	Step 12562	lr 0.02352	Loss 2.7474 (2.4402)	Prec@(1,5) (50.9%, 82.0%)	
07/03 08:42:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][100/781]	Step 12612	lr 0.02352	Loss 2.3277 (2.4642)	Prec@(1,5) (50.2%, 81.9%)	
07/03 08:42:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][150/781]	Step 12662	lr 0.02352	Loss 2.5061 (2.4840)	Prec@(1,5) (50.2%, 81.6%)	
07/03 08:42:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][200/781]	Step 12712	lr 0.02352	Loss 2.5071 (2.4795)	Prec@(1,5) (50.2%, 81.6%)	
07/03 08:43:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][250/781]	Step 12762	lr 0.02352	Loss 2.7406 (2.4999)	Prec@(1,5) (49.9%, 81.2%)	
07/03 08:43:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][300/781]	Step 12812	lr 0.02352	Loss 2.7190 (2.5019)	Prec@(1,5) (49.9%, 81.3%)	
07/03 08:43:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][350/781]	Step 12862	lr 0.02352	Loss 2.0161 (2.4951)	Prec@(1,5) (50.1%, 81.3%)	
07/03 08:43:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][400/781]	Step 12912	lr 0.02352	Loss 3.1241 (2.4938)	Prec@(1,5) (50.2%, 81.3%)	
07/03 08:44:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][450/781]	Step 12962	lr 0.02352	Loss 2.2708 (2.4987)	Prec@(1,5) (50.1%, 81.2%)	
07/03 08:44:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][500/781]	Step 13012	lr 0.02352	Loss 2.8463 (2.5021)	Prec@(1,5) (50.0%, 81.1%)	
07/03 08:44:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][550/781]	Step 13062	lr 0.02352	Loss 2.3142 (2.5071)	Prec@(1,5) (49.9%, 81.0%)	
07/03 08:44:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][600/781]	Step 13112	lr 0.02352	Loss 2.3479 (2.5070)	Prec@(1,5) (49.8%, 81.0%)	
07/03 08:45:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][650/781]	Step 13162	lr 0.02352	Loss 2.2366 (2.5109)	Prec@(1,5) (49.8%, 80.9%)	
07/03 08:45:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][700/781]	Step 13212	lr 0.02352	Loss 2.6807 (2.5195)	Prec@(1,5) (49.6%, 80.8%)	
07/03 08:45:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][750/781]	Step 13262	lr 0.02352	Loss 2.6344 (2.5202)	Prec@(1,5) (49.6%, 80.8%)	
07/03 08:45:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][781/781]	Step 13293	lr 0.02352	Loss 2.2860 (2.5206)	Prec@(1,5) (49.6%, 80.8%)	
07/03 08:45:41午後 evaluateCell_trainer.py:172 [INFO] Train: [ 16/99] Final Prec@1 49.5780%
07/03 08:45:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][50/391]	Step 13294	Loss 1.8137	Prec@(1,5) (50.2%, 81.2%)
07/03 08:45:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][100/391]	Step 13294	Loss 1.7960	Prec@(1,5) (49.8%, 81.6%)
07/03 08:45:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][150/391]	Step 13294	Loss 1.8108	Prec@(1,5) (49.8%, 81.1%)
07/03 08:45:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][200/391]	Step 13294	Loss 1.8125	Prec@(1,5) (49.5%, 81.1%)
07/03 08:46:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][250/391]	Step 13294	Loss 1.8046	Prec@(1,5) (49.9%, 81.2%)
07/03 08:46:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][300/391]	Step 13294	Loss 1.8061	Prec@(1,5) (49.9%, 81.2%)
07/03 08:46:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][350/391]	Step 13294	Loss 1.8127	Prec@(1,5) (49.8%, 81.1%)
07/03 08:46:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][390/391]	Step 13294	Loss 1.8150	Prec@(1,5) (49.7%, 81.1%)
07/03 08:46:13午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 16/99] Final Prec@1 49.6600%
07/03 08:46:13午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 49.6600%
07/03 08:46:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][50/781]	Step 13344	lr 0.02333	Loss 2.6837 (2.4333)	Prec@(1,5) (51.8%, 81.3%)	
07/03 08:46:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][100/781]	Step 13394	lr 0.02333	Loss 2.1399 (2.3962)	Prec@(1,5) (51.8%, 82.1%)	
07/03 08:46:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][150/781]	Step 13444	lr 0.02333	Loss 2.5560 (2.3921)	Prec@(1,5) (51.5%, 82.4%)	
07/03 08:47:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][200/781]	Step 13494	lr 0.02333	Loss 2.7942 (2.3974)	Prec@(1,5) (51.8%, 82.3%)	
07/03 08:47:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][250/781]	Step 13544	lr 0.02333	Loss 2.6367 (2.4063)	Prec@(1,5) (51.5%, 82.1%)	
07/03 08:47:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][300/781]	Step 13594	lr 0.02333	Loss 2.0263 (2.4285)	Prec@(1,5) (51.3%, 82.1%)	
07/03 08:47:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][350/781]	Step 13644	lr 0.02333	Loss 2.1920 (2.4281)	Prec@(1,5) (51.3%, 82.0%)	
07/03 08:48:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][400/781]	Step 13694	lr 0.02333	Loss 2.4545 (2.4444)	Prec@(1,5) (50.9%, 81.7%)	
07/03 08:48:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][450/781]	Step 13744	lr 0.02333	Loss 2.3032 (2.4415)	Prec@(1,5) (51.1%, 81.6%)	
07/03 08:48:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][500/781]	Step 13794	lr 0.02333	Loss 2.1244 (2.4365)	Prec@(1,5) (51.2%, 81.7%)	
07/03 08:48:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][550/781]	Step 13844	lr 0.02333	Loss 2.2322 (2.4448)	Prec@(1,5) (51.0%, 81.7%)	
07/03 08:49:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][600/781]	Step 13894	lr 0.02333	Loss 2.3366 (2.4447)	Prec@(1,5) (51.0%, 81.6%)	
07/03 08:49:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][650/781]	Step 13944	lr 0.02333	Loss 2.2518 (2.4440)	Prec@(1,5) (51.1%, 81.6%)	
07/03 08:49:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][700/781]	Step 13994	lr 0.02333	Loss 2.2816 (2.4460)	Prec@(1,5) (51.1%, 81.6%)	
07/03 08:49:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][750/781]	Step 14044	lr 0.02333	Loss 2.1977 (2.4538)	Prec@(1,5) (51.0%, 81.5%)	
07/03 08:50:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][781/781]	Step 14075	lr 0.02333	Loss 2.3582 (2.4534)	Prec@(1,5) (51.0%, 81.5%)	
07/03 08:50:06午後 evaluateCell_trainer.py:172 [INFO] Train: [ 17/99] Final Prec@1 50.9620%
07/03 08:50:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][50/391]	Step 14076	Loss 1.7805	Prec@(1,5) (49.9%, 81.4%)
07/03 08:50:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][100/391]	Step 14076	Loss 1.7433	Prec@(1,5) (50.9%, 82.4%)
07/03 08:50:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][150/391]	Step 14076	Loss 1.7594	Prec@(1,5) (51.1%, 82.1%)
07/03 08:50:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][200/391]	Step 14076	Loss 1.7556	Prec@(1,5) (51.3%, 82.2%)
07/03 08:50:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][250/391]	Step 14076	Loss 1.7589	Prec@(1,5) (51.4%, 82.2%)
07/03 08:50:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][300/391]	Step 14076	Loss 1.7576	Prec@(1,5) (51.3%, 82.2%)
07/03 08:50:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][350/391]	Step 14076	Loss 1.7575	Prec@(1,5) (51.4%, 82.1%)
07/03 08:50:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][390/391]	Step 14076	Loss 1.7576	Prec@(1,5) (51.4%, 82.1%)
07/03 08:50:39午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 17/99] Final Prec@1 51.4360%
07/03 08:50:39午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 51.4360%
07/03 08:50:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][50/781]	Step 14126	lr 0.02313	Loss 1.9387 (2.2833)	Prec@(1,5) (53.7%, 83.9%)	
07/03 08:51:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][100/781]	Step 14176	lr 0.02313	Loss 2.0350 (2.3113)	Prec@(1,5) (52.8%, 83.4%)	
07/03 08:51:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][150/781]	Step 14226	lr 0.02313	Loss 2.3212 (2.3552)	Prec@(1,5) (52.4%, 82.9%)	
07/03 08:51:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][200/781]	Step 14276	lr 0.02313	Loss 2.5867 (2.3766)	Prec@(1,5) (52.1%, 82.5%)	
07/03 08:51:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][250/781]	Step 14326	lr 0.02313	Loss 2.6520 (2.3849)	Prec@(1,5) (52.1%, 82.4%)	
07/03 08:52:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][300/781]	Step 14376	lr 0.02313	Loss 2.7413 (2.3878)	Prec@(1,5) (52.1%, 82.5%)	
07/03 08:52:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][350/781]	Step 14426	lr 0.02313	Loss 2.1683 (2.3876)	Prec@(1,5) (52.0%, 82.5%)	
07/03 08:52:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][400/781]	Step 14476	lr 0.02313	Loss 2.5402 (2.4009)	Prec@(1,5) (51.8%, 82.3%)	
07/03 08:52:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][450/781]	Step 14526	lr 0.02313	Loss 2.3816 (2.3954)	Prec@(1,5) (51.8%, 82.4%)	
07/03 08:53:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][500/781]	Step 14576	lr 0.02313	Loss 2.8469 (2.3995)	Prec@(1,5) (51.8%, 82.3%)	
07/03 08:53:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][550/781]	Step 14626	lr 0.02313	Loss 2.1881 (2.3921)	Prec@(1,5) (52.0%, 82.4%)	
07/03 08:53:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][600/781]	Step 14676	lr 0.02313	Loss 2.4346 (2.3930)	Prec@(1,5) (51.9%, 82.3%)	
07/03 08:53:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][650/781]	Step 14726	lr 0.02313	Loss 2.2269 (2.3971)	Prec@(1,5) (51.9%, 82.2%)	
07/03 08:54:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][700/781]	Step 14776	lr 0.02313	Loss 2.7303 (2.4003)	Prec@(1,5) (51.8%, 82.2%)	
07/03 08:54:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][750/781]	Step 14826	lr 0.02313	Loss 2.1565 (2.4027)	Prec@(1,5) (51.8%, 82.2%)	
07/03 08:54:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][781/781]	Step 14857	lr 0.02313	Loss 2.5631 (2.4038)	Prec@(1,5) (51.7%, 82.2%)	
07/03 08:54:32午後 evaluateCell_trainer.py:172 [INFO] Train: [ 18/99] Final Prec@1 51.7220%
07/03 08:54:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][50/391]	Step 14858	Loss 1.6926	Prec@(1,5) (52.8%, 83.3%)
07/03 08:54:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][100/391]	Step 14858	Loss 1.6682	Prec@(1,5) (52.7%, 84.1%)
07/03 08:54:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][150/391]	Step 14858	Loss 1.6753	Prec@(1,5) (52.7%, 84.2%)
07/03 08:54:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][200/391]	Step 14858	Loss 1.6781	Prec@(1,5) (52.8%, 84.0%)
07/03 08:54:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][250/391]	Step 14858	Loss 1.6792	Prec@(1,5) (52.8%, 83.9%)
07/03 08:54:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][300/391]	Step 14858	Loss 1.6823	Prec@(1,5) (52.9%, 83.8%)
07/03 08:55:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][350/391]	Step 14858	Loss 1.6911	Prec@(1,5) (52.8%, 83.6%)
07/03 08:55:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][390/391]	Step 14858	Loss 1.6948	Prec@(1,5) (52.8%, 83.5%)
07/03 08:55:03午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 18/99] Final Prec@1 52.8000%
07/03 08:55:03午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 52.8000%
07/03 08:55:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][50/781]	Step 14908	lr 0.02292	Loss 2.4507 (2.2318)	Prec@(1,5) (54.8%, 84.2%)	
07/03 08:55:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][100/781]	Step 14958	lr 0.02292	Loss 1.9899 (2.2605)	Prec@(1,5) (54.1%, 83.8%)	
07/03 08:55:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][150/781]	Step 15008	lr 0.02292	Loss 2.9006 (2.3023)	Prec@(1,5) (53.4%, 83.5%)	
07/03 08:56:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][200/781]	Step 15058	lr 0.02292	Loss 2.4341 (2.3317)	Prec@(1,5) (52.9%, 83.2%)	
07/03 08:56:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][250/781]	Step 15108	lr 0.02292	Loss 2.4017 (2.3316)	Prec@(1,5) (52.7%, 83.3%)	
07/03 08:56:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][300/781]	Step 15158	lr 0.02292	Loss 2.6591 (2.3314)	Prec@(1,5) (52.6%, 83.3%)	
07/03 08:56:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][350/781]	Step 15208	lr 0.02292	Loss 2.4539 (2.3286)	Prec@(1,5) (52.7%, 83.3%)	
07/03 08:57:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][400/781]	Step 15258	lr 0.02292	Loss 2.5263 (2.3343)	Prec@(1,5) (52.7%, 83.2%)	
07/03 08:57:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][450/781]	Step 15308	lr 0.02292	Loss 2.5232 (2.3413)	Prec@(1,5) (52.6%, 83.0%)	
07/03 08:57:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][500/781]	Step 15358	lr 0.02292	Loss 2.3702 (2.3527)	Prec@(1,5) (52.4%, 82.9%)	
07/03 08:57:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][550/781]	Step 15408	lr 0.02292	Loss 2.3803 (2.3470)	Prec@(1,5) (52.5%, 82.9%)	
07/03 08:58:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][600/781]	Step 15458	lr 0.02292	Loss 2.5180 (2.3454)	Prec@(1,5) (52.6%, 83.0%)	
07/03 08:58:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][650/781]	Step 15508	lr 0.02292	Loss 2.3830 (2.3436)	Prec@(1,5) (52.6%, 83.0%)	
07/03 08:58:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][700/781]	Step 15558	lr 0.02292	Loss 2.6701 (2.3455)	Prec@(1,5) (52.5%, 82.9%)	
07/03 08:58:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][750/781]	Step 15608	lr 0.02292	Loss 2.4008 (2.3490)	Prec@(1,5) (52.6%, 82.9%)	
07/03 08:58:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][781/781]	Step 15639	lr 0.02292	Loss 2.0578 (2.3499)	Prec@(1,5) (52.6%, 82.9%)	
07/03 08:58:56午後 evaluateCell_trainer.py:172 [INFO] Train: [ 19/99] Final Prec@1 52.5720%
07/03 08:59:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][50/391]	Step 15640	Loss 1.7746	Prec@(1,5) (51.4%, 81.6%)
07/03 08:59:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][100/391]	Step 15640	Loss 1.8068	Prec@(1,5) (50.2%, 80.4%)
07/03 08:59:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][150/391]	Step 15640	Loss 1.8043	Prec@(1,5) (50.5%, 80.8%)
07/03 08:59:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][200/391]	Step 15640	Loss 1.8023	Prec@(1,5) (50.3%, 80.8%)
07/03 08:59:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][250/391]	Step 15640	Loss 1.8001	Prec@(1,5) (50.2%, 80.8%)
07/03 08:59:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][300/391]	Step 15640	Loss 1.7981	Prec@(1,5) (50.3%, 80.9%)
07/03 08:59:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][350/391]	Step 15640	Loss 1.8007	Prec@(1,5) (50.4%, 81.0%)
07/03 08:59:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][390/391]	Step 15640	Loss 1.8040	Prec@(1,5) (50.3%, 81.0%)
07/03 08:59:29午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 19/99] Final Prec@1 50.2720%
07/03 08:59:29午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 52.8000%
07/03 08:59:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][50/781]	Step 15690	lr 0.02271	Loss 1.8651 (2.2687)	Prec@(1,5) (54.9%, 83.9%)	
07/03 08:59:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][100/781]	Step 15740	lr 0.02271	Loss 2.0974 (2.2560)	Prec@(1,5) (54.7%, 84.1%)	
07/03 09:00:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][150/781]	Step 15790	lr 0.02271	Loss 2.4618 (2.2779)	Prec@(1,5) (54.5%, 83.7%)	
07/03 09:00:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][200/781]	Step 15840	lr 0.02271	Loss 1.9276 (2.2842)	Prec@(1,5) (54.1%, 83.5%)	
07/03 09:00:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][250/781]	Step 15890	lr 0.02271	Loss 2.4755 (2.2888)	Prec@(1,5) (53.9%, 83.4%)	
07/03 09:00:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][300/781]	Step 15940	lr 0.02271	Loss 2.0508 (2.2937)	Prec@(1,5) (53.9%, 83.3%)	
07/03 09:01:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][350/781]	Step 15990	lr 0.02271	Loss 1.9593 (2.2960)	Prec@(1,5) (53.8%, 83.2%)	
07/03 09:01:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][400/781]	Step 16040	lr 0.02271	Loss 2.6864 (2.2995)	Prec@(1,5) (53.8%, 83.2%)	
07/03 09:01:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][450/781]	Step 16090	lr 0.02271	Loss 2.0874 (2.3085)	Prec@(1,5) (53.6%, 83.2%)	
07/03 09:01:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][500/781]	Step 16140	lr 0.02271	Loss 2.2948 (2.3097)	Prec@(1,5) (53.4%, 83.2%)	
07/03 09:02:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][550/781]	Step 16190	lr 0.02271	Loss 2.4524 (2.3047)	Prec@(1,5) (53.6%, 83.3%)	
07/03 09:02:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][600/781]	Step 16240	lr 0.02271	Loss 2.3678 (2.3132)	Prec@(1,5) (53.3%, 83.2%)	
07/03 09:02:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][650/781]	Step 16290	lr 0.02271	Loss 2.3352 (2.3145)	Prec@(1,5) (53.3%, 83.2%)	
07/03 09:02:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][700/781]	Step 16340	lr 0.02271	Loss 2.1767 (2.3136)	Prec@(1,5) (53.3%, 83.3%)	
07/03 09:03:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][750/781]	Step 16390	lr 0.02271	Loss 2.6393 (2.3175)	Prec@(1,5) (53.1%, 83.2%)	
07/03 09:03:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][781/781]	Step 16421	lr 0.02271	Loss 2.6703 (2.3163)	Prec@(1,5) (53.1%, 83.2%)	
07/03 09:03:22午後 evaluateCell_trainer.py:172 [INFO] Train: [ 20/99] Final Prec@1 53.1180%
07/03 09:03:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][50/391]	Step 16422	Loss 1.7261	Prec@(1,5) (52.8%, 82.7%)
07/03 09:03:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][100/391]	Step 16422	Loss 1.7242	Prec@(1,5) (52.6%, 82.7%)
07/03 09:03:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][150/391]	Step 16422	Loss 1.7423	Prec@(1,5) (52.0%, 82.4%)
07/03 09:03:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][200/391]	Step 16422	Loss 1.7294	Prec@(1,5) (52.5%, 82.6%)
07/03 09:03:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][250/391]	Step 16422	Loss 1.7271	Prec@(1,5) (52.6%, 82.7%)
07/03 09:03:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][300/391]	Step 16422	Loss 1.7240	Prec@(1,5) (52.6%, 82.8%)
07/03 09:03:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][350/391]	Step 16422	Loss 1.7251	Prec@(1,5) (52.4%, 82.8%)
07/03 09:03:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][390/391]	Step 16422	Loss 1.7238	Prec@(1,5) (52.5%, 82.7%)
07/03 09:03:55午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 20/99] Final Prec@1 52.4600%
07/03 09:03:55午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 52.8000%
07/03 09:04:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][50/781]	Step 16472	lr 0.02248	Loss 2.6314 (2.2003)	Prec@(1,5) (55.9%, 84.5%)	
07/03 09:04:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][100/781]	Step 16522	lr 0.02248	Loss 2.4695 (2.2051)	Prec@(1,5) (55.2%, 84.6%)	
07/03 09:04:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][150/781]	Step 16572	lr 0.02248	Loss 2.3891 (2.2077)	Prec@(1,5) (55.1%, 84.6%)	
07/03 09:04:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][200/781]	Step 16622	lr 0.02248	Loss 2.5678 (2.2188)	Prec@(1,5) (54.9%, 84.4%)	
07/03 09:05:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][250/781]	Step 16672	lr 0.02248	Loss 2.4798 (2.2414)	Prec@(1,5) (54.5%, 84.1%)	
07/03 09:05:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][300/781]	Step 16722	lr 0.02248	Loss 2.0449 (2.2399)	Prec@(1,5) (54.4%, 84.1%)	
07/03 09:05:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][350/781]	Step 16772	lr 0.02248	Loss 2.4907 (2.2406)	Prec@(1,5) (54.6%, 84.0%)	
07/03 09:05:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][400/781]	Step 16822	lr 0.02248	Loss 2.4168 (2.2475)	Prec@(1,5) (54.5%, 83.9%)	
07/03 09:06:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][450/781]	Step 16872	lr 0.02248	Loss 2.5916 (2.2547)	Prec@(1,5) (54.4%, 83.8%)	
07/03 09:06:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][500/781]	Step 16922	lr 0.02248	Loss 1.9593 (2.2560)	Prec@(1,5) (54.4%, 83.8%)	
07/03 09:06:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][550/781]	Step 16972	lr 0.02248	Loss 2.6277 (2.2539)	Prec@(1,5) (54.4%, 83.8%)	
07/03 09:06:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][600/781]	Step 17022	lr 0.02248	Loss 2.2174 (2.2590)	Prec@(1,5) (54.3%, 83.8%)	
07/03 09:07:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][650/781]	Step 17072	lr 0.02248	Loss 2.3662 (2.2647)	Prec@(1,5) (54.1%, 83.8%)	
07/03 09:07:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][700/781]	Step 17122	lr 0.02248	Loss 1.9771 (2.2708)	Prec@(1,5) (54.0%, 83.7%)	
07/03 09:07:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][750/781]	Step 17172	lr 0.02248	Loss 2.4056 (2.2728)	Prec@(1,5) (54.0%, 83.7%)	
07/03 09:07:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][781/781]	Step 17203	lr 0.02248	Loss 2.0665 (2.2765)	Prec@(1,5) (53.9%, 83.7%)	
07/03 09:07:48午後 evaluateCell_trainer.py:172 [INFO] Train: [ 21/99] Final Prec@1 53.9360%
07/03 09:07:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][50/391]	Step 17204	Loss 1.6177	Prec@(1,5) (54.0%, 84.5%)
07/03 09:07:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][100/391]	Step 17204	Loss 1.6405	Prec@(1,5) (54.0%, 84.1%)
07/03 09:08:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][150/391]	Step 17204	Loss 1.6454	Prec@(1,5) (54.3%, 84.0%)
07/03 09:08:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][200/391]	Step 17204	Loss 1.6532	Prec@(1,5) (54.0%, 84.0%)
07/03 09:08:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][250/391]	Step 17204	Loss 1.6478	Prec@(1,5) (54.2%, 83.9%)
07/03 09:08:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][300/391]	Step 17204	Loss 1.6560	Prec@(1,5) (53.9%, 83.8%)
07/03 09:08:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][350/391]	Step 17204	Loss 1.6538	Prec@(1,5) (54.0%, 83.8%)
07/03 09:08:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][390/391]	Step 17204	Loss 1.6580	Prec@(1,5) (53.9%, 83.7%)
07/03 09:08:20午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 21/99] Final Prec@1 53.9440%
07/03 09:08:20午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 53.9440%
07/03 09:08:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][50/781]	Step 17254	lr 0.02225	Loss 2.2304 (2.0928)	Prec@(1,5) (57.9%, 86.1%)	
07/03 09:08:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][100/781]	Step 17304	lr 0.02225	Loss 2.3820 (2.1125)	Prec@(1,5) (57.2%, 85.9%)	
07/03 09:09:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][150/781]	Step 17354	lr 0.02225	Loss 2.2497 (2.1536)	Prec@(1,5) (56.6%, 85.4%)	
07/03 09:09:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][200/781]	Step 17404	lr 0.02225	Loss 2.0274 (2.1617)	Prec@(1,5) (56.1%, 85.2%)	
07/03 09:09:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][250/781]	Step 17454	lr 0.02225	Loss 2.1912 (2.1826)	Prec@(1,5) (55.8%, 85.0%)	
07/03 09:09:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][300/781]	Step 17504	lr 0.02225	Loss 1.8705 (2.1897)	Prec@(1,5) (55.5%, 84.9%)	
07/03 09:10:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][350/781]	Step 17554	lr 0.02225	Loss 2.5154 (2.1956)	Prec@(1,5) (55.4%, 84.8%)	
07/03 09:10:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][400/781]	Step 17604	lr 0.02225	Loss 2.2224 (2.1991)	Prec@(1,5) (55.4%, 84.8%)	
07/03 09:10:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][450/781]	Step 17654	lr 0.02225	Loss 2.2812 (2.1988)	Prec@(1,5) (55.4%, 84.8%)	
07/03 09:10:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][500/781]	Step 17704	lr 0.02225	Loss 2.3438 (2.2065)	Prec@(1,5) (55.2%, 84.8%)	
07/03 09:11:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][550/781]	Step 17754	lr 0.02225	Loss 2.1918 (2.2076)	Prec@(1,5) (55.2%, 84.7%)	
07/03 09:11:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][600/781]	Step 17804	lr 0.02225	Loss 2.3842 (2.2128)	Prec@(1,5) (55.1%, 84.6%)	
07/03 09:11:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][650/781]	Step 17854	lr 0.02225	Loss 2.0407 (2.2184)	Prec@(1,5) (55.2%, 84.6%)	
07/03 09:11:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][700/781]	Step 17904	lr 0.02225	Loss 2.3554 (2.2203)	Prec@(1,5) (55.1%, 84.5%)	
07/03 09:12:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][750/781]	Step 17954	lr 0.02225	Loss 2.2486 (2.2240)	Prec@(1,5) (55.1%, 84.5%)	
07/03 09:12:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][781/781]	Step 17985	lr 0.02225	Loss 2.1191 (2.2250)	Prec@(1,5) (55.0%, 84.4%)	
07/03 09:12:13午後 evaluateCell_trainer.py:172 [INFO] Train: [ 22/99] Final Prec@1 55.0460%
07/03 09:12:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][50/391]	Step 17986	Loss 1.8159	Prec@(1,5) (50.7%, 80.8%)
07/03 09:12:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][100/391]	Step 17986	Loss 1.7847	Prec@(1,5) (50.7%, 81.7%)
07/03 09:12:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][150/391]	Step 17986	Loss 1.7816	Prec@(1,5) (51.2%, 81.7%)
07/03 09:12:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][200/391]	Step 17986	Loss 1.7679	Prec@(1,5) (51.4%, 82.0%)
07/03 09:12:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][250/391]	Step 17986	Loss 1.7705	Prec@(1,5) (51.3%, 82.0%)
07/03 09:12:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][300/391]	Step 17986	Loss 1.7651	Prec@(1,5) (51.5%, 82.0%)
07/03 09:12:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][350/391]	Step 17986	Loss 1.7646	Prec@(1,5) (51.6%, 82.0%)
07/03 09:12:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][390/391]	Step 17986	Loss 1.7588	Prec@(1,5) (51.7%, 82.1%)
07/03 09:12:45午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 22/99] Final Prec@1 51.7480%
07/03 09:12:45午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 53.9440%
07/03 09:13:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][50/781]	Step 18036	lr 0.022	Loss 1.7424 (2.0600)	Prec@(1,5) (58.0%, 86.0%)	
07/03 09:13:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][100/781]	Step 18086	lr 0.022	Loss 2.1892 (2.1372)	Prec@(1,5) (56.8%, 85.3%)	
07/03 09:13:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][150/781]	Step 18136	lr 0.022	Loss 2.6455 (2.1425)	Prec@(1,5) (56.6%, 85.2%)	
07/03 09:13:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][200/781]	Step 18186	lr 0.022	Loss 1.8954 (2.1778)	Prec@(1,5) (55.9%, 84.9%)	
07/03 09:14:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][250/781]	Step 18236	lr 0.022	Loss 1.6229 (2.1690)	Prec@(1,5) (56.1%, 85.0%)	
07/03 09:14:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][300/781]	Step 18286	lr 0.022	Loss 2.2900 (2.1743)	Prec@(1,5) (55.9%, 85.0%)	
07/03 09:14:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][350/781]	Step 18336	lr 0.022	Loss 2.3860 (2.1685)	Prec@(1,5) (56.0%, 85.1%)	
07/03 09:14:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][400/781]	Step 18386	lr 0.022	Loss 2.6105 (2.1757)	Prec@(1,5) (56.0%, 85.0%)	
07/03 09:15:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][450/781]	Step 18436	lr 0.022	Loss 1.8226 (2.1774)	Prec@(1,5) (55.9%, 84.9%)	
07/03 09:15:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][500/781]	Step 18486	lr 0.022	Loss 1.9919 (2.1739)	Prec@(1,5) (55.9%, 84.9%)	
07/03 09:15:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][550/781]	Step 18536	lr 0.022	Loss 2.2631 (2.1780)	Prec@(1,5) (55.8%, 84.9%)	
07/03 09:15:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][600/781]	Step 18586	lr 0.022	Loss 1.7388 (2.1756)	Prec@(1,5) (55.8%, 85.0%)	
07/03 09:15:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][650/781]	Step 18636	lr 0.022	Loss 2.1812 (2.1797)	Prec@(1,5) (55.8%, 84.9%)	
07/03 09:16:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][700/781]	Step 18686	lr 0.022	Loss 2.4796 (2.1832)	Prec@(1,5) (55.7%, 84.9%)	
07/03 09:16:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][750/781]	Step 18736	lr 0.022	Loss 1.7461 (2.1840)	Prec@(1,5) (55.7%, 84.9%)	
07/03 09:16:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][781/781]	Step 18767	lr 0.022	Loss 2.4526 (2.1875)	Prec@(1,5) (55.6%, 84.9%)	
07/03 09:16:38午後 evaluateCell_trainer.py:172 [INFO] Train: [ 23/99] Final Prec@1 55.6220%
07/03 09:16:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][50/391]	Step 18768	Loss 1.7772	Prec@(1,5) (50.4%, 82.0%)
07/03 09:16:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][100/391]	Step 18768	Loss 1.7613	Prec@(1,5) (51.0%, 82.4%)
07/03 09:16:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][150/391]	Step 18768	Loss 1.7623	Prec@(1,5) (51.2%, 82.3%)
07/03 09:16:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][200/391]	Step 18768	Loss 1.7626	Prec@(1,5) (51.2%, 82.3%)
07/03 09:16:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][250/391]	Step 18768	Loss 1.7750	Prec@(1,5) (50.8%, 82.1%)
07/03 09:17:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][300/391]	Step 18768	Loss 1.7744	Prec@(1,5) (50.8%, 82.1%)
07/03 09:17:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][350/391]	Step 18768	Loss 1.7747	Prec@(1,5) (50.8%, 82.1%)
07/03 09:17:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][390/391]	Step 18768	Loss 1.7747	Prec@(1,5) (50.9%, 82.1%)
07/03 09:17:11午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 23/99] Final Prec@1 50.8640%
07/03 09:17:11午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 53.9440%
07/03 09:17:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][50/781]	Step 18818	lr 0.02175	Loss 1.8815 (2.0794)	Prec@(1,5) (57.9%, 86.3%)	
07/03 09:17:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][100/781]	Step 18868	lr 0.02175	Loss 2.1138 (2.1125)	Prec@(1,5) (56.5%, 85.8%)	
07/03 09:17:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][150/781]	Step 18918	lr 0.02175	Loss 1.8546 (2.0961)	Prec@(1,5) (57.0%, 86.0%)	
07/03 09:18:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][200/781]	Step 18968	lr 0.02175	Loss 1.8083 (2.1071)	Prec@(1,5) (56.8%, 85.9%)	
07/03 09:18:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][250/781]	Step 19018	lr 0.02175	Loss 2.9646 (2.1270)	Prec@(1,5) (56.5%, 85.7%)	
07/03 09:18:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][300/781]	Step 19068	lr 0.02175	Loss 2.0895 (2.1278)	Prec@(1,5) (56.4%, 85.7%)	
07/03 09:18:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][350/781]	Step 19118	lr 0.02175	Loss 3.0380 (2.1374)	Prec@(1,5) (56.2%, 85.5%)	
07/03 09:19:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][400/781]	Step 19168	lr 0.02175	Loss 2.0124 (2.1403)	Prec@(1,5) (56.1%, 85.5%)	
07/03 09:19:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][450/781]	Step 19218	lr 0.02175	Loss 2.2836 (2.1429)	Prec@(1,5) (56.0%, 85.5%)	
07/03 09:19:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][500/781]	Step 19268	lr 0.02175	Loss 2.4632 (2.1479)	Prec@(1,5) (56.0%, 85.3%)	
07/03 09:19:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][550/781]	Step 19318	lr 0.02175	Loss 1.7041 (2.1499)	Prec@(1,5) (55.9%, 85.4%)	
07/03 09:20:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][600/781]	Step 19368	lr 0.02175	Loss 2.3715 (2.1520)	Prec@(1,5) (55.9%, 85.3%)	
07/03 09:20:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][650/781]	Step 19418	lr 0.02175	Loss 2.1048 (2.1552)	Prec@(1,5) (55.9%, 85.3%)	
07/03 09:20:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][700/781]	Step 19468	lr 0.02175	Loss 2.3161 (2.1602)	Prec@(1,5) (55.7%, 85.3%)	
07/03 09:20:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][750/781]	Step 19518	lr 0.02175	Loss 2.1119 (2.1573)	Prec@(1,5) (55.9%, 85.3%)	
07/03 09:21:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][781/781]	Step 19549	lr 0.02175	Loss 1.8986 (2.1591)	Prec@(1,5) (55.9%, 85.2%)	
07/03 09:21:04午後 evaluateCell_trainer.py:172 [INFO] Train: [ 24/99] Final Prec@1 55.9140%
07/03 09:21:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][50/391]	Step 19550	Loss 1.7990	Prec@(1,5) (52.5%, 80.2%)
07/03 09:21:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][100/391]	Step 19550	Loss 1.8347	Prec@(1,5) (51.6%, 80.0%)
07/03 09:21:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][150/391]	Step 19550	Loss 1.8442	Prec@(1,5) (51.4%, 79.9%)
07/03 09:21:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][200/391]	Step 19550	Loss 1.8498	Prec@(1,5) (51.0%, 79.8%)
07/03 09:21:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][250/391]	Step 19550	Loss 1.8509	Prec@(1,5) (50.9%, 79.8%)
07/03 09:21:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][300/391]	Step 19550	Loss 1.8492	Prec@(1,5) (50.9%, 79.8%)
07/03 09:21:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][350/391]	Step 19550	Loss 1.8462	Prec@(1,5) (50.9%, 79.7%)
07/03 09:21:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][390/391]	Step 19550	Loss 1.8513	Prec@(1,5) (50.8%, 79.8%)
07/03 09:21:37午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 24/99] Final Prec@1 50.8120%
07/03 09:21:37午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 53.9440%
07/03 09:21:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][50/781]	Step 19600	lr 0.02149	Loss 2.5258 (2.1360)	Prec@(1,5) (56.4%, 85.2%)	
07/03 09:22:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][100/781]	Step 19650	lr 0.02149	Loss 1.8393 (2.1152)	Prec@(1,5) (57.2%, 85.3%)	
07/03 09:22:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][150/781]	Step 19700	lr 0.02149	Loss 1.9600 (2.1111)	Prec@(1,5) (57.4%, 85.7%)	
07/03 09:22:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][200/781]	Step 19750	lr 0.02149	Loss 2.1280 (2.1326)	Prec@(1,5) (56.7%, 85.3%)	
07/03 09:22:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][250/781]	Step 19800	lr 0.02149	Loss 2.7101 (2.1395)	Prec@(1,5) (56.5%, 85.4%)	
07/03 09:23:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][300/781]	Step 19850	lr 0.02149	Loss 2.0055 (2.1460)	Prec@(1,5) (56.3%, 85.4%)	
07/03 09:23:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][350/781]	Step 19900	lr 0.02149	Loss 2.0031 (2.1357)	Prec@(1,5) (56.7%, 85.5%)	
07/03 09:23:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][400/781]	Step 19950	lr 0.02149	Loss 2.2392 (2.1277)	Prec@(1,5) (56.8%, 85.7%)	
07/03 09:23:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][450/781]	Step 20000	lr 0.02149	Loss 1.9258 (2.1304)	Prec@(1,5) (56.7%, 85.6%)	
07/03 09:24:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][500/781]	Step 20050	lr 0.02149	Loss 1.9425 (2.1325)	Prec@(1,5) (56.7%, 85.7%)	
07/03 09:24:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][550/781]	Step 20100	lr 0.02149	Loss 1.9307 (2.1332)	Prec@(1,5) (56.7%, 85.6%)	
07/03 09:24:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][600/781]	Step 20150	lr 0.02149	Loss 2.0551 (2.1342)	Prec@(1,5) (56.7%, 85.6%)	
07/03 09:24:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][650/781]	Step 20200	lr 0.02149	Loss 2.4180 (2.1364)	Prec@(1,5) (56.7%, 85.6%)	
07/03 09:25:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][700/781]	Step 20250	lr 0.02149	Loss 2.5857 (2.1377)	Prec@(1,5) (56.7%, 85.6%)	
07/03 09:25:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][750/781]	Step 20300	lr 0.02149	Loss 2.1613 (2.1396)	Prec@(1,5) (56.6%, 85.6%)	
07/03 09:25:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][781/781]	Step 20331	lr 0.02149	Loss 1.9883 (2.1359)	Prec@(1,5) (56.7%, 85.6%)	
07/03 09:25:29午後 evaluateCell_trainer.py:172 [INFO] Train: [ 25/99] Final Prec@1 56.6940%
07/03 09:25:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][50/391]	Step 20332	Loss 1.6771	Prec@(1,5) (52.7%, 83.7%)
07/03 09:25:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][100/391]	Step 20332	Loss 1.6729	Prec@(1,5) (53.0%, 83.5%)
07/03 09:25:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][150/391]	Step 20332	Loss 1.6615	Prec@(1,5) (53.5%, 83.7%)
07/03 09:25:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][200/391]	Step 20332	Loss 1.6524	Prec@(1,5) (53.8%, 84.0%)
07/03 09:25:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][250/391]	Step 20332	Loss 1.6397	Prec@(1,5) (54.2%, 84.1%)
07/03 09:25:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][300/391]	Step 20332	Loss 1.6473	Prec@(1,5) (54.0%, 84.0%)
07/03 09:25:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][350/391]	Step 20332	Loss 1.6477	Prec@(1,5) (53.9%, 84.0%)
07/03 09:26:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][390/391]	Step 20332	Loss 1.6433	Prec@(1,5) (54.1%, 84.0%)
07/03 09:26:02午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 25/99] Final Prec@1 54.1240%
07/03 09:26:02午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 54.1240%
07/03 09:26:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][50/781]	Step 20382	lr 0.02121	Loss 2.2700 (2.0955)	Prec@(1,5) (56.7%, 86.5%)	
07/03 09:26:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][100/781]	Step 20432	lr 0.02121	Loss 2.1925 (2.0783)	Prec@(1,5) (57.2%, 86.6%)	
07/03 09:26:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][150/781]	Step 20482	lr 0.02121	Loss 2.2278 (2.0378)	Prec@(1,5) (58.1%, 87.0%)	
07/03 09:27:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][200/781]	Step 20532	lr 0.02121	Loss 1.7226 (2.0430)	Prec@(1,5) (57.8%, 86.8%)	
07/03 09:27:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][250/781]	Step 20582	lr 0.02121	Loss 2.1090 (2.0675)	Prec@(1,5) (57.4%, 86.6%)	
07/03 09:27:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][300/781]	Step 20632	lr 0.02121	Loss 1.8000 (2.0708)	Prec@(1,5) (57.5%, 86.4%)	
07/03 09:27:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][350/781]	Step 20682	lr 0.02121	Loss 1.6130 (2.0752)	Prec@(1,5) (57.4%, 86.3%)	
07/03 09:28:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][400/781]	Step 20732	lr 0.02121	Loss 2.3346 (2.0755)	Prec@(1,5) (57.5%, 86.2%)	
07/03 09:28:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][450/781]	Step 20782	lr 0.02121	Loss 1.5850 (2.0831)	Prec@(1,5) (57.4%, 86.1%)	
07/03 09:28:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][500/781]	Step 20832	lr 0.02121	Loss 2.3571 (2.0888)	Prec@(1,5) (57.3%, 86.1%)	
07/03 09:28:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][550/781]	Step 20882	lr 0.02121	Loss 2.4598 (2.0919)	Prec@(1,5) (57.1%, 86.0%)	
07/03 09:29:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][600/781]	Step 20932	lr 0.02121	Loss 2.1979 (2.0920)	Prec@(1,5) (57.2%, 86.0%)	
07/03 09:29:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][650/781]	Step 20982	lr 0.02121	Loss 2.2675 (2.0970)	Prec@(1,5) (57.2%, 86.0%)	
07/03 09:29:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][700/781]	Step 21032	lr 0.02121	Loss 2.9645 (2.0994)	Prec@(1,5) (57.1%, 85.9%)	
07/03 09:29:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][750/781]	Step 21082	lr 0.02121	Loss 1.8405 (2.1010)	Prec@(1,5) (57.1%, 85.9%)	
07/03 09:29:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][781/781]	Step 21113	lr 0.02121	Loss 1.9507 (2.1003)	Prec@(1,5) (57.1%, 85.9%)	
07/03 09:29:55午後 evaluateCell_trainer.py:172 [INFO] Train: [ 26/99] Final Prec@1 57.1020%
07/03 09:29:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][50/391]	Step 21114	Loss 1.5479	Prec@(1,5) (56.7%, 84.5%)
07/03 09:30:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][100/391]	Step 21114	Loss 1.5473	Prec@(1,5) (57.1%, 84.8%)
07/03 09:30:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][150/391]	Step 21114	Loss 1.5628	Prec@(1,5) (56.8%, 84.6%)
07/03 09:30:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][200/391]	Step 21114	Loss 1.5525	Prec@(1,5) (56.9%, 84.9%)
07/03 09:30:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][250/391]	Step 21114	Loss 1.5447	Prec@(1,5) (57.0%, 85.0%)
07/03 09:30:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][300/391]	Step 21114	Loss 1.5409	Prec@(1,5) (57.2%, 85.0%)
07/03 09:30:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][350/391]	Step 21114	Loss 1.5503	Prec@(1,5) (56.8%, 84.9%)
07/03 09:30:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][390/391]	Step 21114	Loss 1.5467	Prec@(1,5) (57.0%, 84.9%)
07/03 09:30:28午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 26/99] Final Prec@1 56.9760%
07/03 09:30:28午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 56.9760%
07/03 09:30:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][50/781]	Step 21164	lr 0.02094	Loss 2.3102 (2.0710)	Prec@(1,5) (56.1%, 86.8%)	
07/03 09:30:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][100/781]	Step 21214	lr 0.02094	Loss 1.7488 (2.0155)	Prec@(1,5) (58.1%, 87.5%)	
07/03 09:31:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][150/781]	Step 21264	lr 0.02094	Loss 2.4797 (2.0143)	Prec@(1,5) (58.5%, 87.3%)	
07/03 09:31:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][200/781]	Step 21314	lr 0.02094	Loss 2.2932 (2.0396)	Prec@(1,5) (58.0%, 86.9%)	
07/03 09:31:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][250/781]	Step 21364	lr 0.02094	Loss 1.8514 (2.0386)	Prec@(1,5) (58.1%, 86.7%)	
07/03 09:31:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][300/781]	Step 21414	lr 0.02094	Loss 2.2327 (2.0493)	Prec@(1,5) (57.9%, 86.5%)	
07/03 09:32:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][350/781]	Step 21464	lr 0.02094	Loss 1.5725 (2.0543)	Prec@(1,5) (57.8%, 86.4%)	
07/03 09:32:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][400/781]	Step 21514	lr 0.02094	Loss 1.7884 (2.0460)	Prec@(1,5) (58.1%, 86.5%)	
07/03 09:32:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][450/781]	Step 21564	lr 0.02094	Loss 2.0832 (2.0448)	Prec@(1,5) (58.1%, 86.5%)	
07/03 09:32:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][500/781]	Step 21614	lr 0.02094	Loss 1.9323 (2.0488)	Prec@(1,5) (58.1%, 86.4%)	
07/03 09:33:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][550/781]	Step 21664	lr 0.02094	Loss 1.6194 (2.0515)	Prec@(1,5) (58.0%, 86.4%)	
07/03 09:33:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][600/781]	Step 21714	lr 0.02094	Loss 2.7694 (2.0551)	Prec@(1,5) (57.9%, 86.4%)	
07/03 09:33:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][650/781]	Step 21764	lr 0.02094	Loss 1.9732 (2.0594)	Prec@(1,5) (57.8%, 86.3%)	
07/03 09:33:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][700/781]	Step 21814	lr 0.02094	Loss 1.5342 (2.0612)	Prec@(1,5) (57.8%, 86.3%)	
07/03 09:34:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][750/781]	Step 21864	lr 0.02094	Loss 1.8397 (2.0601)	Prec@(1,5) (57.9%, 86.3%)	
07/03 09:34:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][781/781]	Step 21895	lr 0.02094	Loss 1.9381 (2.0592)	Prec@(1,5) (57.9%, 86.3%)	
07/03 09:34:21午後 evaluateCell_trainer.py:172 [INFO] Train: [ 27/99] Final Prec@1 57.8880%
07/03 09:34:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][50/391]	Step 21896	Loss 1.5514	Prec@(1,5) (55.4%, 86.2%)
07/03 09:34:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][100/391]	Step 21896	Loss 1.5416	Prec@(1,5) (56.1%, 85.7%)
07/03 09:34:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][150/391]	Step 21896	Loss 1.5443	Prec@(1,5) (56.2%, 85.5%)
07/03 09:34:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][200/391]	Step 21896	Loss 1.5490	Prec@(1,5) (56.2%, 85.4%)
07/03 09:34:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][250/391]	Step 21896	Loss 1.5376	Prec@(1,5) (56.5%, 85.6%)
07/03 09:34:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][300/391]	Step 21896	Loss 1.5265	Prec@(1,5) (56.8%, 85.9%)
07/03 09:34:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][350/391]	Step 21896	Loss 1.5330	Prec@(1,5) (56.6%, 85.8%)
07/03 09:34:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][390/391]	Step 21896	Loss 1.5410	Prec@(1,5) (56.4%, 85.8%)
07/03 09:34:54午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 27/99] Final Prec@1 56.4000%
07/03 09:34:54午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 56.9760%
07/03 09:35:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][50/781]	Step 21946	lr 0.02065	Loss 2.1033 (2.0654)	Prec@(1,5) (57.7%, 86.3%)	
07/03 09:35:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][100/781]	Step 21996	lr 0.02065	Loss 2.0143 (2.0256)	Prec@(1,5) (59.0%, 86.7%)	
07/03 09:35:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][150/781]	Step 22046	lr 0.02065	Loss 2.4405 (2.0140)	Prec@(1,5) (58.9%, 86.6%)	
07/03 09:35:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][200/781]	Step 22096	lr 0.02065	Loss 2.2132 (2.0036)	Prec@(1,5) (59.0%, 86.9%)	
07/03 09:36:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][250/781]	Step 22146	lr 0.02065	Loss 2.2795 (1.9953)	Prec@(1,5) (59.0%, 87.1%)	
07/03 09:36:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][300/781]	Step 22196	lr 0.02065	Loss 2.2134 (1.9955)	Prec@(1,5) (58.9%, 87.1%)	
07/03 09:36:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][350/781]	Step 22246	lr 0.02065	Loss 1.8392 (1.9961)	Prec@(1,5) (58.8%, 87.1%)	
07/03 09:36:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][400/781]	Step 22296	lr 0.02065	Loss 1.5366 (2.0002)	Prec@(1,5) (58.8%, 87.1%)	
07/03 09:37:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][450/781]	Step 22346	lr 0.02065	Loss 1.7237 (2.0099)	Prec@(1,5) (58.7%, 87.0%)	
07/03 09:37:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][500/781]	Step 22396	lr 0.02065	Loss 2.3091 (2.0162)	Prec@(1,5) (58.5%, 86.9%)	
07/03 09:37:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][550/781]	Step 22446	lr 0.02065	Loss 1.6380 (2.0187)	Prec@(1,5) (58.5%, 86.9%)	
07/03 09:37:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][600/781]	Step 22496	lr 0.02065	Loss 2.4362 (2.0185)	Prec@(1,5) (58.4%, 86.8%)	
07/03 09:38:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][650/781]	Step 22546	lr 0.02065	Loss 1.8382 (2.0225)	Prec@(1,5) (58.4%, 86.7%)	
07/03 09:38:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][700/781]	Step 22596	lr 0.02065	Loss 2.3737 (2.0294)	Prec@(1,5) (58.3%, 86.7%)	
07/03 09:38:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][750/781]	Step 22646	lr 0.02065	Loss 2.4153 (2.0333)	Prec@(1,5) (58.2%, 86.6%)	
07/03 09:38:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][781/781]	Step 22677	lr 0.02065	Loss 2.0731 (2.0309)	Prec@(1,5) (58.3%, 86.6%)	
07/03 09:38:47午後 evaluateCell_trainer.py:172 [INFO] Train: [ 28/99] Final Prec@1 58.2940%
07/03 09:38:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][50/391]	Step 22678	Loss 1.5497	Prec@(1,5) (56.6%, 85.8%)
07/03 09:38:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][100/391]	Step 22678	Loss 1.5300	Prec@(1,5) (57.0%, 86.2%)
07/03 09:38:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][150/391]	Step 22678	Loss 1.5181	Prec@(1,5) (56.9%, 86.4%)
07/03 09:39:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][200/391]	Step 22678	Loss 1.5209	Prec@(1,5) (57.0%, 86.2%)
07/03 09:39:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][250/391]	Step 22678	Loss 1.5168	Prec@(1,5) (57.1%, 86.2%)
07/03 09:39:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][300/391]	Step 22678	Loss 1.5213	Prec@(1,5) (57.0%, 86.1%)
07/03 09:39:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][350/391]	Step 22678	Loss 1.5255	Prec@(1,5) (57.0%, 86.2%)
07/03 09:39:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][390/391]	Step 22678	Loss 1.5238	Prec@(1,5) (57.1%, 86.2%)
07/03 09:39:18午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 28/99] Final Prec@1 57.1280%
07/03 09:39:19午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 57.1280%
07/03 09:39:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][50/781]	Step 22728	lr 0.02035	Loss 2.1290 (1.9290)	Prec@(1,5) (61.2%, 88.0%)	
07/03 09:39:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][100/781]	Step 22778	lr 0.02035	Loss 1.6950 (1.9213)	Prec@(1,5) (60.9%, 88.1%)	
07/03 09:40:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][150/781]	Step 22828	lr 0.02035	Loss 1.8874 (1.9472)	Prec@(1,5) (60.1%, 87.5%)	
07/03 09:40:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][200/781]	Step 22878	lr 0.02035	Loss 1.9689 (1.9546)	Prec@(1,5) (60.0%, 87.4%)	
07/03 09:40:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][250/781]	Step 22928	lr 0.02035	Loss 1.8176 (1.9746)	Prec@(1,5) (59.4%, 87.2%)	
07/03 09:40:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][300/781]	Step 22978	lr 0.02035	Loss 2.0239 (1.9724)	Prec@(1,5) (59.4%, 87.3%)	
07/03 09:41:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][350/781]	Step 23028	lr 0.02035	Loss 1.7389 (1.9737)	Prec@(1,5) (59.5%, 87.3%)	
07/03 09:41:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][400/781]	Step 23078	lr 0.02035	Loss 1.9473 (1.9736)	Prec@(1,5) (59.5%, 87.3%)	
07/03 09:41:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][450/781]	Step 23128	lr 0.02035	Loss 1.9805 (1.9715)	Prec@(1,5) (59.6%, 87.4%)	
07/03 09:41:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][500/781]	Step 23178	lr 0.02035	Loss 1.7789 (1.9806)	Prec@(1,5) (59.4%, 87.3%)	
07/03 09:42:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][550/781]	Step 23228	lr 0.02035	Loss 2.1493 (1.9805)	Prec@(1,5) (59.5%, 87.3%)	
07/03 09:42:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][600/781]	Step 23278	lr 0.02035	Loss 2.5072 (1.9832)	Prec@(1,5) (59.5%, 87.3%)	
07/03 09:42:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][650/781]	Step 23328	lr 0.02035	Loss 2.1750 (1.9873)	Prec@(1,5) (59.3%, 87.3%)	
07/03 09:42:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][700/781]	Step 23378	lr 0.02035	Loss 2.2212 (1.9913)	Prec@(1,5) (59.3%, 87.2%)	
07/03 09:43:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][750/781]	Step 23428	lr 0.02035	Loss 1.7641 (1.9955)	Prec@(1,5) (59.2%, 87.2%)	
07/03 09:43:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][781/781]	Step 23459	lr 0.02035	Loss 2.4269 (2.0005)	Prec@(1,5) (59.1%, 87.1%)	
07/03 09:43:12午後 evaluateCell_trainer.py:172 [INFO] Train: [ 29/99] Final Prec@1 59.1120%
07/03 09:43:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][50/391]	Step 23460	Loss 1.6544	Prec@(1,5) (53.9%, 85.0%)
07/03 09:43:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][100/391]	Step 23460	Loss 1.6019	Prec@(1,5) (55.5%, 85.8%)
07/03 09:43:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][150/391]	Step 23460	Loss 1.5905	Prec@(1,5) (56.0%, 85.7%)
07/03 09:43:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][200/391]	Step 23460	Loss 1.5735	Prec@(1,5) (56.4%, 86.0%)
07/03 09:43:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][250/391]	Step 23460	Loss 1.5708	Prec@(1,5) (56.4%, 85.9%)
07/03 09:43:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][300/391]	Step 23460	Loss 1.5712	Prec@(1,5) (56.3%, 86.0%)
07/03 09:43:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][350/391]	Step 23460	Loss 1.5772	Prec@(1,5) (56.1%, 86.0%)
07/03 09:43:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][390/391]	Step 23460	Loss 1.5780	Prec@(1,5) (56.3%, 86.0%)
07/03 09:43:44午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 29/99] Final Prec@1 56.2760%
07/03 09:43:44午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 57.1280%
07/03 09:44:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][50/781]	Step 23510	lr 0.02005	Loss 2.2336 (1.9562)	Prec@(1,5) (58.4%, 88.0%)	
07/03 09:44:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][100/781]	Step 23560	lr 0.02005	Loss 1.8946 (1.9100)	Prec@(1,5) (59.4%, 88.5%)	
07/03 09:44:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][150/781]	Step 23610	lr 0.02005	Loss 2.3256 (1.9258)	Prec@(1,5) (59.4%, 88.1%)	
07/03 09:44:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][200/781]	Step 23660	lr 0.02005	Loss 1.8618 (1.9338)	Prec@(1,5) (59.6%, 87.7%)	
07/03 09:44:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][250/781]	Step 23710	lr 0.02005	Loss 1.6019 (1.9341)	Prec@(1,5) (59.6%, 87.7%)	
07/03 09:45:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][300/781]	Step 23760	lr 0.02005	Loss 1.8811 (1.9476)	Prec@(1,5) (59.5%, 87.6%)	
07/03 09:45:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][350/781]	Step 23810	lr 0.02005	Loss 2.1001 (1.9510)	Prec@(1,5) (59.6%, 87.5%)	
07/03 09:45:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][400/781]	Step 23860	lr 0.02005	Loss 1.9873 (1.9538)	Prec@(1,5) (59.6%, 87.6%)	
07/03 09:45:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][450/781]	Step 23910	lr 0.02005	Loss 1.8352 (1.9520)	Prec@(1,5) (59.7%, 87.6%)	
07/03 09:46:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][500/781]	Step 23960	lr 0.02005	Loss 1.9049 (1.9523)	Prec@(1,5) (59.7%, 87.6%)	
07/03 09:46:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][550/781]	Step 24010	lr 0.02005	Loss 2.1900 (1.9553)	Prec@(1,5) (59.6%, 87.6%)	
07/03 09:46:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][600/781]	Step 24060	lr 0.02005	Loss 1.8879 (1.9565)	Prec@(1,5) (59.6%, 87.5%)	
07/03 09:46:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][650/781]	Step 24110	lr 0.02005	Loss 1.8062 (1.9665)	Prec@(1,5) (59.5%, 87.4%)	
07/03 09:47:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][700/781]	Step 24160	lr 0.02005	Loss 1.5595 (1.9623)	Prec@(1,5) (59.6%, 87.4%)	
07/03 09:47:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][750/781]	Step 24210	lr 0.02005	Loss 2.2855 (1.9655)	Prec@(1,5) (59.6%, 87.4%)	
07/03 09:47:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][781/781]	Step 24241	lr 0.02005	Loss 1.8948 (1.9718)	Prec@(1,5) (59.5%, 87.3%)	
07/03 09:47:38午後 evaluateCell_trainer.py:172 [INFO] Train: [ 30/99] Final Prec@1 59.4560%
07/03 09:47:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][50/391]	Step 24242	Loss 1.4768	Prec@(1,5) (58.2%, 87.0%)
07/03 09:47:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][100/391]	Step 24242	Loss 1.4949	Prec@(1,5) (57.6%, 86.3%)
07/03 09:47:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][150/391]	Step 24242	Loss 1.4909	Prec@(1,5) (57.8%, 86.4%)
07/03 09:47:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][200/391]	Step 24242	Loss 1.4791	Prec@(1,5) (57.8%, 86.6%)
07/03 09:47:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][250/391]	Step 24242	Loss 1.4715	Prec@(1,5) (58.1%, 86.7%)
07/03 09:48:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][300/391]	Step 24242	Loss 1.4779	Prec@(1,5) (58.2%, 86.6%)
07/03 09:48:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][350/391]	Step 24242	Loss 1.4804	Prec@(1,5) (58.0%, 86.5%)
07/03 09:48:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][390/391]	Step 24242	Loss 1.4751	Prec@(1,5) (58.1%, 86.6%)
07/03 09:48:10午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 30/99] Final Prec@1 58.1240%
07/03 09:48:10午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 58.1240%
07/03 09:48:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][50/781]	Step 24292	lr 0.01975	Loss 1.5745 (1.8475)	Prec@(1,5) (62.2%, 88.6%)	
07/03 09:48:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][100/781]	Step 24342	lr 0.01975	Loss 2.1358 (1.8688)	Prec@(1,5) (61.9%, 88.2%)	
07/03 09:48:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][150/781]	Step 24392	lr 0.01975	Loss 1.9218 (1.8842)	Prec@(1,5) (61.2%, 88.1%)	
07/03 09:49:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][200/781]	Step 24442	lr 0.01975	Loss 1.6675 (1.9002)	Prec@(1,5) (60.9%, 87.9%)	
07/03 09:49:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][250/781]	Step 24492	lr 0.01975	Loss 1.6145 (1.9017)	Prec@(1,5) (60.8%, 87.9%)	
07/03 09:49:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][300/781]	Step 24542	lr 0.01975	Loss 2.0426 (1.9171)	Prec@(1,5) (60.3%, 87.9%)	
07/03 09:49:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][350/781]	Step 24592	lr 0.01975	Loss 1.7372 (1.9125)	Prec@(1,5) (60.6%, 88.1%)	
07/03 09:50:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][400/781]	Step 24642	lr 0.01975	Loss 1.7950 (1.9180)	Prec@(1,5) (60.5%, 88.0%)	
07/03 09:50:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][450/781]	Step 24692	lr 0.01975	Loss 1.7530 (1.9268)	Prec@(1,5) (60.4%, 87.8%)	
07/03 09:50:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][500/781]	Step 24742	lr 0.01975	Loss 2.3192 (1.9284)	Prec@(1,5) (60.5%, 87.8%)	
07/03 09:50:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][550/781]	Step 24792	lr 0.01975	Loss 1.8663 (1.9278)	Prec@(1,5) (60.5%, 87.8%)	
07/03 09:51:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][600/781]	Step 24842	lr 0.01975	Loss 1.7684 (1.9351)	Prec@(1,5) (60.3%, 87.8%)	
07/03 09:51:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][650/781]	Step 24892	lr 0.01975	Loss 2.2055 (1.9358)	Prec@(1,5) (60.2%, 87.8%)	
07/03 09:51:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][700/781]	Step 24942	lr 0.01975	Loss 1.8079 (1.9362)	Prec@(1,5) (60.2%, 87.8%)	
07/03 09:51:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][750/781]	Step 24992	lr 0.01975	Loss 1.6905 (1.9417)	Prec@(1,5) (60.1%, 87.7%)	
07/03 09:52:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][781/781]	Step 25023	lr 0.01975	Loss 1.9656 (1.9418)	Prec@(1,5) (60.1%, 87.7%)	
07/03 09:52:04午後 evaluateCell_trainer.py:172 [INFO] Train: [ 31/99] Final Prec@1 60.1500%
07/03 09:52:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][50/391]	Step 25024	Loss 1.6604	Prec@(1,5) (55.8%, 83.0%)
07/03 09:52:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][100/391]	Step 25024	Loss 1.6553	Prec@(1,5) (55.2%, 83.4%)
07/03 09:52:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][150/391]	Step 25024	Loss 1.6530	Prec@(1,5) (55.0%, 83.6%)
07/03 09:52:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][200/391]	Step 25024	Loss 1.6589	Prec@(1,5) (54.9%, 83.5%)
07/03 09:52:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][250/391]	Step 25024	Loss 1.6551	Prec@(1,5) (55.0%, 83.5%)
07/03 09:52:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][300/391]	Step 25024	Loss 1.6508	Prec@(1,5) (55.2%, 83.6%)
07/03 09:52:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][350/391]	Step 25024	Loss 1.6453	Prec@(1,5) (55.4%, 83.6%)
07/03 09:52:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][390/391]	Step 25024	Loss 1.6475	Prec@(1,5) (55.3%, 83.7%)
07/03 09:52:36午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 31/99] Final Prec@1 55.3600%
07/03 09:52:36午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 58.1240%
07/03 09:52:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][50/781]	Step 25074	lr 0.01943	Loss 1.6816 (1.8610)	Prec@(1,5) (62.2%, 88.2%)	
07/03 09:53:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][100/781]	Step 25124	lr 0.01943	Loss 2.3542 (1.8665)	Prec@(1,5) (61.8%, 88.6%)	
07/03 09:53:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][150/781]	Step 25174	lr 0.01943	Loss 1.9508 (1.8627)	Prec@(1,5) (61.6%, 88.6%)	
07/03 09:53:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][200/781]	Step 25224	lr 0.01943	Loss 1.9129 (1.8772)	Prec@(1,5) (61.2%, 88.6%)	
07/03 09:53:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][250/781]	Step 25274	lr 0.01943	Loss 1.7332 (1.8815)	Prec@(1,5) (61.0%, 88.6%)	
07/03 09:54:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][300/781]	Step 25324	lr 0.01943	Loss 2.1149 (1.8741)	Prec@(1,5) (61.1%, 88.7%)	
07/03 09:54:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][350/781]	Step 25374	lr 0.01943	Loss 2.0770 (1.8879)	Prec@(1,5) (60.9%, 88.5%)	
07/03 09:54:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][400/781]	Step 25424	lr 0.01943	Loss 2.0876 (1.8890)	Prec@(1,5) (60.9%, 88.4%)	
07/03 09:54:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][450/781]	Step 25474	lr 0.01943	Loss 2.0137 (1.8948)	Prec@(1,5) (60.9%, 88.3%)	
07/03 09:55:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][500/781]	Step 25524	lr 0.01943	Loss 1.5897 (1.8963)	Prec@(1,5) (60.9%, 88.3%)	
07/03 09:55:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][550/781]	Step 25574	lr 0.01943	Loss 2.0455 (1.9038)	Prec@(1,5) (60.8%, 88.1%)	
07/03 09:55:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][600/781]	Step 25624	lr 0.01943	Loss 1.4455 (1.9052)	Prec@(1,5) (60.8%, 88.1%)	
07/03 09:55:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][650/781]	Step 25674	lr 0.01943	Loss 1.9407 (1.9053)	Prec@(1,5) (60.8%, 88.1%)	
07/03 09:56:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][700/781]	Step 25724	lr 0.01943	Loss 1.8577 (1.9059)	Prec@(1,5) (60.7%, 88.1%)	
07/03 09:56:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][750/781]	Step 25774	lr 0.01943	Loss 1.7586 (1.9074)	Prec@(1,5) (60.7%, 88.1%)	
07/03 09:56:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][781/781]	Step 25805	lr 0.01943	Loss 1.6962 (1.9081)	Prec@(1,5) (60.7%, 88.1%)	
07/03 09:56:29午後 evaluateCell_trainer.py:172 [INFO] Train: [ 32/99] Final Prec@1 60.6760%
07/03 09:56:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][50/391]	Step 25806	Loss 1.4456	Prec@(1,5) (58.9%, 87.5%)
07/03 09:56:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][100/391]	Step 25806	Loss 1.4221	Prec@(1,5) (59.6%, 87.7%)
07/03 09:56:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][150/391]	Step 25806	Loss 1.4377	Prec@(1,5) (59.6%, 87.4%)
07/03 09:56:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][200/391]	Step 25806	Loss 1.4511	Prec@(1,5) (59.4%, 87.2%)
07/03 09:56:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][250/391]	Step 25806	Loss 1.4490	Prec@(1,5) (59.4%, 87.1%)
07/03 09:56:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][300/391]	Step 25806	Loss 1.4460	Prec@(1,5) (59.4%, 87.2%)
07/03 09:56:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][350/391]	Step 25806	Loss 1.4465	Prec@(1,5) (59.4%, 87.2%)
07/03 09:57:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][390/391]	Step 25806	Loss 1.4461	Prec@(1,5) (59.5%, 87.1%)
07/03 09:57:01午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 32/99] Final Prec@1 59.5320%
07/03 09:57:02午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 59.5320%
07/03 09:57:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][50/781]	Step 25856	lr 0.01911	Loss 1.8020 (1.8174)	Prec@(1,5) (63.7%, 88.8%)	
07/03 09:57:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][100/781]	Step 25906	lr 0.01911	Loss 1.4637 (1.8181)	Prec@(1,5) (63.0%, 88.9%)	
07/03 09:57:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][150/781]	Step 25956	lr 0.01911	Loss 2.0938 (1.8114)	Prec@(1,5) (63.0%, 88.9%)	
07/03 09:58:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][200/781]	Step 26006	lr 0.01911	Loss 2.3789 (1.8231)	Prec@(1,5) (62.8%, 88.8%)	
07/03 09:58:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][250/781]	Step 26056	lr 0.01911	Loss 2.0394 (1.8459)	Prec@(1,5) (62.3%, 88.5%)	
07/03 09:58:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][300/781]	Step 26106	lr 0.01911	Loss 2.4318 (1.8321)	Prec@(1,5) (62.5%, 88.7%)	
07/03 09:58:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][350/781]	Step 26156	lr 0.01911	Loss 1.7296 (1.8434)	Prec@(1,5) (62.3%, 88.5%)	
07/03 09:59:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][400/781]	Step 26206	lr 0.01911	Loss 1.3081 (1.8440)	Prec@(1,5) (62.3%, 88.5%)	
07/03 09:59:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][450/781]	Step 26256	lr 0.01911	Loss 2.0286 (1.8518)	Prec@(1,5) (62.2%, 88.5%)	
07/03 09:59:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][500/781]	Step 26306	lr 0.01911	Loss 2.4438 (1.8512)	Prec@(1,5) (62.2%, 88.5%)	
07/03 09:59:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][550/781]	Step 26356	lr 0.01911	Loss 1.9417 (1.8583)	Prec@(1,5) (61.9%, 88.3%)	
07/03 10:00:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][600/781]	Step 26406	lr 0.01911	Loss 1.4566 (1.8578)	Prec@(1,5) (61.9%, 88.4%)	
07/03 10:00:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][650/781]	Step 26456	lr 0.01911	Loss 1.7229 (1.8589)	Prec@(1,5) (61.8%, 88.4%)	
07/03 10:00:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][700/781]	Step 26506	lr 0.01911	Loss 1.6462 (1.8616)	Prec@(1,5) (61.7%, 88.5%)	
07/03 10:00:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][750/781]	Step 26556	lr 0.01911	Loss 2.4038 (1.8694)	Prec@(1,5) (61.5%, 88.4%)	
07/03 10:00:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][781/781]	Step 26587	lr 0.01911	Loss 2.4506 (1.8701)	Prec@(1,5) (61.5%, 88.4%)	
07/03 10:00:55午後 evaluateCell_trainer.py:172 [INFO] Train: [ 33/99] Final Prec@1 61.4600%
07/03 10:00:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][50/391]	Step 26588	Loss 1.5727	Prec@(1,5) (56.7%, 85.4%)
07/03 10:01:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][100/391]	Step 26588	Loss 1.5581	Prec@(1,5) (57.0%, 85.6%)
07/03 10:01:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][150/391]	Step 26588	Loss 1.5684	Prec@(1,5) (56.9%, 85.2%)
07/03 10:01:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][200/391]	Step 26588	Loss 1.5757	Prec@(1,5) (56.8%, 85.2%)
07/03 10:01:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][250/391]	Step 26588	Loss 1.5776	Prec@(1,5) (56.7%, 85.0%)
07/03 10:01:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][300/391]	Step 26588	Loss 1.5784	Prec@(1,5) (56.5%, 84.9%)
07/03 10:01:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][350/391]	Step 26588	Loss 1.5781	Prec@(1,5) (56.6%, 84.9%)
07/03 10:01:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][390/391]	Step 26588	Loss 1.5825	Prec@(1,5) (56.5%, 84.8%)
07/03 10:01:28午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 33/99] Final Prec@1 56.4800%
07/03 10:01:28午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 59.5320%
07/03 10:01:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][50/781]	Step 26638	lr 0.01878	Loss 1.8593 (1.8111)	Prec@(1,5) (61.8%, 89.2%)	
07/03 10:01:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][100/781]	Step 26688	lr 0.01878	Loss 1.4584 (1.7801)	Prec@(1,5) (63.2%, 89.6%)	
07/03 10:02:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][150/781]	Step 26738	lr 0.01878	Loss 1.8291 (1.8185)	Prec@(1,5) (63.0%, 89.1%)	
07/03 10:02:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][200/781]	Step 26788	lr 0.01878	Loss 1.8749 (1.8263)	Prec@(1,5) (62.6%, 88.9%)	
07/03 10:02:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][250/781]	Step 26838	lr 0.01878	Loss 2.4284 (1.8234)	Prec@(1,5) (62.5%, 89.0%)	
07/03 10:02:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][300/781]	Step 26888	lr 0.01878	Loss 2.2150 (1.8327)	Prec@(1,5) (62.2%, 88.8%)	
07/03 10:03:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][350/781]	Step 26938	lr 0.01878	Loss 1.8412 (1.8293)	Prec@(1,5) (62.4%, 88.9%)	
07/03 10:03:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][400/781]	Step 26988	lr 0.01878	Loss 1.7620 (1.8363)	Prec@(1,5) (62.3%, 88.8%)	
07/03 10:03:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][450/781]	Step 27038	lr 0.01878	Loss 2.0323 (1.8364)	Prec@(1,5) (62.3%, 88.8%)	
07/03 10:03:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][500/781]	Step 27088	lr 0.01878	Loss 1.8955 (1.8417)	Prec@(1,5) (62.1%, 88.8%)	
07/03 10:04:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][550/781]	Step 27138	lr 0.01878	Loss 1.7932 (1.8422)	Prec@(1,5) (62.1%, 88.8%)	
07/03 10:04:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][600/781]	Step 27188	lr 0.01878	Loss 2.1746 (1.8445)	Prec@(1,5) (62.1%, 88.7%)	
07/03 10:04:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][650/781]	Step 27238	lr 0.01878	Loss 1.7378 (1.8439)	Prec@(1,5) (62.0%, 88.8%)	
07/03 10:04:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][700/781]	Step 27288	lr 0.01878	Loss 2.0728 (1.8494)	Prec@(1,5) (61.8%, 88.7%)	
07/03 10:05:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][750/781]	Step 27338	lr 0.01878	Loss 1.8440 (1.8509)	Prec@(1,5) (61.8%, 88.7%)	
07/03 10:05:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][781/781]	Step 27369	lr 0.01878	Loss 2.1099 (1.8521)	Prec@(1,5) (61.7%, 88.7%)	
07/03 10:05:21午後 evaluateCell_trainer.py:172 [INFO] Train: [ 34/99] Final Prec@1 61.7200%
07/03 10:05:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][50/391]	Step 27370	Loss 1.6108	Prec@(1,5) (55.8%, 85.0%)
07/03 10:05:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][100/391]	Step 27370	Loss 1.6588	Prec@(1,5) (54.2%, 84.4%)
07/03 10:05:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][150/391]	Step 27370	Loss 1.6525	Prec@(1,5) (54.3%, 84.4%)
07/03 10:05:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][200/391]	Step 27370	Loss 1.6429	Prec@(1,5) (54.3%, 84.3%)
07/03 10:05:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][250/391]	Step 27370	Loss 1.6445	Prec@(1,5) (54.3%, 84.4%)
07/03 10:05:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][300/391]	Step 27370	Loss 1.6406	Prec@(1,5) (54.5%, 84.4%)
07/03 10:05:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][350/391]	Step 27370	Loss 1.6471	Prec@(1,5) (54.4%, 84.2%)
07/03 10:05:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][390/391]	Step 27370	Loss 1.6429	Prec@(1,5) (54.5%, 84.2%)
07/03 10:05:53午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 34/99] Final Prec@1 54.5240%
07/03 10:05:53午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 59.5320%
07/03 10:06:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][50/781]	Step 27420	lr 0.01845	Loss 1.7175 (1.7579)	Prec@(1,5) (63.4%, 89.2%)	
07/03 10:06:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][100/781]	Step 27470	lr 0.01845	Loss 2.0901 (1.7352)	Prec@(1,5) (64.0%, 89.9%)	
07/03 10:06:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][150/781]	Step 27520	lr 0.01845	Loss 1.6558 (1.7525)	Prec@(1,5) (63.6%, 89.7%)	
07/03 10:06:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][200/781]	Step 27570	lr 0.01845	Loss 1.9466 (1.7749)	Prec@(1,5) (63.2%, 89.5%)	
07/03 10:07:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][250/781]	Step 27620	lr 0.01845	Loss 1.3828 (1.7844)	Prec@(1,5) (63.0%, 89.3%)	
07/03 10:07:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][300/781]	Step 27670	lr 0.01845	Loss 2.0214 (1.7947)	Prec@(1,5) (62.7%, 89.1%)	
07/03 10:07:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][350/781]	Step 27720	lr 0.01845	Loss 1.5647 (1.8022)	Prec@(1,5) (62.6%, 89.1%)	
07/03 10:07:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][400/781]	Step 27770	lr 0.01845	Loss 1.9530 (1.8065)	Prec@(1,5) (62.5%, 89.0%)	
07/03 10:08:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][450/781]	Step 27820	lr 0.01845	Loss 1.8666 (1.8108)	Prec@(1,5) (62.5%, 89.0%)	
07/03 10:08:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][500/781]	Step 27870	lr 0.01845	Loss 1.2045 (1.8122)	Prec@(1,5) (62.6%, 88.9%)	
07/03 10:08:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][550/781]	Step 27920	lr 0.01845	Loss 2.3318 (1.8130)	Prec@(1,5) (62.5%, 89.0%)	
07/03 10:08:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][600/781]	Step 27970	lr 0.01845	Loss 1.9964 (1.8160)	Prec@(1,5) (62.5%, 88.9%)	
07/03 10:09:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][650/781]	Step 28020	lr 0.01845	Loss 1.6173 (1.8221)	Prec@(1,5) (62.4%, 88.9%)	
07/03 10:09:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][700/781]	Step 28070	lr 0.01845	Loss 1.9917 (1.8278)	Prec@(1,5) (62.3%, 88.8%)	
07/03 10:09:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][750/781]	Step 28120	lr 0.01845	Loss 2.2169 (1.8300)	Prec@(1,5) (62.2%, 88.8%)	
07/03 10:09:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][781/781]	Step 28151	lr 0.01845	Loss 1.7154 (1.8305)	Prec@(1,5) (62.2%, 88.8%)	
07/03 10:09:46午後 evaluateCell_trainer.py:172 [INFO] Train: [ 35/99] Final Prec@1 62.1960%
07/03 10:09:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][50/391]	Step 28152	Loss 1.3520	Prec@(1,5) (61.2%, 88.2%)
07/03 10:09:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][100/391]	Step 28152	Loss 1.3580	Prec@(1,5) (61.1%, 88.2%)
07/03 10:09:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][150/391]	Step 28152	Loss 1.3629	Prec@(1,5) (60.9%, 88.3%)
07/03 10:10:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][200/391]	Step 28152	Loss 1.3541	Prec@(1,5) (61.1%, 88.6%)
07/03 10:10:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][250/391]	Step 28152	Loss 1.3567	Prec@(1,5) (61.0%, 88.6%)
07/03 10:10:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][300/391]	Step 28152	Loss 1.3551	Prec@(1,5) (61.0%, 88.7%)
07/03 10:10:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][350/391]	Step 28152	Loss 1.3537	Prec@(1,5) (61.0%, 88.7%)
07/03 10:10:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][390/391]	Step 28152	Loss 1.3564	Prec@(1,5) (61.1%, 88.6%)
07/03 10:10:19午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 35/99] Final Prec@1 61.1080%
07/03 10:10:19午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 61.1080%
07/03 10:10:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][50/781]	Step 28202	lr 0.01811	Loss 1.5312 (1.7405)	Prec@(1,5) (63.5%, 90.1%)	
07/03 10:10:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][100/781]	Step 28252	lr 0.01811	Loss 1.9091 (1.7071)	Prec@(1,5) (64.6%, 90.0%)	
07/03 10:11:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][150/781]	Step 28302	lr 0.01811	Loss 1.9508 (1.7656)	Prec@(1,5) (63.4%, 89.4%)	
07/03 10:11:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][200/781]	Step 28352	lr 0.01811	Loss 1.9766 (1.7840)	Prec@(1,5) (63.0%, 89.3%)	
07/03 10:11:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][250/781]	Step 28402	lr 0.01811	Loss 1.8834 (1.7813)	Prec@(1,5) (63.1%, 89.3%)	
07/03 10:11:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][300/781]	Step 28452	lr 0.01811	Loss 2.0893 (1.7858)	Prec@(1,5) (62.9%, 89.3%)	
07/03 10:12:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][350/781]	Step 28502	lr 0.01811	Loss 1.4754 (1.7899)	Prec@(1,5) (62.9%, 89.2%)	
07/03 10:12:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][400/781]	Step 28552	lr 0.01811	Loss 1.6119 (1.7892)	Prec@(1,5) (63.1%, 89.2%)	
07/03 10:12:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][450/781]	Step 28602	lr 0.01811	Loss 1.7181 (1.7909)	Prec@(1,5) (63.1%, 89.2%)	
07/03 10:12:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][500/781]	Step 28652	lr 0.01811	Loss 1.6019 (1.7887)	Prec@(1,5) (63.1%, 89.3%)	
07/03 10:13:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][550/781]	Step 28702	lr 0.01811	Loss 1.5646 (1.7857)	Prec@(1,5) (63.2%, 89.4%)	
07/03 10:13:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][600/781]	Step 28752	lr 0.01811	Loss 1.5183 (1.7888)	Prec@(1,5) (63.1%, 89.3%)	
07/03 10:13:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][650/781]	Step 28802	lr 0.01811	Loss 1.8153 (1.7921)	Prec@(1,5) (63.0%, 89.3%)	
07/03 10:13:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][700/781]	Step 28852	lr 0.01811	Loss 1.9511 (1.7999)	Prec@(1,5) (62.9%, 89.2%)	
07/03 10:14:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][750/781]	Step 28902	lr 0.01811	Loss 1.9832 (1.8053)	Prec@(1,5) (62.8%, 89.1%)	
07/03 10:14:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][781/781]	Step 28933	lr 0.01811	Loss 2.0806 (1.8066)	Prec@(1,5) (62.8%, 89.1%)	
07/03 10:14:12午後 evaluateCell_trainer.py:172 [INFO] Train: [ 36/99] Final Prec@1 62.8220%
07/03 10:14:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][50/391]	Step 28934	Loss 1.3291	Prec@(1,5) (62.0%, 89.1%)
07/03 10:14:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][100/391]	Step 28934	Loss 1.3246	Prec@(1,5) (62.4%, 88.8%)
07/03 10:14:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][150/391]	Step 28934	Loss 1.3192	Prec@(1,5) (62.6%, 89.0%)
07/03 10:14:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][200/391]	Step 28934	Loss 1.3238	Prec@(1,5) (62.2%, 88.8%)
07/03 10:14:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][250/391]	Step 28934	Loss 1.3228	Prec@(1,5) (61.9%, 88.9%)
07/03 10:14:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][300/391]	Step 28934	Loss 1.3210	Prec@(1,5) (62.0%, 89.0%)
07/03 10:14:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][350/391]	Step 28934	Loss 1.3248	Prec@(1,5) (61.9%, 88.9%)
07/03 10:14:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][390/391]	Step 28934	Loss 1.3258	Prec@(1,5) (62.0%, 88.9%)
07/03 10:14:45午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 36/99] Final Prec@1 61.9400%
07/03 10:14:45午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 61.9400%
07/03 10:15:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][50/781]	Step 28984	lr 0.01777	Loss 1.4732 (1.6395)	Prec@(1,5) (66.1%, 91.3%)	
07/03 10:15:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][100/781]	Step 29034	lr 0.01777	Loss 1.8991 (1.6737)	Prec@(1,5) (65.2%, 90.6%)	
07/03 10:15:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][150/781]	Step 29084	lr 0.01777	Loss 1.8386 (1.6945)	Prec@(1,5) (64.8%, 90.4%)	
07/03 10:15:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][200/781]	Step 29134	lr 0.01777	Loss 1.4159 (1.7042)	Prec@(1,5) (64.6%, 90.3%)	
07/03 10:16:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][250/781]	Step 29184	lr 0.01777	Loss 1.3791 (1.7172)	Prec@(1,5) (64.2%, 90.1%)	
07/03 10:16:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][300/781]	Step 29234	lr 0.01777	Loss 1.7035 (1.7271)	Prec@(1,5) (63.9%, 90.1%)	
07/03 10:16:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][350/781]	Step 29284	lr 0.01777	Loss 1.9867 (1.7423)	Prec@(1,5) (63.7%, 89.9%)	
07/03 10:16:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][400/781]	Step 29334	lr 0.01777	Loss 1.8335 (1.7509)	Prec@(1,5) (63.5%, 89.7%)	
07/03 10:17:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][450/781]	Step 29384	lr 0.01777	Loss 2.2830 (1.7566)	Prec@(1,5) (63.5%, 89.6%)	
07/03 10:17:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][500/781]	Step 29434	lr 0.01777	Loss 2.0534 (1.7660)	Prec@(1,5) (63.2%, 89.5%)	
07/03 10:17:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][550/781]	Step 29484	lr 0.01777	Loss 1.6257 (1.7736)	Prec@(1,5) (63.1%, 89.5%)	
07/03 10:17:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][600/781]	Step 29534	lr 0.01777	Loss 2.0537 (1.7751)	Prec@(1,5) (63.0%, 89.4%)	
07/03 10:18:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][650/781]	Step 29584	lr 0.01777	Loss 1.8974 (1.7751)	Prec@(1,5) (63.0%, 89.4%)	
07/03 10:18:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][700/781]	Step 29634	lr 0.01777	Loss 1.5331 (1.7756)	Prec@(1,5) (63.1%, 89.4%)	
07/03 10:18:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][750/781]	Step 29684	lr 0.01777	Loss 1.3425 (1.7763)	Prec@(1,5) (63.2%, 89.4%)	
07/03 10:18:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][781/781]	Step 29715	lr 0.01777	Loss 1.7759 (1.7740)	Prec@(1,5) (63.2%, 89.5%)	
07/03 10:18:39午後 evaluateCell_trainer.py:172 [INFO] Train: [ 37/99] Final Prec@1 63.1700%
07/03 10:18:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][50/391]	Step 29716	Loss 1.3146	Prec@(1,5) (61.6%, 89.8%)
07/03 10:18:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][100/391]	Step 29716	Loss 1.3186	Prec@(1,5) (62.0%, 89.3%)
07/03 10:18:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][150/391]	Step 29716	Loss 1.3211	Prec@(1,5) (61.8%, 89.2%)
07/03 10:18:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][200/391]	Step 29716	Loss 1.3230	Prec@(1,5) (61.7%, 89.3%)
07/03 10:19:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][250/391]	Step 29716	Loss 1.3208	Prec@(1,5) (61.8%, 89.2%)
07/03 10:19:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][300/391]	Step 29716	Loss 1.3170	Prec@(1,5) (62.0%, 89.4%)
07/03 10:19:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][350/391]	Step 29716	Loss 1.3196	Prec@(1,5) (61.9%, 89.4%)
07/03 10:19:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][390/391]	Step 29716	Loss 1.3198	Prec@(1,5) (61.9%, 89.4%)
07/03 10:19:12午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 37/99] Final Prec@1 61.8280%
07/03 10:19:12午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 61.9400%
07/03 10:19:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][50/781]	Step 29766	lr 0.01742	Loss 2.3134 (1.6429)	Prec@(1,5) (65.3%, 90.8%)	
07/03 10:19:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][100/781]	Step 29816	lr 0.01742	Loss 2.0816 (1.6463)	Prec@(1,5) (66.0%, 90.5%)	
07/03 10:19:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][150/781]	Step 29866	lr 0.01742	Loss 1.8293 (1.6659)	Prec@(1,5) (65.9%, 90.3%)	
07/03 10:20:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][200/781]	Step 29916	lr 0.01742	Loss 1.5271 (1.7020)	Prec@(1,5) (65.0%, 90.1%)	
07/03 10:20:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][250/781]	Step 29966	lr 0.01742	Loss 2.1697 (1.7122)	Prec@(1,5) (64.7%, 90.1%)	
07/03 10:20:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][300/781]	Step 30016	lr 0.01742	Loss 1.4455 (1.7169)	Prec@(1,5) (64.6%, 90.0%)	
07/03 10:20:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][350/781]	Step 30066	lr 0.01742	Loss 1.3375 (1.7151)	Prec@(1,5) (64.7%, 90.0%)	
07/03 10:21:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][400/781]	Step 30116	lr 0.01742	Loss 1.4848 (1.7174)	Prec@(1,5) (64.7%, 90.0%)	
07/03 10:21:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][450/781]	Step 30166	lr 0.01742	Loss 2.0833 (1.7210)	Prec@(1,5) (64.6%, 90.0%)	
07/03 10:21:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][500/781]	Step 30216	lr 0.01742	Loss 2.1443 (1.7206)	Prec@(1,5) (64.5%, 90.0%)	
07/03 10:21:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][550/781]	Step 30266	lr 0.01742	Loss 1.8605 (1.7302)	Prec@(1,5) (64.3%, 89.9%)	
07/03 10:22:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][600/781]	Step 30316	lr 0.01742	Loss 1.9575 (1.7312)	Prec@(1,5) (64.2%, 89.9%)	
07/03 10:22:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][650/781]	Step 30366	lr 0.01742	Loss 2.6766 (1.7399)	Prec@(1,5) (64.1%, 89.8%)	
07/03 10:22:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][700/781]	Step 30416	lr 0.01742	Loss 2.1783 (1.7426)	Prec@(1,5) (64.0%, 89.8%)	
07/03 10:22:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][750/781]	Step 30466	lr 0.01742	Loss 1.8066 (1.7432)	Prec@(1,5) (63.9%, 89.8%)	
07/03 10:23:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][781/781]	Step 30497	lr 0.01742	Loss 1.4463 (1.7479)	Prec@(1,5) (63.8%, 89.8%)	
07/03 10:23:05午後 evaluateCell_trainer.py:172 [INFO] Train: [ 38/99] Final Prec@1 63.8280%
07/03 10:23:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][50/391]	Step 30498	Loss 1.4229	Prec@(1,5) (61.0%, 86.8%)
07/03 10:23:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][100/391]	Step 30498	Loss 1.4563	Prec@(1,5) (59.6%, 86.6%)
07/03 10:23:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][150/391]	Step 30498	Loss 1.4494	Prec@(1,5) (59.6%, 86.7%)
07/03 10:23:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][200/391]	Step 30498	Loss 1.4362	Prec@(1,5) (60.1%, 86.9%)
07/03 10:23:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][250/391]	Step 30498	Loss 1.4326	Prec@(1,5) (60.2%, 87.0%)
07/03 10:23:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][300/391]	Step 30498	Loss 1.4310	Prec@(1,5) (60.2%, 87.1%)
07/03 10:23:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][350/391]	Step 30498	Loss 1.4309	Prec@(1,5) (60.3%, 86.9%)
07/03 10:23:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][390/391]	Step 30498	Loss 1.4326	Prec@(1,5) (60.3%, 86.9%)
07/03 10:23:37午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 38/99] Final Prec@1 60.3200%
07/03 10:23:37午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 61.9400%
07/03 10:23:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][50/781]	Step 30548	lr 0.01706	Loss 1.6942 (1.6560)	Prec@(1,5) (65.1%, 91.1%)	
07/03 10:24:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][100/781]	Step 30598	lr 0.01706	Loss 1.2484 (1.6728)	Prec@(1,5) (65.1%, 90.6%)	
07/03 10:24:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][150/781]	Step 30648	lr 0.01706	Loss 2.1171 (1.6794)	Prec@(1,5) (65.0%, 90.6%)	
07/03 10:24:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][200/781]	Step 30698	lr 0.01706	Loss 2.0969 (1.6947)	Prec@(1,5) (64.8%, 90.6%)	
07/03 10:24:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][250/781]	Step 30748	lr 0.01706	Loss 1.8057 (1.6865)	Prec@(1,5) (65.0%, 90.6%)	
07/03 10:25:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][300/781]	Step 30798	lr 0.01706	Loss 1.6322 (1.6785)	Prec@(1,5) (65.3%, 90.6%)	
07/03 10:25:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][350/781]	Step 30848	lr 0.01706	Loss 1.5374 (1.6807)	Prec@(1,5) (65.2%, 90.6%)	
07/03 10:25:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][400/781]	Step 30898	lr 0.01706	Loss 2.0460 (1.6886)	Prec@(1,5) (65.0%, 90.4%)	
07/03 10:25:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][450/781]	Step 30948	lr 0.01706	Loss 1.4702 (1.6944)	Prec@(1,5) (64.8%, 90.4%)	
07/03 10:26:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][500/781]	Step 30998	lr 0.01706	Loss 1.6580 (1.6953)	Prec@(1,5) (64.7%, 90.4%)	
07/03 10:26:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][550/781]	Step 31048	lr 0.01706	Loss 2.1942 (1.6990)	Prec@(1,5) (64.6%, 90.4%)	
07/03 10:26:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][600/781]	Step 31098	lr 0.01706	Loss 1.6507 (1.7045)	Prec@(1,5) (64.5%, 90.3%)	
07/03 10:26:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][650/781]	Step 31148	lr 0.01706	Loss 1.6335 (1.7077)	Prec@(1,5) (64.4%, 90.3%)	
07/03 10:27:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][700/781]	Step 31198	lr 0.01706	Loss 1.6649 (1.7097)	Prec@(1,5) (64.4%, 90.3%)	
07/03 10:27:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][750/781]	Step 31248	lr 0.01706	Loss 1.6454 (1.7143)	Prec@(1,5) (64.3%, 90.2%)	
07/03 10:27:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][781/781]	Step 31279	lr 0.01706	Loss 1.6411 (1.7157)	Prec@(1,5) (64.3%, 90.2%)	
07/03 10:27:30午後 evaluateCell_trainer.py:172 [INFO] Train: [ 39/99] Final Prec@1 64.3280%
07/03 10:27:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][50/391]	Step 31280	Loss 1.3019	Prec@(1,5) (62.5%, 89.3%)
07/03 10:27:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][100/391]	Step 31280	Loss 1.2882	Prec@(1,5) (63.0%, 89.6%)
07/03 10:27:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][150/391]	Step 31280	Loss 1.2986	Prec@(1,5) (63.0%, 89.3%)
07/03 10:27:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][200/391]	Step 31280	Loss 1.3093	Prec@(1,5) (62.6%, 89.2%)
07/03 10:27:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][250/391]	Step 31280	Loss 1.2987	Prec@(1,5) (62.8%, 89.4%)
07/03 10:27:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][300/391]	Step 31280	Loss 1.2955	Prec@(1,5) (62.9%, 89.3%)
07/03 10:28:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][350/391]	Step 31280	Loss 1.2946	Prec@(1,5) (62.9%, 89.4%)
07/03 10:28:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][390/391]	Step 31280	Loss 1.2957	Prec@(1,5) (63.0%, 89.4%)
07/03 10:28:03午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 39/99] Final Prec@1 62.9440%
07/03 10:28:03午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 62.9440%
07/03 10:28:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][50/781]	Step 31330	lr 0.01671	Loss 1.7034 (1.5798)	Prec@(1,5) (67.4%, 91.6%)	
07/03 10:28:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][100/781]	Step 31380	lr 0.01671	Loss 1.3177 (1.6227)	Prec@(1,5) (66.1%, 91.2%)	
07/03 10:28:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][150/781]	Step 31430	lr 0.01671	Loss 1.3848 (1.6232)	Prec@(1,5) (66.2%, 91.2%)	
07/03 10:29:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][200/781]	Step 31480	lr 0.01671	Loss 2.1814 (1.6324)	Prec@(1,5) (66.1%, 91.0%)	
07/03 10:29:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][250/781]	Step 31530	lr 0.01671	Loss 2.0092 (1.6397)	Prec@(1,5) (65.8%, 90.9%)	
07/03 10:29:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][300/781]	Step 31580	lr 0.01671	Loss 1.6099 (1.6451)	Prec@(1,5) (65.9%, 90.7%)	
07/03 10:29:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][350/781]	Step 31630	lr 0.01671	Loss 1.6792 (1.6515)	Prec@(1,5) (65.8%, 90.7%)	
07/03 10:30:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][400/781]	Step 31680	lr 0.01671	Loss 1.6588 (1.6544)	Prec@(1,5) (65.8%, 90.7%)	
07/03 10:30:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][450/781]	Step 31730	lr 0.01671	Loss 1.9774 (1.6536)	Prec@(1,5) (65.7%, 90.7%)	
07/03 10:30:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][500/781]	Step 31780	lr 0.01671	Loss 1.5265 (1.6588)	Prec@(1,5) (65.7%, 90.7%)	
07/03 10:30:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][550/781]	Step 31830	lr 0.01671	Loss 1.9278 (1.6668)	Prec@(1,5) (65.5%, 90.6%)	
07/03 10:31:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][600/781]	Step 31880	lr 0.01671	Loss 1.9251 (1.6719)	Prec@(1,5) (65.4%, 90.6%)	
07/03 10:31:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][650/781]	Step 31930	lr 0.01671	Loss 1.6950 (1.6815)	Prec@(1,5) (65.2%, 90.5%)	
07/03 10:31:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][700/781]	Step 31980	lr 0.01671	Loss 1.6465 (1.6854)	Prec@(1,5) (65.1%, 90.4%)	
07/03 10:31:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][750/781]	Step 32030	lr 0.01671	Loss 1.8575 (1.6872)	Prec@(1,5) (65.0%, 90.4%)	
07/03 10:31:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][781/781]	Step 32061	lr 0.01671	Loss 1.8434 (1.6894)	Prec@(1,5) (64.9%, 90.4%)	
07/03 10:31:57午後 evaluateCell_trainer.py:172 [INFO] Train: [ 40/99] Final Prec@1 64.9380%
07/03 10:32:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][50/391]	Step 32062	Loss 1.2289	Prec@(1,5) (64.0%, 90.4%)
07/03 10:32:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][100/391]	Step 32062	Loss 1.2239	Prec@(1,5) (64.1%, 90.5%)
07/03 10:32:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][150/391]	Step 32062	Loss 1.2214	Prec@(1,5) (64.0%, 90.6%)
07/03 10:32:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][200/391]	Step 32062	Loss 1.2323	Prec@(1,5) (64.1%, 90.4%)
07/03 10:32:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][250/391]	Step 32062	Loss 1.2315	Prec@(1,5) (64.2%, 90.3%)
07/03 10:32:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][300/391]	Step 32062	Loss 1.2329	Prec@(1,5) (64.1%, 90.2%)
07/03 10:32:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][350/391]	Step 32062	Loss 1.2374	Prec@(1,5) (64.0%, 90.1%)
07/03 10:32:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][390/391]	Step 32062	Loss 1.2399	Prec@(1,5) (63.9%, 90.1%)
07/03 10:32:29午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 40/99] Final Prec@1 63.9320%
07/03 10:32:30午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 63.9320%
07/03 10:32:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][50/781]	Step 32112	lr 0.01635	Loss 1.5567 (1.6163)	Prec@(1,5) (66.1%, 90.9%)	
07/03 10:33:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][100/781]	Step 32162	lr 0.01635	Loss 1.6184 (1.6168)	Prec@(1,5) (66.2%, 91.1%)	
07/03 10:33:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][150/781]	Step 32212	lr 0.01635	Loss 2.0897 (1.6262)	Prec@(1,5) (66.2%, 90.9%)	
07/03 10:33:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][200/781]	Step 32262	lr 0.01635	Loss 1.7394 (1.6321)	Prec@(1,5) (66.1%, 90.9%)	
07/03 10:33:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][250/781]	Step 32312	lr 0.01635	Loss 1.6738 (1.6289)	Prec@(1,5) (66.1%, 91.0%)	
07/03 10:33:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][300/781]	Step 32362	lr 0.01635	Loss 1.4885 (1.6392)	Prec@(1,5) (65.9%, 90.8%)	
07/03 10:34:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][350/781]	Step 32412	lr 0.01635	Loss 1.7656 (1.6425)	Prec@(1,5) (65.9%, 90.8%)	
07/03 10:34:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][400/781]	Step 32462	lr 0.01635	Loss 2.0168 (1.6446)	Prec@(1,5) (65.8%, 90.8%)	
07/03 10:34:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][450/781]	Step 32512	lr 0.01635	Loss 1.3562 (1.6420)	Prec@(1,5) (65.9%, 90.8%)	
07/03 10:34:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][500/781]	Step 32562	lr 0.01635	Loss 1.3781 (1.6419)	Prec@(1,5) (65.9%, 90.7%)	
07/03 10:35:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][550/781]	Step 32612	lr 0.01635	Loss 1.5597 (1.6435)	Prec@(1,5) (65.8%, 90.8%)	
07/03 10:35:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][600/781]	Step 32662	lr 0.01635	Loss 1.6412 (1.6437)	Prec@(1,5) (65.9%, 90.8%)	
07/03 10:35:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][650/781]	Step 32712	lr 0.01635	Loss 1.4232 (1.6486)	Prec@(1,5) (65.7%, 90.7%)	
07/03 10:35:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][700/781]	Step 32762	lr 0.01635	Loss 1.7888 (1.6484)	Prec@(1,5) (65.7%, 90.7%)	
07/03 10:36:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][750/781]	Step 32812	lr 0.01635	Loss 2.1693 (1.6522)	Prec@(1,5) (65.6%, 90.7%)	
07/03 10:36:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][781/781]	Step 32843	lr 0.01635	Loss 1.6566 (1.6564)	Prec@(1,5) (65.5%, 90.7%)	
07/03 10:36:23午後 evaluateCell_trainer.py:172 [INFO] Train: [ 41/99] Final Prec@1 65.5020%
07/03 10:36:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][50/391]	Step 32844	Loss 1.2783	Prec@(1,5) (63.5%, 89.3%)
07/03 10:36:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][100/391]	Step 32844	Loss 1.2855	Prec@(1,5) (63.0%, 89.4%)
07/03 10:36:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][150/391]	Step 32844	Loss 1.2579	Prec@(1,5) (63.7%, 89.7%)
07/03 10:36:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][200/391]	Step 32844	Loss 1.2600	Prec@(1,5) (63.9%, 89.4%)
07/03 10:36:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][250/391]	Step 32844	Loss 1.2508	Prec@(1,5) (64.1%, 89.7%)
07/03 10:36:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][300/391]	Step 32844	Loss 1.2475	Prec@(1,5) (64.0%, 89.7%)
07/03 10:36:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][350/391]	Step 32844	Loss 1.2416	Prec@(1,5) (64.3%, 89.9%)
07/03 10:36:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][390/391]	Step 32844	Loss 1.2348	Prec@(1,5) (64.3%, 90.0%)
07/03 10:36:54午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 41/99] Final Prec@1 64.3400%
07/03 10:36:55午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 64.3400%
07/03 10:37:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][50/781]	Step 32894	lr 0.01598	Loss 1.2282 (1.5722)	Prec@(1,5) (67.4%, 91.7%)	
07/03 10:37:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][100/781]	Step 32944	lr 0.01598	Loss 1.8743 (1.5591)	Prec@(1,5) (67.5%, 91.9%)	
07/03 10:37:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][150/781]	Step 32994	lr 0.01598	Loss 1.1401 (1.5703)	Prec@(1,5) (67.3%, 92.0%)	
07/03 10:37:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][200/781]	Step 33044	lr 0.01598	Loss 1.5412 (1.5728)	Prec@(1,5) (67.2%, 91.9%)	
07/03 10:38:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][250/781]	Step 33094	lr 0.01598	Loss 1.5110 (1.5714)	Prec@(1,5) (67.4%, 91.8%)	
07/03 10:38:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][300/781]	Step 33144	lr 0.01598	Loss 1.8836 (1.5800)	Prec@(1,5) (67.1%, 91.6%)	
07/03 10:38:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][350/781]	Step 33194	lr 0.01598	Loss 1.5845 (1.5900)	Prec@(1,5) (66.9%, 91.5%)	
07/03 10:38:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][400/781]	Step 33244	lr 0.01598	Loss 2.0219 (1.5973)	Prec@(1,5) (66.8%, 91.4%)	
07/03 10:39:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][450/781]	Step 33294	lr 0.01598	Loss 1.8886 (1.6028)	Prec@(1,5) (66.6%, 91.4%)	
07/03 10:39:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][500/781]	Step 33344	lr 0.01598	Loss 2.0135 (1.6070)	Prec@(1,5) (66.5%, 91.3%)	
07/03 10:39:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][550/781]	Step 33394	lr 0.01598	Loss 1.4296 (1.6104)	Prec@(1,5) (66.5%, 91.3%)	
07/03 10:39:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][600/781]	Step 33444	lr 0.01598	Loss 1.7360 (1.6123)	Prec@(1,5) (66.5%, 91.3%)	
07/03 10:40:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][650/781]	Step 33494	lr 0.01598	Loss 1.3987 (1.6176)	Prec@(1,5) (66.4%, 91.2%)	
07/03 10:40:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][700/781]	Step 33544	lr 0.01598	Loss 1.6770 (1.6280)	Prec@(1,5) (66.2%, 91.1%)	
07/03 10:40:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][750/781]	Step 33594	lr 0.01598	Loss 1.6483 (1.6323)	Prec@(1,5) (66.1%, 91.0%)	
07/03 10:40:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][781/781]	Step 33625	lr 0.01598	Loss 1.5565 (1.6303)	Prec@(1,5) (66.1%, 91.1%)	
07/03 10:40:48午後 evaluateCell_trainer.py:172 [INFO] Train: [ 42/99] Final Prec@1 66.0780%
07/03 10:40:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][50/391]	Step 33626	Loss 1.2486	Prec@(1,5) (64.0%, 89.9%)
07/03 10:40:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][100/391]	Step 33626	Loss 1.2443	Prec@(1,5) (64.3%, 89.8%)
07/03 10:41:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][150/391]	Step 33626	Loss 1.2396	Prec@(1,5) (64.4%, 89.9%)
07/03 10:41:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][200/391]	Step 33626	Loss 1.2319	Prec@(1,5) (64.4%, 89.9%)
07/03 10:41:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][250/391]	Step 33626	Loss 1.2379	Prec@(1,5) (64.3%, 89.8%)
07/03 10:41:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][300/391]	Step 33626	Loss 1.2271	Prec@(1,5) (64.5%, 90.0%)
07/03 10:41:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][350/391]	Step 33626	Loss 1.2289	Prec@(1,5) (64.5%, 90.0%)
07/03 10:41:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][390/391]	Step 33626	Loss 1.2251	Prec@(1,5) (64.6%, 90.0%)
07/03 10:41:21午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 42/99] Final Prec@1 64.5720%
07/03 10:41:21午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 64.5720%
07/03 10:41:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][50/781]	Step 33676	lr 0.01562	Loss 1.3568 (1.4860)	Prec@(1,5) (68.2%, 92.4%)	
07/03 10:41:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][100/781]	Step 33726	lr 0.01562	Loss 1.9689 (1.5333)	Prec@(1,5) (67.5%, 91.7%)	
07/03 10:42:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][150/781]	Step 33776	lr 0.01562	Loss 1.3823 (1.5338)	Prec@(1,5) (67.7%, 91.8%)	
07/03 10:42:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][200/781]	Step 33826	lr 0.01562	Loss 1.1740 (1.5379)	Prec@(1,5) (67.8%, 91.7%)	
07/03 10:42:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][250/781]	Step 33876	lr 0.01562	Loss 1.7676 (1.5546)	Prec@(1,5) (67.6%, 91.5%)	
07/03 10:42:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][300/781]	Step 33926	lr 0.01562	Loss 1.8608 (1.5647)	Prec@(1,5) (67.4%, 91.6%)	
07/03 10:43:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][350/781]	Step 33976	lr 0.01562	Loss 2.4769 (1.5735)	Prec@(1,5) (67.3%, 91.5%)	
07/03 10:43:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][400/781]	Step 34026	lr 0.01562	Loss 1.5380 (1.5737)	Prec@(1,5) (67.2%, 91.5%)	
07/03 10:43:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][450/781]	Step 34076	lr 0.01562	Loss 1.7964 (1.5806)	Prec@(1,5) (67.0%, 91.5%)	
07/03 10:43:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][500/781]	Step 34126	lr 0.01562	Loss 1.9114 (1.5867)	Prec@(1,5) (66.8%, 91.4%)	
07/03 10:44:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][550/781]	Step 34176	lr 0.01562	Loss 1.3345 (1.5903)	Prec@(1,5) (66.8%, 91.4%)	
07/03 10:44:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][600/781]	Step 34226	lr 0.01562	Loss 1.7295 (1.5982)	Prec@(1,5) (66.6%, 91.3%)	
07/03 10:44:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][650/781]	Step 34276	lr 0.01562	Loss 1.1624 (1.5995)	Prec@(1,5) (66.5%, 91.3%)	
07/03 10:44:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][700/781]	Step 34326	lr 0.01562	Loss 1.3793 (1.6034)	Prec@(1,5) (66.4%, 91.3%)	
07/03 10:45:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][750/781]	Step 34376	lr 0.01562	Loss 1.5339 (1.6076)	Prec@(1,5) (66.3%, 91.2%)	
07/03 10:45:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][781/781]	Step 34407	lr 0.01562	Loss 1.3400 (1.6094)	Prec@(1,5) (66.3%, 91.2%)	
07/03 10:45:15午後 evaluateCell_trainer.py:172 [INFO] Train: [ 43/99] Final Prec@1 66.3340%
07/03 10:45:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][50/391]	Step 34408	Loss 1.1386	Prec@(1,5) (66.8%, 91.7%)
07/03 10:45:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][100/391]	Step 34408	Loss 1.1554	Prec@(1,5) (66.3%, 91.4%)
07/03 10:45:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][150/391]	Step 34408	Loss 1.1464	Prec@(1,5) (66.8%, 91.4%)
07/03 10:45:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][200/391]	Step 34408	Loss 1.1383	Prec@(1,5) (66.7%, 91.6%)
07/03 10:45:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][250/391]	Step 34408	Loss 1.1340	Prec@(1,5) (66.8%, 91.6%)
07/03 10:45:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][300/391]	Step 34408	Loss 1.1310	Prec@(1,5) (66.9%, 91.6%)
07/03 10:45:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][350/391]	Step 34408	Loss 1.1329	Prec@(1,5) (66.9%, 91.6%)
07/03 10:45:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][390/391]	Step 34408	Loss 1.1327	Prec@(1,5) (66.9%, 91.7%)
07/03 10:45:47午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 43/99] Final Prec@1 66.8960%
07/03 10:45:48午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 66.8960%
07/03 10:46:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][50/781]	Step 34458	lr 0.01525	Loss 1.2997 (1.5142)	Prec@(1,5) (68.5%, 92.0%)	
07/03 10:46:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][100/781]	Step 34508	lr 0.01525	Loss 1.6110 (1.5047)	Prec@(1,5) (68.8%, 92.2%)	
07/03 10:46:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][150/781]	Step 34558	lr 0.01525	Loss 1.0464 (1.5057)	Prec@(1,5) (68.8%, 92.2%)	
07/03 10:46:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][200/781]	Step 34608	lr 0.01525	Loss 1.2766 (1.5222)	Prec@(1,5) (68.3%, 92.0%)	
07/03 10:47:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][250/781]	Step 34658	lr 0.01525	Loss 1.5041 (1.5320)	Prec@(1,5) (68.0%, 91.9%)	
07/03 10:47:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][300/781]	Step 34708	lr 0.01525	Loss 1.5192 (1.5316)	Prec@(1,5) (68.1%, 91.9%)	
07/03 10:47:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][350/781]	Step 34758	lr 0.01525	Loss 2.3395 (1.5402)	Prec@(1,5) (67.8%, 91.8%)	
07/03 10:47:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][400/781]	Step 34808	lr 0.01525	Loss 1.5276 (1.5512)	Prec@(1,5) (67.5%, 91.7%)	
07/03 10:48:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][450/781]	Step 34858	lr 0.01525	Loss 1.5218 (1.5569)	Prec@(1,5) (67.4%, 91.7%)	
07/03 10:48:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][500/781]	Step 34908	lr 0.01525	Loss 1.2406 (1.5595)	Prec@(1,5) (67.3%, 91.7%)	
07/03 10:48:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][550/781]	Step 34958	lr 0.01525	Loss 1.7663 (1.5629)	Prec@(1,5) (67.2%, 91.6%)	
07/03 10:48:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][600/781]	Step 35008	lr 0.01525	Loss 1.2642 (1.5623)	Prec@(1,5) (67.3%, 91.6%)	
07/03 10:49:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][650/781]	Step 35058	lr 0.01525	Loss 1.4950 (1.5712)	Prec@(1,5) (67.2%, 91.5%)	
07/03 10:49:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][700/781]	Step 35108	lr 0.01525	Loss 1.5645 (1.5732)	Prec@(1,5) (67.1%, 91.5%)	
07/03 10:49:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][750/781]	Step 35158	lr 0.01525	Loss 1.4502 (1.5769)	Prec@(1,5) (67.0%, 91.5%)	
07/03 10:49:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][781/781]	Step 35189	lr 0.01525	Loss 1.7573 (1.5767)	Prec@(1,5) (67.0%, 91.5%)	
07/03 10:49:41午後 evaluateCell_trainer.py:172 [INFO] Train: [ 44/99] Final Prec@1 66.9680%
07/03 10:49:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][50/391]	Step 35190	Loss 1.3427	Prec@(1,5) (63.2%, 87.6%)
07/03 10:49:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][100/391]	Step 35190	Loss 1.3579	Prec@(1,5) (62.1%, 87.6%)
07/03 10:49:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][150/391]	Step 35190	Loss 1.3619	Prec@(1,5) (61.9%, 87.6%)
07/03 10:49:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][200/391]	Step 35190	Loss 1.3495	Prec@(1,5) (62.1%, 87.9%)
07/03 10:50:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][250/391]	Step 35190	Loss 1.3563	Prec@(1,5) (62.0%, 88.0%)
07/03 10:50:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][300/391]	Step 35190	Loss 1.3486	Prec@(1,5) (62.2%, 88.1%)
07/03 10:50:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][350/391]	Step 35190	Loss 1.3506	Prec@(1,5) (62.1%, 88.1%)
07/03 10:50:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][390/391]	Step 35190	Loss 1.3490	Prec@(1,5) (62.1%, 88.2%)
07/03 10:50:13午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 44/99] Final Prec@1 62.0960%
07/03 10:50:13午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 66.8960%
07/03 10:50:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][50/781]	Step 35240	lr 0.01488	Loss 1.3851 (1.4774)	Prec@(1,5) (69.9%, 91.9%)	
07/03 10:50:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][100/781]	Step 35290	lr 0.01488	Loss 1.3205 (1.4755)	Prec@(1,5) (69.8%, 92.2%)	
07/03 10:50:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][150/781]	Step 35340	lr 0.01488	Loss 1.9887 (1.4900)	Prec@(1,5) (69.2%, 92.1%)	
07/03 10:51:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][200/781]	Step 35390	lr 0.01488	Loss 1.7509 (1.5098)	Prec@(1,5) (68.7%, 91.9%)	
07/03 10:51:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][250/781]	Step 35440	lr 0.01488	Loss 1.2386 (1.5166)	Prec@(1,5) (68.4%, 92.0%)	
07/03 10:51:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][300/781]	Step 35490	lr 0.01488	Loss 1.0855 (1.5075)	Prec@(1,5) (68.5%, 92.1%)	
07/03 10:51:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][350/781]	Step 35540	lr 0.01488	Loss 1.5574 (1.5165)	Prec@(1,5) (68.4%, 92.0%)	
07/03 10:52:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][400/781]	Step 35590	lr 0.01488	Loss 1.4439 (1.5134)	Prec@(1,5) (68.5%, 92.0%)	
07/03 10:52:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][450/781]	Step 35640	lr 0.01488	Loss 1.1747 (1.5121)	Prec@(1,5) (68.6%, 92.0%)	
07/03 10:52:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][500/781]	Step 35690	lr 0.01488	Loss 1.3969 (1.5160)	Prec@(1,5) (68.4%, 92.0%)	
07/03 10:52:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][550/781]	Step 35740	lr 0.01488	Loss 1.1965 (1.5241)	Prec@(1,5) (68.3%, 91.9%)	
07/03 10:53:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][600/781]	Step 35790	lr 0.01488	Loss 1.7423 (1.5224)	Prec@(1,5) (68.3%, 92.0%)	
07/03 10:53:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][650/781]	Step 35840	lr 0.01488	Loss 1.7241 (1.5244)	Prec@(1,5) (68.3%, 92.0%)	
07/03 10:53:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][700/781]	Step 35890	lr 0.01488	Loss 1.5205 (1.5280)	Prec@(1,5) (68.1%, 91.9%)	
07/03 10:53:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][750/781]	Step 35940	lr 0.01488	Loss 1.7331 (1.5312)	Prec@(1,5) (68.1%, 91.9%)	
07/03 10:54:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][781/781]	Step 35971	lr 0.01488	Loss 1.8260 (1.5374)	Prec@(1,5) (67.9%, 91.8%)	
07/03 10:54:07午後 evaluateCell_trainer.py:172 [INFO] Train: [ 45/99] Final Prec@1 67.9100%
07/03 10:54:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][50/391]	Step 35972	Loss 1.0875	Prec@(1,5) (67.6%, 91.7%)
07/03 10:54:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][100/391]	Step 35972	Loss 1.0895	Prec@(1,5) (67.7%, 91.9%)
07/03 10:54:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][150/391]	Step 35972	Loss 1.0801	Prec@(1,5) (68.2%, 92.0%)
07/03 10:54:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][200/391]	Step 35972	Loss 1.0747	Prec@(1,5) (68.6%, 91.9%)
07/03 10:54:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][250/391]	Step 35972	Loss 1.0693	Prec@(1,5) (68.8%, 91.9%)
07/03 10:54:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][300/391]	Step 35972	Loss 1.0730	Prec@(1,5) (68.6%, 91.8%)
07/03 10:54:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][350/391]	Step 35972	Loss 1.0757	Prec@(1,5) (68.5%, 91.9%)
07/03 10:54:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][390/391]	Step 35972	Loss 1.0755	Prec@(1,5) (68.6%, 91.8%)
07/03 10:54:39午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 45/99] Final Prec@1 68.6520%
07/03 10:54:39午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 68.6520%
07/03 10:54:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][50/781]	Step 36022	lr 0.0145	Loss 1.2914 (1.5159)	Prec@(1,5) (68.4%, 92.5%)	
07/03 10:55:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][100/781]	Step 36072	lr 0.0145	Loss 1.5900 (1.5050)	Prec@(1,5) (69.3%, 92.2%)	
07/03 10:55:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][150/781]	Step 36122	lr 0.0145	Loss 1.5238 (1.5149)	Prec@(1,5) (68.4%, 92.1%)	
07/03 10:55:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][200/781]	Step 36172	lr 0.0145	Loss 1.3774 (1.5163)	Prec@(1,5) (68.4%, 92.1%)	
07/03 10:55:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][250/781]	Step 36222	lr 0.0145	Loss 1.7221 (1.5115)	Prec@(1,5) (68.6%, 92.0%)	
07/03 10:56:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][300/781]	Step 36272	lr 0.0145	Loss 1.1526 (1.5119)	Prec@(1,5) (68.6%, 92.0%)	
07/03 10:56:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][350/781]	Step 36322	lr 0.0145	Loss 1.3184 (1.5137)	Prec@(1,5) (68.6%, 92.0%)	
07/03 10:56:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][400/781]	Step 36372	lr 0.0145	Loss 1.6874 (1.5137)	Prec@(1,5) (68.4%, 92.0%)	
07/03 10:56:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][450/781]	Step 36422	lr 0.0145	Loss 1.8857 (1.5178)	Prec@(1,5) (68.3%, 92.0%)	
07/03 10:57:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][500/781]	Step 36472	lr 0.0145	Loss 1.8239 (1.5217)	Prec@(1,5) (68.2%, 91.9%)	
07/03 10:57:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][550/781]	Step 36522	lr 0.0145	Loss 1.9347 (1.5250)	Prec@(1,5) (68.1%, 91.9%)	
07/03 10:57:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][600/781]	Step 36572	lr 0.0145	Loss 1.5887 (1.5267)	Prec@(1,5) (68.1%, 91.9%)	
07/03 10:57:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][650/781]	Step 36622	lr 0.0145	Loss 1.9088 (1.5266)	Prec@(1,5) (68.1%, 91.9%)	
07/03 10:58:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][700/781]	Step 36672	lr 0.0145	Loss 1.6014 (1.5305)	Prec@(1,5) (68.1%, 91.8%)	
07/03 10:58:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][750/781]	Step 36722	lr 0.0145	Loss 1.7700 (1.5327)	Prec@(1,5) (68.0%, 91.8%)	
07/03 10:58:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][781/781]	Step 36753	lr 0.0145	Loss 1.2613 (1.5356)	Prec@(1,5) (68.0%, 91.8%)	
07/03 10:58:33午後 evaluateCell_trainer.py:172 [INFO] Train: [ 46/99] Final Prec@1 67.9540%
07/03 10:58:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][50/391]	Step 36754	Loss 1.1334	Prec@(1,5) (67.7%, 90.9%)
07/03 10:58:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][100/391]	Step 36754	Loss 1.1156	Prec@(1,5) (67.7%, 91.4%)
07/03 10:58:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][150/391]	Step 36754	Loss 1.1070	Prec@(1,5) (67.8%, 91.7%)
07/03 10:58:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][200/391]	Step 36754	Loss 1.1059	Prec@(1,5) (67.7%, 91.6%)
07/03 10:58:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][250/391]	Step 36754	Loss 1.1094	Prec@(1,5) (67.6%, 91.6%)
07/03 10:58:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][300/391]	Step 36754	Loss 1.1058	Prec@(1,5) (67.6%, 91.7%)
07/03 10:59:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][350/391]	Step 36754	Loss 1.1040	Prec@(1,5) (67.6%, 91.8%)
07/03 10:59:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][390/391]	Step 36754	Loss 1.1048	Prec@(1,5) (67.6%, 91.7%)
07/03 10:59:06午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 46/99] Final Prec@1 67.6640%
07/03 10:59:06午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 68.6520%
07/03 10:59:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][50/781]	Step 36804	lr 0.01413	Loss 1.4285 (1.4540)	Prec@(1,5) (69.2%, 93.0%)	
07/03 10:59:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][100/781]	Step 36854	lr 0.01413	Loss 1.3276 (1.4533)	Prec@(1,5) (70.0%, 92.9%)	
07/03 10:59:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][150/781]	Step 36904	lr 0.01413	Loss 1.3364 (1.4516)	Prec@(1,5) (69.7%, 93.1%)	
07/03 11:00:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][200/781]	Step 36954	lr 0.01413	Loss 1.3307 (1.4616)	Prec@(1,5) (69.4%, 93.0%)	
07/03 11:00:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][250/781]	Step 37004	lr 0.01413	Loss 1.4071 (1.4567)	Prec@(1,5) (69.4%, 93.0%)	
07/03 11:00:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][300/781]	Step 37054	lr 0.01413	Loss 1.1663 (1.4690)	Prec@(1,5) (69.1%, 92.7%)	
07/03 11:00:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][350/781]	Step 37104	lr 0.01413	Loss 1.3773 (1.4780)	Prec@(1,5) (68.9%, 92.6%)	
07/03 11:01:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][400/781]	Step 37154	lr 0.01413	Loss 1.3685 (1.4791)	Prec@(1,5) (68.9%, 92.6%)	
07/03 11:01:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][450/781]	Step 37204	lr 0.01413	Loss 1.6979 (1.4773)	Prec@(1,5) (68.9%, 92.6%)	
07/03 11:01:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][500/781]	Step 37254	lr 0.01413	Loss 1.3139 (1.4815)	Prec@(1,5) (68.9%, 92.5%)	
07/03 11:01:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][550/781]	Step 37304	lr 0.01413	Loss 1.4267 (1.4875)	Prec@(1,5) (68.8%, 92.5%)	
07/03 11:02:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][600/781]	Step 37354	lr 0.01413	Loss 1.1614 (1.4892)	Prec@(1,5) (68.8%, 92.4%)	
07/03 11:02:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][650/781]	Step 37404	lr 0.01413	Loss 1.6008 (1.4880)	Prec@(1,5) (68.8%, 92.4%)	
07/03 11:02:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][700/781]	Step 37454	lr 0.01413	Loss 1.4805 (1.4918)	Prec@(1,5) (68.8%, 92.3%)	
07/03 11:02:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][750/781]	Step 37504	lr 0.01413	Loss 1.3451 (1.4917)	Prec@(1,5) (68.8%, 92.3%)	
07/03 11:02:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][781/781]	Step 37535	lr 0.01413	Loss 1.5505 (1.4938)	Prec@(1,5) (68.8%, 92.3%)	
07/03 11:03:00午後 evaluateCell_trainer.py:172 [INFO] Train: [ 47/99] Final Prec@1 68.7580%
07/03 11:03:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][50/391]	Step 37536	Loss 1.1307	Prec@(1,5) (67.2%, 92.0%)
07/03 11:03:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][100/391]	Step 37536	Loss 1.1205	Prec@(1,5) (67.1%, 92.2%)
07/03 11:03:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][150/391]	Step 37536	Loss 1.1205	Prec@(1,5) (67.6%, 92.0%)
07/03 11:03:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][200/391]	Step 37536	Loss 1.1312	Prec@(1,5) (67.3%, 91.9%)
07/03 11:03:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][250/391]	Step 37536	Loss 1.1252	Prec@(1,5) (67.4%, 91.9%)
07/03 11:03:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][300/391]	Step 37536	Loss 1.1196	Prec@(1,5) (67.6%, 91.9%)
07/03 11:03:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][350/391]	Step 37536	Loss 1.1208	Prec@(1,5) (67.5%, 91.9%)
07/03 11:03:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][390/391]	Step 37536	Loss 1.1172	Prec@(1,5) (67.6%, 91.9%)
07/03 11:03:32午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 47/99] Final Prec@1 67.5680%
07/03 11:03:32午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 68.6520%
07/03 11:03:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][50/781]	Step 37586	lr 0.01375	Loss 0.8679 (1.4236)	Prec@(1,5) (70.3%, 93.1%)	
07/03 11:04:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][100/781]	Step 37636	lr 0.01375	Loss 1.6575 (1.4274)	Prec@(1,5) (70.4%, 92.8%)	
07/03 11:04:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][150/781]	Step 37686	lr 0.01375	Loss 1.2164 (1.4314)	Prec@(1,5) (70.3%, 93.0%)	
07/03 11:04:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][200/781]	Step 37736	lr 0.01375	Loss 1.6059 (1.4364)	Prec@(1,5) (70.2%, 92.8%)	
07/03 11:04:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][250/781]	Step 37786	lr 0.01375	Loss 1.6366 (1.4366)	Prec@(1,5) (70.2%, 92.8%)	
07/03 11:05:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][300/781]	Step 37836	lr 0.01375	Loss 1.5739 (1.4351)	Prec@(1,5) (70.1%, 92.9%)	
07/03 11:05:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][350/781]	Step 37886	lr 0.01375	Loss 1.7833 (1.4460)	Prec@(1,5) (69.9%, 92.7%)	
07/03 11:05:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][400/781]	Step 37936	lr 0.01375	Loss 1.0459 (1.4408)	Prec@(1,5) (70.0%, 92.7%)	
07/03 11:05:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][450/781]	Step 37986	lr 0.01375	Loss 1.6702 (1.4494)	Prec@(1,5) (69.7%, 92.6%)	
07/03 11:06:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][500/781]	Step 38036	lr 0.01375	Loss 1.5334 (1.4509)	Prec@(1,5) (69.7%, 92.5%)	
07/03 11:06:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][550/781]	Step 38086	lr 0.01375	Loss 1.4668 (1.4494)	Prec@(1,5) (69.8%, 92.6%)	
07/03 11:06:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][600/781]	Step 38136	lr 0.01375	Loss 2.3992 (1.4512)	Prec@(1,5) (69.7%, 92.6%)	
07/03 11:06:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][650/781]	Step 38186	lr 0.01375	Loss 1.0910 (1.4509)	Prec@(1,5) (69.7%, 92.6%)	
07/03 11:07:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][700/781]	Step 38236	lr 0.01375	Loss 1.4365 (1.4546)	Prec@(1,5) (69.6%, 92.6%)	
07/03 11:07:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][750/781]	Step 38286	lr 0.01375	Loss 1.7038 (1.4575)	Prec@(1,5) (69.6%, 92.6%)	
07/03 11:07:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][781/781]	Step 38317	lr 0.01375	Loss 1.5326 (1.4592)	Prec@(1,5) (69.6%, 92.5%)	
07/03 11:07:25午後 evaluateCell_trainer.py:172 [INFO] Train: [ 48/99] Final Prec@1 69.5600%
07/03 11:07:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][50/391]	Step 38318	Loss 1.0521	Prec@(1,5) (68.0%, 92.1%)
07/03 11:07:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][100/391]	Step 38318	Loss 1.0660	Prec@(1,5) (67.7%, 92.2%)
07/03 11:07:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][150/391]	Step 38318	Loss 1.0591	Prec@(1,5) (68.3%, 92.3%)
07/03 11:07:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][200/391]	Step 38318	Loss 1.0680	Prec@(1,5) (68.4%, 92.1%)
07/03 11:07:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][250/391]	Step 38318	Loss 1.0634	Prec@(1,5) (68.5%, 92.2%)
07/03 11:07:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][300/391]	Step 38318	Loss 1.0625	Prec@(1,5) (68.6%, 92.1%)
07/03 11:07:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][350/391]	Step 38318	Loss 1.0695	Prec@(1,5) (68.4%, 92.1%)
07/03 11:07:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][390/391]	Step 38318	Loss 1.0705	Prec@(1,5) (68.5%, 92.1%)
07/03 11:07:57午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 48/99] Final Prec@1 68.4600%
07/03 11:07:57午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 68.6520%
07/03 11:08:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][50/781]	Step 38368	lr 0.01338	Loss 1.4327 (1.3992)	Prec@(1,5) (70.7%, 93.3%)	
07/03 11:08:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][100/781]	Step 38418	lr 0.01338	Loss 1.6763 (1.3931)	Prec@(1,5) (71.0%, 93.1%)	
07/03 11:08:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][150/781]	Step 38468	lr 0.01338	Loss 1.4989 (1.4002)	Prec@(1,5) (70.6%, 93.3%)	
07/03 11:08:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][200/781]	Step 38518	lr 0.01338	Loss 1.6259 (1.3954)	Prec@(1,5) (70.8%, 93.4%)	
07/03 11:09:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][250/781]	Step 38568	lr 0.01338	Loss 1.1632 (1.3929)	Prec@(1,5) (70.9%, 93.4%)	
07/03 11:09:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][300/781]	Step 38618	lr 0.01338	Loss 1.3393 (1.4019)	Prec@(1,5) (70.6%, 93.3%)	
07/03 11:09:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][350/781]	Step 38668	lr 0.01338	Loss 1.4975 (1.4065)	Prec@(1,5) (70.5%, 93.2%)	
07/03 11:09:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][400/781]	Step 38718	lr 0.01338	Loss 1.7846 (1.4147)	Prec@(1,5) (70.4%, 93.1%)	
07/03 11:10:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][450/781]	Step 38768	lr 0.01338	Loss 1.4253 (1.4168)	Prec@(1,5) (70.3%, 93.1%)	
07/03 11:10:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][500/781]	Step 38818	lr 0.01338	Loss 1.1013 (1.4141)	Prec@(1,5) (70.3%, 93.2%)	
07/03 11:10:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][550/781]	Step 38868	lr 0.01338	Loss 1.3021 (1.4205)	Prec@(1,5) (70.2%, 93.1%)	
07/03 11:10:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][600/781]	Step 38918	lr 0.01338	Loss 1.2009 (1.4255)	Prec@(1,5) (70.1%, 93.0%)	
07/03 11:11:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][650/781]	Step 38968	lr 0.01338	Loss 1.8969 (1.4272)	Prec@(1,5) (70.0%, 93.0%)	
07/03 11:11:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][700/781]	Step 39018	lr 0.01338	Loss 1.4509 (1.4314)	Prec@(1,5) (69.9%, 92.9%)	
07/03 11:11:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][750/781]	Step 39068	lr 0.01338	Loss 1.4380 (1.4327)	Prec@(1,5) (70.0%, 92.9%)	
07/03 11:11:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][781/781]	Step 39099	lr 0.01338	Loss 1.5586 (1.4357)	Prec@(1,5) (69.9%, 92.9%)	
07/03 11:11:51午後 evaluateCell_trainer.py:172 [INFO] Train: [ 49/99] Final Prec@1 69.8860%
07/03 11:11:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][50/391]	Step 39100	Loss 1.0701	Prec@(1,5) (68.6%, 92.0%)
07/03 11:12:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][100/391]	Step 39100	Loss 1.0762	Prec@(1,5) (68.7%, 91.9%)
07/03 11:12:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][150/391]	Step 39100	Loss 1.0521	Prec@(1,5) (69.2%, 92.3%)
07/03 11:12:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][200/391]	Step 39100	Loss 1.0548	Prec@(1,5) (69.0%, 92.2%)
07/03 11:12:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][250/391]	Step 39100	Loss 1.0521	Prec@(1,5) (68.9%, 92.3%)
07/03 11:12:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][300/391]	Step 39100	Loss 1.0514	Prec@(1,5) (69.0%, 92.4%)
07/03 11:12:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][350/391]	Step 39100	Loss 1.0561	Prec@(1,5) (68.8%, 92.4%)
07/03 11:12:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][390/391]	Step 39100	Loss 1.0604	Prec@(1,5) (68.8%, 92.3%)
07/03 11:12:24午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 49/99] Final Prec@1 68.7880%
07/03 11:12:24午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 68.7880%
07/03 11:12:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][50/781]	Step 39150	lr 0.013	Loss 1.3077 (1.3231)	Prec@(1,5) (72.4%, 94.4%)	
07/03 11:12:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][100/781]	Step 39200	lr 0.013	Loss 1.5233 (1.2973)	Prec@(1,5) (72.9%, 94.3%)	
07/03 11:13:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][150/781]	Step 39250	lr 0.013	Loss 1.5270 (1.3152)	Prec@(1,5) (72.5%, 94.0%)	
07/03 11:13:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][200/781]	Step 39300	lr 0.013	Loss 1.0310 (1.3479)	Prec@(1,5) (71.8%, 93.8%)	
07/03 11:13:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][250/781]	Step 39350	lr 0.013	Loss 1.7280 (1.3505)	Prec@(1,5) (71.8%, 93.6%)	
07/03 11:13:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][300/781]	Step 39400	lr 0.013	Loss 1.1036 (1.3591)	Prec@(1,5) (71.6%, 93.6%)	
07/03 11:14:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][350/781]	Step 39450	lr 0.013	Loss 1.3859 (1.3605)	Prec@(1,5) (71.5%, 93.7%)	
07/03 11:14:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][400/781]	Step 39500	lr 0.013	Loss 1.0328 (1.3620)	Prec@(1,5) (71.5%, 93.6%)	
07/03 11:14:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][450/781]	Step 39550	lr 0.013	Loss 1.2967 (1.3653)	Prec@(1,5) (71.5%, 93.5%)	
07/03 11:14:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][500/781]	Step 39600	lr 0.013	Loss 1.2802 (1.3759)	Prec@(1,5) (71.3%, 93.4%)	
07/03 11:15:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][550/781]	Step 39650	lr 0.013	Loss 1.3474 (1.3781)	Prec@(1,5) (71.3%, 93.4%)	
07/03 11:15:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][600/781]	Step 39700	lr 0.013	Loss 1.6076 (1.3795)	Prec@(1,5) (71.2%, 93.5%)	
07/03 11:15:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][650/781]	Step 39750	lr 0.013	Loss 1.7004 (1.3878)	Prec@(1,5) (71.1%, 93.4%)	
07/03 11:15:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][700/781]	Step 39800	lr 0.013	Loss 1.2596 (1.3940)	Prec@(1,5) (71.0%, 93.3%)	
07/03 11:16:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][750/781]	Step 39850	lr 0.013	Loss 1.2351 (1.3959)	Prec@(1,5) (70.9%, 93.3%)	
07/03 11:16:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][781/781]	Step 39881	lr 0.013	Loss 1.0461 (1.3977)	Prec@(1,5) (70.9%, 93.3%)	
07/03 11:16:18午後 evaluateCell_trainer.py:172 [INFO] Train: [ 50/99] Final Prec@1 70.9400%
07/03 11:16:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][50/391]	Step 39882	Loss 1.1088	Prec@(1,5) (67.7%, 92.4%)
07/03 11:16:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][100/391]	Step 39882	Loss 1.0899	Prec@(1,5) (68.1%, 92.5%)
07/03 11:16:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][150/391]	Step 39882	Loss 1.0893	Prec@(1,5) (68.0%, 92.5%)
07/03 11:16:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][200/391]	Step 39882	Loss 1.0925	Prec@(1,5) (68.2%, 92.3%)
07/03 11:16:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][250/391]	Step 39882	Loss 1.0911	Prec@(1,5) (68.3%, 92.3%)
07/03 11:16:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][300/391]	Step 39882	Loss 1.0933	Prec@(1,5) (68.2%, 92.3%)
07/03 11:16:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][350/391]	Step 39882	Loss 1.0865	Prec@(1,5) (68.4%, 92.2%)
07/03 11:16:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][390/391]	Step 39882	Loss 1.0856	Prec@(1,5) (68.5%, 92.3%)
07/03 11:16:50午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 50/99] Final Prec@1 68.5000%
07/03 11:16:50午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 68.7880%
07/03 11:17:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][50/781]	Step 39932	lr 0.01262	Loss 1.1800 (1.3395)	Prec@(1,5) (71.7%, 94.3%)	
07/03 11:17:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][100/781]	Step 39982	lr 0.01262	Loss 1.5994 (1.3477)	Prec@(1,5) (71.6%, 93.9%)	
07/03 11:17:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][150/781]	Step 40032	lr 0.01262	Loss 1.5784 (1.3533)	Prec@(1,5) (71.1%, 94.0%)	
07/03 11:17:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][200/781]	Step 40082	lr 0.01262	Loss 1.1525 (1.3545)	Prec@(1,5) (71.1%, 93.9%)	
07/03 11:18:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][250/781]	Step 40132	lr 0.01262	Loss 1.3051 (1.3591)	Prec@(1,5) (71.3%, 93.8%)	
07/03 11:18:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][300/781]	Step 40182	lr 0.01262	Loss 1.2653 (1.3549)	Prec@(1,5) (71.4%, 93.9%)	
07/03 11:18:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][350/781]	Step 40232	lr 0.01262	Loss 1.5212 (1.3598)	Prec@(1,5) (71.4%, 93.7%)	
07/03 11:18:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][400/781]	Step 40282	lr 0.01262	Loss 1.1588 (1.3589)	Prec@(1,5) (71.4%, 93.8%)	
07/03 11:19:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][450/781]	Step 40332	lr 0.01262	Loss 1.5227 (1.3609)	Prec@(1,5) (71.5%, 93.8%)	
07/03 11:19:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][500/781]	Step 40382	lr 0.01262	Loss 1.4209 (1.3646)	Prec@(1,5) (71.3%, 93.8%)	
07/03 11:19:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][550/781]	Step 40432	lr 0.01262	Loss 1.3914 (1.3591)	Prec@(1,5) (71.4%, 93.8%)	
07/03 11:19:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][600/781]	Step 40482	lr 0.01262	Loss 1.5502 (1.3620)	Prec@(1,5) (71.4%, 93.7%)	
07/03 11:20:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][650/781]	Step 40532	lr 0.01262	Loss 1.1573 (1.3668)	Prec@(1,5) (71.3%, 93.6%)	
07/03 11:20:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][700/781]	Step 40582	lr 0.01262	Loss 1.5127 (1.3672)	Prec@(1,5) (71.3%, 93.6%)	
07/03 11:20:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][750/781]	Step 40632	lr 0.01262	Loss 1.5153 (1.3691)	Prec@(1,5) (71.3%, 93.6%)	
07/03 11:20:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][781/781]	Step 40663	lr 0.01262	Loss 1.3703 (1.3702)	Prec@(1,5) (71.3%, 93.6%)	
07/03 11:20:44午後 evaluateCell_trainer.py:172 [INFO] Train: [ 51/99] Final Prec@1 71.2520%
07/03 11:20:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][50/391]	Step 40664	Loss 0.9640	Prec@(1,5) (72.0%, 92.6%)
07/03 11:20:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][100/391]	Step 40664	Loss 0.9398	Prec@(1,5) (72.5%, 93.3%)
07/03 11:20:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][150/391]	Step 40664	Loss 0.9317	Prec@(1,5) (72.7%, 93.6%)
07/03 11:21:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][200/391]	Step 40664	Loss 0.9352	Prec@(1,5) (72.4%, 93.6%)
07/03 11:21:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][250/391]	Step 40664	Loss 0.9311	Prec@(1,5) (72.3%, 93.7%)
07/03 11:21:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][300/391]	Step 40664	Loss 0.9335	Prec@(1,5) (72.4%, 93.7%)
07/03 11:21:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][350/391]	Step 40664	Loss 0.9339	Prec@(1,5) (72.4%, 93.8%)
07/03 11:21:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][390/391]	Step 40664	Loss 0.9346	Prec@(1,5) (72.4%, 93.8%)
07/03 11:21:15午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 51/99] Final Prec@1 72.4200%
07/03 11:21:16午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 72.4200%
07/03 11:21:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][50/781]	Step 40714	lr 0.01225	Loss 1.6214 (1.2619)	Prec@(1,5) (72.5%, 94.6%)	
07/03 11:21:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][100/781]	Step 40764	lr 0.01225	Loss 0.8717 (1.2740)	Prec@(1,5) (73.1%, 94.2%)	
07/03 11:22:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][150/781]	Step 40814	lr 0.01225	Loss 1.1268 (1.2930)	Prec@(1,5) (72.5%, 94.2%)	
07/03 11:22:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][200/781]	Step 40864	lr 0.01225	Loss 1.5352 (1.3057)	Prec@(1,5) (72.2%, 94.2%)	
07/03 11:22:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][250/781]	Step 40914	lr 0.01225	Loss 1.0130 (1.3098)	Prec@(1,5) (72.2%, 94.0%)	
07/03 11:22:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][300/781]	Step 40964	lr 0.01225	Loss 1.2243 (1.3190)	Prec@(1,5) (72.1%, 94.0%)	
07/03 11:23:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][350/781]	Step 41014	lr 0.01225	Loss 1.3900 (1.3248)	Prec@(1,5) (72.0%, 93.9%)	
07/03 11:23:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][400/781]	Step 41064	lr 0.01225	Loss 1.4305 (1.3199)	Prec@(1,5) (72.1%, 94.0%)	
07/03 11:23:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][450/781]	Step 41114	lr 0.01225	Loss 1.4436 (1.3203)	Prec@(1,5) (72.1%, 93.9%)	
07/03 11:23:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][500/781]	Step 41164	lr 0.01225	Loss 1.2851 (1.3230)	Prec@(1,5) (72.1%, 93.9%)	
07/03 11:24:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][550/781]	Step 41214	lr 0.01225	Loss 1.2151 (1.3283)	Prec@(1,5) (72.1%, 93.8%)	
07/03 11:24:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][600/781]	Step 41264	lr 0.01225	Loss 1.2225 (1.3304)	Prec@(1,5) (72.1%, 93.7%)	
07/03 11:24:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][650/781]	Step 41314	lr 0.01225	Loss 1.5779 (1.3318)	Prec@(1,5) (71.9%, 93.8%)	
07/03 11:24:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][700/781]	Step 41364	lr 0.01225	Loss 1.4753 (1.3357)	Prec@(1,5) (71.9%, 93.7%)	
07/03 11:25:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][750/781]	Step 41414	lr 0.01225	Loss 1.4428 (1.3384)	Prec@(1,5) (71.9%, 93.7%)	
07/03 11:25:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][781/781]	Step 41445	lr 0.01225	Loss 1.5135 (1.3383)	Prec@(1,5) (71.9%, 93.6%)	
07/03 11:25:10午後 evaluateCell_trainer.py:172 [INFO] Train: [ 52/99] Final Prec@1 71.8840%
07/03 11:25:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][50/391]	Step 41446	Loss 0.9536	Prec@(1,5) (71.7%, 93.4%)
07/03 11:25:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][100/391]	Step 41446	Loss 0.9948	Prec@(1,5) (70.5%, 93.1%)
07/03 11:25:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][150/391]	Step 41446	Loss 1.0010	Prec@(1,5) (70.6%, 93.2%)
07/03 11:25:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][200/391]	Step 41446	Loss 1.0043	Prec@(1,5) (70.6%, 93.0%)
07/03 11:25:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][250/391]	Step 41446	Loss 0.9954	Prec@(1,5) (70.8%, 93.1%)
07/03 11:25:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][300/391]	Step 41446	Loss 0.9960	Prec@(1,5) (70.7%, 93.2%)
07/03 11:25:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][350/391]	Step 41446	Loss 0.9989	Prec@(1,5) (70.7%, 93.2%)
07/03 11:25:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][390/391]	Step 41446	Loss 1.0019	Prec@(1,5) (70.5%, 93.2%)
07/03 11:25:42午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 52/99] Final Prec@1 70.4640%
07/03 11:25:42午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 72.4200%
07/03 11:25:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][50/781]	Step 41496	lr 0.01187	Loss 1.5044 (1.2526)	Prec@(1,5) (75.0%, 94.6%)	
07/03 11:26:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][100/781]	Step 41546	lr 0.01187	Loss 1.2309 (1.2350)	Prec@(1,5) (75.1%, 94.8%)	
07/03 11:26:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][150/781]	Step 41596	lr 0.01187	Loss 1.2078 (1.2581)	Prec@(1,5) (74.2%, 94.5%)	
07/03 11:26:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][200/781]	Step 41646	lr 0.01187	Loss 1.6018 (1.2675)	Prec@(1,5) (73.8%, 94.4%)	
07/03 11:26:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][250/781]	Step 41696	lr 0.01187	Loss 1.4473 (1.2653)	Prec@(1,5) (73.8%, 94.4%)	
07/03 11:27:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][300/781]	Step 41746	lr 0.01187	Loss 1.1822 (1.2639)	Prec@(1,5) (73.8%, 94.5%)	
07/03 11:27:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][350/781]	Step 41796	lr 0.01187	Loss 1.7176 (1.2766)	Prec@(1,5) (73.5%, 94.4%)	
07/03 11:27:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][400/781]	Step 41846	lr 0.01187	Loss 1.3989 (1.2746)	Prec@(1,5) (73.5%, 94.4%)	
07/03 11:27:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][450/781]	Step 41896	lr 0.01187	Loss 0.9141 (1.2810)	Prec@(1,5) (73.4%, 94.2%)	
07/03 11:28:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][500/781]	Step 41946	lr 0.01187	Loss 1.2323 (1.2859)	Prec@(1,5) (73.2%, 94.2%)	
07/03 11:28:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][550/781]	Step 41996	lr 0.01187	Loss 1.3096 (1.2872)	Prec@(1,5) (73.2%, 94.2%)	
07/03 11:28:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][600/781]	Step 42046	lr 0.01187	Loss 1.4938 (1.2878)	Prec@(1,5) (73.1%, 94.2%)	
07/03 11:28:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][650/781]	Step 42096	lr 0.01187	Loss 1.3666 (1.2919)	Prec@(1,5) (73.0%, 94.2%)	
07/03 11:29:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][700/781]	Step 42146	lr 0.01187	Loss 1.3007 (1.2962)	Prec@(1,5) (72.9%, 94.1%)	
07/03 11:29:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][750/781]	Step 42196	lr 0.01187	Loss 1.2695 (1.2970)	Prec@(1,5) (72.9%, 94.2%)	
07/03 11:29:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][781/781]	Step 42227	lr 0.01187	Loss 1.0867 (1.3003)	Prec@(1,5) (72.7%, 94.1%)	
07/03 11:29:36午後 evaluateCell_trainer.py:172 [INFO] Train: [ 53/99] Final Prec@1 72.7240%
07/03 11:29:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][50/391]	Step 42228	Loss 0.8789	Prec@(1,5) (73.4%, 95.0%)
07/03 11:29:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][100/391]	Step 42228	Loss 0.8860	Prec@(1,5) (73.3%, 94.9%)
07/03 11:29:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][150/391]	Step 42228	Loss 0.8872	Prec@(1,5) (73.5%, 94.8%)
07/03 11:29:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][200/391]	Step 42228	Loss 0.8797	Prec@(1,5) (73.6%, 94.8%)
07/03 11:29:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][250/391]	Step 42228	Loss 0.8894	Prec@(1,5) (73.3%, 94.7%)
07/03 11:30:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][300/391]	Step 42228	Loss 0.8895	Prec@(1,5) (73.3%, 94.6%)
07/03 11:30:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][350/391]	Step 42228	Loss 0.8900	Prec@(1,5) (73.2%, 94.5%)
07/03 11:30:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][390/391]	Step 42228	Loss 0.8923	Prec@(1,5) (73.1%, 94.5%)
07/03 11:30:09午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 53/99] Final Prec@1 73.0920%
07/03 11:30:09午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 73.0920%
07/03 11:30:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][50/781]	Step 42278	lr 0.0115	Loss 1.3639 (1.2444)	Prec@(1,5) (73.7%, 94.5%)	
07/03 11:30:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][100/781]	Step 42328	lr 0.0115	Loss 1.2560 (1.2734)	Prec@(1,5) (73.0%, 94.4%)	
07/03 11:30:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][150/781]	Step 42378	lr 0.0115	Loss 1.5527 (1.2638)	Prec@(1,5) (73.3%, 94.4%)	
07/03 11:31:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][200/781]	Step 42428	lr 0.0115	Loss 1.3994 (1.2616)	Prec@(1,5) (73.3%, 94.4%)	
07/03 11:31:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][250/781]	Step 42478	lr 0.0115	Loss 1.0168 (1.2570)	Prec@(1,5) (73.4%, 94.4%)	
07/03 11:31:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][300/781]	Step 42528	lr 0.0115	Loss 1.3492 (1.2696)	Prec@(1,5) (73.2%, 94.3%)	
07/03 11:31:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][350/781]	Step 42578	lr 0.0115	Loss 1.4226 (1.2668)	Prec@(1,5) (73.3%, 94.2%)	
07/03 11:32:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][400/781]	Step 42628	lr 0.0115	Loss 1.5818 (1.2716)	Prec@(1,5) (73.2%, 94.3%)	
07/03 11:32:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][450/781]	Step 42678	lr 0.0115	Loss 1.1287 (1.2750)	Prec@(1,5) (73.2%, 94.2%)	
07/03 11:32:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][500/781]	Step 42728	lr 0.0115	Loss 1.0673 (1.2747)	Prec@(1,5) (73.2%, 94.2%)	
07/03 11:32:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][550/781]	Step 42778	lr 0.0115	Loss 1.3977 (1.2726)	Prec@(1,5) (73.3%, 94.3%)	
07/03 11:33:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][600/781]	Step 42828	lr 0.0115	Loss 1.0751 (1.2725)	Prec@(1,5) (73.3%, 94.3%)	
07/03 11:33:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][650/781]	Step 42878	lr 0.0115	Loss 1.6038 (1.2742)	Prec@(1,5) (73.2%, 94.2%)	
07/03 11:33:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][700/781]	Step 42928	lr 0.0115	Loss 1.4281 (1.2759)	Prec@(1,5) (73.2%, 94.2%)	
07/03 11:33:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][750/781]	Step 42978	lr 0.0115	Loss 1.1071 (1.2798)	Prec@(1,5) (73.2%, 94.1%)	
07/03 11:34:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][781/781]	Step 43009	lr 0.0115	Loss 1.2585 (1.2774)	Prec@(1,5) (73.2%, 94.1%)	
07/03 11:34:02午後 evaluateCell_trainer.py:172 [INFO] Train: [ 54/99] Final Prec@1 73.2180%
07/03 11:34:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][50/391]	Step 43010	Loss 0.8819	Prec@(1,5) (73.6%, 94.2%)
07/03 11:34:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][100/391]	Step 43010	Loss 0.8862	Prec@(1,5) (73.6%, 94.3%)
07/03 11:34:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][150/391]	Step 43010	Loss 0.8903	Prec@(1,5) (73.4%, 94.3%)
07/03 11:34:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][200/391]	Step 43010	Loss 0.8932	Prec@(1,5) (73.3%, 94.2%)
07/03 11:34:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][250/391]	Step 43010	Loss 0.8929	Prec@(1,5) (73.3%, 94.3%)
07/03 11:34:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][300/391]	Step 43010	Loss 0.8913	Prec@(1,5) (73.3%, 94.3%)
07/03 11:34:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][350/391]	Step 43010	Loss 0.8921	Prec@(1,5) (73.3%, 94.3%)
07/03 11:34:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][390/391]	Step 43010	Loss 0.8913	Prec@(1,5) (73.4%, 94.3%)
07/03 11:34:34午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 54/99] Final Prec@1 73.3920%
07/03 11:34:34午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 73.3920%
07/03 11:34:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][50/781]	Step 43060	lr 0.01112	Loss 1.0899 (1.2150)	Prec@(1,5) (74.9%, 94.6%)	
07/03 11:35:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][100/781]	Step 43110	lr 0.01112	Loss 1.3629 (1.1914)	Prec@(1,5) (75.0%, 95.1%)	
07/03 11:35:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][150/781]	Step 43160	lr 0.01112	Loss 0.9878 (1.1819)	Prec@(1,5) (75.1%, 94.9%)	
07/03 11:35:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][200/781]	Step 43210	lr 0.01112	Loss 1.1753 (1.1964)	Prec@(1,5) (74.8%, 94.9%)	
07/03 11:35:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][250/781]	Step 43260	lr 0.01112	Loss 1.0212 (1.2098)	Prec@(1,5) (74.6%, 94.8%)	
07/03 11:36:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][300/781]	Step 43310	lr 0.01112	Loss 1.5071 (1.2143)	Prec@(1,5) (74.3%, 94.7%)	
07/03 11:36:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][350/781]	Step 43360	lr 0.01112	Loss 1.3012 (1.2228)	Prec@(1,5) (74.2%, 94.7%)	
07/03 11:36:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][400/781]	Step 43410	lr 0.01112	Loss 0.8506 (1.2305)	Prec@(1,5) (74.1%, 94.6%)	
07/03 11:36:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][450/781]	Step 43460	lr 0.01112	Loss 0.9305 (1.2321)	Prec@(1,5) (74.1%, 94.5%)	
07/03 11:37:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][500/781]	Step 43510	lr 0.01112	Loss 0.8940 (1.2333)	Prec@(1,5) (74.0%, 94.5%)	
07/03 11:37:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][550/781]	Step 43560	lr 0.01112	Loss 1.3110 (1.2376)	Prec@(1,5) (74.0%, 94.5%)	
07/03 11:37:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][600/781]	Step 43610	lr 0.01112	Loss 1.3428 (1.2440)	Prec@(1,5) (73.8%, 94.5%)	
07/03 11:37:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][650/781]	Step 43660	lr 0.01112	Loss 0.8198 (1.2450)	Prec@(1,5) (73.8%, 94.5%)	
07/03 11:38:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][700/781]	Step 43710	lr 0.01112	Loss 1.4071 (1.2473)	Prec@(1,5) (73.8%, 94.5%)	
07/03 11:38:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][750/781]	Step 43760	lr 0.01112	Loss 1.2131 (1.2518)	Prec@(1,5) (73.7%, 94.5%)	
07/03 11:38:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][781/781]	Step 43791	lr 0.01112	Loss 1.6891 (1.2556)	Prec@(1,5) (73.6%, 94.5%)	
07/03 11:38:28午後 evaluateCell_trainer.py:172 [INFO] Train: [ 55/99] Final Prec@1 73.6160%
07/03 11:38:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][50/391]	Step 43792	Loss 0.8199	Prec@(1,5) (76.1%, 95.0%)
07/03 11:38:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][100/391]	Step 43792	Loss 0.8199	Prec@(1,5) (75.3%, 95.3%)
07/03 11:38:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][150/391]	Step 43792	Loss 0.8077	Prec@(1,5) (75.9%, 95.4%)
07/03 11:38:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][200/391]	Step 43792	Loss 0.8079	Prec@(1,5) (76.0%, 95.3%)
07/03 11:38:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][250/391]	Step 43792	Loss 0.8093	Prec@(1,5) (75.8%, 95.2%)
07/03 11:38:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][300/391]	Step 43792	Loss 0.8066	Prec@(1,5) (75.8%, 95.3%)
07/03 11:38:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][350/391]	Step 43792	Loss 0.8092	Prec@(1,5) (75.7%, 95.2%)
07/03 11:39:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][390/391]	Step 43792	Loss 0.8101	Prec@(1,5) (75.7%, 95.2%)
07/03 11:39:00午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 55/99] Final Prec@1 75.6920%
07/03 11:39:01午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 75.6920%
07/03 11:39:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][50/781]	Step 43842	lr 0.01075	Loss 1.0284 (1.1497)	Prec@(1,5) (75.9%, 95.3%)	
07/03 11:39:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][100/781]	Step 43892	lr 0.01075	Loss 1.2013 (1.1500)	Prec@(1,5) (76.1%, 95.4%)	
07/03 11:39:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][150/781]	Step 43942	lr 0.01075	Loss 1.1846 (1.1516)	Prec@(1,5) (75.9%, 95.3%)	
07/03 11:40:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][200/781]	Step 43992	lr 0.01075	Loss 0.8423 (1.1747)	Prec@(1,5) (75.7%, 95.2%)	
07/03 11:40:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][250/781]	Step 44042	lr 0.01075	Loss 1.2820 (1.1952)	Prec@(1,5) (75.1%, 95.0%)	
07/03 11:40:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][300/781]	Step 44092	lr 0.01075	Loss 1.3261 (1.1965)	Prec@(1,5) (75.2%, 95.0%)	
07/03 11:40:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][350/781]	Step 44142	lr 0.01075	Loss 0.8620 (1.1996)	Prec@(1,5) (75.0%, 95.0%)	
07/03 11:41:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][400/781]	Step 44192	lr 0.01075	Loss 1.2791 (1.1978)	Prec@(1,5) (75.1%, 94.9%)	
07/03 11:41:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][450/781]	Step 44242	lr 0.01075	Loss 1.1245 (1.1995)	Prec@(1,5) (74.9%, 94.9%)	
07/03 11:41:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][500/781]	Step 44292	lr 0.01075	Loss 1.3568 (1.1991)	Prec@(1,5) (75.0%, 94.9%)	
07/03 11:41:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][550/781]	Step 44342	lr 0.01075	Loss 1.0610 (1.2031)	Prec@(1,5) (74.9%, 94.9%)	
07/03 11:42:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][600/781]	Step 44392	lr 0.01075	Loss 1.2780 (1.2084)	Prec@(1,5) (74.8%, 94.8%)	
07/03 11:42:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][650/781]	Step 44442	lr 0.01075	Loss 0.8035 (1.2118)	Prec@(1,5) (74.7%, 94.8%)	
07/03 11:42:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][700/781]	Step 44492	lr 0.01075	Loss 1.2746 (1.2141)	Prec@(1,5) (74.6%, 94.8%)	
07/03 11:42:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][750/781]	Step 44542	lr 0.01075	Loss 1.2163 (1.2169)	Prec@(1,5) (74.6%, 94.8%)	
07/03 11:42:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][781/781]	Step 44573	lr 0.01075	Loss 1.0181 (1.2166)	Prec@(1,5) (74.6%, 94.8%)	
07/03 11:42:54午後 evaluateCell_trainer.py:172 [INFO] Train: [ 56/99] Final Prec@1 74.6160%
07/03 11:42:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][50/391]	Step 44574	Loss 0.9095	Prec@(1,5) (72.4%, 93.9%)
07/03 11:43:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][100/391]	Step 44574	Loss 0.8993	Prec@(1,5) (73.0%, 94.3%)
07/03 11:43:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][150/391]	Step 44574	Loss 0.9078	Prec@(1,5) (72.8%, 94.2%)
07/03 11:43:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][200/391]	Step 44574	Loss 0.9082	Prec@(1,5) (72.6%, 94.2%)
07/03 11:43:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][250/391]	Step 44574	Loss 0.9055	Prec@(1,5) (72.7%, 94.2%)
07/03 11:43:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][300/391]	Step 44574	Loss 0.9154	Prec@(1,5) (72.5%, 94.2%)
07/03 11:43:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][350/391]	Step 44574	Loss 0.9101	Prec@(1,5) (72.6%, 94.3%)
07/03 11:43:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][390/391]	Step 44574	Loss 0.9104	Prec@(1,5) (72.6%, 94.3%)
07/03 11:43:27午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 56/99] Final Prec@1 72.6080%
07/03 11:43:27午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 75.6920%
07/03 11:43:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][50/781]	Step 44624	lr 0.01038	Loss 1.1918 (1.1389)	Prec@(1,5) (76.4%, 95.3%)	
07/03 11:43:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][100/781]	Step 44674	lr 0.01038	Loss 0.9602 (1.1271)	Prec@(1,5) (76.4%, 95.7%)	
07/03 11:44:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][150/781]	Step 44724	lr 0.01038	Loss 1.2976 (1.1286)	Prec@(1,5) (76.4%, 95.6%)	
07/03 11:44:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][200/781]	Step 44774	lr 0.01038	Loss 1.3865 (1.1398)	Prec@(1,5) (76.1%, 95.4%)	
07/03 11:44:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][250/781]	Step 44824	lr 0.01038	Loss 0.7485 (1.1343)	Prec@(1,5) (76.2%, 95.5%)	
07/03 11:44:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][300/781]	Step 44874	lr 0.01038	Loss 1.2303 (1.1418)	Prec@(1,5) (75.9%, 95.4%)	
07/03 11:45:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][350/781]	Step 44924	lr 0.01038	Loss 1.3639 (1.1456)	Prec@(1,5) (75.9%, 95.4%)	
07/03 11:45:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][400/781]	Step 44974	lr 0.01038	Loss 1.0931 (1.1519)	Prec@(1,5) (75.8%, 95.2%)	
07/03 11:45:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][450/781]	Step 45024	lr 0.01038	Loss 1.1652 (1.1564)	Prec@(1,5) (75.8%, 95.2%)	
07/03 11:45:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][500/781]	Step 45074	lr 0.01038	Loss 1.2928 (1.1646)	Prec@(1,5) (75.6%, 95.1%)	
07/03 11:46:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][550/781]	Step 45124	lr 0.01038	Loss 1.1796 (1.1711)	Prec@(1,5) (75.5%, 95.1%)	
07/03 11:46:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][600/781]	Step 45174	lr 0.01038	Loss 1.1315 (1.1765)	Prec@(1,5) (75.4%, 95.0%)	
07/03 11:46:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][650/781]	Step 45224	lr 0.01038	Loss 1.1714 (1.1783)	Prec@(1,5) (75.3%, 95.0%)	
07/03 11:46:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][700/781]	Step 45274	lr 0.01038	Loss 1.0826 (1.1781)	Prec@(1,5) (75.3%, 95.0%)	
07/03 11:47:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][750/781]	Step 45324	lr 0.01038	Loss 1.2743 (1.1788)	Prec@(1,5) (75.3%, 95.0%)	
07/03 11:47:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][781/781]	Step 45355	lr 0.01038	Loss 1.3452 (1.1810)	Prec@(1,5) (75.2%, 95.0%)	
07/03 11:47:21午後 evaluateCell_trainer.py:172 [INFO] Train: [ 57/99] Final Prec@1 75.2260%
07/03 11:47:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][50/391]	Step 45356	Loss 0.7830	Prec@(1,5) (76.5%, 95.4%)
07/03 11:47:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][100/391]	Step 45356	Loss 0.8069	Prec@(1,5) (76.0%, 95.1%)
07/03 11:47:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][150/391]	Step 45356	Loss 0.8229	Prec@(1,5) (75.2%, 94.9%)
07/03 11:47:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][200/391]	Step 45356	Loss 0.8332	Prec@(1,5) (75.0%, 94.8%)
07/03 11:47:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][250/391]	Step 45356	Loss 0.8296	Prec@(1,5) (75.1%, 94.9%)
07/03 11:47:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][300/391]	Step 45356	Loss 0.8323	Prec@(1,5) (75.0%, 94.8%)
07/03 11:47:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][350/391]	Step 45356	Loss 0.8333	Prec@(1,5) (75.0%, 94.8%)
07/03 11:47:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][390/391]	Step 45356	Loss 0.8349	Prec@(1,5) (75.0%, 94.8%)
07/03 11:47:53午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 57/99] Final Prec@1 74.9840%
07/03 11:47:53午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 75.6920%
07/03 11:48:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][50/781]	Step 45406	lr 0.01002	Loss 1.2027 (1.0988)	Prec@(1,5) (76.6%, 96.0%)	
07/03 11:48:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][100/781]	Step 45456	lr 0.01002	Loss 1.3770 (1.1031)	Prec@(1,5) (76.9%, 95.6%)	
07/03 11:48:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][150/781]	Step 45506	lr 0.01002	Loss 1.1819 (1.1157)	Prec@(1,5) (76.7%, 95.5%)	
07/03 11:48:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][200/781]	Step 45556	lr 0.01002	Loss 1.0765 (1.1206)	Prec@(1,5) (76.7%, 95.4%)	
07/03 11:49:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][250/781]	Step 45606	lr 0.01002	Loss 0.9975 (1.1269)	Prec@(1,5) (76.5%, 95.3%)	
07/03 11:49:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][300/781]	Step 45656	lr 0.01002	Loss 1.2829 (1.1336)	Prec@(1,5) (76.4%, 95.3%)	
07/03 11:49:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][350/781]	Step 45706	lr 0.01002	Loss 1.5364 (1.1376)	Prec@(1,5) (76.5%, 95.3%)	
07/03 11:49:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][400/781]	Step 45756	lr 0.01002	Loss 0.9488 (1.1405)	Prec@(1,5) (76.3%, 95.2%)	
07/03 11:50:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][450/781]	Step 45806	lr 0.01002	Loss 1.5253 (1.1504)	Prec@(1,5) (76.1%, 95.2%)	
07/03 11:50:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][500/781]	Step 45856	lr 0.01002	Loss 1.0129 (1.1471)	Prec@(1,5) (76.2%, 95.2%)	
07/03 11:50:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][550/781]	Step 45906	lr 0.01002	Loss 1.2787 (1.1513)	Prec@(1,5) (76.0%, 95.2%)	
07/03 11:50:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][600/781]	Step 45956	lr 0.01002	Loss 1.0821 (1.1550)	Prec@(1,5) (75.9%, 95.2%)	
07/03 11:51:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][650/781]	Step 46006	lr 0.01002	Loss 1.4176 (1.1578)	Prec@(1,5) (75.8%, 95.2%)	
07/03 11:51:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][700/781]	Step 46056	lr 0.01002	Loss 1.3087 (1.1599)	Prec@(1,5) (75.7%, 95.2%)	
07/03 11:51:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][750/781]	Step 46106	lr 0.01002	Loss 1.3999 (1.1601)	Prec@(1,5) (75.7%, 95.2%)	
07/03 11:51:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][781/781]	Step 46137	lr 0.01002	Loss 1.6376 (1.1637)	Prec@(1,5) (75.7%, 95.1%)	
07/03 11:51:46午後 evaluateCell_trainer.py:172 [INFO] Train: [ 58/99] Final Prec@1 75.6500%
07/03 11:51:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][50/391]	Step 46138	Loss 0.7564	Prec@(1,5) (77.8%, 95.3%)
07/03 11:51:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][100/391]	Step 46138	Loss 0.7608	Prec@(1,5) (77.3%, 95.4%)
07/03 11:51:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][150/391]	Step 46138	Loss 0.7536	Prec@(1,5) (77.2%, 95.6%)
07/03 11:52:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][200/391]	Step 46138	Loss 0.7567	Prec@(1,5) (77.1%, 95.6%)
07/03 11:52:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][250/391]	Step 46138	Loss 0.7540	Prec@(1,5) (77.3%, 95.6%)
07/03 11:52:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][300/391]	Step 46138	Loss 0.7529	Prec@(1,5) (77.5%, 95.7%)
07/03 11:52:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][350/391]	Step 46138	Loss 0.7523	Prec@(1,5) (77.5%, 95.7%)
07/03 11:52:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][390/391]	Step 46138	Loss 0.7565	Prec@(1,5) (77.4%, 95.6%)
07/03 11:52:19午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 58/99] Final Prec@1 77.3760%
07/03 11:52:19午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 77.3760%
07/03 11:52:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][50/781]	Step 46188	lr 0.00965	Loss 1.0330 (1.0366)	Prec@(1,5) (78.4%, 95.9%)	
07/03 11:52:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][100/781]	Step 46238	lr 0.00965	Loss 1.1750 (1.0618)	Prec@(1,5) (78.0%, 96.0%)	
07/03 11:53:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][150/781]	Step 46288	lr 0.00965	Loss 1.2108 (1.0772)	Prec@(1,5) (77.4%, 96.0%)	
07/03 11:53:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][200/781]	Step 46338	lr 0.00965	Loss 1.6394 (1.0776)	Prec@(1,5) (77.4%, 95.9%)	
07/03 11:53:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][250/781]	Step 46388	lr 0.00965	Loss 1.2314 (1.0781)	Prec@(1,5) (77.4%, 96.0%)	
07/03 11:53:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][300/781]	Step 46438	lr 0.00965	Loss 1.0225 (1.0879)	Prec@(1,5) (77.3%, 95.8%)	
07/03 11:54:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][350/781]	Step 46488	lr 0.00965	Loss 0.8141 (1.0906)	Prec@(1,5) (77.3%, 95.8%)	
07/03 11:54:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][400/781]	Step 46538	lr 0.00965	Loss 1.2052 (1.1006)	Prec@(1,5) (77.1%, 95.7%)	
07/03 11:54:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][450/781]	Step 46588	lr 0.00965	Loss 1.2166 (1.1054)	Prec@(1,5) (77.0%, 95.7%)	
07/03 11:54:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][500/781]	Step 46638	lr 0.00965	Loss 0.9587 (1.1087)	Prec@(1,5) (77.0%, 95.6%)	
07/03 11:55:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][550/781]	Step 46688	lr 0.00965	Loss 1.5090 (1.1146)	Prec@(1,5) (76.9%, 95.6%)	
07/03 11:55:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][600/781]	Step 46738	lr 0.00965	Loss 1.3368 (1.1183)	Prec@(1,5) (76.8%, 95.6%)	
07/03 11:55:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][650/781]	Step 46788	lr 0.00965	Loss 1.2931 (1.1243)	Prec@(1,5) (76.7%, 95.5%)	
07/03 11:55:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][700/781]	Step 46838	lr 0.00965	Loss 1.0388 (1.1219)	Prec@(1,5) (76.7%, 95.5%)	
07/03 11:56:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][750/781]	Step 46888	lr 0.00965	Loss 0.8288 (1.1232)	Prec@(1,5) (76.7%, 95.5%)	
07/03 11:56:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][781/781]	Step 46919	lr 0.00965	Loss 1.2317 (1.1249)	Prec@(1,5) (76.7%, 95.5%)	
07/03 11:56:13午後 evaluateCell_trainer.py:172 [INFO] Train: [ 59/99] Final Prec@1 76.6520%
07/03 11:56:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][50/391]	Step 46920	Loss 0.7985	Prec@(1,5) (75.8%, 95.2%)
07/03 11:56:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][100/391]	Step 46920	Loss 0.8058	Prec@(1,5) (76.0%, 95.1%)
07/03 11:56:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][150/391]	Step 46920	Loss 0.7951	Prec@(1,5) (76.1%, 95.2%)
07/03 11:56:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][200/391]	Step 46920	Loss 0.7948	Prec@(1,5) (76.2%, 95.3%)
07/03 11:56:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][250/391]	Step 46920	Loss 0.7978	Prec@(1,5) (76.1%, 95.3%)
07/03 11:56:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][300/391]	Step 46920	Loss 0.8034	Prec@(1,5) (75.9%, 95.2%)
07/03 11:56:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][350/391]	Step 46920	Loss 0.8040	Prec@(1,5) (76.0%, 95.2%)
07/03 11:56:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][390/391]	Step 46920	Loss 0.8040	Prec@(1,5) (75.8%, 95.3%)
07/03 11:56:46午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 59/99] Final Prec@1 75.8320%
07/03 11:56:46午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 77.3760%
07/03 11:57:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][50/781]	Step 46970	lr 0.00929	Loss 1.2172 (1.1012)	Prec@(1,5) (77.6%, 95.8%)	
07/03 11:57:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][100/781]	Step 47020	lr 0.00929	Loss 0.7709 (1.0490)	Prec@(1,5) (78.5%, 96.1%)	
07/03 11:57:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][150/781]	Step 47070	lr 0.00929	Loss 1.1419 (1.0423)	Prec@(1,5) (78.6%, 96.3%)	
07/03 11:57:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][200/781]	Step 47120	lr 0.00929	Loss 0.9043 (1.0509)	Prec@(1,5) (78.4%, 96.2%)	
07/03 11:58:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][250/781]	Step 47170	lr 0.00929	Loss 1.2187 (1.0456)	Prec@(1,5) (78.4%, 96.2%)	
07/03 11:58:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][300/781]	Step 47220	lr 0.00929	Loss 0.9741 (1.0615)	Prec@(1,5) (78.0%, 96.1%)	
07/03 11:58:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][350/781]	Step 47270	lr 0.00929	Loss 1.1068 (1.0609)	Prec@(1,5) (78.0%, 96.0%)	
07/03 11:58:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][400/781]	Step 47320	lr 0.00929	Loss 1.0258 (1.0579)	Prec@(1,5) (78.1%, 96.1%)	
07/03 11:59:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][450/781]	Step 47370	lr 0.00929	Loss 1.3171 (1.0644)	Prec@(1,5) (78.0%, 96.0%)	
07/03 11:59:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][500/781]	Step 47420	lr 0.00929	Loss 0.9405 (1.0678)	Prec@(1,5) (77.9%, 96.0%)	
07/03 11:59:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][550/781]	Step 47470	lr 0.00929	Loss 0.9115 (1.0697)	Prec@(1,5) (77.9%, 96.0%)	
07/03 11:59:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][600/781]	Step 47520	lr 0.00929	Loss 1.2322 (1.0721)	Prec@(1,5) (77.8%, 95.9%)	
07/04 12:00:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][650/781]	Step 47570	lr 0.00929	Loss 1.2645 (1.0774)	Prec@(1,5) (77.7%, 95.9%)	
07/04 12:00:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][700/781]	Step 47620	lr 0.00929	Loss 1.1670 (1.0806)	Prec@(1,5) (77.7%, 95.8%)	
07/04 12:00:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][750/781]	Step 47670	lr 0.00929	Loss 1.2747 (1.0888)	Prec@(1,5) (77.4%, 95.7%)	
07/04 12:00:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][781/781]	Step 47701	lr 0.00929	Loss 0.6908 (1.0904)	Prec@(1,5) (77.4%, 95.7%)	
07/04 12:00:40午前 evaluateCell_trainer.py:172 [INFO] Train: [ 60/99] Final Prec@1 77.4160%
07/04 12:00:44午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][50/391]	Step 47702	Loss 0.7262	Prec@(1,5) (78.8%, 96.1%)
07/04 12:00:48午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][100/391]	Step 47702	Loss 0.7452	Prec@(1,5) (77.8%, 95.8%)
07/04 12:00:52午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][150/391]	Step 47702	Loss 0.7224	Prec@(1,5) (78.4%, 96.3%)
07/04 12:00:57午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][200/391]	Step 47702	Loss 0.7200	Prec@(1,5) (78.5%, 96.3%)
07/04 12:01:01午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][250/391]	Step 47702	Loss 0.7106	Prec@(1,5) (78.6%, 96.4%)
07/04 12:01:05午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][300/391]	Step 47702	Loss 0.7091	Prec@(1,5) (78.7%, 96.4%)
07/04 12:01:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][350/391]	Step 47702	Loss 0.7042	Prec@(1,5) (78.8%, 96.4%)
07/04 12:01:12午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][390/391]	Step 47702	Loss 0.7067	Prec@(1,5) (78.7%, 96.3%)
07/04 12:01:12午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 60/99] Final Prec@1 78.7360%
07/04 12:01:12午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 78.7360%
07/04 12:01:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][50/781]	Step 47752	lr 0.00894	Loss 1.0725 (0.9774)	Prec@(1,5) (79.8%, 96.9%)	
07/04 12:01:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][100/781]	Step 47802	lr 0.00894	Loss 0.8004 (1.0067)	Prec@(1,5) (79.1%, 96.3%)	
07/04 12:01:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][150/781]	Step 47852	lr 0.00894	Loss 0.9753 (1.0135)	Prec@(1,5) (79.1%, 96.2%)	
07/04 12:02:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][200/781]	Step 47902	lr 0.00894	Loss 0.8019 (1.0107)	Prec@(1,5) (79.1%, 96.2%)	
07/04 12:02:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][250/781]	Step 47952	lr 0.00894	Loss 0.9749 (1.0134)	Prec@(1,5) (79.0%, 96.3%)	
07/04 12:02:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][300/781]	Step 48002	lr 0.00894	Loss 0.9101 (1.0224)	Prec@(1,5) (78.7%, 96.2%)	
07/04 12:02:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][350/781]	Step 48052	lr 0.00894	Loss 0.7040 (1.0207)	Prec@(1,5) (78.8%, 96.2%)	
07/04 12:03:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][400/781]	Step 48102	lr 0.00894	Loss 0.8101 (1.0306)	Prec@(1,5) (78.6%, 96.1%)	
07/04 12:03:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][450/781]	Step 48152	lr 0.00894	Loss 1.0832 (1.0359)	Prec@(1,5) (78.6%, 96.0%)	
07/04 12:03:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][500/781]	Step 48202	lr 0.00894	Loss 0.8850 (1.0447)	Prec@(1,5) (78.3%, 96.0%)	
07/04 12:03:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][550/781]	Step 48252	lr 0.00894	Loss 1.2150 (1.0467)	Prec@(1,5) (78.3%, 96.0%)	
07/04 12:04:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][600/781]	Step 48302	lr 0.00894	Loss 1.3072 (1.0509)	Prec@(1,5) (78.2%, 95.9%)	
07/04 12:04:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][650/781]	Step 48352	lr 0.00894	Loss 1.2689 (1.0547)	Prec@(1,5) (78.1%, 95.9%)	
07/04 12:04:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][700/781]	Step 48402	lr 0.00894	Loss 1.0846 (1.0617)	Prec@(1,5) (77.9%, 95.8%)	
07/04 12:04:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][750/781]	Step 48452	lr 0.00894	Loss 0.9179 (1.0631)	Prec@(1,5) (77.9%, 95.8%)	
07/04 12:05:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][781/781]	Step 48483	lr 0.00894	Loss 1.0205 (1.0658)	Prec@(1,5) (77.8%, 95.8%)	
07/04 12:05:05午前 evaluateCell_trainer.py:172 [INFO] Train: [ 61/99] Final Prec@1 77.7880%
07/04 12:05:10午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][50/391]	Step 48484	Loss 0.6727	Prec@(1,5) (79.6%, 96.9%)
07/04 12:05:14午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][100/391]	Step 48484	Loss 0.6749	Prec@(1,5) (79.4%, 96.9%)
07/04 12:05:18午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][150/391]	Step 48484	Loss 0.6772	Prec@(1,5) (79.2%, 97.0%)
07/04 12:05:22午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][200/391]	Step 48484	Loss 0.6835	Prec@(1,5) (78.9%, 96.8%)
07/04 12:05:26午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][250/391]	Step 48484	Loss 0.6881	Prec@(1,5) (78.8%, 96.6%)
07/04 12:05:31午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][300/391]	Step 48484	Loss 0.6950	Prec@(1,5) (78.7%, 96.5%)
07/04 12:05:35午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][350/391]	Step 48484	Loss 0.6940	Prec@(1,5) (78.7%, 96.5%)
07/04 12:05:38午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][390/391]	Step 48484	Loss 0.6907	Prec@(1,5) (78.8%, 96.5%)
07/04 12:05:38午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 61/99] Final Prec@1 78.8320%
07/04 12:05:38午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 78.8320%
07/04 12:05:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][50/781]	Step 48534	lr 0.00858	Loss 1.0350 (1.0000)	Prec@(1,5) (79.7%, 96.1%)	
07/04 12:06:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][100/781]	Step 48584	lr 0.00858	Loss 0.6698 (0.9982)	Prec@(1,5) (79.4%, 96.2%)	
07/04 12:06:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][150/781]	Step 48634	lr 0.00858	Loss 1.3270 (0.9897)	Prec@(1,5) (79.1%, 96.4%)	
07/04 12:06:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][200/781]	Step 48684	lr 0.00858	Loss 0.9333 (1.0008)	Prec@(1,5) (79.1%, 96.4%)	
07/04 12:06:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][250/781]	Step 48734	lr 0.00858	Loss 0.8102 (0.9942)	Prec@(1,5) (79.3%, 96.5%)	
07/04 12:07:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][300/781]	Step 48784	lr 0.00858	Loss 0.7362 (1.0049)	Prec@(1,5) (79.1%, 96.5%)	
07/04 12:07:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][350/781]	Step 48834	lr 0.00858	Loss 0.9575 (1.0061)	Prec@(1,5) (79.1%, 96.5%)	
07/04 12:07:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][400/781]	Step 48884	lr 0.00858	Loss 1.0523 (1.0104)	Prec@(1,5) (79.0%, 96.4%)	
07/04 12:07:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][450/781]	Step 48934	lr 0.00858	Loss 1.1711 (1.0152)	Prec@(1,5) (78.9%, 96.3%)	
07/04 12:08:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][500/781]	Step 48984	lr 0.00858	Loss 1.2269 (1.0218)	Prec@(1,5) (78.7%, 96.3%)	
07/04 12:08:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][550/781]	Step 49034	lr 0.00858	Loss 0.9516 (1.0257)	Prec@(1,5) (78.6%, 96.2%)	
07/04 12:08:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][600/781]	Step 49084	lr 0.00858	Loss 0.6810 (1.0260)	Prec@(1,5) (78.7%, 96.2%)	
07/04 12:08:53午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][650/781]	Step 49134	lr 0.00858	Loss 0.7020 (1.0253)	Prec@(1,5) (78.7%, 96.2%)	
07/04 12:09:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][700/781]	Step 49184	lr 0.00858	Loss 1.2361 (1.0275)	Prec@(1,5) (78.6%, 96.2%)	
07/04 12:09:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][750/781]	Step 49234	lr 0.00858	Loss 0.8639 (1.0303)	Prec@(1,5) (78.5%, 96.2%)	
07/04 12:09:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][781/781]	Step 49265	lr 0.00858	Loss 1.0541 (1.0301)	Prec@(1,5) (78.5%, 96.2%)	
07/04 12:09:32午前 evaluateCell_trainer.py:172 [INFO] Train: [ 62/99] Final Prec@1 78.5300%
07/04 12:09:37午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][50/391]	Step 49266	Loss 0.6641	Prec@(1,5) (79.8%, 96.9%)
07/04 12:09:41午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][100/391]	Step 49266	Loss 0.6772	Prec@(1,5) (79.6%, 96.7%)
07/04 12:09:45午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][150/391]	Step 49266	Loss 0.6807	Prec@(1,5) (79.5%, 96.7%)
07/04 12:09:49午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][200/391]	Step 49266	Loss 0.6851	Prec@(1,5) (79.2%, 96.7%)
07/04 12:09:53午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][250/391]	Step 49266	Loss 0.6881	Prec@(1,5) (79.1%, 96.7%)
07/04 12:09:57午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][300/391]	Step 49266	Loss 0.6824	Prec@(1,5) (79.5%, 96.8%)
07/04 12:10:01午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][350/391]	Step 49266	Loss 0.6824	Prec@(1,5) (79.4%, 96.8%)
07/04 12:10:05午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][390/391]	Step 49266	Loss 0.6833	Prec@(1,5) (79.4%, 96.7%)
07/04 12:10:05午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 62/99] Final Prec@1 79.4400%
07/04 12:10:05午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 79.4400%
07/04 12:10:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][50/781]	Step 49316	lr 0.00823	Loss 0.9371 (0.9704)	Prec@(1,5) (80.0%, 96.3%)	
07/04 12:10:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][100/781]	Step 49366	lr 0.00823	Loss 0.9999 (0.9344)	Prec@(1,5) (80.8%, 96.5%)	
07/04 12:10:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][150/781]	Step 49416	lr 0.00823	Loss 0.8729 (0.9452)	Prec@(1,5) (80.6%, 96.6%)	
07/04 12:11:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][200/781]	Step 49466	lr 0.00823	Loss 0.7760 (0.9597)	Prec@(1,5) (80.1%, 96.5%)	
07/04 12:11:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][250/781]	Step 49516	lr 0.00823	Loss 1.2565 (0.9715)	Prec@(1,5) (79.7%, 96.5%)	
07/04 12:11:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][300/781]	Step 49566	lr 0.00823	Loss 1.0127 (0.9709)	Prec@(1,5) (79.7%, 96.5%)	
07/04 12:11:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][350/781]	Step 49616	lr 0.00823	Loss 0.9380 (0.9759)	Prec@(1,5) (79.6%, 96.5%)	
07/04 12:12:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][400/781]	Step 49666	lr 0.00823	Loss 0.8543 (0.9826)	Prec@(1,5) (79.5%, 96.5%)	
07/04 12:12:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][450/781]	Step 49716	lr 0.00823	Loss 0.9492 (0.9823)	Prec@(1,5) (79.5%, 96.5%)	
07/04 12:12:34午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][500/781]	Step 49766	lr 0.00823	Loss 1.3790 (0.9844)	Prec@(1,5) (79.4%, 96.5%)	
07/04 12:12:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][550/781]	Step 49816	lr 0.00823	Loss 0.9257 (0.9871)	Prec@(1,5) (79.3%, 96.4%)	
07/04 12:13:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][600/781]	Step 49866	lr 0.00823	Loss 0.9055 (0.9901)	Prec@(1,5) (79.3%, 96.4%)	
07/04 12:13:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][650/781]	Step 49916	lr 0.00823	Loss 1.1988 (0.9919)	Prec@(1,5) (79.3%, 96.4%)	
07/04 12:13:34午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][700/781]	Step 49966	lr 0.00823	Loss 0.6964 (0.9951)	Prec@(1,5) (79.3%, 96.4%)	
07/04 12:13:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][750/781]	Step 50016	lr 0.00823	Loss 0.9155 (0.9961)	Prec@(1,5) (79.2%, 96.4%)	
07/04 12:13:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][781/781]	Step 50047	lr 0.00823	Loss 0.9949 (0.9986)	Prec@(1,5) (79.2%, 96.4%)	
07/04 12:13:59午前 evaluateCell_trainer.py:172 [INFO] Train: [ 63/99] Final Prec@1 79.2120%
07/04 12:14:03午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][50/391]	Step 50048	Loss 0.6111	Prec@(1,5) (81.6%, 97.0%)
07/04 12:14:07午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][100/391]	Step 50048	Loss 0.6444	Prec@(1,5) (80.7%, 96.6%)
07/04 12:14:11午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][150/391]	Step 50048	Loss 0.6471	Prec@(1,5) (80.4%, 96.6%)
07/04 12:14:16午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][200/391]	Step 50048	Loss 0.6537	Prec@(1,5) (80.1%, 96.6%)
07/04 12:14:20午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][250/391]	Step 50048	Loss 0.6550	Prec@(1,5) (80.2%, 96.6%)
07/04 12:14:24午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][300/391]	Step 50048	Loss 0.6576	Prec@(1,5) (80.0%, 96.6%)
07/04 12:14:28午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][350/391]	Step 50048	Loss 0.6515	Prec@(1,5) (80.2%, 96.6%)
07/04 12:14:31午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][390/391]	Step 50048	Loss 0.6501	Prec@(1,5) (80.3%, 96.6%)
07/04 12:14:32午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 63/99] Final Prec@1 80.3040%
07/04 12:14:32午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 80.3040%
07/04 12:14:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][50/781]	Step 50098	lr 0.00789	Loss 0.7680 (0.9459)	Prec@(1,5) (81.2%, 96.7%)	
07/04 12:15:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][100/781]	Step 50148	lr 0.00789	Loss 0.8609 (0.9374)	Prec@(1,5) (81.5%, 96.6%)	
07/04 12:15:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][150/781]	Step 50198	lr 0.00789	Loss 1.0916 (0.9454)	Prec@(1,5) (81.3%, 96.6%)	
07/04 12:15:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][200/781]	Step 50248	lr 0.00789	Loss 1.2776 (0.9524)	Prec@(1,5) (81.0%, 96.6%)	
07/04 12:15:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][250/781]	Step 50298	lr 0.00789	Loss 1.1215 (0.9524)	Prec@(1,5) (80.8%, 96.6%)	
07/04 12:16:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][300/781]	Step 50348	lr 0.00789	Loss 0.8659 (0.9569)	Prec@(1,5) (80.7%, 96.7%)	
07/04 12:16:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][350/781]	Step 50398	lr 0.00789	Loss 1.0025 (0.9569)	Prec@(1,5) (80.7%, 96.7%)	
07/04 12:16:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][400/781]	Step 50448	lr 0.00789	Loss 0.7096 (0.9638)	Prec@(1,5) (80.5%, 96.6%)	
07/04 12:16:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][450/781]	Step 50498	lr 0.00789	Loss 1.1815 (0.9620)	Prec@(1,5) (80.5%, 96.7%)	
07/04 12:17:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][500/781]	Step 50548	lr 0.00789	Loss 0.7234 (0.9629)	Prec@(1,5) (80.4%, 96.7%)	
07/04 12:17:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][550/781]	Step 50598	lr 0.00789	Loss 1.3523 (0.9636)	Prec@(1,5) (80.4%, 96.7%)	
07/04 12:17:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][600/781]	Step 50648	lr 0.00789	Loss 0.6563 (0.9642)	Prec@(1,5) (80.4%, 96.7%)	
07/04 12:17:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][650/781]	Step 50698	lr 0.00789	Loss 0.8036 (0.9626)	Prec@(1,5) (80.4%, 96.7%)	
07/04 12:18:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][700/781]	Step 50748	lr 0.00789	Loss 0.9124 (0.9643)	Prec@(1,5) (80.3%, 96.7%)	
07/04 12:18:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][750/781]	Step 50798	lr 0.00789	Loss 1.2103 (0.9671)	Prec@(1,5) (80.2%, 96.6%)	
07/04 12:18:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][781/781]	Step 50829	lr 0.00789	Loss 1.2208 (0.9707)	Prec@(1,5) (80.1%, 96.6%)	
07/04 12:18:25午前 evaluateCell_trainer.py:172 [INFO] Train: [ 64/99] Final Prec@1 80.1360%
07/04 12:18:30午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][50/391]	Step 50830	Loss 0.6012	Prec@(1,5) (82.1%, 97.4%)
07/04 12:18:33午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][100/391]	Step 50830	Loss 0.6081	Prec@(1,5) (81.7%, 97.4%)
07/04 12:18:37午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][150/391]	Step 50830	Loss 0.6056	Prec@(1,5) (81.7%, 97.3%)
07/04 12:18:41午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][200/391]	Step 50830	Loss 0.6048	Prec@(1,5) (81.7%, 97.3%)
07/04 12:18:46午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][250/391]	Step 50830	Loss 0.6080	Prec@(1,5) (81.4%, 97.2%)
07/04 12:18:50午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][300/391]	Step 50830	Loss 0.6086	Prec@(1,5) (81.4%, 97.2%)
07/04 12:18:54午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][350/391]	Step 50830	Loss 0.6082	Prec@(1,5) (81.3%, 97.2%)
07/04 12:18:57午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][390/391]	Step 50830	Loss 0.6092	Prec@(1,5) (81.2%, 97.2%)
07/04 12:18:57午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 64/99] Final Prec@1 81.1840%
07/04 12:18:58午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 81.1840%
07/04 12:19:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][50/781]	Step 50880	lr 0.00755	Loss 0.9272 (0.8789)	Prec@(1,5) (82.2%, 97.2%)	
07/04 12:19:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][100/781]	Step 50930	lr 0.00755	Loss 0.9084 (0.8849)	Prec@(1,5) (82.0%, 97.2%)	
07/04 12:19:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][150/781]	Step 50980	lr 0.00755	Loss 1.2247 (0.8844)	Prec@(1,5) (82.1%, 97.2%)	
07/04 12:19:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][200/781]	Step 51030	lr 0.00755	Loss 1.0850 (0.8976)	Prec@(1,5) (81.6%, 97.1%)	
07/04 12:20:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][250/781]	Step 51080	lr 0.00755	Loss 0.8112 (0.9102)	Prec@(1,5) (81.4%, 97.0%)	
07/04 12:20:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][300/781]	Step 51130	lr 0.00755	Loss 0.7926 (0.9103)	Prec@(1,5) (81.3%, 97.0%)	
07/04 12:20:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][350/781]	Step 51180	lr 0.00755	Loss 0.6116 (0.9123)	Prec@(1,5) (81.2%, 97.0%)	
07/04 12:20:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][400/781]	Step 51230	lr 0.00755	Loss 0.7209 (0.9195)	Prec@(1,5) (81.1%, 96.9%)	
07/04 12:21:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][450/781]	Step 51280	lr 0.00755	Loss 0.9097 (0.9198)	Prec@(1,5) (81.1%, 96.9%)	
07/04 12:21:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][500/781]	Step 51330	lr 0.00755	Loss 1.0020 (0.9277)	Prec@(1,5) (80.9%, 96.9%)	
07/04 12:21:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][550/781]	Step 51380	lr 0.00755	Loss 1.0610 (0.9272)	Prec@(1,5) (81.0%, 96.9%)	
07/04 12:21:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][600/781]	Step 51430	lr 0.00755	Loss 1.1715 (0.9296)	Prec@(1,5) (81.0%, 96.9%)	
07/04 12:22:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][650/781]	Step 51480	lr 0.00755	Loss 0.7254 (0.9286)	Prec@(1,5) (81.0%, 96.9%)	
07/04 12:22:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][700/781]	Step 51530	lr 0.00755	Loss 0.8691 (0.9294)	Prec@(1,5) (81.0%, 96.9%)	
07/04 12:22:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][750/781]	Step 51580	lr 0.00755	Loss 0.9531 (0.9322)	Prec@(1,5) (80.9%, 96.9%)	
07/04 12:22:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][781/781]	Step 51611	lr 0.00755	Loss 1.1782 (0.9384)	Prec@(1,5) (80.8%, 96.8%)	
07/04 12:22:51午前 evaluateCell_trainer.py:172 [INFO] Train: [ 65/99] Final Prec@1 80.7800%
07/04 12:22:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][50/391]	Step 51612	Loss 0.5789	Prec@(1,5) (82.5%, 97.5%)
07/04 12:23:00午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][100/391]	Step 51612	Loss 0.5639	Prec@(1,5) (83.0%, 97.6%)
07/04 12:23:04午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][150/391]	Step 51612	Loss 0.5613	Prec@(1,5) (83.1%, 97.4%)
07/04 12:23:08午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][200/391]	Step 51612	Loss 0.5667	Prec@(1,5) (82.9%, 97.4%)
07/04 12:23:12午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][250/391]	Step 51612	Loss 0.5661	Prec@(1,5) (82.8%, 97.4%)
07/04 12:23:17午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][300/391]	Step 51612	Loss 0.5642	Prec@(1,5) (83.0%, 97.4%)
07/04 12:23:21午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][350/391]	Step 51612	Loss 0.5669	Prec@(1,5) (82.9%, 97.4%)
07/04 12:23:24午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][390/391]	Step 51612	Loss 0.5657	Prec@(1,5) (82.9%, 97.4%)
07/04 12:23:24午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 65/99] Final Prec@1 82.9120%
07/04 12:23:24午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 82.9120%
07/04 12:23:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][50/781]	Step 51662	lr 0.00722	Loss 0.9709 (0.8374)	Prec@(1,5) (83.5%, 97.9%)	
07/04 12:23:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][100/781]	Step 51712	lr 0.00722	Loss 0.6186 (0.8348)	Prec@(1,5) (83.2%, 97.9%)	
07/04 12:24:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][150/781]	Step 51762	lr 0.00722	Loss 0.9639 (0.8590)	Prec@(1,5) (82.6%, 97.6%)	
07/04 12:24:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][200/781]	Step 51812	lr 0.00722	Loss 0.8013 (0.8570)	Prec@(1,5) (82.5%, 97.6%)	
07/04 12:24:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][250/781]	Step 51862	lr 0.00722	Loss 1.1844 (0.8622)	Prec@(1,5) (82.4%, 97.5%)	
07/04 12:24:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][300/781]	Step 51912	lr 0.00722	Loss 0.8346 (0.8681)	Prec@(1,5) (82.2%, 97.4%)	
07/04 12:25:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][350/781]	Step 51962	lr 0.00722	Loss 0.8985 (0.8708)	Prec@(1,5) (82.2%, 97.3%)	
07/04 12:25:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][400/781]	Step 52012	lr 0.00722	Loss 0.8527 (0.8734)	Prec@(1,5) (82.1%, 97.3%)	
07/04 12:25:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][450/781]	Step 52062	lr 0.00722	Loss 1.2131 (0.8778)	Prec@(1,5) (82.0%, 97.3%)	
07/04 12:25:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][500/781]	Step 52112	lr 0.00722	Loss 0.9288 (0.8792)	Prec@(1,5) (81.9%, 97.3%)	
07/04 12:26:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][550/781]	Step 52162	lr 0.00722	Loss 1.0342 (0.8816)	Prec@(1,5) (81.9%, 97.3%)	
07/04 12:26:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][600/781]	Step 52212	lr 0.00722	Loss 1.0244 (0.8882)	Prec@(1,5) (81.7%, 97.3%)	
07/04 12:26:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][650/781]	Step 52262	lr 0.00722	Loss 0.7549 (0.8882)	Prec@(1,5) (81.7%, 97.3%)	
07/04 12:26:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][700/781]	Step 52312	lr 0.00722	Loss 0.9795 (0.8918)	Prec@(1,5) (81.7%, 97.3%)	
07/04 12:27:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][750/781]	Step 52362	lr 0.00722	Loss 1.0951 (0.8938)	Prec@(1,5) (81.6%, 97.3%)	
07/04 12:27:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][781/781]	Step 52393	lr 0.00722	Loss 1.0447 (0.8974)	Prec@(1,5) (81.5%, 97.2%)	
07/04 12:27:18午前 evaluateCell_trainer.py:172 [INFO] Train: [ 66/99] Final Prec@1 81.5260%
07/04 12:27:23午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][50/391]	Step 52394	Loss 0.5354	Prec@(1,5) (84.1%, 97.7%)
07/04 12:27:27午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][100/391]	Step 52394	Loss 0.5615	Prec@(1,5) (83.0%, 97.6%)
07/04 12:27:31午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][150/391]	Step 52394	Loss 0.5623	Prec@(1,5) (83.1%, 97.4%)
07/04 12:27:35午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][200/391]	Step 52394	Loss 0.5579	Prec@(1,5) (83.1%, 97.5%)
07/04 12:27:39午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][250/391]	Step 52394	Loss 0.5588	Prec@(1,5) (83.1%, 97.5%)
07/04 12:27:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][300/391]	Step 52394	Loss 0.5588	Prec@(1,5) (83.1%, 97.4%)
07/04 12:27:48午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][350/391]	Step 52394	Loss 0.5679	Prec@(1,5) (82.8%, 97.4%)
07/04 12:27:51午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][390/391]	Step 52394	Loss 0.5656	Prec@(1,5) (82.9%, 97.4%)
07/04 12:27:51午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 66/99] Final Prec@1 82.8760%
07/04 12:27:51午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 82.9120%
07/04 12:28:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][50/781]	Step 52444	lr 0.00689	Loss 0.7187 (0.8552)	Prec@(1,5) (81.9%, 97.7%)	
07/04 12:28:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][100/781]	Step 52494	lr 0.00689	Loss 0.7875 (0.8679)	Prec@(1,5) (82.1%, 97.5%)	
07/04 12:28:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][150/781]	Step 52544	lr 0.00689	Loss 0.5999 (0.8481)	Prec@(1,5) (82.5%, 97.6%)	
07/04 12:28:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][200/781]	Step 52594	lr 0.00689	Loss 1.2656 (0.8625)	Prec@(1,5) (82.2%, 97.4%)	
07/04 12:29:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][250/781]	Step 52644	lr 0.00689	Loss 0.7804 (0.8607)	Prec@(1,5) (82.4%, 97.4%)	
07/04 12:29:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][300/781]	Step 52694	lr 0.00689	Loss 0.9340 (0.8644)	Prec@(1,5) (82.3%, 97.4%)	
07/04 12:29:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][350/781]	Step 52744	lr 0.00689	Loss 0.8176 (0.8597)	Prec@(1,5) (82.4%, 97.4%)	
07/04 12:29:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][400/781]	Step 52794	lr 0.00689	Loss 0.6459 (0.8600)	Prec@(1,5) (82.3%, 97.4%)	
07/04 12:30:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][450/781]	Step 52844	lr 0.00689	Loss 0.7979 (0.8617)	Prec@(1,5) (82.3%, 97.4%)	
07/04 12:30:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][500/781]	Step 52894	lr 0.00689	Loss 1.2054 (0.8607)	Prec@(1,5) (82.3%, 97.4%)	
07/04 12:30:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][550/781]	Step 52944	lr 0.00689	Loss 0.9257 (0.8643)	Prec@(1,5) (82.2%, 97.4%)	
07/04 12:30:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][600/781]	Step 52994	lr 0.00689	Loss 1.1444 (0.8680)	Prec@(1,5) (82.2%, 97.4%)	
07/04 12:31:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][650/781]	Step 53044	lr 0.00689	Loss 0.7287 (0.8694)	Prec@(1,5) (82.2%, 97.3%)	
07/04 12:31:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][700/781]	Step 53094	lr 0.00689	Loss 1.1130 (0.8748)	Prec@(1,5) (82.1%, 97.3%)	
07/04 12:31:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][750/781]	Step 53144	lr 0.00689	Loss 0.6558 (0.8746)	Prec@(1,5) (82.1%, 97.3%)	
07/04 12:31:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][781/781]	Step 53175	lr 0.00689	Loss 0.9522 (0.8765)	Prec@(1,5) (82.0%, 97.3%)	
07/04 12:31:44午前 evaluateCell_trainer.py:172 [INFO] Train: [ 67/99] Final Prec@1 82.0360%
07/04 12:31:49午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][50/391]	Step 53176	Loss 0.4902	Prec@(1,5) (85.3%, 97.9%)
07/04 12:31:53午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][100/391]	Step 53176	Loss 0.5022	Prec@(1,5) (84.8%, 98.0%)
07/04 12:31:57午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][150/391]	Step 53176	Loss 0.5070	Prec@(1,5) (84.5%, 97.9%)
07/04 12:32:01午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][200/391]	Step 53176	Loss 0.5019	Prec@(1,5) (84.6%, 97.9%)
07/04 12:32:05午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][250/391]	Step 53176	Loss 0.4954	Prec@(1,5) (84.7%, 98.0%)
07/04 12:32:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][300/391]	Step 53176	Loss 0.4945	Prec@(1,5) (84.7%, 98.0%)
07/04 12:32:13午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][350/391]	Step 53176	Loss 0.4940	Prec@(1,5) (84.7%, 98.0%)
07/04 12:32:16午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][390/391]	Step 53176	Loss 0.4938	Prec@(1,5) (84.8%, 98.0%)
07/04 12:32:16午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 67/99] Final Prec@1 84.7680%
07/04 12:32:17午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 84.7680%
07/04 12:32:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][50/781]	Step 53226	lr 0.00657	Loss 0.6967 (0.7858)	Prec@(1,5) (84.4%, 98.0%)	
07/04 12:32:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][100/781]	Step 53276	lr 0.00657	Loss 1.0072 (0.8028)	Prec@(1,5) (83.5%, 97.9%)	
07/04 12:33:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][150/781]	Step 53326	lr 0.00657	Loss 1.0057 (0.8123)	Prec@(1,5) (83.1%, 97.8%)	
07/04 12:33:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][200/781]	Step 53376	lr 0.00657	Loss 0.9462 (0.8234)	Prec@(1,5) (82.9%, 97.7%)	
07/04 12:33:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][250/781]	Step 53426	lr 0.00657	Loss 1.1770 (0.8238)	Prec@(1,5) (82.9%, 97.7%)	
07/04 12:33:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][300/781]	Step 53476	lr 0.00657	Loss 0.8487 (0.8274)	Prec@(1,5) (82.9%, 97.6%)	
07/04 12:34:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][350/781]	Step 53526	lr 0.00657	Loss 0.9682 (0.8296)	Prec@(1,5) (82.9%, 97.6%)	
07/04 12:34:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][400/781]	Step 53576	lr 0.00657	Loss 0.8588 (0.8299)	Prec@(1,5) (83.0%, 97.6%)	
07/04 12:34:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][450/781]	Step 53626	lr 0.00657	Loss 0.6350 (0.8295)	Prec@(1,5) (83.0%, 97.6%)	
07/04 12:34:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][500/781]	Step 53676	lr 0.00657	Loss 0.6795 (0.8322)	Prec@(1,5) (83.0%, 97.6%)	
07/04 12:35:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][550/781]	Step 53726	lr 0.00657	Loss 1.0465 (0.8342)	Prec@(1,5) (82.9%, 97.6%)	
07/04 12:35:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][600/781]	Step 53776	lr 0.00657	Loss 0.8572 (0.8387)	Prec@(1,5) (82.8%, 97.6%)	
07/04 12:35:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][650/781]	Step 53826	lr 0.00657	Loss 0.9296 (0.8428)	Prec@(1,5) (82.7%, 97.5%)	
07/04 12:35:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][700/781]	Step 53876	lr 0.00657	Loss 0.9435 (0.8443)	Prec@(1,5) (82.7%, 97.5%)	
07/04 12:36:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][750/781]	Step 53926	lr 0.00657	Loss 0.7084 (0.8472)	Prec@(1,5) (82.6%, 97.5%)	
07/04 12:36:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][781/781]	Step 53957	lr 0.00657	Loss 0.8899 (0.8474)	Prec@(1,5) (82.6%, 97.5%)	
07/04 12:36:10午前 evaluateCell_trainer.py:172 [INFO] Train: [ 68/99] Final Prec@1 82.6320%
07/04 12:36:15午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][50/391]	Step 53958	Loss 0.5151	Prec@(1,5) (84.7%, 97.8%)
07/04 12:36:19午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][100/391]	Step 53958	Loss 0.5184	Prec@(1,5) (84.2%, 97.7%)
07/04 12:36:23午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][150/391]	Step 53958	Loss 0.5175	Prec@(1,5) (84.1%, 97.8%)
07/04 12:36:27午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][200/391]	Step 53958	Loss 0.5177	Prec@(1,5) (84.3%, 97.7%)
07/04 12:36:31午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][250/391]	Step 53958	Loss 0.5241	Prec@(1,5) (84.0%, 97.8%)
07/04 12:36:36午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][300/391]	Step 53958	Loss 0.5271	Prec@(1,5) (83.9%, 97.7%)
07/04 12:36:40午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][350/391]	Step 53958	Loss 0.5301	Prec@(1,5) (83.8%, 97.7%)
07/04 12:36:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][390/391]	Step 53958	Loss 0.5307	Prec@(1,5) (83.8%, 97.7%)
07/04 12:36:43午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 68/99] Final Prec@1 83.8120%
07/04 12:36:43午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 84.7680%
07/04 12:36:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][50/781]	Step 54008	lr 0.00625	Loss 0.7509 (0.8033)	Prec@(1,5) (83.6%, 97.7%)	
07/04 12:37:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][100/781]	Step 54058	lr 0.00625	Loss 0.8456 (0.7959)	Prec@(1,5) (83.6%, 97.8%)	
07/04 12:37:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][150/781]	Step 54108	lr 0.00625	Loss 0.6354 (0.7934)	Prec@(1,5) (84.0%, 97.7%)	
07/04 12:37:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][200/781]	Step 54158	lr 0.00625	Loss 0.6447 (0.7965)	Prec@(1,5) (84.0%, 97.7%)	
07/04 12:37:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][250/781]	Step 54208	lr 0.00625	Loss 0.8103 (0.8023)	Prec@(1,5) (84.0%, 97.7%)	
07/04 12:38:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][300/781]	Step 54258	lr 0.00625	Loss 1.0502 (0.7974)	Prec@(1,5) (84.0%, 97.7%)	
07/04 12:38:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][350/781]	Step 54308	lr 0.00625	Loss 0.5387 (0.7949)	Prec@(1,5) (84.1%, 97.7%)	
07/04 12:38:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][400/781]	Step 54358	lr 0.00625	Loss 0.9007 (0.8007)	Prec@(1,5) (84.0%, 97.7%)	
07/04 12:38:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][450/781]	Step 54408	lr 0.00625	Loss 1.0229 (0.7993)	Prec@(1,5) (84.0%, 97.7%)	
07/04 12:39:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][500/781]	Step 54458	lr 0.00625	Loss 0.7672 (0.8042)	Prec@(1,5) (83.9%, 97.6%)	
07/04 12:39:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][550/781]	Step 54508	lr 0.00625	Loss 0.9487 (0.8089)	Prec@(1,5) (83.8%, 97.6%)	
07/04 12:39:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][600/781]	Step 54558	lr 0.00625	Loss 0.8107 (0.8129)	Prec@(1,5) (83.6%, 97.6%)	
07/04 12:39:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][650/781]	Step 54608	lr 0.00625	Loss 0.7862 (0.8111)	Prec@(1,5) (83.6%, 97.6%)	
07/04 12:40:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][700/781]	Step 54658	lr 0.00625	Loss 1.0315 (0.8143)	Prec@(1,5) (83.6%, 97.6%)	
07/04 12:40:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][750/781]	Step 54708	lr 0.00625	Loss 0.8176 (0.8153)	Prec@(1,5) (83.6%, 97.6%)	
07/04 12:40:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][781/781]	Step 54739	lr 0.00625	Loss 0.8798 (0.8186)	Prec@(1,5) (83.5%, 97.6%)	
07/04 12:40:37午前 evaluateCell_trainer.py:172 [INFO] Train: [ 69/99] Final Prec@1 83.4980%
07/04 12:40:42午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][50/391]	Step 54740	Loss 0.4520	Prec@(1,5) (85.8%, 98.1%)
07/04 12:40:46午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][100/391]	Step 54740	Loss 0.4604	Prec@(1,5) (85.6%, 98.3%)
07/04 12:40:50午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][150/391]	Step 54740	Loss 0.4688	Prec@(1,5) (85.5%, 98.2%)
07/04 12:40:54午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][200/391]	Step 54740	Loss 0.4783	Prec@(1,5) (85.3%, 98.1%)
07/04 12:40:58午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][250/391]	Step 54740	Loss 0.4827	Prec@(1,5) (85.4%, 98.0%)
07/04 12:41:02午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][300/391]	Step 54740	Loss 0.4863	Prec@(1,5) (85.3%, 98.0%)
07/04 12:41:07午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][350/391]	Step 54740	Loss 0.4846	Prec@(1,5) (85.3%, 98.0%)
07/04 12:41:10午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][390/391]	Step 54740	Loss 0.4837	Prec@(1,5) (85.3%, 98.1%)
07/04 12:41:10午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 69/99] Final Prec@1 85.2720%
07/04 12:41:10午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 85.2720%
07/04 12:41:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][50/781]	Step 54790	lr 0.00595	Loss 0.8021 (0.6950)	Prec@(1,5) (86.3%, 98.5%)	
07/04 12:41:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][100/781]	Step 54840	lr 0.00595	Loss 0.8623 (0.7159)	Prec@(1,5) (85.8%, 98.2%)	
07/04 12:41:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][150/781]	Step 54890	lr 0.00595	Loss 0.7126 (0.7287)	Prec@(1,5) (85.5%, 98.1%)	
07/04 12:42:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][200/781]	Step 54940	lr 0.00595	Loss 0.8944 (0.7366)	Prec@(1,5) (85.2%, 98.1%)	
07/04 12:42:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][250/781]	Step 54990	lr 0.00595	Loss 1.1018 (0.7415)	Prec@(1,5) (85.1%, 98.1%)	
07/04 12:42:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][300/781]	Step 55040	lr 0.00595	Loss 0.6541 (0.7555)	Prec@(1,5) (84.7%, 98.1%)	
07/04 12:42:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][350/781]	Step 55090	lr 0.00595	Loss 1.1267 (0.7599)	Prec@(1,5) (84.7%, 98.0%)	
07/04 12:43:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][400/781]	Step 55140	lr 0.00595	Loss 0.8375 (0.7621)	Prec@(1,5) (84.7%, 98.0%)	
07/04 12:43:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][450/781]	Step 55190	lr 0.00595	Loss 0.8241 (0.7715)	Prec@(1,5) (84.6%, 97.9%)	
07/04 12:43:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][500/781]	Step 55240	lr 0.00595	Loss 0.7193 (0.7704)	Prec@(1,5) (84.6%, 97.9%)	
07/04 12:43:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][550/781]	Step 55290	lr 0.00595	Loss 0.9150 (0.7724)	Prec@(1,5) (84.6%, 97.9%)	
07/04 12:44:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][600/781]	Step 55340	lr 0.00595	Loss 1.0223 (0.7743)	Prec@(1,5) (84.6%, 97.8%)	
07/04 12:44:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][650/781]	Step 55390	lr 0.00595	Loss 0.5757 (0.7747)	Prec@(1,5) (84.6%, 97.8%)	
07/04 12:44:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][700/781]	Step 55440	lr 0.00595	Loss 0.8208 (0.7755)	Prec@(1,5) (84.5%, 97.8%)	
07/04 12:44:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][750/781]	Step 55490	lr 0.00595	Loss 1.1257 (0.7820)	Prec@(1,5) (84.4%, 97.8%)	
07/04 12:45:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][781/781]	Step 55521	lr 0.00595	Loss 0.5861 (0.7823)	Prec@(1,5) (84.4%, 97.8%)	
07/04 12:45:04午前 evaluateCell_trainer.py:172 [INFO] Train: [ 70/99] Final Prec@1 84.3840%
07/04 12:45:08午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][50/391]	Step 55522	Loss 0.4577	Prec@(1,5) (85.5%, 98.4%)
07/04 12:45:13午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][100/391]	Step 55522	Loss 0.4726	Prec@(1,5) (84.9%, 98.3%)
07/04 12:45:17午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][150/391]	Step 55522	Loss 0.4785	Prec@(1,5) (84.9%, 98.1%)
07/04 12:45:21午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][200/391]	Step 55522	Loss 0.4783	Prec@(1,5) (84.8%, 98.2%)
07/04 12:45:25午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][250/391]	Step 55522	Loss 0.4795	Prec@(1,5) (85.0%, 98.2%)
07/04 12:45:29午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][300/391]	Step 55522	Loss 0.4795	Prec@(1,5) (85.0%, 98.1%)
07/04 12:45:33午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][350/391]	Step 55522	Loss 0.4760	Prec@(1,5) (85.0%, 98.2%)
07/04 12:45:36午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][390/391]	Step 55522	Loss 0.4725	Prec@(1,5) (85.2%, 98.1%)
07/04 12:45:36午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 70/99] Final Prec@1 85.2040%
07/04 12:45:36午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 85.2720%
07/04 12:45:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][50/781]	Step 55572	lr 0.00565	Loss 0.6581 (0.7102)	Prec@(1,5) (85.6%, 98.2%)	
07/04 12:46:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][100/781]	Step 55622	lr 0.00565	Loss 0.9834 (0.7226)	Prec@(1,5) (85.3%, 98.2%)	
07/04 12:46:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][150/781]	Step 55672	lr 0.00565	Loss 0.6551 (0.7260)	Prec@(1,5) (85.4%, 98.1%)	
07/04 12:46:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][200/781]	Step 55722	lr 0.00565	Loss 1.0725 (0.7258)	Prec@(1,5) (85.5%, 98.1%)	
07/04 12:46:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][250/781]	Step 55772	lr 0.00565	Loss 0.6101 (0.7157)	Prec@(1,5) (85.7%, 98.2%)	
07/04 12:47:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][300/781]	Step 55822	lr 0.00565	Loss 0.9653 (0.7308)	Prec@(1,5) (85.5%, 98.1%)	
07/04 12:47:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][350/781]	Step 55872	lr 0.00565	Loss 0.8998 (0.7350)	Prec@(1,5) (85.3%, 98.1%)	
07/04 12:47:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][400/781]	Step 55922	lr 0.00565	Loss 0.6732 (0.7393)	Prec@(1,5) (85.2%, 98.1%)	
07/04 12:47:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][450/781]	Step 55972	lr 0.00565	Loss 0.7601 (0.7444)	Prec@(1,5) (85.1%, 98.0%)	
07/04 12:48:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][500/781]	Step 56022	lr 0.00565	Loss 0.8712 (0.7495)	Prec@(1,5) (85.0%, 98.0%)	
07/04 12:48:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][550/781]	Step 56072	lr 0.00565	Loss 0.9204 (0.7530)	Prec@(1,5) (84.9%, 98.0%)	
07/04 12:48:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][600/781]	Step 56122	lr 0.00565	Loss 0.7520 (0.7563)	Prec@(1,5) (84.8%, 98.0%)	
07/04 12:48:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][650/781]	Step 56172	lr 0.00565	Loss 0.5543 (0.7588)	Prec@(1,5) (84.8%, 98.0%)	
07/04 12:49:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][700/781]	Step 56222	lr 0.00565	Loss 1.1110 (0.7607)	Prec@(1,5) (84.7%, 98.0%)	
07/04 12:49:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][750/781]	Step 56272	lr 0.00565	Loss 0.9329 (0.7632)	Prec@(1,5) (84.6%, 98.0%)	
07/04 12:49:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][781/781]	Step 56303	lr 0.00565	Loss 0.7486 (0.7658)	Prec@(1,5) (84.6%, 98.0%)	
07/04 12:49:29午前 evaluateCell_trainer.py:172 [INFO] Train: [ 71/99] Final Prec@1 84.5900%
07/04 12:49:34午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][50/391]	Step 56304	Loss 0.4762	Prec@(1,5) (85.9%, 98.1%)
07/04 12:49:38午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][100/391]	Step 56304	Loss 0.4709	Prec@(1,5) (85.8%, 98.1%)
07/04 12:49:42午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][150/391]	Step 56304	Loss 0.4770	Prec@(1,5) (85.5%, 98.0%)
07/04 12:49:46午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][200/391]	Step 56304	Loss 0.4867	Prec@(1,5) (85.2%, 97.9%)
07/04 12:49:50午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][250/391]	Step 56304	Loss 0.4945	Prec@(1,5) (85.0%, 97.9%)
07/04 12:49:54午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][300/391]	Step 56304	Loss 0.4899	Prec@(1,5) (85.1%, 97.9%)
07/04 12:49:59午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][350/391]	Step 56304	Loss 0.4872	Prec@(1,5) (85.3%, 97.9%)
07/04 12:50:02午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][390/391]	Step 56304	Loss 0.4842	Prec@(1,5) (85.3%, 98.0%)
07/04 12:50:02午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 71/99] Final Prec@1 85.3040%
07/04 12:50:02午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 85.3040%
07/04 12:50:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][50/781]	Step 56354	lr 0.00535	Loss 0.5265 (0.7148)	Prec@(1,5) (86.6%, 97.9%)	
07/04 12:50:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][100/781]	Step 56404	lr 0.00535	Loss 0.6333 (0.7179)	Prec@(1,5) (86.7%, 98.2%)	
07/04 12:50:48午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][150/781]	Step 56454	lr 0.00535	Loss 0.4293 (0.7093)	Prec@(1,5) (86.4%, 98.3%)	
07/04 12:51:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][200/781]	Step 56504	lr 0.00535	Loss 0.6312 (0.7063)	Prec@(1,5) (86.4%, 98.3%)	
07/04 12:51:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][250/781]	Step 56554	lr 0.00535	Loss 0.7084 (0.7073)	Prec@(1,5) (86.3%, 98.2%)	
07/04 12:51:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][300/781]	Step 56604	lr 0.00535	Loss 0.8271 (0.7118)	Prec@(1,5) (86.2%, 98.2%)	
07/04 12:51:48午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][350/781]	Step 56654	lr 0.00535	Loss 0.8536 (0.7176)	Prec@(1,5) (86.1%, 98.2%)	
07/04 12:52:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][400/781]	Step 56704	lr 0.00535	Loss 0.4810 (0.7174)	Prec@(1,5) (86.0%, 98.1%)	
07/04 12:52:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][450/781]	Step 56754	lr 0.00535	Loss 0.6668 (0.7231)	Prec@(1,5) (85.9%, 98.1%)	
07/04 12:52:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][500/781]	Step 56804	lr 0.00535	Loss 0.7510 (0.7281)	Prec@(1,5) (85.8%, 98.1%)	
07/04 12:52:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][550/781]	Step 56854	lr 0.00535	Loss 0.6767 (0.7311)	Prec@(1,5) (85.7%, 98.1%)	
07/04 12:53:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][600/781]	Step 56904	lr 0.00535	Loss 0.6804 (0.7379)	Prec@(1,5) (85.5%, 98.1%)	
07/04 12:53:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][650/781]	Step 56954	lr 0.00535	Loss 0.8737 (0.7371)	Prec@(1,5) (85.5%, 98.1%)	
07/04 12:53:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][700/781]	Step 57004	lr 0.00535	Loss 0.6744 (0.7380)	Prec@(1,5) (85.5%, 98.1%)	
07/04 12:53:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][750/781]	Step 57054	lr 0.00535	Loss 0.7451 (0.7396)	Prec@(1,5) (85.4%, 98.1%)	
07/04 12:53:56午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][781/781]	Step 57085	lr 0.00535	Loss 0.6951 (0.7400)	Prec@(1,5) (85.3%, 98.1%)	
07/04 12:53:56午前 evaluateCell_trainer.py:172 [INFO] Train: [ 72/99] Final Prec@1 85.3120%
07/04 12:54:01午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][50/391]	Step 57086	Loss 0.3952	Prec@(1,5) (88.8%, 98.3%)
07/04 12:54:05午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][100/391]	Step 57086	Loss 0.3993	Prec@(1,5) (88.4%, 98.3%)
07/04 12:54:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][150/391]	Step 57086	Loss 0.4059	Prec@(1,5) (88.0%, 98.5%)
07/04 12:54:13午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][200/391]	Step 57086	Loss 0.4049	Prec@(1,5) (88.0%, 98.4%)
07/04 12:54:17午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][250/391]	Step 57086	Loss 0.4089	Prec@(1,5) (87.9%, 98.4%)
07/04 12:54:21午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][300/391]	Step 57086	Loss 0.4054	Prec@(1,5) (87.9%, 98.5%)
07/04 12:54:26午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][350/391]	Step 57086	Loss 0.4070	Prec@(1,5) (87.8%, 98.5%)
07/04 12:54:29午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][390/391]	Step 57086	Loss 0.4067	Prec@(1,5) (87.8%, 98.5%)
07/04 12:54:29午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 72/99] Final Prec@1 87.8320%
07/04 12:54:29午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 87.8320%
07/04 12:54:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][50/781]	Step 57136	lr 0.00506	Loss 0.5666 (0.6805)	Prec@(1,5) (86.8%, 98.3%)	
07/04 12:55:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][100/781]	Step 57186	lr 0.00506	Loss 0.4793 (0.6776)	Prec@(1,5) (86.9%, 98.4%)	
07/04 12:55:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][150/781]	Step 57236	lr 0.00506	Loss 0.6245 (0.6874)	Prec@(1,5) (86.7%, 98.2%)	
07/04 12:55:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][200/781]	Step 57286	lr 0.00506	Loss 0.8131 (0.6821)	Prec@(1,5) (86.6%, 98.2%)	
07/04 12:55:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][250/781]	Step 57336	lr 0.00506	Loss 0.6336 (0.6828)	Prec@(1,5) (86.6%, 98.2%)	
07/04 12:55:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][300/781]	Step 57386	lr 0.00506	Loss 0.6341 (0.6883)	Prec@(1,5) (86.4%, 98.3%)	
07/04 12:56:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][350/781]	Step 57436	lr 0.00506	Loss 0.5284 (0.6896)	Prec@(1,5) (86.4%, 98.3%)	
07/04 12:56:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][400/781]	Step 57486	lr 0.00506	Loss 0.7226 (0.6965)	Prec@(1,5) (86.3%, 98.2%)	
07/04 12:56:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][450/781]	Step 57536	lr 0.00506	Loss 1.0066 (0.6968)	Prec@(1,5) (86.3%, 98.2%)	
07/04 12:56:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][500/781]	Step 57586	lr 0.00506	Loss 0.5220 (0.6968)	Prec@(1,5) (86.2%, 98.2%)	
07/04 12:57:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][550/781]	Step 57636	lr 0.00506	Loss 0.6435 (0.6986)	Prec@(1,5) (86.2%, 98.2%)	
07/04 12:57:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][600/781]	Step 57686	lr 0.00506	Loss 0.9089 (0.6980)	Prec@(1,5) (86.2%, 98.3%)	
07/04 12:57:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][650/781]	Step 57736	lr 0.00506	Loss 0.5647 (0.6978)	Prec@(1,5) (86.3%, 98.3%)	
07/04 12:57:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][700/781]	Step 57786	lr 0.00506	Loss 0.5285 (0.6987)	Prec@(1,5) (86.3%, 98.3%)	
07/04 12:58:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][750/781]	Step 57836	lr 0.00506	Loss 0.5606 (0.7012)	Prec@(1,5) (86.2%, 98.3%)	
07/04 12:58:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][781/781]	Step 57867	lr 0.00506	Loss 0.6487 (0.7025)	Prec@(1,5) (86.2%, 98.3%)	
07/04 12:58:23午前 evaluateCell_trainer.py:172 [INFO] Train: [ 73/99] Final Prec@1 86.1920%
07/04 12:58:28午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][50/391]	Step 57868	Loss 0.3854	Prec@(1,5) (88.3%, 98.4%)
07/04 12:58:32午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][100/391]	Step 57868	Loss 0.3956	Prec@(1,5) (87.7%, 98.4%)
07/04 12:58:36午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][150/391]	Step 57868	Loss 0.3986	Prec@(1,5) (87.7%, 98.4%)
07/04 12:58:40午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][200/391]	Step 57868	Loss 0.3949	Prec@(1,5) (87.9%, 98.5%)
07/04 12:58:44午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][250/391]	Step 57868	Loss 0.3954	Prec@(1,5) (87.9%, 98.5%)
07/04 12:58:48午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][300/391]	Step 57868	Loss 0.3964	Prec@(1,5) (87.9%, 98.5%)
07/04 12:58:52午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][350/391]	Step 57868	Loss 0.3944	Prec@(1,5) (88.0%, 98.6%)
07/04 12:58:55午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][390/391]	Step 57868	Loss 0.3938	Prec@(1,5) (88.0%, 98.6%)
07/04 12:58:56午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 73/99] Final Prec@1 88.0440%
07/04 12:58:56午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 88.0440%
07/04 12:59:11午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][50/781]	Step 57918	lr 0.00479	Loss 0.5078 (0.6472)	Prec@(1,5) (88.1%, 98.5%)	
07/04 12:59:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][100/781]	Step 57968	lr 0.00479	Loss 0.9888 (0.6489)	Prec@(1,5) (87.8%, 98.4%)	
07/04 12:59:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][150/781]	Step 58018	lr 0.00479	Loss 0.4600 (0.6455)	Prec@(1,5) (87.8%, 98.4%)	
07/04 12:59:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][200/781]	Step 58068	lr 0.00479	Loss 0.5735 (0.6519)	Prec@(1,5) (87.8%, 98.4%)	
07/04 01:00:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][250/781]	Step 58118	lr 0.00479	Loss 0.4264 (0.6578)	Prec@(1,5) (87.5%, 98.4%)	
07/04 01:00:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][300/781]	Step 58168	lr 0.00479	Loss 0.7041 (0.6703)	Prec@(1,5) (87.2%, 98.4%)	
07/04 01:00:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][350/781]	Step 58218	lr 0.00479	Loss 0.7158 (0.6750)	Prec@(1,5) (87.0%, 98.4%)	
07/04 01:00:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][400/781]	Step 58268	lr 0.00479	Loss 0.7981 (0.6780)	Prec@(1,5) (86.9%, 98.3%)	
07/04 01:01:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][450/781]	Step 58318	lr 0.00479	Loss 0.6900 (0.6808)	Prec@(1,5) (86.8%, 98.3%)	
07/04 01:01:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][500/781]	Step 58368	lr 0.00479	Loss 0.7816 (0.6853)	Prec@(1,5) (86.8%, 98.3%)	
07/04 01:01:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][550/781]	Step 58418	lr 0.00479	Loss 0.4252 (0.6887)	Prec@(1,5) (86.7%, 98.3%)	
07/04 01:01:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][600/781]	Step 58468	lr 0.00479	Loss 0.4821 (0.6874)	Prec@(1,5) (86.7%, 98.3%)	
07/04 01:02:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][650/781]	Step 58518	lr 0.00479	Loss 0.8398 (0.6871)	Prec@(1,5) (86.6%, 98.3%)	
07/04 01:02:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][700/781]	Step 58568	lr 0.00479	Loss 0.6401 (0.6848)	Prec@(1,5) (86.6%, 98.3%)	
07/04 01:02:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][750/781]	Step 58618	lr 0.00479	Loss 0.8525 (0.6858)	Prec@(1,5) (86.5%, 98.3%)	
07/04 01:02:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][781/781]	Step 58649	lr 0.00479	Loss 0.9713 (0.6872)	Prec@(1,5) (86.5%, 98.3%)	
07/04 01:02:49午前 evaluateCell_trainer.py:172 [INFO] Train: [ 74/99] Final Prec@1 86.5180%
07/04 01:02:53午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][50/391]	Step 58650	Loss 0.3663	Prec@(1,5) (89.1%, 98.8%)
07/04 01:02:57午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][100/391]	Step 58650	Loss 0.3666	Prec@(1,5) (89.2%, 98.7%)
07/04 01:03:01午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][150/391]	Step 58650	Loss 0.3698	Prec@(1,5) (88.9%, 98.7%)
07/04 01:03:06午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][200/391]	Step 58650	Loss 0.3628	Prec@(1,5) (88.9%, 98.8%)
07/04 01:03:10午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][250/391]	Step 58650	Loss 0.3613	Prec@(1,5) (89.0%, 98.8%)
07/04 01:03:14午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][300/391]	Step 58650	Loss 0.3620	Prec@(1,5) (88.9%, 98.8%)
07/04 01:03:18午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][350/391]	Step 58650	Loss 0.3646	Prec@(1,5) (88.8%, 98.7%)
07/04 01:03:21午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][390/391]	Step 58650	Loss 0.3618	Prec@(1,5) (89.0%, 98.8%)
07/04 01:03:21午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 74/99] Final Prec@1 88.9680%
07/04 01:03:22午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 88.9680%
07/04 01:03:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][50/781]	Step 58700	lr 0.00451	Loss 0.7269 (0.6185)	Prec@(1,5) (87.9%, 98.5%)	
07/04 01:03:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][100/781]	Step 58750	lr 0.00451	Loss 0.4609 (0.6110)	Prec@(1,5) (88.2%, 98.6%)	
07/04 01:04:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][150/781]	Step 58800	lr 0.00451	Loss 0.5258 (0.6200)	Prec@(1,5) (88.0%, 98.6%)	
07/04 01:04:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][200/781]	Step 58850	lr 0.00451	Loss 0.6956 (0.6290)	Prec@(1,5) (87.9%, 98.5%)	
07/04 01:04:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][250/781]	Step 58900	lr 0.00451	Loss 0.6957 (0.6243)	Prec@(1,5) (87.9%, 98.6%)	
07/04 01:04:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][300/781]	Step 58950	lr 0.00451	Loss 0.5655 (0.6330)	Prec@(1,5) (88.0%, 98.6%)	
07/04 01:05:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][350/781]	Step 59000	lr 0.00451	Loss 0.6390 (0.6397)	Prec@(1,5) (87.8%, 98.5%)	
07/04 01:05:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][400/781]	Step 59050	lr 0.00451	Loss 0.6014 (0.6384)	Prec@(1,5) (87.8%, 98.6%)	
07/04 01:05:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][450/781]	Step 59100	lr 0.00451	Loss 0.8536 (0.6424)	Prec@(1,5) (87.7%, 98.6%)	
07/04 01:05:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][500/781]	Step 59150	lr 0.00451	Loss 0.5312 (0.6442)	Prec@(1,5) (87.7%, 98.5%)	
07/04 01:06:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][550/781]	Step 59200	lr 0.00451	Loss 1.0164 (0.6484)	Prec@(1,5) (87.6%, 98.5%)	
07/04 01:06:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][600/781]	Step 59250	lr 0.00451	Loss 0.6064 (0.6499)	Prec@(1,5) (87.6%, 98.6%)	
07/04 01:06:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][650/781]	Step 59300	lr 0.00451	Loss 0.8627 (0.6522)	Prec@(1,5) (87.5%, 98.6%)	
07/04 01:06:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][700/781]	Step 59350	lr 0.00451	Loss 0.7938 (0.6526)	Prec@(1,5) (87.5%, 98.5%)	
07/04 01:07:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][750/781]	Step 59400	lr 0.00451	Loss 0.4821 (0.6551)	Prec@(1,5) (87.5%, 98.5%)	
07/04 01:07:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][781/781]	Step 59431	lr 0.00451	Loss 0.4697 (0.6550)	Prec@(1,5) (87.5%, 98.5%)	
07/04 01:07:16午前 evaluateCell_trainer.py:172 [INFO] Train: [ 75/99] Final Prec@1 87.4660%
07/04 01:07:20午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][50/391]	Step 59432	Loss 0.3370	Prec@(1,5) (90.4%, 98.8%)
07/04 01:07:24午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][100/391]	Step 59432	Loss 0.3430	Prec@(1,5) (89.7%, 98.7%)
07/04 01:07:29午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][150/391]	Step 59432	Loss 0.3450	Prec@(1,5) (89.6%, 98.7%)
07/04 01:07:33午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][200/391]	Step 59432	Loss 0.3488	Prec@(1,5) (89.5%, 98.8%)
07/04 01:07:37午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][250/391]	Step 59432	Loss 0.3464	Prec@(1,5) (89.5%, 98.8%)
07/04 01:07:41午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][300/391]	Step 59432	Loss 0.3479	Prec@(1,5) (89.4%, 98.8%)
07/04 01:07:45午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][350/391]	Step 59432	Loss 0.3485	Prec@(1,5) (89.5%, 98.7%)
07/04 01:07:48午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][390/391]	Step 59432	Loss 0.3470	Prec@(1,5) (89.4%, 98.8%)
07/04 01:07:49午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 75/99] Final Prec@1 89.4280%
07/04 01:07:49午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 89.4280%
07/04 01:08:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][50/781]	Step 59482	lr 0.00425	Loss 0.6516 (0.6290)	Prec@(1,5) (88.2%, 98.3%)	
07/04 01:08:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][100/781]	Step 59532	lr 0.00425	Loss 0.5315 (0.6124)	Prec@(1,5) (88.5%, 98.5%)	
07/04 01:08:34午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][150/781]	Step 59582	lr 0.00425	Loss 0.6200 (0.6183)	Prec@(1,5) (88.4%, 98.5%)	
07/04 01:08:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][200/781]	Step 59632	lr 0.00425	Loss 0.8856 (0.6246)	Prec@(1,5) (88.1%, 98.5%)	
07/04 01:09:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][250/781]	Step 59682	lr 0.00425	Loss 0.4252 (0.6243)	Prec@(1,5) (88.0%, 98.5%)	
07/04 01:09:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][300/781]	Step 59732	lr 0.00425	Loss 0.6528 (0.6262)	Prec@(1,5) (88.0%, 98.5%)	
07/04 01:09:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][350/781]	Step 59782	lr 0.00425	Loss 0.5393 (0.6260)	Prec@(1,5) (88.0%, 98.5%)	
07/04 01:09:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][400/781]	Step 59832	lr 0.00425	Loss 0.6295 (0.6266)	Prec@(1,5) (88.0%, 98.5%)	
07/04 01:09:45午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][450/781]	Step 59882	lr 0.00425	Loss 0.7258 (0.6266)	Prec@(1,5) (88.0%, 98.6%)	
07/04 01:09:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][500/781]	Step 59932	lr 0.00425	Loss 0.6608 (0.6310)	Prec@(1,5) (87.9%, 98.6%)	
07/04 01:10:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][550/781]	Step 59982	lr 0.00425	Loss 0.7216 (0.6291)	Prec@(1,5) (88.0%, 98.6%)	
07/04 01:10:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][600/781]	Step 60032	lr 0.00425	Loss 0.6583 (0.6308)	Prec@(1,5) (87.9%, 98.6%)	
07/04 01:10:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][650/781]	Step 60082	lr 0.00425	Loss 0.6772 (0.6323)	Prec@(1,5) (87.9%, 98.6%)	
07/04 01:10:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][700/781]	Step 60132	lr 0.00425	Loss 0.7286 (0.6332)	Prec@(1,5) (87.9%, 98.5%)	
07/04 01:10:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][750/781]	Step 60182	lr 0.00425	Loss 0.6696 (0.6313)	Prec@(1,5) (87.9%, 98.5%)	
07/04 01:10:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][781/781]	Step 60213	lr 0.00425	Loss 0.4646 (0.6314)	Prec@(1,5) (87.9%, 98.6%)	
07/04 01:10:35午前 evaluateCell_trainer.py:172 [INFO] Train: [ 76/99] Final Prec@1 87.8940%
07/04 01:10:37午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][50/391]	Step 60214	Loss 0.3256	Prec@(1,5) (89.6%, 99.1%)
07/04 01:10:39午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][100/391]	Step 60214	Loss 0.3231	Prec@(1,5) (89.8%, 99.1%)
07/04 01:10:41午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][150/391]	Step 60214	Loss 0.3213	Prec@(1,5) (90.0%, 99.0%)
07/04 01:10:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][200/391]	Step 60214	Loss 0.3238	Prec@(1,5) (89.9%, 99.0%)
07/04 01:10:44午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][250/391]	Step 60214	Loss 0.3234	Prec@(1,5) (89.9%, 99.0%)
07/04 01:10:46午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][300/391]	Step 60214	Loss 0.3239	Prec@(1,5) (89.9%, 99.0%)
07/04 01:10:48午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][350/391]	Step 60214	Loss 0.3272	Prec@(1,5) (89.8%, 99.0%)
07/04 01:10:50午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][390/391]	Step 60214	Loss 0.3274	Prec@(1,5) (89.8%, 99.0%)
07/04 01:10:50午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 76/99] Final Prec@1 89.8000%
07/04 01:10:50午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 89.8000%
07/04 01:10:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][50/781]	Step 60264	lr 0.004	Loss 0.8157 (0.6037)	Prec@(1,5) (89.0%, 98.7%)	
07/04 01:11:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][100/781]	Step 60314	lr 0.004	Loss 0.4882 (0.5965)	Prec@(1,5) (89.2%, 98.8%)	
07/04 01:11:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][150/781]	Step 60364	lr 0.004	Loss 0.5900 (0.5998)	Prec@(1,5) (88.9%, 98.7%)	
07/04 01:11:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][200/781]	Step 60414	lr 0.004	Loss 0.8418 (0.6078)	Prec@(1,5) (88.7%, 98.6%)	
07/04 01:11:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][250/781]	Step 60464	lr 0.004	Loss 0.4243 (0.6031)	Prec@(1,5) (88.8%, 98.6%)	
07/04 01:11:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][300/781]	Step 60514	lr 0.004	Loss 0.7442 (0.5957)	Prec@(1,5) (89.0%, 98.7%)	
07/04 01:11:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][350/781]	Step 60564	lr 0.004	Loss 0.6196 (0.5949)	Prec@(1,5) (89.0%, 98.7%)	
07/04 01:11:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][400/781]	Step 60614	lr 0.004	Loss 0.4209 (0.5966)	Prec@(1,5) (88.9%, 98.7%)	
07/04 01:11:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][450/781]	Step 60664	lr 0.004	Loss 0.6734 (0.5979)	Prec@(1,5) (88.8%, 98.7%)	
07/04 01:12:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][500/781]	Step 60714	lr 0.004	Loss 0.6436 (0.6034)	Prec@(1,5) (88.6%, 98.7%)	
07/04 01:12:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][550/781]	Step 60764	lr 0.004	Loss 0.5166 (0.6064)	Prec@(1,5) (88.5%, 98.7%)	
07/04 01:12:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][600/781]	Step 60814	lr 0.004	Loss 0.5204 (0.6060)	Prec@(1,5) (88.5%, 98.7%)	
07/04 01:12:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][650/781]	Step 60864	lr 0.004	Loss 0.5355 (0.6049)	Prec@(1,5) (88.6%, 98.7%)	
07/04 01:12:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][700/781]	Step 60914	lr 0.004	Loss 0.7638 (0.6084)	Prec@(1,5) (88.5%, 98.7%)	
07/04 01:12:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][750/781]	Step 60964	lr 0.004	Loss 0.6197 (0.6066)	Prec@(1,5) (88.5%, 98.7%)	
07/04 01:12:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][781/781]	Step 60995	lr 0.004	Loss 0.7566 (0.6070)	Prec@(1,5) (88.5%, 98.7%)	
07/04 01:12:47午前 evaluateCell_trainer.py:172 [INFO] Train: [ 77/99] Final Prec@1 88.5180%
07/04 01:12:49午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][50/391]	Step 60996	Loss 0.2897	Prec@(1,5) (91.4%, 99.1%)
07/04 01:12:51午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][100/391]	Step 60996	Loss 0.2926	Prec@(1,5) (91.5%, 99.1%)
07/04 01:12:53午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][150/391]	Step 60996	Loss 0.2989	Prec@(1,5) (91.1%, 99.1%)
07/04 01:12:55午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][200/391]	Step 60996	Loss 0.3012	Prec@(1,5) (90.9%, 99.1%)
07/04 01:12:57午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][250/391]	Step 60996	Loss 0.3022	Prec@(1,5) (90.9%, 99.1%)
07/04 01:12:59午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][300/391]	Step 60996	Loss 0.3016	Prec@(1,5) (90.9%, 99.1%)
07/04 01:13:01午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][350/391]	Step 60996	Loss 0.3031	Prec@(1,5) (90.9%, 99.1%)
07/04 01:13:03午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][390/391]	Step 60996	Loss 0.3025	Prec@(1,5) (90.9%, 99.1%)
07/04 01:13:03午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 77/99] Final Prec@1 90.8960%
07/04 01:13:03午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 90.8960%
07/04 01:13:11午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][50/781]	Step 61046	lr 0.00375	Loss 0.5329 (0.5522)	Prec@(1,5) (90.1%, 99.1%)	
07/04 01:13:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][100/781]	Step 61096	lr 0.00375	Loss 0.3212 (0.5611)	Prec@(1,5) (89.7%, 99.0%)	
07/04 01:13:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][150/781]	Step 61146	lr 0.00375	Loss 0.6627 (0.5625)	Prec@(1,5) (89.7%, 98.9%)	
07/04 01:13:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][200/781]	Step 61196	lr 0.00375	Loss 0.5643 (0.5684)	Prec@(1,5) (89.6%, 98.8%)	
07/04 01:13:41午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][250/781]	Step 61246	lr 0.00375	Loss 0.9300 (0.5750)	Prec@(1,5) (89.5%, 98.7%)	
07/04 01:13:48午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][300/781]	Step 61296	lr 0.00375	Loss 0.7635 (0.5719)	Prec@(1,5) (89.5%, 98.7%)	
07/04 01:13:56午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][350/781]	Step 61346	lr 0.00375	Loss 0.8299 (0.5737)	Prec@(1,5) (89.5%, 98.7%)	
07/04 01:14:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][400/781]	Step 61396	lr 0.00375	Loss 0.5092 (0.5725)	Prec@(1,5) (89.5%, 98.7%)	
07/04 01:14:11午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][450/781]	Step 61446	lr 0.00375	Loss 0.5274 (0.5752)	Prec@(1,5) (89.5%, 98.7%)	
07/04 01:14:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][500/781]	Step 61496	lr 0.00375	Loss 0.6460 (0.5791)	Prec@(1,5) (89.3%, 98.7%)	
07/04 01:14:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][550/781]	Step 61546	lr 0.00375	Loss 0.5037 (0.5821)	Prec@(1,5) (89.3%, 98.7%)	
07/04 01:14:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][600/781]	Step 61596	lr 0.00375	Loss 0.6257 (0.5821)	Prec@(1,5) (89.2%, 98.7%)	
07/04 01:14:41午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][650/781]	Step 61646	lr 0.00375	Loss 0.5210 (0.5857)	Prec@(1,5) (89.1%, 98.7%)	
07/04 01:14:48午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][700/781]	Step 61696	lr 0.00375	Loss 0.7207 (0.5855)	Prec@(1,5) (89.1%, 98.7%)	
07/04 01:14:56午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][750/781]	Step 61746	lr 0.00375	Loss 0.7358 (0.5842)	Prec@(1,5) (89.2%, 98.7%)	
07/04 01:15:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][781/781]	Step 61777	lr 0.00375	Loss 0.7321 (0.5855)	Prec@(1,5) (89.1%, 98.7%)	
07/04 01:15:01午前 evaluateCell_trainer.py:172 [INFO] Train: [ 78/99] Final Prec@1 89.1460%
07/04 01:15:03午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][50/391]	Step 61778	Loss 0.2842	Prec@(1,5) (91.2%, 99.3%)
07/04 01:15:05午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][100/391]	Step 61778	Loss 0.2912	Prec@(1,5) (91.2%, 99.2%)
07/04 01:15:07午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][150/391]	Step 61778	Loss 0.2879	Prec@(1,5) (91.5%, 99.2%)
07/04 01:15:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][200/391]	Step 61778	Loss 0.2886	Prec@(1,5) (91.4%, 99.2%)
07/04 01:15:11午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][250/391]	Step 61778	Loss 0.2860	Prec@(1,5) (91.4%, 99.2%)
07/04 01:15:13午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][300/391]	Step 61778	Loss 0.2860	Prec@(1,5) (91.4%, 99.1%)
07/04 01:15:15午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][350/391]	Step 61778	Loss 0.2880	Prec@(1,5) (91.2%, 99.1%)
07/04 01:15:16午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][390/391]	Step 61778	Loss 0.2884	Prec@(1,5) (91.2%, 99.1%)
07/04 01:15:16午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 78/99] Final Prec@1 91.2280%
07/04 01:15:17午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 91.2280%
07/04 01:15:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][50/781]	Step 61828	lr 0.00352	Loss 0.5200 (0.5099)	Prec@(1,5) (90.2%, 99.1%)	
07/04 01:15:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][100/781]	Step 61878	lr 0.00352	Loss 0.5773 (0.5313)	Prec@(1,5) (89.9%, 98.9%)	
07/04 01:15:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][150/781]	Step 61928	lr 0.00352	Loss 0.4761 (0.5442)	Prec@(1,5) (89.7%, 98.9%)	
07/04 01:15:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][200/781]	Step 61978	lr 0.00352	Loss 0.4975 (0.5530)	Prec@(1,5) (89.6%, 98.9%)	
07/04 01:15:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][250/781]	Step 62028	lr 0.00352	Loss 0.5419 (0.5485)	Prec@(1,5) (89.7%, 99.0%)	
07/04 01:16:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][300/781]	Step 62078	lr 0.00352	Loss 0.3996 (0.5513)	Prec@(1,5) (89.7%, 98.9%)	
07/04 01:16:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][350/781]	Step 62128	lr 0.00352	Loss 0.6149 (0.5530)	Prec@(1,5) (89.6%, 99.0%)	
07/04 01:16:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][400/781]	Step 62178	lr 0.00352	Loss 0.3870 (0.5555)	Prec@(1,5) (89.6%, 98.9%)	
07/04 01:16:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][450/781]	Step 62228	lr 0.00352	Loss 0.6628 (0.5570)	Prec@(1,5) (89.5%, 98.9%)	
07/04 01:16:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][500/781]	Step 62278	lr 0.00352	Loss 0.6608 (0.5584)	Prec@(1,5) (89.5%, 98.9%)	
07/04 01:16:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][550/781]	Step 62328	lr 0.00352	Loss 0.5941 (0.5593)	Prec@(1,5) (89.4%, 98.9%)	
07/04 01:16:48午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][600/781]	Step 62378	lr 0.00352	Loss 0.5196 (0.5604)	Prec@(1,5) (89.4%, 98.9%)	
07/04 01:16:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][650/781]	Step 62428	lr 0.00352	Loss 0.6194 (0.5605)	Prec@(1,5) (89.4%, 98.9%)	
07/04 01:17:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][700/781]	Step 62478	lr 0.00352	Loss 0.2886 (0.5595)	Prec@(1,5) (89.5%, 98.9%)	
07/04 01:17:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][750/781]	Step 62528	lr 0.00352	Loss 0.9441 (0.5610)	Prec@(1,5) (89.4%, 98.9%)	
07/04 01:17:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][781/781]	Step 62559	lr 0.00352	Loss 0.4883 (0.5617)	Prec@(1,5) (89.4%, 98.9%)	
07/04 01:17:15午前 evaluateCell_trainer.py:172 [INFO] Train: [ 79/99] Final Prec@1 89.3980%
07/04 01:17:18午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][50/391]	Step 62560	Loss 0.2850	Prec@(1,5) (91.4%, 99.3%)
07/04 01:17:20午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][100/391]	Step 62560	Loss 0.2804	Prec@(1,5) (91.5%, 99.3%)
07/04 01:17:22午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][150/391]	Step 62560	Loss 0.2778	Prec@(1,5) (91.5%, 99.2%)
07/04 01:17:23午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][200/391]	Step 62560	Loss 0.2780	Prec@(1,5) (91.6%, 99.2%)
07/04 01:17:25午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][250/391]	Step 62560	Loss 0.2748	Prec@(1,5) (91.7%, 99.2%)
07/04 01:17:27午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][300/391]	Step 62560	Loss 0.2754	Prec@(1,5) (91.7%, 99.2%)
07/04 01:17:29午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][350/391]	Step 62560	Loss 0.2750	Prec@(1,5) (91.7%, 99.3%)
07/04 01:17:31午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][390/391]	Step 62560	Loss 0.2759	Prec@(1,5) (91.7%, 99.2%)
07/04 01:17:31午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 79/99] Final Prec@1 91.6800%
07/04 01:17:31午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 91.6800%
07/04 01:17:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][50/781]	Step 62610	lr 0.00329	Loss 0.4044 (0.5449)	Prec@(1,5) (90.4%, 99.1%)	
07/04 01:17:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][100/781]	Step 62660	lr 0.00329	Loss 0.4855 (0.5287)	Prec@(1,5) (90.5%, 99.2%)	
07/04 01:17:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][150/781]	Step 62710	lr 0.00329	Loss 0.3024 (0.5227)	Prec@(1,5) (90.7%, 99.1%)	
07/04 01:18:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][200/781]	Step 62760	lr 0.00329	Loss 0.4966 (0.5246)	Prec@(1,5) (90.6%, 99.1%)	
07/04 01:18:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][250/781]	Step 62810	lr 0.00329	Loss 0.6508 (0.5270)	Prec@(1,5) (90.4%, 99.1%)	
07/04 01:18:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][300/781]	Step 62860	lr 0.00329	Loss 0.7336 (0.5335)	Prec@(1,5) (90.4%, 99.0%)	
07/04 01:18:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][350/781]	Step 62910	lr 0.00329	Loss 0.4325 (0.5319)	Prec@(1,5) (90.6%, 99.0%)	
07/04 01:18:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][400/781]	Step 62960	lr 0.00329	Loss 0.5923 (0.5328)	Prec@(1,5) (90.5%, 99.0%)	
07/04 01:18:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][450/781]	Step 63010	lr 0.00329	Loss 0.3310 (0.5376)	Prec@(1,5) (90.4%, 99.0%)	
07/04 01:18:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][500/781]	Step 63060	lr 0.00329	Loss 0.6089 (0.5392)	Prec@(1,5) (90.4%, 99.0%)	
07/04 01:18:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][550/781]	Step 63110	lr 0.00329	Loss 0.5554 (0.5415)	Prec@(1,5) (90.4%, 99.0%)	
07/04 01:19:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][600/781]	Step 63160	lr 0.00329	Loss 0.5174 (0.5426)	Prec@(1,5) (90.3%, 99.0%)	
07/04 01:19:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][650/781]	Step 63210	lr 0.00329	Loss 0.5908 (0.5433)	Prec@(1,5) (90.3%, 99.0%)	
07/04 01:19:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][700/781]	Step 63260	lr 0.00329	Loss 0.3961 (0.5431)	Prec@(1,5) (90.2%, 99.0%)	
07/04 01:19:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][750/781]	Step 63310	lr 0.00329	Loss 0.6987 (0.5425)	Prec@(1,5) (90.3%, 99.0%)	
07/04 01:19:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][781/781]	Step 63341	lr 0.00329	Loss 0.4912 (0.5437)	Prec@(1,5) (90.2%, 99.0%)	
07/04 01:19:30午前 evaluateCell_trainer.py:172 [INFO] Train: [ 80/99] Final Prec@1 90.1960%
07/04 01:19:32午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][50/391]	Step 63342	Loss 0.2341	Prec@(1,5) (92.8%, 99.3%)
07/04 01:19:34午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][100/391]	Step 63342	Loss 0.2405	Prec@(1,5) (92.7%, 99.3%)
07/04 01:19:36午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][150/391]	Step 63342	Loss 0.2414	Prec@(1,5) (92.8%, 99.3%)
07/04 01:19:38午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][200/391]	Step 63342	Loss 0.2452	Prec@(1,5) (92.7%, 99.3%)
07/04 01:19:40午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][250/391]	Step 63342	Loss 0.2452	Prec@(1,5) (92.7%, 99.3%)
07/04 01:19:42午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][300/391]	Step 63342	Loss 0.2445	Prec@(1,5) (92.8%, 99.3%)
07/04 01:19:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][350/391]	Step 63342	Loss 0.2462	Prec@(1,5) (92.8%, 99.3%)
07/04 01:19:45午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][390/391]	Step 63342	Loss 0.2470	Prec@(1,5) (92.7%, 99.3%)
07/04 01:19:45午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 80/99] Final Prec@1 92.7040%
07/04 01:19:45午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 92.7040%
07/04 01:19:53午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][50/781]	Step 63392	lr 0.00308	Loss 0.3691 (0.4837)	Prec@(1,5) (91.1%, 99.2%)	
07/04 01:20:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][100/781]	Step 63442	lr 0.00308	Loss 0.3948 (0.4948)	Prec@(1,5) (91.3%, 99.1%)	
07/04 01:20:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][150/781]	Step 63492	lr 0.00308	Loss 0.3347 (0.4995)	Prec@(1,5) (91.2%, 99.1%)	
07/04 01:20:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][200/781]	Step 63542	lr 0.00308	Loss 0.4605 (0.4944)	Prec@(1,5) (91.2%, 99.1%)	
07/04 01:20:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][250/781]	Step 63592	lr 0.00308	Loss 0.3277 (0.4982)	Prec@(1,5) (91.2%, 99.1%)	
07/04 01:20:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][300/781]	Step 63642	lr 0.00308	Loss 0.3062 (0.5033)	Prec@(1,5) (91.2%, 99.0%)	
07/04 01:20:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][350/781]	Step 63692	lr 0.00308	Loss 0.4661 (0.5020)	Prec@(1,5) (91.2%, 99.1%)	
07/04 01:20:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][400/781]	Step 63742	lr 0.00308	Loss 0.5107 (0.5020)	Prec@(1,5) (91.1%, 99.1%)	
07/04 01:20:53午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][450/781]	Step 63792	lr 0.00308	Loss 0.5692 (0.5051)	Prec@(1,5) (91.0%, 99.1%)	
07/04 01:21:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][500/781]	Step 63842	lr 0.00308	Loss 0.3424 (0.5045)	Prec@(1,5) (91.0%, 99.1%)	
07/04 01:21:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][550/781]	Step 63892	lr 0.00308	Loss 0.6022 (0.5081)	Prec@(1,5) (90.9%, 99.1%)	
07/04 01:21:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][600/781]	Step 63942	lr 0.00308	Loss 0.6320 (0.5074)	Prec@(1,5) (90.9%, 99.1%)	
07/04 01:21:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][650/781]	Step 63992	lr 0.00308	Loss 0.6336 (0.5096)	Prec@(1,5) (90.9%, 99.1%)	
07/04 01:21:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][700/781]	Step 64042	lr 0.00308	Loss 0.5067 (0.5110)	Prec@(1,5) (90.8%, 99.1%)	
07/04 01:21:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][750/781]	Step 64092	lr 0.00308	Loss 0.7794 (0.5138)	Prec@(1,5) (90.8%, 99.1%)	
07/04 01:21:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][781/781]	Step 64123	lr 0.00308	Loss 0.5025 (0.5160)	Prec@(1,5) (90.7%, 99.0%)	
07/04 01:21:44午前 evaluateCell_trainer.py:172 [INFO] Train: [ 81/99] Final Prec@1 90.7420%
07/04 01:21:46午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][50/391]	Step 64124	Loss 0.2358	Prec@(1,5) (93.2%, 99.2%)
07/04 01:21:48午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][100/391]	Step 64124	Loss 0.2274	Prec@(1,5) (93.2%, 99.4%)
07/04 01:21:50午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][150/391]	Step 64124	Loss 0.2359	Prec@(1,5) (93.0%, 99.3%)
07/04 01:21:52午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][200/391]	Step 64124	Loss 0.2315	Prec@(1,5) (93.2%, 99.4%)
07/04 01:21:54午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][250/391]	Step 64124	Loss 0.2324	Prec@(1,5) (93.2%, 99.4%)
07/04 01:21:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][300/391]	Step 64124	Loss 0.2336	Prec@(1,5) (93.2%, 99.4%)
07/04 01:21:57午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][350/391]	Step 64124	Loss 0.2358	Prec@(1,5) (93.0%, 99.4%)
07/04 01:21:59午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][390/391]	Step 64124	Loss 0.2365	Prec@(1,5) (93.0%, 99.4%)
07/04 01:21:59午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 81/99] Final Prec@1 92.9800%
07/04 01:21:59午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 92.9800%
07/04 01:22:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][50/781]	Step 64174	lr 0.00287	Loss 0.5081 (0.4740)	Prec@(1,5) (91.8%, 99.0%)	
07/04 01:22:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][100/781]	Step 64224	lr 0.00287	Loss 0.4408 (0.4640)	Prec@(1,5) (92.0%, 99.1%)	
07/04 01:22:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][150/781]	Step 64274	lr 0.00287	Loss 0.4256 (0.4719)	Prec@(1,5) (92.1%, 99.1%)	
07/04 01:22:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][200/781]	Step 64324	lr 0.00287	Loss 0.2075 (0.4691)	Prec@(1,5) (92.1%, 99.1%)	
07/04 01:22:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][250/781]	Step 64374	lr 0.00287	Loss 0.5853 (0.4688)	Prec@(1,5) (92.0%, 99.2%)	
07/04 01:22:45午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][300/781]	Step 64424	lr 0.00287	Loss 0.4853 (0.4722)	Prec@(1,5) (91.9%, 99.2%)	
07/04 01:22:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][350/781]	Step 64474	lr 0.00287	Loss 0.5513 (0.4754)	Prec@(1,5) (91.7%, 99.2%)	
07/04 01:23:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][400/781]	Step 64524	lr 0.00287	Loss 0.4205 (0.4835)	Prec@(1,5) (91.6%, 99.1%)	
07/04 01:23:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][450/781]	Step 64574	lr 0.00287	Loss 0.3907 (0.4853)	Prec@(1,5) (91.5%, 99.1%)	
07/04 01:23:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][500/781]	Step 64624	lr 0.00287	Loss 0.5264 (0.4875)	Prec@(1,5) (91.5%, 99.1%)	
07/04 01:23:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][550/781]	Step 64674	lr 0.00287	Loss 0.4162 (0.4900)	Prec@(1,5) (91.4%, 99.1%)	
07/04 01:23:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][600/781]	Step 64724	lr 0.00287	Loss 0.4506 (0.4919)	Prec@(1,5) (91.3%, 99.1%)	
07/04 01:23:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][650/781]	Step 64774	lr 0.00287	Loss 0.4816 (0.4939)	Prec@(1,5) (91.3%, 99.1%)	
07/04 01:23:45午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][700/781]	Step 64824	lr 0.00287	Loss 0.5384 (0.4956)	Prec@(1,5) (91.3%, 99.1%)	
07/04 01:23:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][750/781]	Step 64874	lr 0.00287	Loss 0.5385 (0.4972)	Prec@(1,5) (91.2%, 99.0%)	
07/04 01:23:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][781/781]	Step 64905	lr 0.00287	Loss 0.5745 (0.4979)	Prec@(1,5) (91.2%, 99.0%)	
07/04 01:23:57午前 evaluateCell_trainer.py:172 [INFO] Train: [ 82/99] Final Prec@1 91.1900%
07/04 01:23:59午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][50/391]	Step 64906	Loss 0.2263	Prec@(1,5) (93.3%, 99.4%)
07/04 01:24:01午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][100/391]	Step 64906	Loss 0.2305	Prec@(1,5) (93.2%, 99.3%)
07/04 01:24:03午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][150/391]	Step 64906	Loss 0.2352	Prec@(1,5) (93.1%, 99.3%)
07/04 01:24:05午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][200/391]	Step 64906	Loss 0.2335	Prec@(1,5) (93.2%, 99.2%)
07/04 01:24:07午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][250/391]	Step 64906	Loss 0.2341	Prec@(1,5) (93.1%, 99.3%)
07/04 01:24:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][300/391]	Step 64906	Loss 0.2322	Prec@(1,5) (93.2%, 99.3%)
07/04 01:24:11午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][350/391]	Step 64906	Loss 0.2290	Prec@(1,5) (93.3%, 99.3%)
07/04 01:24:12午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][390/391]	Step 64906	Loss 0.2269	Prec@(1,5) (93.3%, 99.3%)
07/04 01:24:12午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 82/99] Final Prec@1 93.3360%
07/04 01:24:13午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 93.3360%
07/04 01:24:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][50/781]	Step 64956	lr 0.00267	Loss 0.5334 (0.4651)	Prec@(1,5) (92.0%, 99.1%)	
07/04 01:24:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][100/781]	Step 65006	lr 0.00267	Loss 0.4504 (0.4628)	Prec@(1,5) (92.0%, 99.2%)	
07/04 01:24:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][150/781]	Step 65056	lr 0.00267	Loss 0.2985 (0.4583)	Prec@(1,5) (92.2%, 99.2%)	
07/04 01:24:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][200/781]	Step 65106	lr 0.00267	Loss 0.6922 (0.4608)	Prec@(1,5) (92.1%, 99.2%)	
07/04 01:24:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][250/781]	Step 65156	lr 0.00267	Loss 0.6413 (0.4586)	Prec@(1,5) (92.2%, 99.2%)	
07/04 01:24:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][300/781]	Step 65206	lr 0.00267	Loss 0.3453 (0.4632)	Prec@(1,5) (92.0%, 99.2%)	
07/04 01:25:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][350/781]	Step 65256	lr 0.00267	Loss 0.8514 (0.4676)	Prec@(1,5) (91.9%, 99.2%)	
07/04 01:25:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][400/781]	Step 65306	lr 0.00267	Loss 0.6389 (0.4700)	Prec@(1,5) (91.9%, 99.2%)	
07/04 01:25:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][450/781]	Step 65356	lr 0.00267	Loss 0.5324 (0.4716)	Prec@(1,5) (91.8%, 99.2%)	
07/04 01:25:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][500/781]	Step 65406	lr 0.00267	Loss 0.3657 (0.4714)	Prec@(1,5) (91.8%, 99.2%)	
07/04 01:25:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][550/781]	Step 65456	lr 0.00267	Loss 0.6771 (0.4705)	Prec@(1,5) (91.8%, 99.2%)	
07/04 01:25:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][600/781]	Step 65506	lr 0.00267	Loss 0.5623 (0.4737)	Prec@(1,5) (91.8%, 99.2%)	
07/04 01:25:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][650/781]	Step 65556	lr 0.00267	Loss 0.5766 (0.4751)	Prec@(1,5) (91.7%, 99.2%)	
07/04 01:25:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][700/781]	Step 65606	lr 0.00267	Loss 0.4050 (0.4748)	Prec@(1,5) (91.7%, 99.2%)	
07/04 01:26:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][750/781]	Step 65656	lr 0.00267	Loss 0.4607 (0.4759)	Prec@(1,5) (91.7%, 99.2%)	
07/04 01:26:11午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][781/781]	Step 65687	lr 0.00267	Loss 0.4717 (0.4768)	Prec@(1,5) (91.7%, 99.2%)	
07/04 01:26:11午前 evaluateCell_trainer.py:172 [INFO] Train: [ 83/99] Final Prec@1 91.6840%
07/04 01:26:13午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][50/391]	Step 65688	Loss 0.2455	Prec@(1,5) (92.9%, 99.2%)
07/04 01:26:15午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][100/391]	Step 65688	Loss 0.2445	Prec@(1,5) (92.9%, 99.3%)
07/04 01:26:17午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][150/391]	Step 65688	Loss 0.2361	Prec@(1,5) (93.2%, 99.3%)
07/04 01:26:19午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][200/391]	Step 65688	Loss 0.2326	Prec@(1,5) (93.4%, 99.3%)
07/04 01:26:21午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][250/391]	Step 65688	Loss 0.2314	Prec@(1,5) (93.4%, 99.3%)
07/04 01:26:23午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][300/391]	Step 65688	Loss 0.2279	Prec@(1,5) (93.5%, 99.3%)
07/04 01:26:25午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][350/391]	Step 65688	Loss 0.2286	Prec@(1,5) (93.5%, 99.3%)
07/04 01:26:26午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][390/391]	Step 65688	Loss 0.2274	Prec@(1,5) (93.5%, 99.3%)
07/04 01:26:27午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 83/99] Final Prec@1 93.5160%
07/04 01:26:27午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 93.5160%
07/04 01:26:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][50/781]	Step 65738	lr 0.00248	Loss 0.4372 (0.4282)	Prec@(1,5) (92.8%, 99.4%)	
07/04 01:26:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][100/781]	Step 65788	lr 0.00248	Loss 0.4482 (0.4328)	Prec@(1,5) (92.8%, 99.3%)	
07/04 01:26:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][150/781]	Step 65838	lr 0.00248	Loss 0.4055 (0.4462)	Prec@(1,5) (92.4%, 99.2%)	
07/04 01:26:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][200/781]	Step 65888	lr 0.00248	Loss 0.6164 (0.4376)	Prec@(1,5) (92.6%, 99.3%)	
07/04 01:27:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][250/781]	Step 65938	lr 0.00248	Loss 0.4473 (0.4426)	Prec@(1,5) (92.5%, 99.3%)	
07/04 01:27:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][300/781]	Step 65988	lr 0.00248	Loss 0.5421 (0.4448)	Prec@(1,5) (92.5%, 99.3%)	
07/04 01:27:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][350/781]	Step 66038	lr 0.00248	Loss 0.3280 (0.4450)	Prec@(1,5) (92.5%, 99.3%)	
07/04 01:27:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][400/781]	Step 66088	lr 0.00248	Loss 0.3748 (0.4475)	Prec@(1,5) (92.4%, 99.3%)	
07/04 01:27:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][450/781]	Step 66138	lr 0.00248	Loss 0.4825 (0.4495)	Prec@(1,5) (92.3%, 99.3%)	
07/04 01:27:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][500/781]	Step 66188	lr 0.00248	Loss 0.6022 (0.4573)	Prec@(1,5) (92.2%, 99.3%)	
07/04 01:27:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][550/781]	Step 66238	lr 0.00248	Loss 0.3735 (0.4579)	Prec@(1,5) (92.2%, 99.3%)	
07/04 01:27:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][600/781]	Step 66288	lr 0.00248	Loss 0.3870 (0.4598)	Prec@(1,5) (92.1%, 99.3%)	
07/04 01:28:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][650/781]	Step 66338	lr 0.00248	Loss 0.6736 (0.4600)	Prec@(1,5) (92.1%, 99.2%)	
07/04 01:28:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][700/781]	Step 66388	lr 0.00248	Loss 0.4186 (0.4605)	Prec@(1,5) (92.1%, 99.2%)	
07/04 01:28:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][750/781]	Step 66438	lr 0.00248	Loss 0.5934 (0.4595)	Prec@(1,5) (92.1%, 99.2%)	
07/04 01:28:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][781/781]	Step 66469	lr 0.00248	Loss 0.2569 (0.4593)	Prec@(1,5) (92.1%, 99.3%)	
07/04 01:28:25午前 evaluateCell_trainer.py:172 [INFO] Train: [ 84/99] Final Prec@1 92.0940%
07/04 01:28:27午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][50/391]	Step 66470	Loss 0.2098	Prec@(1,5) (93.9%, 99.2%)
07/04 01:28:29午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][100/391]	Step 66470	Loss 0.2200	Prec@(1,5) (93.5%, 99.2%)
07/04 01:28:31午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][150/391]	Step 66470	Loss 0.2176	Prec@(1,5) (93.6%, 99.3%)
07/04 01:28:33午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][200/391]	Step 66470	Loss 0.2155	Prec@(1,5) (93.7%, 99.3%)
07/04 01:28:35午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][250/391]	Step 66470	Loss 0.2139	Prec@(1,5) (93.7%, 99.3%)
07/04 01:28:37午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][300/391]	Step 66470	Loss 0.2141	Prec@(1,5) (93.7%, 99.3%)
07/04 01:28:39午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][350/391]	Step 66470	Loss 0.2135	Prec@(1,5) (93.7%, 99.4%)
07/04 01:28:41午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][390/391]	Step 66470	Loss 0.2111	Prec@(1,5) (93.7%, 99.4%)
07/04 01:28:41午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 84/99] Final Prec@1 93.7160%
07/04 01:28:41午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 93.7160%
07/04 01:28:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][50/781]	Step 66520	lr 0.00231	Loss 0.5179 (0.4172)	Prec@(1,5) (93.1%, 99.4%)	
07/04 01:28:56午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][100/781]	Step 66570	lr 0.00231	Loss 0.3216 (0.4171)	Prec@(1,5) (92.9%, 99.5%)	
07/04 01:29:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][150/781]	Step 66620	lr 0.00231	Loss 0.4513 (0.4313)	Prec@(1,5) (92.7%, 99.3%)	
07/04 01:29:11午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][200/781]	Step 66670	lr 0.00231	Loss 0.5842 (0.4321)	Prec@(1,5) (92.6%, 99.3%)	
07/04 01:29:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][250/781]	Step 66720	lr 0.00231	Loss 0.4358 (0.4342)	Prec@(1,5) (92.6%, 99.2%)	
07/04 01:29:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][300/781]	Step 66770	lr 0.00231	Loss 0.2893 (0.4413)	Prec@(1,5) (92.4%, 99.2%)	
07/04 01:29:34午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][350/781]	Step 66820	lr 0.00231	Loss 0.5466 (0.4414)	Prec@(1,5) (92.3%, 99.3%)	
07/04 01:29:41午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][400/781]	Step 66870	lr 0.00231	Loss 0.4879 (0.4430)	Prec@(1,5) (92.4%, 99.3%)	
07/04 01:29:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][450/781]	Step 66920	lr 0.00231	Loss 0.4388 (0.4463)	Prec@(1,5) (92.3%, 99.3%)	
07/04 01:29:56午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][500/781]	Step 66970	lr 0.00231	Loss 0.3864 (0.4452)	Prec@(1,5) (92.3%, 99.3%)	
07/04 01:30:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][550/781]	Step 67020	lr 0.00231	Loss 0.4902 (0.4484)	Prec@(1,5) (92.3%, 99.3%)	
07/04 01:30:11午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][600/781]	Step 67070	lr 0.00231	Loss 0.2931 (0.4480)	Prec@(1,5) (92.2%, 99.3%)	
07/04 01:30:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][650/781]	Step 67120	lr 0.00231	Loss 0.4729 (0.4479)	Prec@(1,5) (92.3%, 99.3%)	
07/04 01:30:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][700/781]	Step 67170	lr 0.00231	Loss 0.3318 (0.4489)	Prec@(1,5) (92.3%, 99.2%)	
07/04 01:30:34午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][750/781]	Step 67220	lr 0.00231	Loss 0.5973 (0.4495)	Prec@(1,5) (92.3%, 99.2%)	
07/04 01:30:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][781/781]	Step 67251	lr 0.00231	Loss 0.4361 (0.4507)	Prec@(1,5) (92.3%, 99.2%)	
07/04 01:30:39午前 evaluateCell_trainer.py:172 [INFO] Train: [ 85/99] Final Prec@1 92.3000%
07/04 01:30:41午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][50/391]	Step 67252	Loss 0.2136	Prec@(1,5) (93.5%, 99.4%)
07/04 01:30:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][100/391]	Step 67252	Loss 0.2004	Prec@(1,5) (93.9%, 99.5%)
07/04 01:30:45午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][150/391]	Step 67252	Loss 0.2004	Prec@(1,5) (94.0%, 99.5%)
07/04 01:30:47午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][200/391]	Step 67252	Loss 0.1950	Prec@(1,5) (94.3%, 99.5%)
07/04 01:30:49午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][250/391]	Step 67252	Loss 0.1987	Prec@(1,5) (94.1%, 99.5%)
07/04 01:30:51午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][300/391]	Step 67252	Loss 0.1966	Prec@(1,5) (94.2%, 99.5%)
07/04 01:30:53午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][350/391]	Step 67252	Loss 0.1990	Prec@(1,5) (94.1%, 99.5%)
07/04 01:30:54午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][390/391]	Step 67252	Loss 0.2009	Prec@(1,5) (94.1%, 99.5%)
07/04 01:30:54午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 85/99] Final Prec@1 94.1280%
07/04 01:30:55午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 94.1280%
07/04 01:31:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][50/781]	Step 67302	lr 0.00214	Loss 0.3608 (0.4169)	Prec@(1,5) (93.1%, 99.5%)	
07/04 01:31:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][100/781]	Step 67352	lr 0.00214	Loss 0.4342 (0.4213)	Prec@(1,5) (93.0%, 99.4%)	
07/04 01:31:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][150/781]	Step 67402	lr 0.00214	Loss 0.4604 (0.4264)	Prec@(1,5) (92.8%, 99.4%)	
07/04 01:31:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][200/781]	Step 67452	lr 0.00214	Loss 0.5383 (0.4217)	Prec@(1,5) (92.8%, 99.4%)	
07/04 01:31:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][250/781]	Step 67502	lr 0.00214	Loss 0.4716 (0.4212)	Prec@(1,5) (92.9%, 99.4%)	
07/04 01:31:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][300/781]	Step 67552	lr 0.00214	Loss 0.5022 (0.4259)	Prec@(1,5) (92.8%, 99.3%)	
07/04 01:31:48午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][350/781]	Step 67602	lr 0.00214	Loss 0.5061 (0.4292)	Prec@(1,5) (92.8%, 99.3%)	
07/04 01:31:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][400/781]	Step 67652	lr 0.00214	Loss 0.6914 (0.4303)	Prec@(1,5) (92.7%, 99.3%)	
07/04 01:32:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][450/781]	Step 67702	lr 0.00214	Loss 0.4702 (0.4276)	Prec@(1,5) (92.9%, 99.3%)	
07/04 01:32:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][500/781]	Step 67752	lr 0.00214	Loss 0.4802 (0.4301)	Prec@(1,5) (92.8%, 99.3%)	
07/04 01:32:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][550/781]	Step 67802	lr 0.00214	Loss 0.6006 (0.4297)	Prec@(1,5) (92.8%, 99.3%)	
07/04 01:32:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][600/781]	Step 67852	lr 0.00214	Loss 0.3726 (0.4337)	Prec@(1,5) (92.8%, 99.3%)	
07/04 01:32:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][650/781]	Step 67902	lr 0.00214	Loss 0.5871 (0.4341)	Prec@(1,5) (92.8%, 99.3%)	
07/04 01:32:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][700/781]	Step 67952	lr 0.00214	Loss 0.6491 (0.4360)	Prec@(1,5) (92.7%, 99.3%)	
07/04 01:32:48午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][750/781]	Step 68002	lr 0.00214	Loss 0.3649 (0.4347)	Prec@(1,5) (92.7%, 99.3%)	
07/04 01:32:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][781/781]	Step 68033	lr 0.00214	Loss 0.2839 (0.4348)	Prec@(1,5) (92.7%, 99.3%)	
07/04 01:32:53午前 evaluateCell_trainer.py:172 [INFO] Train: [ 86/99] Final Prec@1 92.6800%
07/04 01:32:55午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][50/391]	Step 68034	Loss 0.1796	Prec@(1,5) (94.6%, 99.7%)
07/04 01:32:57午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][100/391]	Step 68034	Loss 0.1918	Prec@(1,5) (94.2%, 99.6%)
07/04 01:32:59午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][150/391]	Step 68034	Loss 0.1896	Prec@(1,5) (94.2%, 99.6%)
07/04 01:33:01午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][200/391]	Step 68034	Loss 0.1906	Prec@(1,5) (94.2%, 99.6%)
07/04 01:33:03午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][250/391]	Step 68034	Loss 0.1909	Prec@(1,5) (94.3%, 99.6%)
07/04 01:33:05午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][300/391]	Step 68034	Loss 0.1906	Prec@(1,5) (94.2%, 99.5%)
07/04 01:33:07午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][350/391]	Step 68034	Loss 0.1898	Prec@(1,5) (94.2%, 99.5%)
07/04 01:33:08午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][390/391]	Step 68034	Loss 0.1904	Prec@(1,5) (94.2%, 99.5%)
07/04 01:33:08午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 86/99] Final Prec@1 94.2280%
07/04 01:33:09午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 94.2280%
07/04 01:33:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][50/781]	Step 68084	lr 0.00199	Loss 0.2895 (0.4254)	Prec@(1,5) (93.2%, 99.3%)	
07/04 01:33:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][100/781]	Step 68134	lr 0.00199	Loss 0.3512 (0.4203)	Prec@(1,5) (93.2%, 99.2%)	
07/04 01:33:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][150/781]	Step 68184	lr 0.00199	Loss 0.4247 (0.4216)	Prec@(1,5) (93.1%, 99.3%)	
07/04 01:33:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][200/781]	Step 68234	lr 0.00199	Loss 0.3442 (0.4142)	Prec@(1,5) (93.2%, 99.4%)	
07/04 01:33:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][250/781]	Step 68284	lr 0.00199	Loss 0.3238 (0.4139)	Prec@(1,5) (93.3%, 99.3%)	
07/04 01:33:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][300/781]	Step 68334	lr 0.00199	Loss 0.3458 (0.4171)	Prec@(1,5) (93.2%, 99.4%)	
07/04 01:35:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][350/781]	Step 68384	lr 0.00199	Loss 0.5396 (0.4173)	Prec@(1,5) (93.1%, 99.3%)	
07/04 01:35:45午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][400/781]	Step 68434	lr 0.00199	Loss 0.2996 (0.4139)	Prec@(1,5) (93.2%, 99.4%)	
07/04 01:35:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][450/781]	Step 68484	lr 0.00199	Loss 0.2738 (0.4185)	Prec@(1,5) (93.1%, 99.3%)	
07/04 01:36:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][500/781]	Step 68534	lr 0.00199	Loss 0.3766 (0.4198)	Prec@(1,5) (93.1%, 99.3%)	
07/04 01:36:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][550/781]	Step 68584	lr 0.00199	Loss 0.3774 (0.4209)	Prec@(1,5) (93.1%, 99.3%)	
07/04 01:36:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][600/781]	Step 68634	lr 0.00199	Loss 0.5447 (0.4200)	Prec@(1,5) (93.1%, 99.3%)	
07/04 01:36:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][650/781]	Step 68684	lr 0.00199	Loss 0.4945 (0.4195)	Prec@(1,5) (93.1%, 99.3%)	
07/04 01:36:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][700/781]	Step 68734	lr 0.00199	Loss 0.5200 (0.4182)	Prec@(1,5) (93.2%, 99.3%)	
07/04 01:36:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][750/781]	Step 68784	lr 0.00199	Loss 0.3969 (0.4182)	Prec@(1,5) (93.2%, 99.3%)	
07/04 01:36:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][781/781]	Step 68815	lr 0.00199	Loss 0.4247 (0.4181)	Prec@(1,5) (93.2%, 99.3%)	
07/04 01:36:42午前 evaluateCell_trainer.py:172 [INFO] Train: [ 87/99] Final Prec@1 93.1560%
07/04 01:36:44午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][50/391]	Step 68816	Loss 0.1802	Prec@(1,5) (94.9%, 99.5%)
07/04 01:36:46午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][100/391]	Step 68816	Loss 0.1824	Prec@(1,5) (94.7%, 99.6%)
07/04 01:36:48午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][150/391]	Step 68816	Loss 0.1764	Prec@(1,5) (94.9%, 99.6%)
07/04 01:36:50午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][200/391]	Step 68816	Loss 0.1744	Prec@(1,5) (94.9%, 99.6%)
07/04 01:36:52午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][250/391]	Step 68816	Loss 0.1740	Prec@(1,5) (94.9%, 99.6%)
07/04 01:36:54午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][300/391]	Step 68816	Loss 0.1731	Prec@(1,5) (94.9%, 99.6%)
07/04 01:36:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][350/391]	Step 68816	Loss 0.1721	Prec@(1,5) (94.9%, 99.6%)
07/04 01:36:57午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][390/391]	Step 68816	Loss 0.1709	Prec@(1,5) (94.9%, 99.6%)
07/04 01:36:57午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 87/99] Final Prec@1 94.9400%
07/04 01:36:58午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 94.9400%
07/04 01:37:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][50/781]	Step 68866	lr 0.00184	Loss 0.4236 (0.3787)	Prec@(1,5) (94.0%, 99.5%)	
07/04 01:37:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][100/781]	Step 68916	lr 0.00184	Loss 0.3217 (0.3838)	Prec@(1,5) (94.1%, 99.4%)	
07/04 01:37:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][150/781]	Step 68966	lr 0.00184	Loss 0.2682 (0.3925)	Prec@(1,5) (94.0%, 99.4%)	
07/04 01:37:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][200/781]	Step 69016	lr 0.00184	Loss 0.4722 (0.3945)	Prec@(1,5) (93.9%, 99.4%)	
07/04 01:37:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][250/781]	Step 69066	lr 0.00184	Loss 0.3675 (0.3955)	Prec@(1,5) (93.7%, 99.4%)	
07/04 01:37:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][300/781]	Step 69116	lr 0.00184	Loss 0.4163 (0.3986)	Prec@(1,5) (93.7%, 99.4%)	
07/04 01:37:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][350/781]	Step 69166	lr 0.00184	Loss 0.5325 (0.3970)	Prec@(1,5) (93.6%, 99.4%)	
07/04 01:37:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][400/781]	Step 69216	lr 0.00184	Loss 0.2878 (0.4000)	Prec@(1,5) (93.5%, 99.4%)	
07/04 01:38:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][450/781]	Step 69266	lr 0.00184	Loss 0.3483 (0.3985)	Prec@(1,5) (93.6%, 99.4%)	
07/04 01:38:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][500/781]	Step 69316	lr 0.00184	Loss 0.6898 (0.4004)	Prec@(1,5) (93.5%, 99.4%)	
07/04 01:38:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][550/781]	Step 69366	lr 0.00184	Loss 0.5340 (0.4001)	Prec@(1,5) (93.5%, 99.4%)	
07/04 01:38:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][600/781]	Step 69416	lr 0.00184	Loss 0.2755 (0.4005)	Prec@(1,5) (93.5%, 99.4%)	
07/04 01:38:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][650/781]	Step 69466	lr 0.00184	Loss 0.4146 (0.4028)	Prec@(1,5) (93.5%, 99.3%)	
07/04 01:38:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][700/781]	Step 69516	lr 0.00184	Loss 0.2035 (0.4014)	Prec@(1,5) (93.5%, 99.3%)	
07/04 01:38:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][750/781]	Step 69566	lr 0.00184	Loss 0.5999 (0.4035)	Prec@(1,5) (93.4%, 99.3%)	
07/04 01:38:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][781/781]	Step 69597	lr 0.00184	Loss 0.3813 (0.4029)	Prec@(1,5) (93.4%, 99.4%)	
07/04 01:38:54午前 evaluateCell_trainer.py:172 [INFO] Train: [ 88/99] Final Prec@1 93.4240%
07/04 01:38:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][50/391]	Step 69598	Loss 0.1856	Prec@(1,5) (94.2%, 99.4%)
07/04 01:38:58午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][100/391]	Step 69598	Loss 0.1850	Prec@(1,5) (94.4%, 99.4%)
07/04 01:39:00午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][150/391]	Step 69598	Loss 0.1822	Prec@(1,5) (94.6%, 99.5%)
07/04 01:39:02午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][200/391]	Step 69598	Loss 0.1775	Prec@(1,5) (94.7%, 99.5%)
07/04 01:39:04午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][250/391]	Step 69598	Loss 0.1768	Prec@(1,5) (94.8%, 99.5%)
07/04 01:39:06午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][300/391]	Step 69598	Loss 0.1789	Prec@(1,5) (94.9%, 99.5%)
07/04 01:39:08午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][350/391]	Step 69598	Loss 0.1766	Prec@(1,5) (94.9%, 99.5%)
07/04 01:39:10午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][390/391]	Step 69598	Loss 0.1767	Prec@(1,5) (94.9%, 99.5%)
07/04 01:39:10午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 88/99] Final Prec@1 94.9200%
07/04 01:39:10午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 94.9400%
07/04 01:39:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][50/781]	Step 69648	lr 0.00171	Loss 0.3435 (0.3786)	Prec@(1,5) (94.1%, 99.5%)	
07/04 01:39:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][100/781]	Step 69698	lr 0.00171	Loss 0.5750 (0.3778)	Prec@(1,5) (94.0%, 99.5%)	
07/04 01:39:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][150/781]	Step 69748	lr 0.00171	Loss 0.5804 (0.3773)	Prec@(1,5) (94.0%, 99.5%)	
07/04 01:39:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][200/781]	Step 69798	lr 0.00171	Loss 0.4075 (0.3816)	Prec@(1,5) (93.9%, 99.5%)	
07/04 01:39:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][250/781]	Step 69848	lr 0.00171	Loss 0.4001 (0.3809)	Prec@(1,5) (94.0%, 99.5%)	
07/04 01:39:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][300/781]	Step 69898	lr 0.00171	Loss 0.2919 (0.3827)	Prec@(1,5) (94.0%, 99.5%)	
07/04 01:40:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][350/781]	Step 69948	lr 0.00171	Loss 0.5316 (0.3868)	Prec@(1,5) (93.9%, 99.5%)	
07/04 01:40:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][400/781]	Step 69998	lr 0.00171	Loss 0.2671 (0.3832)	Prec@(1,5) (94.0%, 99.5%)	
07/04 01:40:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][450/781]	Step 70048	lr 0.00171	Loss 0.3081 (0.3850)	Prec@(1,5) (93.9%, 99.5%)	
07/04 01:40:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][500/781]	Step 70098	lr 0.00171	Loss 0.2813 (0.3886)	Prec@(1,5) (93.8%, 99.5%)	
07/04 01:40:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][550/781]	Step 70148	lr 0.00171	Loss 0.4182 (0.3903)	Prec@(1,5) (93.8%, 99.4%)	
07/04 01:40:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][600/781]	Step 70198	lr 0.00171	Loss 0.3922 (0.3920)	Prec@(1,5) (93.7%, 99.4%)	
07/04 01:40:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][650/781]	Step 70248	lr 0.00171	Loss 0.4687 (0.3907)	Prec@(1,5) (93.8%, 99.4%)	
07/04 01:40:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][700/781]	Step 70298	lr 0.00171	Loss 0.4542 (0.3914)	Prec@(1,5) (93.8%, 99.4%)	
07/04 01:41:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][750/781]	Step 70348	lr 0.00171	Loss 0.1982 (0.3907)	Prec@(1,5) (93.8%, 99.4%)	
07/04 01:41:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][781/781]	Step 70379	lr 0.00171	Loss 0.4833 (0.3906)	Prec@(1,5) (93.8%, 99.4%)	
07/04 01:41:07午前 evaluateCell_trainer.py:172 [INFO] Train: [ 89/99] Final Prec@1 93.7640%
07/04 01:41:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][50/391]	Step 70380	Loss 0.1745	Prec@(1,5) (95.0%, 99.6%)
07/04 01:41:11午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][100/391]	Step 70380	Loss 0.1688	Prec@(1,5) (95.2%, 99.6%)
07/04 01:41:13午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][150/391]	Step 70380	Loss 0.1681	Prec@(1,5) (95.2%, 99.6%)
07/04 01:41:15午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][200/391]	Step 70380	Loss 0.1698	Prec@(1,5) (95.2%, 99.6%)
07/04 01:41:17午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][250/391]	Step 70380	Loss 0.1701	Prec@(1,5) (95.2%, 99.6%)
07/04 01:41:19午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][300/391]	Step 70380	Loss 0.1692	Prec@(1,5) (95.2%, 99.5%)
07/04 01:41:21午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][350/391]	Step 70380	Loss 0.1674	Prec@(1,5) (95.2%, 99.6%)
07/04 01:41:22午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][390/391]	Step 70380	Loss 0.1672	Prec@(1,5) (95.3%, 99.6%)
07/04 01:41:23午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 89/99] Final Prec@1 95.2640%
07/04 01:41:23午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 95.2640%
07/04 01:41:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][50/781]	Step 70430	lr 0.00159	Loss 0.3448 (0.3425)	Prec@(1,5) (95.0%, 99.7%)	
07/04 01:41:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][100/781]	Step 70480	lr 0.00159	Loss 0.3562 (0.3655)	Prec@(1,5) (94.5%, 99.5%)	
07/04 01:41:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][150/781]	Step 70530	lr 0.00159	Loss 0.3408 (0.3796)	Prec@(1,5) (94.3%, 99.4%)	
07/04 01:41:53午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][200/781]	Step 70580	lr 0.00159	Loss 0.1365 (0.3747)	Prec@(1,5) (94.4%, 99.5%)	
07/04 01:42:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][250/781]	Step 70630	lr 0.00159	Loss 0.3309 (0.3775)	Prec@(1,5) (94.2%, 99.4%)	
07/04 01:42:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][300/781]	Step 70680	lr 0.00159	Loss 0.4440 (0.3726)	Prec@(1,5) (94.3%, 99.5%)	
07/04 01:42:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][350/781]	Step 70730	lr 0.00159	Loss 0.3249 (0.3712)	Prec@(1,5) (94.3%, 99.5%)	
07/04 01:42:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][400/781]	Step 70780	lr 0.00159	Loss 0.1950 (0.3720)	Prec@(1,5) (94.3%, 99.5%)	
07/04 01:42:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][450/781]	Step 70830	lr 0.00159	Loss 0.3305 (0.3719)	Prec@(1,5) (94.2%, 99.5%)	
07/04 01:42:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][500/781]	Step 70880	lr 0.00159	Loss 0.2748 (0.3702)	Prec@(1,5) (94.2%, 99.5%)	
07/04 01:42:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][550/781]	Step 70930	lr 0.00159	Loss 0.3299 (0.3687)	Prec@(1,5) (94.2%, 99.5%)	
07/04 01:42:53午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][600/781]	Step 70980	lr 0.00159	Loss 0.3207 (0.3702)	Prec@(1,5) (94.2%, 99.5%)	
07/04 01:43:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][650/781]	Step 71030	lr 0.00159	Loss 0.1708 (0.3706)	Prec@(1,5) (94.2%, 99.5%)	
07/04 01:43:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][700/781]	Step 71080	lr 0.00159	Loss 0.3454 (0.3698)	Prec@(1,5) (94.2%, 99.5%)	
07/04 01:43:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][750/781]	Step 71130	lr 0.00159	Loss 0.4775 (0.3723)	Prec@(1,5) (94.1%, 99.5%)	
07/04 01:43:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][781/781]	Step 71161	lr 0.00159	Loss 0.5449 (0.3734)	Prec@(1,5) (94.1%, 99.5%)	
07/04 01:43:21午前 evaluateCell_trainer.py:172 [INFO] Train: [ 90/99] Final Prec@1 94.1220%
07/04 01:43:23午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][50/391]	Step 71162	Loss 0.1503	Prec@(1,5) (95.6%, 99.6%)
07/04 01:43:25午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][100/391]	Step 71162	Loss 0.1513	Prec@(1,5) (95.4%, 99.6%)
07/04 01:43:27午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][150/391]	Step 71162	Loss 0.1542	Prec@(1,5) (95.5%, 99.6%)
07/04 01:43:29午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][200/391]	Step 71162	Loss 0.1551	Prec@(1,5) (95.5%, 99.6%)
07/04 01:43:31午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][250/391]	Step 71162	Loss 0.1542	Prec@(1,5) (95.5%, 99.6%)
07/04 01:43:33午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][300/391]	Step 71162	Loss 0.1519	Prec@(1,5) (95.6%, 99.6%)
07/04 01:43:34午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][350/391]	Step 71162	Loss 0.1512	Prec@(1,5) (95.7%, 99.6%)
07/04 01:43:36午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][390/391]	Step 71162	Loss 0.1526	Prec@(1,5) (95.6%, 99.6%)
07/04 01:43:36午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 90/99] Final Prec@1 95.5720%
07/04 01:43:36午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 95.5720%
07/04 01:43:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][50/781]	Step 71212	lr 0.00148	Loss 0.4434 (0.3436)	Prec@(1,5) (94.7%, 99.6%)	
07/04 01:43:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][100/781]	Step 71262	lr 0.00148	Loss 0.7112 (0.3417)	Prec@(1,5) (94.8%, 99.5%)	
07/04 01:43:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][150/781]	Step 71312	lr 0.00148	Loss 0.2428 (0.3426)	Prec@(1,5) (94.7%, 99.5%)	
07/04 01:44:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][200/781]	Step 71362	lr 0.00148	Loss 0.3074 (0.3481)	Prec@(1,5) (94.6%, 99.6%)	
07/04 01:44:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][250/781]	Step 71412	lr 0.00148	Loss 0.3138 (0.3530)	Prec@(1,5) (94.5%, 99.6%)	
07/04 01:44:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][300/781]	Step 71462	lr 0.00148	Loss 0.3899 (0.3571)	Prec@(1,5) (94.5%, 99.6%)	
07/04 01:44:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][350/781]	Step 71512	lr 0.00148	Loss 0.4049 (0.3587)	Prec@(1,5) (94.4%, 99.6%)	
07/04 01:44:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][400/781]	Step 71562	lr 0.00148	Loss 0.2525 (0.3608)	Prec@(1,5) (94.4%, 99.6%)	
07/04 01:44:45午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][450/781]	Step 71612	lr 0.00148	Loss 0.3208 (0.3622)	Prec@(1,5) (94.4%, 99.6%)	
07/04 01:44:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][500/781]	Step 71662	lr 0.00148	Loss 0.4471 (0.3645)	Prec@(1,5) (94.3%, 99.5%)	
07/04 01:45:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][550/781]	Step 71712	lr 0.00148	Loss 0.4306 (0.3637)	Prec@(1,5) (94.4%, 99.5%)	
07/04 01:45:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][600/781]	Step 71762	lr 0.00148	Loss 0.3093 (0.3630)	Prec@(1,5) (94.4%, 99.5%)	
07/04 01:45:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][650/781]	Step 71812	lr 0.00148	Loss 0.3082 (0.3640)	Prec@(1,5) (94.3%, 99.6%)	
07/04 01:45:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][700/781]	Step 71862	lr 0.00148	Loss 0.4014 (0.3636)	Prec@(1,5) (94.3%, 99.6%)	
07/04 01:45:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][750/781]	Step 71912	lr 0.00148	Loss 0.3167 (0.3638)	Prec@(1,5) (94.3%, 99.6%)	
07/04 01:45:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][781/781]	Step 71943	lr 0.00148	Loss 0.3339 (0.3643)	Prec@(1,5) (94.3%, 99.5%)	
07/04 01:45:35午前 evaluateCell_trainer.py:172 [INFO] Train: [ 91/99] Final Prec@1 94.3080%
07/04 01:45:37午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][50/391]	Step 71944	Loss 0.1356	Prec@(1,5) (95.8%, 99.7%)
07/04 01:45:39午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][100/391]	Step 71944	Loss 0.1439	Prec@(1,5) (95.5%, 99.7%)
07/04 01:45:41午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][150/391]	Step 71944	Loss 0.1454	Prec@(1,5) (95.5%, 99.7%)
07/04 01:45:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][200/391]	Step 71944	Loss 0.1463	Prec@(1,5) (95.6%, 99.7%)
07/04 01:45:45午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][250/391]	Step 71944	Loss 0.1464	Prec@(1,5) (95.6%, 99.7%)
07/04 01:45:47午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][300/391]	Step 71944	Loss 0.1502	Prec@(1,5) (95.5%, 99.7%)
07/04 01:45:49午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][350/391]	Step 71944	Loss 0.1498	Prec@(1,5) (95.6%, 99.7%)
07/04 01:45:51午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][390/391]	Step 71944	Loss 0.1506	Prec@(1,5) (95.5%, 99.7%)
07/04 01:45:51午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 91/99] Final Prec@1 95.5160%
07/04 01:45:51午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 95.5720%
07/04 01:45:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][50/781]	Step 71994	lr 0.00138	Loss 0.4848 (0.3749)	Prec@(1,5) (94.0%, 99.5%)	
07/04 01:46:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][100/781]	Step 72044	lr 0.00138	Loss 0.5975 (0.3631)	Prec@(1,5) (94.3%, 99.5%)	
07/04 01:46:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][150/781]	Step 72094	lr 0.00138	Loss 0.5192 (0.3625)	Prec@(1,5) (94.4%, 99.4%)	
07/04 01:46:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][200/781]	Step 72144	lr 0.00138	Loss 0.4245 (0.3586)	Prec@(1,5) (94.6%, 99.4%)	
07/04 01:46:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][250/781]	Step 72194	lr 0.00138	Loss 0.4246 (0.3558)	Prec@(1,5) (94.6%, 99.4%)	
07/04 01:46:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][300/781]	Step 72244	lr 0.00138	Loss 0.2575 (0.3575)	Prec@(1,5) (94.5%, 99.4%)	
07/04 01:46:45午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][350/781]	Step 72294	lr 0.00138	Loss 0.2747 (0.3585)	Prec@(1,5) (94.4%, 99.4%)	
07/04 01:46:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][400/781]	Step 72344	lr 0.00138	Loss 0.3249 (0.3581)	Prec@(1,5) (94.4%, 99.4%)	
07/04 01:47:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][450/781]	Step 72394	lr 0.00138	Loss 0.2800 (0.3568)	Prec@(1,5) (94.5%, 99.4%)	
07/04 01:47:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][500/781]	Step 72444	lr 0.00138	Loss 0.2648 (0.3580)	Prec@(1,5) (94.4%, 99.4%)	
07/04 01:47:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][550/781]	Step 72494	lr 0.00138	Loss 0.3218 (0.3580)	Prec@(1,5) (94.4%, 99.4%)	
07/04 01:47:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][600/781]	Step 72544	lr 0.00138	Loss 0.3642 (0.3586)	Prec@(1,5) (94.4%, 99.4%)	
07/04 01:47:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][650/781]	Step 72594	lr 0.00138	Loss 0.2129 (0.3607)	Prec@(1,5) (94.4%, 99.4%)	
07/04 01:47:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][700/781]	Step 72644	lr 0.00138	Loss 0.2510 (0.3625)	Prec@(1,5) (94.4%, 99.4%)	
07/04 01:47:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][750/781]	Step 72694	lr 0.00138	Loss 0.3268 (0.3619)	Prec@(1,5) (94.4%, 99.4%)	
07/04 01:47:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][781/781]	Step 72725	lr 0.00138	Loss 0.4897 (0.3601)	Prec@(1,5) (94.4%, 99.4%)	
07/04 01:47:50午前 evaluateCell_trainer.py:172 [INFO] Train: [ 92/99] Final Prec@1 94.4020%
07/04 01:47:52午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][50/391]	Step 72726	Loss 0.1445	Prec@(1,5) (95.9%, 99.5%)
07/04 01:47:54午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][100/391]	Step 72726	Loss 0.1382	Prec@(1,5) (96.0%, 99.6%)
07/04 01:47:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][150/391]	Step 72726	Loss 0.1391	Prec@(1,5) (95.9%, 99.6%)
07/04 01:47:58午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][200/391]	Step 72726	Loss 0.1402	Prec@(1,5) (95.8%, 99.6%)
07/04 01:48:00午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][250/391]	Step 72726	Loss 0.1425	Prec@(1,5) (95.8%, 99.6%)
07/04 01:48:02午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][300/391]	Step 72726	Loss 0.1429	Prec@(1,5) (95.7%, 99.6%)
07/04 01:48:04午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][350/391]	Step 72726	Loss 0.1438	Prec@(1,5) (95.7%, 99.6%)
07/04 01:48:06午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][390/391]	Step 72726	Loss 0.1438	Prec@(1,5) (95.7%, 99.6%)
07/04 01:48:06午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 92/99] Final Prec@1 95.6800%
07/04 01:48:06午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 95.6800%
07/04 01:48:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][50/781]	Step 72776	lr 0.00129	Loss 0.4344 (0.3465)	Prec@(1,5) (95.3%, 99.5%)	
07/04 01:48:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][100/781]	Step 72826	lr 0.00129	Loss 0.3911 (0.3387)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:48:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][150/781]	Step 72876	lr 0.00129	Loss 0.5330 (0.3440)	Prec@(1,5) (94.9%, 99.6%)	
07/04 01:48:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][200/781]	Step 72926	lr 0.00129	Loss 0.2892 (0.3402)	Prec@(1,5) (94.9%, 99.6%)	
07/04 01:48:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][250/781]	Step 72976	lr 0.00129	Loss 0.3422 (0.3440)	Prec@(1,5) (94.8%, 99.5%)	
07/04 01:48:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][300/781]	Step 73026	lr 0.00129	Loss 0.3116 (0.3432)	Prec@(1,5) (94.8%, 99.6%)	
07/04 01:48:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][350/781]	Step 73076	lr 0.00129	Loss 0.4461 (0.3458)	Prec@(1,5) (94.8%, 99.6%)	
07/04 01:49:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][400/781]	Step 73126	lr 0.00129	Loss 0.3709 (0.3461)	Prec@(1,5) (94.7%, 99.5%)	
07/04 01:49:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][450/781]	Step 73176	lr 0.00129	Loss 0.3074 (0.3467)	Prec@(1,5) (94.8%, 99.6%)	
07/04 01:49:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][500/781]	Step 73226	lr 0.00129	Loss 0.4007 (0.3445)	Prec@(1,5) (94.8%, 99.6%)	
07/04 01:49:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][550/781]	Step 73276	lr 0.00129	Loss 0.3848 (0.3459)	Prec@(1,5) (94.8%, 99.6%)	
07/04 01:49:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][600/781]	Step 73326	lr 0.00129	Loss 0.2805 (0.3466)	Prec@(1,5) (94.8%, 99.5%)	
07/04 01:49:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][650/781]	Step 73376	lr 0.00129	Loss 0.2269 (0.3458)	Prec@(1,5) (94.8%, 99.5%)	
07/04 01:49:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][700/781]	Step 73426	lr 0.00129	Loss 0.3361 (0.3457)	Prec@(1,5) (94.8%, 99.5%)	
07/04 01:49:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][750/781]	Step 73476	lr 0.00129	Loss 0.1396 (0.3463)	Prec@(1,5) (94.8%, 99.5%)	
07/04 01:50:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][781/781]	Step 73507	lr 0.00129	Loss 0.4453 (0.3459)	Prec@(1,5) (94.8%, 99.5%)	
07/04 01:50:05午前 evaluateCell_trainer.py:172 [INFO] Train: [ 93/99] Final Prec@1 94.8080%
07/04 01:50:07午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][50/391]	Step 73508	Loss 0.1389	Prec@(1,5) (96.2%, 99.6%)
07/04 01:50:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][100/391]	Step 73508	Loss 0.1433	Prec@(1,5) (96.0%, 99.6%)
07/04 01:50:10午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][150/391]	Step 73508	Loss 0.1361	Prec@(1,5) (96.3%, 99.6%)
07/04 01:50:12午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][200/391]	Step 73508	Loss 0.1365	Prec@(1,5) (96.3%, 99.6%)
07/04 01:50:14午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][250/391]	Step 73508	Loss 0.1350	Prec@(1,5) (96.3%, 99.7%)
07/04 01:50:16午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][300/391]	Step 73508	Loss 0.1328	Prec@(1,5) (96.3%, 99.7%)
07/04 01:50:18午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][350/391]	Step 73508	Loss 0.1325	Prec@(1,5) (96.3%, 99.7%)
07/04 01:50:20午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][390/391]	Step 73508	Loss 0.1329	Prec@(1,5) (96.3%, 99.7%)
07/04 01:50:20午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 93/99] Final Prec@1 96.3240%
07/04 01:50:20午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.3240%
07/04 01:50:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][50/781]	Step 73558	lr 0.00121	Loss 0.2741 (0.3154)	Prec@(1,5) (96.1%, 99.4%)	
07/04 01:50:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][100/781]	Step 73608	lr 0.00121	Loss 0.2897 (0.3217)	Prec@(1,5) (95.6%, 99.5%)	
07/04 01:50:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][150/781]	Step 73658	lr 0.00121	Loss 0.3121 (0.3233)	Prec@(1,5) (95.6%, 99.5%)	
07/04 01:50:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][200/781]	Step 73708	lr 0.00121	Loss 0.3710 (0.3195)	Prec@(1,5) (95.6%, 99.5%)	
07/04 01:50:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][250/781]	Step 73758	lr 0.00121	Loss 0.3094 (0.3214)	Prec@(1,5) (95.4%, 99.5%)	
07/04 01:51:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][300/781]	Step 73808	lr 0.00121	Loss 0.2517 (0.3277)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:51:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][350/781]	Step 73858	lr 0.00121	Loss 0.2831 (0.3333)	Prec@(1,5) (95.2%, 99.5%)	
07/04 01:51:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][400/781]	Step 73908	lr 0.00121	Loss 0.2801 (0.3341)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:51:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][450/781]	Step 73958	lr 0.00121	Loss 0.3536 (0.3352)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:51:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][500/781]	Step 74008	lr 0.00121	Loss 0.2602 (0.3356)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:51:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][550/781]	Step 74058	lr 0.00121	Loss 0.4061 (0.3368)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:51:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][600/781]	Step 74108	lr 0.00121	Loss 0.2899 (0.3362)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:51:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][650/781]	Step 74158	lr 0.00121	Loss 0.3745 (0.3356)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:52:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][700/781]	Step 74208	lr 0.00121	Loss 0.3142 (0.3362)	Prec@(1,5) (95.0%, 99.6%)	
07/04 01:52:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][750/781]	Step 74258	lr 0.00121	Loss 0.3260 (0.3374)	Prec@(1,5) (95.0%, 99.6%)	
07/04 01:52:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][781/781]	Step 74289	lr 0.00121	Loss 0.4228 (0.3379)	Prec@(1,5) (95.0%, 99.5%)	
07/04 01:52:19午前 evaluateCell_trainer.py:172 [INFO] Train: [ 94/99] Final Prec@1 94.9940%
07/04 01:52:21午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][50/391]	Step 74290	Loss 0.1366	Prec@(1,5) (96.0%, 99.6%)
07/04 01:52:23午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][100/391]	Step 74290	Loss 0.1372	Prec@(1,5) (96.1%, 99.6%)
07/04 01:52:25午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][150/391]	Step 74290	Loss 0.1362	Prec@(1,5) (96.1%, 99.6%)
07/04 01:52:27午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][200/391]	Step 74290	Loss 0.1345	Prec@(1,5) (96.2%, 99.6%)
07/04 01:52:29午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][250/391]	Step 74290	Loss 0.1339	Prec@(1,5) (96.2%, 99.6%)
07/04 01:52:30午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][300/391]	Step 74290	Loss 0.1358	Prec@(1,5) (96.3%, 99.6%)
07/04 01:52:32午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][350/391]	Step 74290	Loss 0.1357	Prec@(1,5) (96.2%, 99.6%)
07/04 01:52:34午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][390/391]	Step 74290	Loss 0.1350	Prec@(1,5) (96.3%, 99.6%)
07/04 01:52:34午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 94/99] Final Prec@1 96.2840%
07/04 01:52:34午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.3240%
07/04 01:52:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][50/781]	Step 74340	lr 0.00115	Loss 0.4351 (0.3530)	Prec@(1,5) (95.0%, 99.4%)	
07/04 01:52:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][100/781]	Step 74390	lr 0.00115	Loss 0.2819 (0.3419)	Prec@(1,5) (95.2%, 99.5%)	
07/04 01:52:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][150/781]	Step 74440	lr 0.00115	Loss 0.3699 (0.3349)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:53:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][200/781]	Step 74490	lr 0.00115	Loss 0.2485 (0.3354)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:53:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][250/781]	Step 74540	lr 0.00115	Loss 0.2875 (0.3353)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:53:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][300/781]	Step 74590	lr 0.00115	Loss 0.7630 (0.3336)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:53:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][350/781]	Step 74640	lr 0.00115	Loss 0.3089 (0.3345)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:53:34午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][400/781]	Step 74690	lr 0.00115	Loss 0.2368 (0.3321)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:53:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][450/781]	Step 74740	lr 0.00115	Loss 0.2096 (0.3351)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:53:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][500/781]	Step 74790	lr 0.00115	Loss 0.4470 (0.3358)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:53:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][550/781]	Step 74840	lr 0.00115	Loss 0.3662 (0.3365)	Prec@(1,5) (95.0%, 99.5%)	
07/04 01:54:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][600/781]	Step 74890	lr 0.00115	Loss 0.2751 (0.3362)	Prec@(1,5) (95.0%, 99.5%)	
07/04 01:54:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][650/781]	Step 74940	lr 0.00115	Loss 0.2737 (0.3356)	Prec@(1,5) (95.0%, 99.6%)	
07/04 01:54:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][700/781]	Step 74990	lr 0.00115	Loss 0.3812 (0.3356)	Prec@(1,5) (95.0%, 99.6%)	
07/04 01:54:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][750/781]	Step 75040	lr 0.00115	Loss 0.2840 (0.3360)	Prec@(1,5) (95.0%, 99.6%)	
07/04 01:54:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][781/781]	Step 75071	lr 0.00115	Loss 0.3853 (0.3369)	Prec@(1,5) (95.0%, 99.5%)	
07/04 01:54:31午前 evaluateCell_trainer.py:172 [INFO] Train: [ 95/99] Final Prec@1 95.0080%
07/04 01:54:34午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][50/391]	Step 75072	Loss 0.1255	Prec@(1,5) (96.5%, 99.8%)
07/04 01:54:35午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][100/391]	Step 75072	Loss 0.1315	Prec@(1,5) (96.3%, 99.7%)
07/04 01:54:37午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][150/391]	Step 75072	Loss 0.1349	Prec@(1,5) (96.2%, 99.7%)
07/04 01:54:39午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][200/391]	Step 75072	Loss 0.1352	Prec@(1,5) (96.2%, 99.7%)
07/04 01:54:41午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][250/391]	Step 75072	Loss 0.1347	Prec@(1,5) (96.2%, 99.6%)
07/04 01:54:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][300/391]	Step 75072	Loss 0.1333	Prec@(1,5) (96.2%, 99.6%)
07/04 01:54:45午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][350/391]	Step 75072	Loss 0.1341	Prec@(1,5) (96.2%, 99.7%)
07/04 01:54:47午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][390/391]	Step 75072	Loss 0.1338	Prec@(1,5) (96.2%, 99.7%)
07/04 01:54:47午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 95/99] Final Prec@1 96.2200%
07/04 01:54:47午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.3240%
07/04 01:54:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][50/781]	Step 75122	lr 0.00109	Loss 0.4101 (0.3267)	Prec@(1,5) (95.1%, 99.4%)	
07/04 01:55:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][100/781]	Step 75172	lr 0.00109	Loss 0.3101 (0.3177)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:55:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][150/781]	Step 75222	lr 0.00109	Loss 0.3301 (0.3224)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:55:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][200/781]	Step 75272	lr 0.00109	Loss 0.4516 (0.3217)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:55:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][250/781]	Step 75322	lr 0.00109	Loss 0.3399 (0.3218)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:55:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][300/781]	Step 75372	lr 0.00109	Loss 0.5697 (0.3193)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:55:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][350/781]	Step 75422	lr 0.00109	Loss 0.3769 (0.3208)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:55:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][400/781]	Step 75472	lr 0.00109	Loss 0.5062 (0.3212)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:55:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][450/781]	Step 75522	lr 0.00109	Loss 0.2872 (0.3220)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:56:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][500/781]	Step 75572	lr 0.00109	Loss 0.3344 (0.3240)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:56:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][550/781]	Step 75622	lr 0.00109	Loss 0.5032 (0.3256)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:56:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][600/781]	Step 75672	lr 0.00109	Loss 0.3663 (0.3251)	Prec@(1,5) (95.2%, 99.5%)	
07/04 01:56:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][650/781]	Step 75722	lr 0.00109	Loss 0.4761 (0.3256)	Prec@(1,5) (95.2%, 99.5%)	
07/04 01:56:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][700/781]	Step 75772	lr 0.00109	Loss 0.4274 (0.3265)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:56:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][750/781]	Step 75822	lr 0.00109	Loss 0.3672 (0.3267)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:56:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][781/781]	Step 75853	lr 0.00109	Loss 0.2758 (0.3274)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:56:44午前 evaluateCell_trainer.py:172 [INFO] Train: [ 96/99] Final Prec@1 95.1680%
07/04 01:56:46午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][50/391]	Step 75854	Loss 0.1182	Prec@(1,5) (96.4%, 99.8%)
07/04 01:56:48午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][100/391]	Step 75854	Loss 0.1195	Prec@(1,5) (96.5%, 99.7%)
07/04 01:56:50午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][150/391]	Step 75854	Loss 0.1169	Prec@(1,5) (96.6%, 99.7%)
07/04 01:56:52午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][200/391]	Step 75854	Loss 0.1182	Prec@(1,5) (96.6%, 99.7%)
07/04 01:56:54午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][250/391]	Step 75854	Loss 0.1197	Prec@(1,5) (96.6%, 99.7%)
07/04 01:56:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][300/391]	Step 75854	Loss 0.1205	Prec@(1,5) (96.6%, 99.7%)
07/04 01:56:58午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][350/391]	Step 75854	Loss 0.1231	Prec@(1,5) (96.5%, 99.7%)
07/04 01:56:59午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][390/391]	Step 75854	Loss 0.1234	Prec@(1,5) (96.5%, 99.7%)
07/04 01:56:59午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 96/99] Final Prec@1 96.5120%
07/04 01:56:59午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.5120%
07/04 01:57:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][50/781]	Step 75904	lr 0.00105	Loss 0.1815 (0.3018)	Prec@(1,5) (95.5%, 99.5%)	
07/04 01:57:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][100/781]	Step 75954	lr 0.00105	Loss 0.4733 (0.3173)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:57:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][150/781]	Step 76004	lr 0.00105	Loss 0.2389 (0.3126)	Prec@(1,5) (95.4%, 99.6%)	
07/04 01:57:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][200/781]	Step 76054	lr 0.00105	Loss 0.3635 (0.3176)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:57:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][250/781]	Step 76104	lr 0.00105	Loss 0.3009 (0.3232)	Prec@(1,5) (95.2%, 99.5%)	
07/04 01:57:45午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][300/781]	Step 76154	lr 0.00105	Loss 0.2673 (0.3292)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:57:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][350/781]	Step 76204	lr 0.00105	Loss 0.2869 (0.3300)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:58:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][400/781]	Step 76254	lr 0.00105	Loss 0.3567 (0.3292)	Prec@(1,5) (95.2%, 99.5%)	
07/04 01:58:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][450/781]	Step 76304	lr 0.00105	Loss 0.3542 (0.3264)	Prec@(1,5) (95.2%, 99.5%)	
07/04 01:58:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][500/781]	Step 76354	lr 0.00105	Loss 0.3013 (0.3278)	Prec@(1,5) (95.2%, 99.5%)	
07/04 01:58:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][550/781]	Step 76404	lr 0.00105	Loss 0.3600 (0.3276)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:58:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][600/781]	Step 76454	lr 0.00105	Loss 0.2450 (0.3288)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:58:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][650/781]	Step 76504	lr 0.00105	Loss 0.2236 (0.3282)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:58:45午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][700/781]	Step 76554	lr 0.00105	Loss 0.2576 (0.3274)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:58:53午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][750/781]	Step 76604	lr 0.00105	Loss 0.4764 (0.3275)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:58:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][781/781]	Step 76635	lr 0.00105	Loss 0.3056 (0.3265)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:58:58午前 evaluateCell_trainer.py:172 [INFO] Train: [ 97/99] Final Prec@1 95.1060%
07/04 01:59:00午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][50/391]	Step 76636	Loss 0.1242	Prec@(1,5) (96.7%, 99.7%)
07/04 01:59:02午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][100/391]	Step 76636	Loss 0.1339	Prec@(1,5) (96.4%, 99.7%)
07/04 01:59:04午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][150/391]	Step 76636	Loss 0.1265	Prec@(1,5) (96.5%, 99.7%)
07/04 01:59:06午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][200/391]	Step 76636	Loss 0.1264	Prec@(1,5) (96.5%, 99.7%)
07/04 01:59:08午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][250/391]	Step 76636	Loss 0.1273	Prec@(1,5) (96.5%, 99.7%)
07/04 01:59:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][300/391]	Step 76636	Loss 0.1255	Prec@(1,5) (96.6%, 99.7%)
07/04 01:59:11午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][350/391]	Step 76636	Loss 0.1249	Prec@(1,5) (96.6%, 99.7%)
07/04 01:59:13午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][390/391]	Step 76636	Loss 0.1258	Prec@(1,5) (96.5%, 99.7%)
07/04 01:59:13午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 97/99] Final Prec@1 96.5240%
07/04 01:59:13午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.5240%
07/04 01:59:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][50/781]	Step 76686	lr 0.00102	Loss 0.2408 (0.3217)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:59:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][100/781]	Step 76736	lr 0.00102	Loss 0.2814 (0.3224)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:59:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][150/781]	Step 76786	lr 0.00102	Loss 0.2082 (0.3161)	Prec@(1,5) (95.4%, 99.7%)	
07/04 01:59:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][200/781]	Step 76836	lr 0.00102	Loss 0.2940 (0.3138)	Prec@(1,5) (95.4%, 99.6%)	
07/04 01:59:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][250/781]	Step 76886	lr 0.00102	Loss 0.2794 (0.3085)	Prec@(1,5) (95.5%, 99.7%)	
07/04 01:59:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][300/781]	Step 76936	lr 0.00102	Loss 0.2418 (0.3109)	Prec@(1,5) (95.6%, 99.6%)	
07/04 02:00:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][350/781]	Step 76986	lr 0.00102	Loss 0.1409 (0.3098)	Prec@(1,5) (95.7%, 99.6%)	
07/04 02:00:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][400/781]	Step 77036	lr 0.00102	Loss 0.1930 (0.3115)	Prec@(1,5) (95.7%, 99.6%)	
07/04 02:00:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][450/781]	Step 77086	lr 0.00102	Loss 0.3659 (0.3114)	Prec@(1,5) (95.7%, 99.6%)	
07/04 02:00:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][500/781]	Step 77136	lr 0.00102	Loss 0.3941 (0.3127)	Prec@(1,5) (95.6%, 99.6%)	
07/04 02:00:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][550/781]	Step 77186	lr 0.00102	Loss 0.3829 (0.3138)	Prec@(1,5) (95.6%, 99.6%)	
07/04 02:00:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][600/781]	Step 77236	lr 0.00102	Loss 0.2109 (0.3142)	Prec@(1,5) (95.6%, 99.6%)	
07/04 02:00:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][650/781]	Step 77286	lr 0.00102	Loss 0.3944 (0.3158)	Prec@(1,5) (95.5%, 99.6%)	
07/04 02:00:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][700/781]	Step 77336	lr 0.00102	Loss 0.2109 (0.3159)	Prec@(1,5) (95.5%, 99.6%)	
07/04 02:01:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][750/781]	Step 77386	lr 0.00102	Loss 0.3937 (0.3169)	Prec@(1,5) (95.5%, 99.6%)	
07/04 02:01:11午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][781/781]	Step 77417	lr 0.00102	Loss 0.3448 (0.3164)	Prec@(1,5) (95.5%, 99.6%)	
07/04 02:01:11午前 evaluateCell_trainer.py:172 [INFO] Train: [ 98/99] Final Prec@1 95.4720%
07/04 02:01:13午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][50/391]	Step 77418	Loss 0.1218	Prec@(1,5) (96.8%, 99.7%)
07/04 02:01:15午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][100/391]	Step 77418	Loss 0.1188	Prec@(1,5) (96.7%, 99.7%)
07/04 02:01:17午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][150/391]	Step 77418	Loss 0.1238	Prec@(1,5) (96.6%, 99.7%)
07/04 02:01:19午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][200/391]	Step 77418	Loss 0.1226	Prec@(1,5) (96.6%, 99.7%)
07/04 02:01:21午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][250/391]	Step 77418	Loss 0.1238	Prec@(1,5) (96.6%, 99.7%)
07/04 02:01:23午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][300/391]	Step 77418	Loss 0.1227	Prec@(1,5) (96.7%, 99.7%)
07/04 02:01:25午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][350/391]	Step 77418	Loss 0.1213	Prec@(1,5) (96.7%, 99.7%)
07/04 02:01:26午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][390/391]	Step 77418	Loss 0.1218	Prec@(1,5) (96.7%, 99.7%)
07/04 02:01:26午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 98/99] Final Prec@1 96.6720%
07/04 02:01:26午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.6720%
07/04 02:01:34午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][50/781]	Step 77468	lr 0.00101	Loss 0.1637 (0.3066)	Prec@(1,5) (95.3%, 99.6%)	
07/04 02:01:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][100/781]	Step 77518	lr 0.00101	Loss 0.3663 (0.3132)	Prec@(1,5) (95.3%, 99.6%)	
07/04 02:01:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][150/781]	Step 77568	lr 0.00101	Loss 0.3082 (0.3161)	Prec@(1,5) (95.4%, 99.5%)	
07/04 02:01:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][200/781]	Step 77618	lr 0.00101	Loss 0.2719 (0.3121)	Prec@(1,5) (95.6%, 99.5%)	
07/04 02:02:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][250/781]	Step 77668	lr 0.00101	Loss 0.2422 (0.3117)	Prec@(1,5) (95.6%, 99.5%)	
07/04 02:02:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][300/781]	Step 77718	lr 0.00101	Loss 0.2134 (0.3096)	Prec@(1,5) (95.7%, 99.5%)	
07/04 02:02:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][350/781]	Step 77768	lr 0.00101	Loss 0.3728 (0.3074)	Prec@(1,5) (95.7%, 99.5%)	
07/04 02:02:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][400/781]	Step 77818	lr 0.00101	Loss 0.3164 (0.3070)	Prec@(1,5) (95.7%, 99.6%)	
07/04 02:02:34午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][450/781]	Step 77868	lr 0.00101	Loss 0.3167 (0.3080)	Prec@(1,5) (95.7%, 99.6%)	
07/04 02:02:41午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][500/781]	Step 77918	lr 0.00101	Loss 0.5275 (0.3072)	Prec@(1,5) (95.7%, 99.6%)	
07/04 02:02:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][550/781]	Step 77968	lr 0.00101	Loss 0.3763 (0.3083)	Prec@(1,5) (95.6%, 99.6%)	
07/04 02:02:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][600/781]	Step 78018	lr 0.00101	Loss 0.3668 (0.3111)	Prec@(1,5) (95.6%, 99.5%)	
07/04 02:03:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][650/781]	Step 78068	lr 0.00101	Loss 0.2034 (0.3114)	Prec@(1,5) (95.6%, 99.5%)	
07/04 02:03:11午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][700/781]	Step 78118	lr 0.00101	Loss 0.1743 (0.3127)	Prec@(1,5) (95.5%, 99.5%)	
07/04 02:03:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][750/781]	Step 78168	lr 0.00101	Loss 0.2110 (0.3132)	Prec@(1,5) (95.5%, 99.5%)	
07/04 02:03:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][781/781]	Step 78199	lr 0.00101	Loss 0.2590 (0.3147)	Prec@(1,5) (95.5%, 99.5%)	
07/04 02:03:24午前 evaluateCell_trainer.py:172 [INFO] Train: [ 99/99] Final Prec@1 95.4980%
07/04 02:03:26午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][50/391]	Step 78200	Loss 0.1241	Prec@(1,5) (96.7%, 99.7%)
07/04 02:03:28午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][100/391]	Step 78200	Loss 0.1153	Prec@(1,5) (96.7%, 99.7%)
07/04 02:03:30午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][150/391]	Step 78200	Loss 0.1177	Prec@(1,5) (96.6%, 99.8%)
07/04 02:03:32午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][200/391]	Step 78200	Loss 0.1162	Prec@(1,5) (96.7%, 99.8%)
07/04 02:03:34午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][250/391]	Step 78200	Loss 0.1205	Prec@(1,5) (96.6%, 99.7%)
07/04 02:03:36午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][300/391]	Step 78200	Loss 0.1208	Prec@(1,5) (96.6%, 99.7%)
07/04 02:03:37午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][350/391]	Step 78200	Loss 0.1215	Prec@(1,5) (96.6%, 99.7%)
07/04 02:03:39午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][390/391]	Step 78200	Loss 0.1221	Prec@(1,5) (96.6%, 99.7%)
07/04 02:03:39午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 99/99] Final Prec@1 96.5920%
07/04 02:03:39午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.6720%
07/04 02:03:39午前 evaluateCell_main.py:67 [INFO] Final best Prec@1 = 96.6720%
