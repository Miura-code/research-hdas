07/03 07:30:53PM parser.py:28 [INFO] 
07/03 07:30:53PM parser.py:29 [INFO] Parameters:
07/03 07:30:53PM parser.py:31 [INFO] AUX_WEIGHT=0.4
07/03 07:30:53PM parser.py:31 [INFO] BATCH_SIZE=64
07/03 07:30:53PM parser.py:31 [INFO] CUTOUT_LENGTH=16
07/03 07:30:53PM parser.py:31 [INFO] DATA_PATH=../data/
07/03 07:30:53PM parser.py:31 [INFO] DATASET=cifar100
07/03 07:30:53PM parser.py:31 [INFO] DIST=False
07/03 07:30:53PM parser.py:31 [INFO] DROP_PATH_PROB=0.2
07/03 07:30:53PM parser.py:31 [INFO] EPOCHS=100
07/03 07:30:53PM parser.py:31 [INFO] EXCLUDE_BIAS_AND_BN=True
07/03 07:30:53PM parser.py:31 [INFO] EXP_NAME=eval-20240703-193053
07/03 07:30:53PM parser.py:31 [INFO] GENOTYPE=Genotype3(normal1=[[('sep_conv_3x3', 1), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 2)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 0)], [('sep_conv_3x3', 0), ('sep_conv_3x3', 4)]], normal1_concat=range(2, 6), reduce1=[[('sep_conv_3x3', 0), ('sep_conv_3x3', 1)], [('skip_connect', 1), ('avg_pool_3x3', 0)], [('sep_conv_3x3', 2), ('sep_conv_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 3)]], reduce1_concat=range(2, 6), normal2=[[('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 1), ('skip_connect', 0)], [('skip_connect', 0), ('sep_conv_3x3', 2)], [('skip_connect', 0), ('skip_connect', 1)]], normal2_concat=range(2, 6), reduce2=[[('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('avg_pool_3x3', 1)], [('avg_pool_3x3', 0), ('skip_connect', 2)], [('avg_pool_3x3', 0), ('skip_connect', 2)]], reduce2_concat=range(2, 6), normal3=[[('skip_connect', 0), ('dil_conv_3x3', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 1)], [('skip_connect', 0), ('skip_connect', 2)]], normal3_concat=range(2, 6))
07/03 07:30:53PM parser.py:31 [INFO] GPUS=[0]
07/03 07:30:53PM parser.py:31 [INFO] GRAD_CLIP=5.0
07/03 07:30:53PM parser.py:31 [INFO] INIT_CHANNELS=32
07/03 07:30:53PM parser.py:31 [INFO] LAYERS=20
07/03 07:30:53PM parser.py:31 [INFO] LOCAL_RANK=0
07/03 07:30:53PM parser.py:31 [INFO] LOGGER=<Logger H-DAS (INFO)>
07/03 07:30:53PM parser.py:31 [INFO] LR=0.025
07/03 07:30:53PM parser.py:31 [INFO] LR_MIN=0.001
07/03 07:30:53PM parser.py:31 [INFO] MOMENTUM=0.9
07/03 07:30:53PM parser.py:31 [INFO] NAME=BASELINE
07/03 07:30:53PM parser.py:31 [INFO] PATH=results/evaluate_Cell/cifar100/BASELINE/eval-20240703-193053
07/03 07:30:53PM parser.py:31 [INFO] PRINT_FREQ=50
07/03 07:30:53PM parser.py:31 [INFO] RESUME_PATH=None
07/03 07:30:53PM parser.py:31 [INFO] SAVE=eval
07/03 07:30:53PM parser.py:31 [INFO] SEED=0
07/03 07:30:53PM parser.py:31 [INFO] TRAIN_PORTION=1.0
07/03 07:30:53PM parser.py:31 [INFO] WEIGHT_DECAY=0.0003
07/03 07:30:53PM parser.py:31 [INFO] WORKERS=4
07/03 07:30:53PM parser.py:32 [INFO] 
07/03 07:30:54PM evaluateCell_trainer.py:103 [INFO] --> No loaded checkpoint!
07/03 07:31:01PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][50/781]	Step 50	lr 0.025	Loss 6.1043 (6.4091)	Prec@(1,5) (2.0%, 8.7%)	
07/03 07:31:06PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][100/781]	Step 100	lr 0.025	Loss 6.1369 (6.3131)	Prec@(1,5) (2.6%, 11.5%)	
07/03 07:31:11PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][150/781]	Step 150	lr 0.025	Loss 5.9383 (6.2322)	Prec@(1,5) (3.2%, 13.4%)	
07/03 07:31:17PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][200/781]	Step 200	lr 0.025	Loss 5.8666 (6.1403)	Prec@(1,5) (3.9%, 15.4%)	
07/03 07:31:28PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][250/781]	Step 250	lr 0.025	Loss 5.7853 (6.0680)	Prec@(1,5) (4.3%, 16.8%)	
07/03 07:31:39PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][300/781]	Step 300	lr 0.025	Loss 5.9952 (6.0032)	Prec@(1,5) (4.7%, 18.2%)	
07/03 07:31:50PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][350/781]	Step 350	lr 0.025	Loss 5.5291 (5.9411)	Prec@(1,5) (5.2%, 19.6%)	
07/03 07:32:01PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][400/781]	Step 400	lr 0.025	Loss 5.1140 (5.8786)	Prec@(1,5) (5.8%, 21.0%)	
07/03 07:32:12PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][450/781]	Step 450	lr 0.025	Loss 5.2954 (5.8281)	Prec@(1,5) (6.2%, 22.1%)	
07/03 07:32:23PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][500/781]	Step 500	lr 0.025	Loss 5.4219 (5.7812)	Prec@(1,5) (6.7%, 23.3%)	
07/03 07:32:34PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][550/781]	Step 550	lr 0.025	Loss 5.4161 (5.7407)	Prec@(1,5) (7.1%, 24.2%)	
07/03 07:32:46PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][600/781]	Step 600	lr 0.025	Loss 4.9051 (5.7039)	Prec@(1,5) (7.5%, 25.1%)	
07/03 07:32:57PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][650/781]	Step 650	lr 0.025	Loss 4.9142 (5.6621)	Prec@(1,5) (7.9%, 26.0%)	
07/03 07:33:08PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][700/781]	Step 700	lr 0.025	Loss 5.1535 (5.6265)	Prec@(1,5) (8.3%, 26.8%)	
07/03 07:33:19PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][750/781]	Step 750	lr 0.025	Loss 4.9761 (5.5965)	Prec@(1,5) (8.6%, 27.5%)	
07/03 07:33:26PM evaluateCell_trainer.py:162 [INFO] Train: Epoch: [0][781/781]	Step 781	lr 0.025	Loss 5.1794 (5.5776)	Prec@(1,5) (8.8%, 27.9%)	
07/03 07:33:28PM evaluateCell_trainer.py:172 [INFO] Train: [  0/99] Final Prec@1 8.8340%
07/03 07:33:31PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][50/391]	Step 782	Loss 3.6359	Prec@(1,5) (13.6%, 39.0%)
07/03 07:33:35PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][100/391]	Step 782	Loss 3.6600	Prec@(1,5) (13.6%, 37.9%)
07/03 07:33:38PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][150/391]	Step 782	Loss 3.6656	Prec@(1,5) (13.5%, 37.8%)
07/03 07:33:41PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][200/391]	Step 782	Loss 3.6736	Prec@(1,5) (13.5%, 37.8%)
07/03 07:33:44PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][250/391]	Step 782	Loss 3.6666	Prec@(1,5) (13.5%, 38.0%)
07/03 07:33:47PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][300/391]	Step 782	Loss 3.6727	Prec@(1,5) (13.3%, 37.8%)
07/03 07:33:50PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][350/391]	Step 782	Loss 3.6692	Prec@(1,5) (13.5%, 37.9%)
07/03 07:33:53PM evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [0][390/391]	Step 782	Loss 3.6700	Prec@(1,5) (13.4%, 37.8%)
07/03 07:33:54PM evaluateCell_trainer.py:208 [INFO] Valid: [  0/99] Final Prec@1 13.4440%
07/03 07:33:54PM evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 13.4440%
07/03 07:34:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][50/781]	Step 832	lr 0.02499	Loss 4.8496 (5.0088)	Prec@(1,5) (14.2%, 39.9%)	
07/03 07:34:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][100/781]	Step 882	lr 0.02499	Loss 5.2712 (5.0021)	Prec@(1,5) (14.7%, 40.9%)	
07/03 07:34:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][150/781]	Step 932	lr 0.02499	Loss 5.0883 (4.9810)	Prec@(1,5) (15.3%, 41.1%)	
07/03 07:34:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][200/781]	Step 982	lr 0.02499	Loss 4.6876 (4.9681)	Prec@(1,5) (15.5%, 41.4%)	
07/03 07:34:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][250/781]	Step 1032	lr 0.02499	Loss 4.7069 (4.9216)	Prec@(1,5) (16.2%, 42.3%)	
07/03 07:35:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][300/781]	Step 1082	lr 0.02499	Loss 5.1011 (4.8971)	Prec@(1,5) (16.5%, 42.6%)	
07/03 07:35:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][350/781]	Step 1132	lr 0.02499	Loss 4.9097 (4.8803)	Prec@(1,5) (16.6%, 43.0%)	
07/03 07:35:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][400/781]	Step 1182	lr 0.02499	Loss 5.1987 (4.8602)	Prec@(1,5) (16.7%, 43.5%)	
07/03 07:35:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][450/781]	Step 1232	lr 0.02499	Loss 4.5135 (4.8369)	Prec@(1,5) (17.0%, 44.0%)	
07/03 07:35:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][500/781]	Step 1282	lr 0.02499	Loss 4.0621 (4.8146)	Prec@(1,5) (17.3%, 44.4%)	
07/03 07:35:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][550/781]	Step 1332	lr 0.02499	Loss 4.3053 (4.7936)	Prec@(1,5) (17.5%, 44.8%)	
07/03 07:36:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][600/781]	Step 1382	lr 0.02499	Loss 4.8465 (4.7757)	Prec@(1,5) (17.7%, 45.2%)	
07/03 07:36:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][650/781]	Step 1432	lr 0.02499	Loss 4.5335 (4.7568)	Prec@(1,5) (18.0%, 45.6%)	
07/03 07:36:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][700/781]	Step 1482	lr 0.02499	Loss 3.9246 (4.7342)	Prec@(1,5) (18.2%, 46.1%)	
07/03 07:36:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][750/781]	Step 1532	lr 0.02499	Loss 4.4863 (4.7176)	Prec@(1,5) (18.4%, 46.4%)	
07/03 07:36:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [1][781/781]	Step 1563	lr 0.02499	Loss 4.9731 (4.7032)	Prec@(1,5) (18.5%, 46.6%)	
07/03 07:36:46午後 evaluateCell_trainer.py:172 [INFO] Train: [  1/99] Final Prec@1 18.5200%
07/03 07:36:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][50/391]	Step 1564	Loss 3.3545	Prec@(1,5) (19.6%, 48.4%)
07/03 07:36:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][100/391]	Step 1564	Loss 3.3273	Prec@(1,5) (19.9%, 48.8%)
07/03 07:36:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][150/391]	Step 1564	Loss 3.3335	Prec@(1,5) (20.0%, 48.1%)
07/03 07:36:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][200/391]	Step 1564	Loss 3.3352	Prec@(1,5) (19.9%, 48.4%)
07/03 07:37:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][250/391]	Step 1564	Loss 3.3372	Prec@(1,5) (19.7%, 48.2%)
07/03 07:37:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][300/391]	Step 1564	Loss 3.3357	Prec@(1,5) (19.8%, 48.4%)
07/03 07:37:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][350/391]	Step 1564	Loss 3.3412	Prec@(1,5) (19.8%, 48.3%)
07/03 07:37:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [1][390/391]	Step 1564	Loss 3.3419	Prec@(1,5) (19.7%, 48.3%)
07/03 07:37:12午後 evaluateCell_trainer.py:208 [INFO] Valid: [  1/99] Final Prec@1 19.6720%
07/03 07:37:12午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 19.6720%
07/03 07:37:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][50/781]	Step 1614	lr 0.02498	Loss 4.6497 (4.3022)	Prec@(1,5) (23.5%, 53.9%)	
07/03 07:37:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][100/781]	Step 1664	lr 0.02498	Loss 4.0866 (4.2881)	Prec@(1,5) (23.5%, 54.6%)	
07/03 07:37:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][150/781]	Step 1714	lr 0.02498	Loss 4.0608 (4.2690)	Prec@(1,5) (23.9%, 55.0%)	
07/03 07:37:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][200/781]	Step 1764	lr 0.02498	Loss 4.2066 (4.2394)	Prec@(1,5) (24.5%, 55.8%)	
07/03 07:38:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][250/781]	Step 1814	lr 0.02498	Loss 4.4862 (4.2055)	Prec@(1,5) (24.9%, 56.4%)	
07/03 07:38:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][300/781]	Step 1864	lr 0.02498	Loss 3.9419 (4.1819)	Prec@(1,5) (25.3%, 56.8%)	
07/03 07:38:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][350/781]	Step 1914	lr 0.02498	Loss 3.8223 (4.1710)	Prec@(1,5) (25.5%, 57.1%)	
07/03 07:38:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][400/781]	Step 1964	lr 0.02498	Loss 3.9484 (4.1523)	Prec@(1,5) (25.8%, 57.5%)	
07/03 07:38:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][450/781]	Step 2014	lr 0.02498	Loss 3.9547 (4.1394)	Prec@(1,5) (25.9%, 57.7%)	
07/03 07:39:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][500/781]	Step 2064	lr 0.02498	Loss 4.3275 (4.1285)	Prec@(1,5) (26.0%, 58.0%)	
07/03 07:39:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][550/781]	Step 2114	lr 0.02498	Loss 3.9915 (4.1112)	Prec@(1,5) (26.2%, 58.2%)	
07/03 07:39:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][600/781]	Step 2164	lr 0.02498	Loss 4.0709 (4.0919)	Prec@(1,5) (26.5%, 58.6%)	
07/03 07:39:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][650/781]	Step 2214	lr 0.02498	Loss 3.4391 (4.0708)	Prec@(1,5) (26.9%, 58.9%)	
07/03 07:39:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][700/781]	Step 2264	lr 0.02498	Loss 3.4961 (4.0528)	Prec@(1,5) (27.2%, 59.2%)	
07/03 07:40:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][750/781]	Step 2314	lr 0.02498	Loss 3.5687 (4.0369)	Prec@(1,5) (27.4%, 59.5%)	
07/03 07:40:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [2][781/781]	Step 2345	lr 0.02498	Loss 3.7367 (4.0301)	Prec@(1,5) (27.5%, 59.6%)	
07/03 07:40:09午後 evaluateCell_trainer.py:172 [INFO] Train: [  2/99] Final Prec@1 27.5420%
07/03 07:40:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][50/391]	Step 2346	Loss 3.0932	Prec@(1,5) (24.7%, 54.5%)
07/03 07:40:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][100/391]	Step 2346	Loss 3.0849	Prec@(1,5) (24.5%, 54.4%)
07/03 07:40:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][150/391]	Step 2346	Loss 3.0938	Prec@(1,5) (24.4%, 54.5%)
07/03 07:40:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][200/391]	Step 2346	Loss 3.0736	Prec@(1,5) (24.6%, 55.0%)
07/03 07:40:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][250/391]	Step 2346	Loss 3.0766	Prec@(1,5) (24.4%, 55.0%)
07/03 07:40:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][300/391]	Step 2346	Loss 3.0810	Prec@(1,5) (24.2%, 54.9%)
07/03 07:40:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][350/391]	Step 2346	Loss 3.0792	Prec@(1,5) (24.1%, 54.8%)
07/03 07:40:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [2][390/391]	Step 2346	Loss 3.0890	Prec@(1,5) (24.1%, 54.7%)
07/03 07:40:35午後 evaluateCell_trainer.py:208 [INFO] Valid: [  2/99] Final Prec@1 24.0600%
07/03 07:40:35午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 24.0600%
07/03 07:40:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][50/781]	Step 2396	lr 0.02495	Loss 3.9891 (3.7546)	Prec@(1,5) (31.8%, 64.9%)	
07/03 07:40:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][100/781]	Step 2446	lr 0.02495	Loss 3.8028 (3.7346)	Prec@(1,5) (31.9%, 65.0%)	
07/03 07:41:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][150/781]	Step 2496	lr 0.02495	Loss 3.4780 (3.7268)	Prec@(1,5) (32.0%, 65.1%)	
07/03 07:41:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][200/781]	Step 2546	lr 0.02495	Loss 3.4338 (3.6988)	Prec@(1,5) (32.7%, 65.3%)	
07/03 07:41:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][250/781]	Step 2596	lr 0.02495	Loss 3.8378 (3.6953)	Prec@(1,5) (32.6%, 65.5%)	
07/03 07:41:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][300/781]	Step 2646	lr 0.02495	Loss 3.1381 (3.6863)	Prec@(1,5) (32.7%, 65.7%)	
07/03 07:41:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][350/781]	Step 2696	lr 0.02495	Loss 4.0811 (3.6841)	Prec@(1,5) (32.8%, 65.6%)	
07/03 07:42:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][400/781]	Step 2746	lr 0.02495	Loss 3.7281 (3.6680)	Prec@(1,5) (33.0%, 65.9%)	
07/03 07:42:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][450/781]	Step 2796	lr 0.02495	Loss 3.4600 (3.6516)	Prec@(1,5) (33.4%, 66.1%)	
07/03 07:42:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][500/781]	Step 2846	lr 0.02495	Loss 3.3960 (3.6419)	Prec@(1,5) (33.5%, 66.2%)	
07/03 07:42:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][550/781]	Step 2896	lr 0.02495	Loss 3.4423 (3.6225)	Prec@(1,5) (33.7%, 66.5%)	
07/03 07:42:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][600/781]	Step 2946	lr 0.02495	Loss 3.7983 (3.6179)	Prec@(1,5) (33.8%, 66.6%)	
07/03 07:43:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][650/781]	Step 2996	lr 0.02495	Loss 3.9070 (3.6096)	Prec@(1,5) (34.0%, 66.8%)	
07/03 07:43:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][700/781]	Step 3046	lr 0.02495	Loss 4.0212 (3.6011)	Prec@(1,5) (34.1%, 66.9%)	
07/03 07:43:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][750/781]	Step 3096	lr 0.02495	Loss 2.9529 (3.5884)	Prec@(1,5) (34.3%, 67.1%)	
07/03 07:43:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [3][781/781]	Step 3127	lr 0.02495	Loss 3.3499 (3.5819)	Prec@(1,5) (34.4%, 67.2%)	
07/03 07:43:33午後 evaluateCell_trainer.py:172 [INFO] Train: [  3/99] Final Prec@1 34.3900%
07/03 07:43:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][50/391]	Step 3128	Loss 2.5668	Prec@(1,5) (33.3%, 66.0%)
07/03 07:43:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][100/391]	Step 3128	Loss 2.5751	Prec@(1,5) (33.5%, 65.9%)
07/03 07:43:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][150/391]	Step 3128	Loss 2.5812	Prec@(1,5) (33.2%, 65.9%)
07/03 07:43:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][200/391]	Step 3128	Loss 2.5777	Prec@(1,5) (33.3%, 65.9%)
07/03 07:43:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][250/391]	Step 3128	Loss 2.5866	Prec@(1,5) (33.3%, 65.7%)
07/03 07:43:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][300/391]	Step 3128	Loss 2.5843	Prec@(1,5) (33.3%, 65.7%)
07/03 07:43:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][350/391]	Step 3128	Loss 2.5781	Prec@(1,5) (33.5%, 65.8%)
07/03 07:43:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [3][390/391]	Step 3128	Loss 2.5800	Prec@(1,5) (33.5%, 65.7%)
07/03 07:43:58午後 evaluateCell_trainer.py:208 [INFO] Valid: [  3/99] Final Prec@1 33.4520%
07/03 07:43:58午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 33.4520%
07/03 07:44:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][50/781]	Step 3178	lr 0.02491	Loss 3.6026 (3.3763)	Prec@(1,5) (37.9%, 70.8%)	
07/03 07:44:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][100/781]	Step 3228	lr 0.02491	Loss 3.7256 (3.3679)	Prec@(1,5) (38.0%, 70.9%)	
07/03 07:44:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][150/781]	Step 3278	lr 0.02491	Loss 3.6720 (3.3646)	Prec@(1,5) (37.8%, 70.7%)	
07/03 07:44:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][200/781]	Step 3328	lr 0.02491	Loss 3.3789 (3.3606)	Prec@(1,5) (37.8%, 70.9%)	
07/03 07:44:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][250/781]	Step 3378	lr 0.02491	Loss 3.0858 (3.3510)	Prec@(1,5) (38.0%, 71.0%)	
07/03 07:45:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][300/781]	Step 3428	lr 0.02491	Loss 3.5090 (3.3520)	Prec@(1,5) (38.1%, 70.8%)	
07/03 07:45:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][350/781]	Step 3478	lr 0.02491	Loss 3.1258 (3.3336)	Prec@(1,5) (38.5%, 71.1%)	
07/03 07:45:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][400/781]	Step 3528	lr 0.02491	Loss 3.1673 (3.3302)	Prec@(1,5) (38.5%, 71.2%)	
07/03 07:45:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][450/781]	Step 3578	lr 0.02491	Loss 2.9057 (3.3190)	Prec@(1,5) (38.6%, 71.3%)	
07/03 07:45:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][500/781]	Step 3628	lr 0.02491	Loss 3.3687 (3.3121)	Prec@(1,5) (38.7%, 71.4%)	
07/03 07:46:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][550/781]	Step 3678	lr 0.02491	Loss 3.2967 (3.3043)	Prec@(1,5) (38.7%, 71.5%)	
07/03 07:46:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][600/781]	Step 3728	lr 0.02491	Loss 3.2521 (3.2985)	Prec@(1,5) (38.7%, 71.6%)	
07/03 07:46:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][650/781]	Step 3778	lr 0.02491	Loss 3.0175 (3.2942)	Prec@(1,5) (38.8%, 71.6%)	
07/03 07:46:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][700/781]	Step 3828	lr 0.02491	Loss 3.7830 (3.2889)	Prec@(1,5) (39.0%, 71.7%)	
07/03 07:46:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][750/781]	Step 3878	lr 0.02491	Loss 3.1970 (3.2844)	Prec@(1,5) (39.0%, 71.8%)	
07/03 07:46:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [4][781/781]	Step 3909	lr 0.02491	Loss 3.3924 (3.2797)	Prec@(1,5) (39.1%, 71.9%)	
07/03 07:46:56午後 evaluateCell_trainer.py:172 [INFO] Train: [  4/99] Final Prec@1 39.0540%
07/03 07:46:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][50/391]	Step 3910	Loss 2.3650	Prec@(1,5) (37.4%, 70.5%)
07/03 07:47:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][100/391]	Step 3910	Loss 2.3916	Prec@(1,5) (37.2%, 70.3%)
07/03 07:47:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][150/391]	Step 3910	Loss 2.3805	Prec@(1,5) (37.1%, 70.6%)
07/03 07:47:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][200/391]	Step 3910	Loss 2.3905	Prec@(1,5) (36.9%, 70.3%)
07/03 07:47:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][250/391]	Step 3910	Loss 2.4057	Prec@(1,5) (36.8%, 70.2%)
07/03 07:47:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][300/391]	Step 3910	Loss 2.4050	Prec@(1,5) (36.9%, 70.3%)
07/03 07:47:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][350/391]	Step 3910	Loss 2.4080	Prec@(1,5) (36.8%, 70.3%)
07/03 07:47:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [4][390/391]	Step 3910	Loss 2.4043	Prec@(1,5) (37.0%, 70.2%)
07/03 07:47:21午後 evaluateCell_trainer.py:208 [INFO] Valid: [  4/99] Final Prec@1 36.9560%
07/03 07:47:21午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 36.9560%
07/03 07:47:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][50/781]	Step 3960	lr 0.02485	Loss 3.0676 (3.1388)	Prec@(1,5) (40.8%, 74.7%)	
07/03 07:47:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][100/781]	Step 4010	lr 0.02485	Loss 2.6963 (3.1315)	Prec@(1,5) (40.7%, 74.7%)	
07/03 07:47:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][150/781]	Step 4060	lr 0.02485	Loss 2.9634 (3.1005)	Prec@(1,5) (41.5%, 74.9%)	
07/03 07:48:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][200/781]	Step 4110	lr 0.02485	Loss 3.3156 (3.0994)	Prec@(1,5) (41.7%, 74.7%)	
07/03 07:48:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][250/781]	Step 4160	lr 0.02485	Loss 2.8840 (3.1054)	Prec@(1,5) (41.7%, 74.6%)	
07/03 07:48:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][300/781]	Step 4210	lr 0.02485	Loss 3.1079 (3.0900)	Prec@(1,5) (42.1%, 75.0%)	
07/03 07:48:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][350/781]	Step 4260	lr 0.02485	Loss 3.0642 (3.0900)	Prec@(1,5) (42.1%, 75.1%)	
07/03 07:48:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][400/781]	Step 4310	lr 0.02485	Loss 3.0101 (3.0875)	Prec@(1,5) (42.2%, 75.2%)	
07/03 07:49:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][450/781]	Step 4360	lr 0.02485	Loss 2.8454 (3.0804)	Prec@(1,5) (42.3%, 75.1%)	
07/03 07:49:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][500/781]	Step 4410	lr 0.02485	Loss 3.0210 (3.0741)	Prec@(1,5) (42.4%, 75.2%)	
07/03 07:49:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][550/781]	Step 4460	lr 0.02485	Loss 2.4216 (3.0717)	Prec@(1,5) (42.5%, 75.2%)	
07/03 07:49:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][600/781]	Step 4510	lr 0.02485	Loss 3.0981 (3.0695)	Prec@(1,5) (42.5%, 75.2%)	
07/03 07:49:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][650/781]	Step 4560	lr 0.02485	Loss 3.1426 (3.0696)	Prec@(1,5) (42.5%, 75.2%)	
07/03 07:50:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][700/781]	Step 4610	lr 0.02485	Loss 2.9820 (3.0651)	Prec@(1,5) (42.6%, 75.3%)	
07/03 07:50:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][750/781]	Step 4660	lr 0.02485	Loss 2.3855 (3.0582)	Prec@(1,5) (42.7%, 75.3%)	
07/03 07:50:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [5][781/781]	Step 4691	lr 0.02485	Loss 2.5564 (3.0547)	Prec@(1,5) (42.8%, 75.4%)	
07/03 07:50:19午後 evaluateCell_trainer.py:172 [INFO] Train: [  5/99] Final Prec@1 42.7700%
07/03 07:50:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][50/391]	Step 4692	Loss 2.1238	Prec@(1,5) (43.1%, 75.7%)
07/03 07:50:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][100/391]	Step 4692	Loss 2.1011	Prec@(1,5) (43.4%, 75.7%)
07/03 07:50:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][150/391]	Step 4692	Loss 2.0878	Prec@(1,5) (43.7%, 76.1%)
07/03 07:50:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][200/391]	Step 4692	Loss 2.0831	Prec@(1,5) (43.5%, 76.0%)
07/03 07:50:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][250/391]	Step 4692	Loss 2.0883	Prec@(1,5) (43.5%, 76.0%)
07/03 07:50:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][300/391]	Step 4692	Loss 2.0923	Prec@(1,5) (43.5%, 75.9%)
07/03 07:50:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][350/391]	Step 4692	Loss 2.0906	Prec@(1,5) (43.4%, 75.9%)
07/03 07:50:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [5][390/391]	Step 4692	Loss 2.0917	Prec@(1,5) (43.3%, 76.0%)
07/03 07:50:44午後 evaluateCell_trainer.py:208 [INFO] Valid: [  5/99] Final Prec@1 43.3440%
07/03 07:50:44午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 43.3440%
07/03 07:50:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][50/781]	Step 4742	lr 0.02479	Loss 2.9879 (2.8996)	Prec@(1,5) (46.3%, 77.9%)	
07/03 07:51:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][100/781]	Step 4792	lr 0.02479	Loss 2.8858 (2.9039)	Prec@(1,5) (44.9%, 77.6%)	
07/03 07:51:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][150/781]	Step 4842	lr 0.02479	Loss 2.2085 (2.8938)	Prec@(1,5) (45.7%, 77.6%)	
07/03 07:51:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][200/781]	Step 4892	lr 0.02479	Loss 2.4961 (2.8971)	Prec@(1,5) (45.6%, 77.4%)	
07/03 07:51:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][250/781]	Step 4942	lr 0.02479	Loss 3.1538 (2.9142)	Prec@(1,5) (45.1%, 77.2%)	
07/03 07:51:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][300/781]	Step 4992	lr 0.02479	Loss 2.5515 (2.9156)	Prec@(1,5) (45.0%, 77.3%)	
07/03 07:52:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][350/781]	Step 5042	lr 0.02479	Loss 2.4330 (2.9067)	Prec@(1,5) (45.3%, 77.4%)	
07/03 07:52:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][400/781]	Step 5092	lr 0.02479	Loss 2.3072 (2.8942)	Prec@(1,5) (45.5%, 77.6%)	
07/03 07:52:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][450/781]	Step 5142	lr 0.02479	Loss 3.1091 (2.8982)	Prec@(1,5) (45.4%, 77.5%)	
07/03 07:52:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][500/781]	Step 5192	lr 0.02479	Loss 2.9204 (2.8961)	Prec@(1,5) (45.5%, 77.5%)	
07/03 07:52:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][550/781]	Step 5242	lr 0.02479	Loss 2.9393 (2.8913)	Prec@(1,5) (45.5%, 77.6%)	
07/03 07:53:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][600/781]	Step 5292	lr 0.02479	Loss 3.0274 (2.8853)	Prec@(1,5) (45.5%, 77.6%)	
07/03 07:53:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][650/781]	Step 5342	lr 0.02479	Loss 2.7304 (2.8811)	Prec@(1,5) (45.5%, 77.6%)	
07/03 07:53:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][700/781]	Step 5392	lr 0.02479	Loss 2.9760 (2.8797)	Prec@(1,5) (45.7%, 77.6%)	
07/03 07:53:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][750/781]	Step 5442	lr 0.02479	Loss 2.8146 (2.8773)	Prec@(1,5) (45.7%, 77.6%)	
07/03 07:53:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [6][781/781]	Step 5473	lr 0.02479	Loss 2.5225 (2.8769)	Prec@(1,5) (45.7%, 77.7%)	
07/03 07:53:42午後 evaluateCell_trainer.py:172 [INFO] Train: [  6/99] Final Prec@1 45.7440%
07/03 07:53:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][50/391]	Step 5474	Loss 2.1146	Prec@(1,5) (44.6%, 75.1%)
07/03 07:53:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][100/391]	Step 5474	Loss 2.0885	Prec@(1,5) (44.5%, 75.6%)
07/03 07:53:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][150/391]	Step 5474	Loss 2.0970	Prec@(1,5) (44.7%, 75.6%)
07/03 07:53:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][200/391]	Step 5474	Loss 2.0916	Prec@(1,5) (44.6%, 75.5%)
07/03 07:53:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][250/391]	Step 5474	Loss 2.0818	Prec@(1,5) (44.7%, 75.7%)
07/03 07:54:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][300/391]	Step 5474	Loss 2.0813	Prec@(1,5) (44.6%, 75.7%)
07/03 07:54:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][350/391]	Step 5474	Loss 2.0785	Prec@(1,5) (44.6%, 75.8%)
07/03 07:54:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [6][390/391]	Step 5474	Loss 2.0777	Prec@(1,5) (44.8%, 75.9%)
07/03 07:54:07午後 evaluateCell_trainer.py:208 [INFO] Valid: [  6/99] Final Prec@1 44.7560%
07/03 07:54:07午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 44.7560%
07/03 07:54:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][50/781]	Step 5524	lr 0.02471	Loss 3.2973 (2.7074)	Prec@(1,5) (48.1%, 79.7%)	
07/03 07:54:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][100/781]	Step 5574	lr 0.02471	Loss 2.5523 (2.6958)	Prec@(1,5) (48.0%, 79.9%)	
07/03 07:54:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][150/781]	Step 5624	lr 0.02471	Loss 3.1563 (2.7149)	Prec@(1,5) (47.7%, 80.0%)	
07/03 07:54:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][200/781]	Step 5674	lr 0.02471	Loss 2.6622 (2.7262)	Prec@(1,5) (47.7%, 79.7%)	
07/03 07:55:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][250/781]	Step 5724	lr 0.02471	Loss 2.2705 (2.7291)	Prec@(1,5) (47.4%, 79.6%)	
07/03 07:55:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][300/781]	Step 5774	lr 0.02471	Loss 3.2327 (2.7451)	Prec@(1,5) (47.3%, 79.3%)	
07/03 07:55:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][350/781]	Step 5824	lr 0.02471	Loss 2.7513 (2.7496)	Prec@(1,5) (47.3%, 79.3%)	
07/03 07:55:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][400/781]	Step 5874	lr 0.02471	Loss 2.8979 (2.7431)	Prec@(1,5) (47.6%, 79.4%)	
07/03 07:55:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][450/781]	Step 5924	lr 0.02471	Loss 2.6144 (2.7475)	Prec@(1,5) (47.6%, 79.3%)	
07/03 07:56:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][500/781]	Step 5974	lr 0.02471	Loss 3.1921 (2.7539)	Prec@(1,5) (47.6%, 79.2%)	
07/03 07:56:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][550/781]	Step 6024	lr 0.02471	Loss 3.0401 (2.7505)	Prec@(1,5) (47.7%, 79.2%)	
07/03 07:56:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][600/781]	Step 6074	lr 0.02471	Loss 2.3882 (2.7492)	Prec@(1,5) (47.7%, 79.2%)	
07/03 07:56:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][650/781]	Step 6124	lr 0.02471	Loss 2.7522 (2.7435)	Prec@(1,5) (47.8%, 79.3%)	
07/03 07:56:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][700/781]	Step 6174	lr 0.02471	Loss 2.6172 (2.7443)	Prec@(1,5) (47.7%, 79.3%)	
07/03 07:56:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][750/781]	Step 6224	lr 0.02471	Loss 2.5193 (2.7411)	Prec@(1,5) (47.8%, 79.3%)	
07/03 07:57:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [7][781/781]	Step 6255	lr 0.02471	Loss 3.0726 (2.7413)	Prec@(1,5) (47.8%, 79.3%)	
07/03 07:57:05午後 evaluateCell_trainer.py:172 [INFO] Train: [  7/99] Final Prec@1 47.8360%
07/03 07:57:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][50/391]	Step 6256	Loss 1.9892	Prec@(1,5) (45.1%, 78.2%)
07/03 07:57:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][100/391]	Step 6256	Loss 1.9884	Prec@(1,5) (45.7%, 77.8%)
07/03 07:57:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][150/391]	Step 6256	Loss 1.9893	Prec@(1,5) (45.9%, 78.0%)
07/03 07:57:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][200/391]	Step 6256	Loss 1.9785	Prec@(1,5) (46.4%, 78.2%)
07/03 07:57:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][250/391]	Step 6256	Loss 1.9771	Prec@(1,5) (46.5%, 78.2%)
07/03 07:57:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][300/391]	Step 6256	Loss 1.9723	Prec@(1,5) (46.4%, 78.3%)
07/03 07:57:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][350/391]	Step 6256	Loss 1.9627	Prec@(1,5) (46.6%, 78.7%)
07/03 07:57:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [7][390/391]	Step 6256	Loss 1.9636	Prec@(1,5) (46.5%, 78.6%)
07/03 07:57:30午後 evaluateCell_trainer.py:208 [INFO] Valid: [  7/99] Final Prec@1 46.5240%
07/03 07:57:31午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 46.5240%
07/03 07:57:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][50/781]	Step 6306	lr 0.02462	Loss 2.4683 (2.5716)	Prec@(1,5) (51.3%, 82.0%)	
07/03 07:57:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][100/781]	Step 6356	lr 0.02462	Loss 2.6895 (2.5961)	Prec@(1,5) (50.6%, 81.5%)	
07/03 07:58:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][150/781]	Step 6406	lr 0.02462	Loss 2.4686 (2.5906)	Prec@(1,5) (50.6%, 81.6%)	
07/03 07:58:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][200/781]	Step 6456	lr 0.02462	Loss 2.7044 (2.6101)	Prec@(1,5) (50.0%, 81.3%)	
07/03 07:58:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][250/781]	Step 6506	lr 0.02462	Loss 3.0872 (2.6201)	Prec@(1,5) (49.8%, 81.2%)	
07/03 07:58:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][300/781]	Step 6556	lr 0.02462	Loss 2.7606 (2.6194)	Prec@(1,5) (49.8%, 81.2%)	
07/03 07:58:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][350/781]	Step 6606	lr 0.02462	Loss 2.2942 (2.6255)	Prec@(1,5) (49.7%, 81.0%)	
07/03 07:59:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][400/781]	Step 6656	lr 0.02462	Loss 2.1232 (2.6295)	Prec@(1,5) (49.6%, 81.0%)	
07/03 07:59:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][450/781]	Step 6706	lr 0.02462	Loss 2.6217 (2.6300)	Prec@(1,5) (49.5%, 81.0%)	
07/03 07:59:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][500/781]	Step 6756	lr 0.02462	Loss 3.0410 (2.6304)	Prec@(1,5) (49.5%, 80.9%)	
07/03 07:59:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][550/781]	Step 6806	lr 0.02462	Loss 2.8315 (2.6283)	Prec@(1,5) (49.6%, 80.9%)	
07/03 07:59:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][600/781]	Step 6856	lr 0.02462	Loss 2.9095 (2.6271)	Prec@(1,5) (49.7%, 80.9%)	
07/03 07:59:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][650/781]	Step 6906	lr 0.02462	Loss 2.9389 (2.6235)	Prec@(1,5) (49.7%, 80.9%)	
07/03 08:00:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][700/781]	Step 6956	lr 0.02462	Loss 2.8226 (2.6197)	Prec@(1,5) (49.8%, 81.0%)	
07/03 08:00:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][750/781]	Step 7006	lr 0.02462	Loss 2.5898 (2.6171)	Prec@(1,5) (49.9%, 81.0%)	
07/03 08:00:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [8][781/781]	Step 7037	lr 0.02462	Loss 3.3415 (2.6199)	Prec@(1,5) (49.9%, 81.0%)	
07/03 08:00:28午後 evaluateCell_trainer.py:172 [INFO] Train: [  8/99] Final Prec@1 49.8600%
07/03 08:00:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][50/391]	Step 7038	Loss 1.7994	Prec@(1,5) (50.6%, 81.9%)
07/03 08:00:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][100/391]	Step 7038	Loss 1.8006	Prec@(1,5) (50.3%, 81.5%)
07/03 08:00:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][150/391]	Step 7038	Loss 1.8156	Prec@(1,5) (50.1%, 81.4%)
07/03 08:00:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][200/391]	Step 7038	Loss 1.8084	Prec@(1,5) (50.2%, 81.5%)
07/03 08:00:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][250/391]	Step 7038	Loss 1.7997	Prec@(1,5) (50.6%, 81.5%)
07/03 08:00:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][300/391]	Step 7038	Loss 1.8104	Prec@(1,5) (50.2%, 81.3%)
07/03 08:00:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][350/391]	Step 7038	Loss 1.8089	Prec@(1,5) (50.1%, 81.4%)
07/03 08:00:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [8][390/391]	Step 7038	Loss 1.8063	Prec@(1,5) (50.2%, 81.5%)
07/03 08:00:53午後 evaluateCell_trainer.py:208 [INFO] Valid: [  8/99] Final Prec@1 50.1480%
07/03 08:00:54午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 50.1480%
07/03 08:01:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][50/781]	Step 7088	lr 0.02452	Loss 2.5525 (2.4987)	Prec@(1,5) (51.8%, 82.9%)	
07/03 08:01:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][100/781]	Step 7138	lr 0.02452	Loss 2.5297 (2.5380)	Prec@(1,5) (51.4%, 82.5%)	
07/03 08:01:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][150/781]	Step 7188	lr 0.02452	Loss 2.7424 (2.5041)	Prec@(1,5) (51.8%, 82.7%)	
07/03 08:01:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][200/781]	Step 7238	lr 0.02452	Loss 2.2847 (2.4969)	Prec@(1,5) (51.8%, 82.8%)	
07/03 08:01:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][250/781]	Step 7288	lr 0.02452	Loss 2.7227 (2.4983)	Prec@(1,5) (51.9%, 82.6%)	
07/03 08:02:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][300/781]	Step 7338	lr 0.02452	Loss 2.3746 (2.5059)	Prec@(1,5) (51.7%, 82.5%)	
07/03 08:02:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][350/781]	Step 7388	lr 0.02452	Loss 2.4059 (2.5186)	Prec@(1,5) (51.6%, 82.2%)	
07/03 08:02:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][400/781]	Step 7438	lr 0.02452	Loss 2.4595 (2.5131)	Prec@(1,5) (51.7%, 82.2%)	
07/03 08:02:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][450/781]	Step 7488	lr 0.02452	Loss 2.7171 (2.5087)	Prec@(1,5) (51.8%, 82.3%)	
07/03 08:02:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][500/781]	Step 7538	lr 0.02452	Loss 2.7939 (2.5090)	Prec@(1,5) (51.8%, 82.2%)	
07/03 08:02:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][550/781]	Step 7588	lr 0.02452	Loss 2.5904 (2.5116)	Prec@(1,5) (51.7%, 82.2%)	
07/03 08:03:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][600/781]	Step 7638	lr 0.02452	Loss 2.3825 (2.5125)	Prec@(1,5) (51.8%, 82.1%)	
07/03 08:03:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][650/781]	Step 7688	lr 0.02452	Loss 2.0509 (2.5154)	Prec@(1,5) (51.7%, 82.1%)	
07/03 08:03:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][700/781]	Step 7738	lr 0.02452	Loss 2.1068 (2.5144)	Prec@(1,5) (51.7%, 82.1%)	
07/03 08:03:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][750/781]	Step 7788	lr 0.02452	Loss 3.3622 (2.5166)	Prec@(1,5) (51.6%, 82.1%)	
07/03 08:03:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [9][781/781]	Step 7819	lr 0.02452	Loss 2.3740 (2.5145)	Prec@(1,5) (51.7%, 82.1%)	
07/03 08:03:51午後 evaluateCell_trainer.py:172 [INFO] Train: [  9/99] Final Prec@1 51.6620%
07/03 08:03:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][50/391]	Step 7820	Loss 1.8117	Prec@(1,5) (50.9%, 81.0%)
07/03 08:03:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][100/391]	Step 7820	Loss 1.8163	Prec@(1,5) (50.4%, 81.3%)
07/03 08:04:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][150/391]	Step 7820	Loss 1.8097	Prec@(1,5) (50.8%, 81.2%)
07/03 08:04:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][200/391]	Step 7820	Loss 1.8061	Prec@(1,5) (50.6%, 81.3%)
07/03 08:04:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][250/391]	Step 7820	Loss 1.8056	Prec@(1,5) (50.6%, 81.2%)
07/03 08:04:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][300/391]	Step 7820	Loss 1.8070	Prec@(1,5) (50.7%, 81.2%)
07/03 08:04:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][350/391]	Step 7820	Loss 1.8076	Prec@(1,5) (50.5%, 81.1%)
07/03 08:04:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [9][390/391]	Step 7820	Loss 1.8089	Prec@(1,5) (50.6%, 81.0%)
07/03 08:04:16午後 evaluateCell_trainer.py:208 [INFO] Valid: [  9/99] Final Prec@1 50.6200%
07/03 08:04:17午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 50.6200%
07/03 08:04:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][50/781]	Step 7870	lr 0.02441	Loss 2.1449 (2.4522)	Prec@(1,5) (53.3%, 83.3%)	
07/03 08:04:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][100/781]	Step 7920	lr 0.02441	Loss 2.3624 (2.4656)	Prec@(1,5) (52.4%, 83.0%)	
07/03 08:04:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][150/781]	Step 7970	lr 0.02441	Loss 2.1610 (2.4498)	Prec@(1,5) (52.7%, 83.2%)	
07/03 08:05:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][200/781]	Step 8020	lr 0.02441	Loss 2.1069 (2.4418)	Prec@(1,5) (52.7%, 83.3%)	
07/03 08:05:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][250/781]	Step 8070	lr 0.02441	Loss 2.1210 (2.4491)	Prec@(1,5) (52.7%, 83.3%)	
07/03 08:05:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][300/781]	Step 8120	lr 0.02441	Loss 2.6654 (2.4447)	Prec@(1,5) (52.6%, 83.3%)	
07/03 08:05:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][350/781]	Step 8170	lr 0.02441	Loss 2.3104 (2.4509)	Prec@(1,5) (52.5%, 83.3%)	
07/03 08:05:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][400/781]	Step 8220	lr 0.02441	Loss 2.7460 (2.4509)	Prec@(1,5) (52.6%, 83.3%)	
07/03 08:05:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][450/781]	Step 8270	lr 0.02441	Loss 2.5629 (2.4487)	Prec@(1,5) (52.7%, 83.3%)	
07/03 08:06:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][500/781]	Step 8320	lr 0.02441	Loss 2.5121 (2.4444)	Prec@(1,5) (52.7%, 83.4%)	
07/03 08:06:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][550/781]	Step 8370	lr 0.02441	Loss 2.4084 (2.4438)	Prec@(1,5) (52.8%, 83.4%)	
07/03 08:06:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][600/781]	Step 8420	lr 0.02441	Loss 2.6720 (2.4411)	Prec@(1,5) (52.9%, 83.4%)	
07/03 08:06:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][650/781]	Step 8470	lr 0.02441	Loss 2.5616 (2.4431)	Prec@(1,5) (52.8%, 83.3%)	
07/03 08:06:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][700/781]	Step 8520	lr 0.02441	Loss 2.1488 (2.4397)	Prec@(1,5) (52.9%, 83.5%)	
07/03 08:07:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][750/781]	Step 8570	lr 0.02441	Loss 2.8721 (2.4427)	Prec@(1,5) (52.9%, 83.4%)	
07/03 08:07:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [10][781/781]	Step 8601	lr 0.02441	Loss 2.3935 (2.4424)	Prec@(1,5) (52.9%, 83.4%)	
07/03 08:07:14午後 evaluateCell_trainer.py:172 [INFO] Train: [ 10/99] Final Prec@1 52.8700%
07/03 08:07:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][50/391]	Step 8602	Loss 1.7436	Prec@(1,5) (51.2%, 82.7%)
07/03 08:07:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][100/391]	Step 8602	Loss 1.7511	Prec@(1,5) (50.8%, 82.4%)
07/03 08:07:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][150/391]	Step 8602	Loss 1.7618	Prec@(1,5) (50.9%, 81.9%)
07/03 08:07:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][200/391]	Step 8602	Loss 1.7622	Prec@(1,5) (51.1%, 82.0%)
07/03 08:07:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][250/391]	Step 8602	Loss 1.7666	Prec@(1,5) (51.2%, 81.9%)
07/03 08:07:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][300/391]	Step 8602	Loss 1.7686	Prec@(1,5) (51.2%, 81.7%)
07/03 08:07:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][350/391]	Step 8602	Loss 1.7651	Prec@(1,5) (51.3%, 81.9%)
07/03 08:07:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [10][390/391]	Step 8602	Loss 1.7636	Prec@(1,5) (51.4%, 81.9%)
07/03 08:07:39午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 10/99] Final Prec@1 51.4040%
07/03 08:07:39午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 51.4040%
07/03 08:07:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][50/781]	Step 8652	lr 0.02429	Loss 3.0145 (2.3468)	Prec@(1,5) (54.5%, 84.7%)	
07/03 08:08:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][100/781]	Step 8702	lr 0.02429	Loss 2.2524 (2.3229)	Prec@(1,5) (54.9%, 84.4%)	
07/03 08:08:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][150/781]	Step 8752	lr 0.02429	Loss 2.5014 (2.3623)	Prec@(1,5) (54.1%, 83.8%)	
07/03 08:08:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][200/781]	Step 8802	lr 0.02429	Loss 1.7460 (2.3496)	Prec@(1,5) (54.5%, 84.1%)	
07/03 08:08:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][250/781]	Step 8852	lr 0.02429	Loss 2.3255 (2.3400)	Prec@(1,5) (54.6%, 84.2%)	
07/03 08:08:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][300/781]	Step 8902	lr 0.02429	Loss 2.6732 (2.3343)	Prec@(1,5) (54.7%, 84.3%)	
07/03 08:08:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][350/781]	Step 8952	lr 0.02429	Loss 2.3074 (2.3411)	Prec@(1,5) (54.7%, 84.3%)	
07/03 08:09:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][400/781]	Step 9002	lr 0.02429	Loss 2.4608 (2.3373)	Prec@(1,5) (54.7%, 84.3%)	
07/03 08:09:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][450/781]	Step 9052	lr 0.02429	Loss 2.1536 (2.3366)	Prec@(1,5) (54.8%, 84.4%)	
07/03 08:09:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][500/781]	Step 9102	lr 0.02429	Loss 2.0426 (2.3408)	Prec@(1,5) (54.8%, 84.3%)	
07/03 08:09:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][550/781]	Step 9152	lr 0.02429	Loss 2.3089 (2.3414)	Prec@(1,5) (54.7%, 84.3%)	
07/03 08:09:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][600/781]	Step 9202	lr 0.02429	Loss 2.3969 (2.3463)	Prec@(1,5) (54.6%, 84.3%)	
07/03 08:10:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][650/781]	Step 9252	lr 0.02429	Loss 2.7207 (2.3484)	Prec@(1,5) (54.6%, 84.3%)	
07/03 08:10:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][700/781]	Step 9302	lr 0.02429	Loss 2.9832 (2.3524)	Prec@(1,5) (54.6%, 84.2%)	
07/03 08:10:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][750/781]	Step 9352	lr 0.02429	Loss 2.4942 (2.3557)	Prec@(1,5) (54.5%, 84.1%)	
07/03 08:10:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [11][781/781]	Step 9383	lr 0.02429	Loss 2.5097 (2.3571)	Prec@(1,5) (54.5%, 84.1%)	
07/03 08:10:37午後 evaluateCell_trainer.py:172 [INFO] Train: [ 11/99] Final Prec@1 54.4900%
07/03 08:10:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][50/391]	Step 9384	Loss 1.8048	Prec@(1,5) (51.0%, 81.3%)
07/03 08:10:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][100/391]	Step 9384	Loss 1.7890	Prec@(1,5) (50.6%, 81.6%)
07/03 08:10:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][150/391]	Step 9384	Loss 1.7958	Prec@(1,5) (50.1%, 81.6%)
07/03 08:10:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][200/391]	Step 9384	Loss 1.8122	Prec@(1,5) (50.1%, 81.3%)
07/03 08:10:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][250/391]	Step 9384	Loss 1.8105	Prec@(1,5) (50.0%, 81.3%)
07/03 08:10:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][300/391]	Step 9384	Loss 1.8140	Prec@(1,5) (50.0%, 81.2%)
07/03 08:10:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][350/391]	Step 9384	Loss 1.8153	Prec@(1,5) (50.2%, 81.3%)
07/03 08:11:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [11][390/391]	Step 9384	Loss 1.8120	Prec@(1,5) (50.2%, 81.3%)
07/03 08:11:02午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 11/99] Final Prec@1 50.1560%
07/03 08:11:02午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 51.4040%
07/03 08:11:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][50/781]	Step 9434	lr 0.02416	Loss 2.6312 (2.2410)	Prec@(1,5) (56.6%, 85.7%)	
07/03 08:11:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][100/781]	Step 9484	lr 0.02416	Loss 2.3013 (2.2475)	Prec@(1,5) (56.1%, 86.0%)	
07/03 08:11:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][150/781]	Step 9534	lr 0.02416	Loss 2.2621 (2.2609)	Prec@(1,5) (56.0%, 85.7%)	
07/03 08:11:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][200/781]	Step 9584	lr 0.02416	Loss 2.4612 (2.2675)	Prec@(1,5) (56.1%, 85.5%)	
07/03 08:11:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][250/781]	Step 9634	lr 0.02416	Loss 1.9717 (2.2674)	Prec@(1,5) (56.1%, 85.5%)	
07/03 08:12:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][300/781]	Step 9684	lr 0.02416	Loss 2.2231 (2.2694)	Prec@(1,5) (56.2%, 85.5%)	
07/03 08:12:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][350/781]	Step 9734	lr 0.02416	Loss 1.9821 (2.2775)	Prec@(1,5) (56.0%, 85.4%)	
07/03 08:12:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][400/781]	Step 9784	lr 0.02416	Loss 1.7843 (2.2816)	Prec@(1,5) (55.9%, 85.4%)	
07/03 08:12:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][450/781]	Step 9834	lr 0.02416	Loss 2.3926 (2.2854)	Prec@(1,5) (55.9%, 85.2%)	
07/03 08:12:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][500/781]	Step 9884	lr 0.02416	Loss 2.7934 (2.2853)	Prec@(1,5) (55.9%, 85.1%)	
07/03 08:13:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][550/781]	Step 9934	lr 0.02416	Loss 2.4949 (2.2944)	Prec@(1,5) (55.7%, 85.0%)	
07/03 08:13:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][600/781]	Step 9984	lr 0.02416	Loss 2.5831 (2.2922)	Prec@(1,5) (55.7%, 85.1%)	
07/03 08:13:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][650/781]	Step 10034	lr 0.02416	Loss 2.2089 (2.2895)	Prec@(1,5) (55.8%, 85.1%)	
07/03 08:13:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][700/781]	Step 10084	lr 0.02416	Loss 2.4631 (2.2912)	Prec@(1,5) (55.8%, 85.1%)	
07/03 08:13:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][750/781]	Step 10134	lr 0.02416	Loss 2.2413 (2.2945)	Prec@(1,5) (55.7%, 85.0%)	
07/03 08:14:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [12][781/781]	Step 10165	lr 0.02416	Loss 2.6167 (2.2942)	Prec@(1,5) (55.7%, 85.0%)	
07/03 08:14:00午後 evaluateCell_trainer.py:172 [INFO] Train: [ 12/99] Final Prec@1 55.6900%
07/03 08:14:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][50/391]	Step 10166	Loss 1.6189	Prec@(1,5) (54.9%, 85.1%)
07/03 08:14:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][100/391]	Step 10166	Loss 1.6023	Prec@(1,5) (55.0%, 84.9%)
07/03 08:14:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][150/391]	Step 10166	Loss 1.5921	Prec@(1,5) (55.6%, 84.8%)
07/03 08:14:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][200/391]	Step 10166	Loss 1.5845	Prec@(1,5) (55.7%, 84.9%)
07/03 08:14:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][250/391]	Step 10166	Loss 1.5819	Prec@(1,5) (56.0%, 84.9%)
07/03 08:14:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][300/391]	Step 10166	Loss 1.5849	Prec@(1,5) (55.7%, 84.8%)
07/03 08:14:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][350/391]	Step 10166	Loss 1.5828	Prec@(1,5) (55.7%, 84.9%)
07/03 08:14:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [12][390/391]	Step 10166	Loss 1.5866	Prec@(1,5) (55.6%, 84.8%)
07/03 08:14:25午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 12/99] Final Prec@1 55.6320%
07/03 08:14:26午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 55.6320%
07/03 08:14:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][50/781]	Step 10216	lr 0.02401	Loss 1.9990 (2.2097)	Prec@(1,5) (57.1%, 85.7%)	
07/03 08:14:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][100/781]	Step 10266	lr 0.02401	Loss 1.6799 (2.1965)	Prec@(1,5) (56.9%, 85.9%)	
07/03 08:15:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][150/781]	Step 10316	lr 0.02401	Loss 2.4545 (2.1975)	Prec@(1,5) (56.9%, 86.2%)	
07/03 08:15:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][200/781]	Step 10366	lr 0.02401	Loss 2.2667 (2.2133)	Prec@(1,5) (56.9%, 85.7%)	
07/03 08:15:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][250/781]	Step 10416	lr 0.02401	Loss 2.3409 (2.2128)	Prec@(1,5) (57.0%, 85.7%)	
07/03 08:15:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][300/781]	Step 10466	lr 0.02401	Loss 2.6533 (2.2127)	Prec@(1,5) (56.9%, 85.8%)	
07/03 08:15:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][350/781]	Step 10516	lr 0.02401	Loss 1.5468 (2.2104)	Prec@(1,5) (56.7%, 85.9%)	
07/03 08:15:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][400/781]	Step 10566	lr 0.02401	Loss 1.8082 (2.2154)	Prec@(1,5) (56.8%, 85.8%)	
07/03 08:16:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][450/781]	Step 10616	lr 0.02401	Loss 2.0678 (2.2181)	Prec@(1,5) (56.7%, 85.7%)	
07/03 08:16:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][500/781]	Step 10666	lr 0.02401	Loss 1.6624 (2.2251)	Prec@(1,5) (56.6%, 85.6%)	
07/03 08:16:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][550/781]	Step 10716	lr 0.02401	Loss 2.1104 (2.2220)	Prec@(1,5) (56.7%, 85.7%)	
07/03 08:16:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][600/781]	Step 10766	lr 0.02401	Loss 2.3054 (2.2299)	Prec@(1,5) (56.6%, 85.7%)	
07/03 08:16:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][650/781]	Step 10816	lr 0.02401	Loss 2.5009 (2.2350)	Prec@(1,5) (56.5%, 85.6%)	
07/03 08:17:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][700/781]	Step 10866	lr 0.02401	Loss 2.6045 (2.2350)	Prec@(1,5) (56.5%, 85.6%)	
07/03 08:17:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][750/781]	Step 10916	lr 0.02401	Loss 1.8162 (2.2376)	Prec@(1,5) (56.5%, 85.6%)	
07/03 08:17:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [13][781/781]	Step 10947	lr 0.02401	Loss 2.5975 (2.2410)	Prec@(1,5) (56.4%, 85.5%)	
07/03 08:17:23午後 evaluateCell_trainer.py:172 [INFO] Train: [ 13/99] Final Prec@1 56.3500%
07/03 08:17:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][50/391]	Step 10948	Loss 1.6831	Prec@(1,5) (53.7%, 82.7%)
07/03 08:17:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][100/391]	Step 10948	Loss 1.6885	Prec@(1,5) (53.1%, 83.0%)
07/03 08:17:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][150/391]	Step 10948	Loss 1.6942	Prec@(1,5) (52.7%, 83.1%)
07/03 08:17:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][200/391]	Step 10948	Loss 1.6877	Prec@(1,5) (52.8%, 83.1%)
07/03 08:17:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][250/391]	Step 10948	Loss 1.6883	Prec@(1,5) (53.2%, 83.0%)
07/03 08:17:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][300/391]	Step 10948	Loss 1.6903	Prec@(1,5) (53.1%, 83.0%)
07/03 08:17:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][350/391]	Step 10948	Loss 1.6935	Prec@(1,5) (53.0%, 83.0%)
07/03 08:17:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [13][390/391]	Step 10948	Loss 1.6954	Prec@(1,5) (53.0%, 82.9%)
07/03 08:17:48午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 13/99] Final Prec@1 53.0360%
07/03 08:17:48午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 55.6320%
07/03 08:18:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][50/781]	Step 10998	lr 0.02386	Loss 2.0680 (2.0585)	Prec@(1,5) (60.2%, 87.6%)	
07/03 08:18:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][100/781]	Step 11048	lr 0.02386	Loss 2.1807 (2.1174)	Prec@(1,5) (59.3%, 86.6%)	
07/03 08:18:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][150/781]	Step 11098	lr 0.02386	Loss 2.5069 (2.1309)	Prec@(1,5) (59.1%, 86.5%)	
07/03 08:18:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][200/781]	Step 11148	lr 0.02386	Loss 1.8384 (2.1402)	Prec@(1,5) (58.9%, 86.3%)	
07/03 08:18:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][250/781]	Step 11198	lr 0.02386	Loss 2.4325 (2.1526)	Prec@(1,5) (58.5%, 86.2%)	
07/03 08:18:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][300/781]	Step 11248	lr 0.02386	Loss 2.1895 (2.1450)	Prec@(1,5) (58.7%, 86.3%)	
07/03 08:19:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][350/781]	Step 11298	lr 0.02386	Loss 2.5286 (2.1500)	Prec@(1,5) (58.6%, 86.2%)	
07/03 08:19:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][400/781]	Step 11348	lr 0.02386	Loss 2.7005 (2.1492)	Prec@(1,5) (58.6%, 86.4%)	
07/03 08:19:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][450/781]	Step 11398	lr 0.02386	Loss 2.3555 (2.1496)	Prec@(1,5) (58.6%, 86.5%)	
07/03 08:19:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][500/781]	Step 11448	lr 0.02386	Loss 2.3087 (2.1568)	Prec@(1,5) (58.4%, 86.4%)	
07/03 08:19:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][550/781]	Step 11498	lr 0.02386	Loss 1.9202 (2.1640)	Prec@(1,5) (58.1%, 86.4%)	
07/03 08:20:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][600/781]	Step 11548	lr 0.02386	Loss 2.3433 (2.1704)	Prec@(1,5) (58.1%, 86.2%)	
07/03 08:20:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][650/781]	Step 11598	lr 0.02386	Loss 1.9518 (2.1698)	Prec@(1,5) (58.1%, 86.2%)	
07/03 08:20:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][700/781]	Step 11648	lr 0.02386	Loss 2.3275 (2.1755)	Prec@(1,5) (58.0%, 86.2%)	
07/03 08:20:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][750/781]	Step 11698	lr 0.02386	Loss 2.0443 (2.1844)	Prec@(1,5) (57.8%, 86.1%)	
07/03 08:20:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [14][781/781]	Step 11729	lr 0.02386	Loss 2.0774 (2.1840)	Prec@(1,5) (57.7%, 86.1%)	
07/03 08:20:45午後 evaluateCell_trainer.py:172 [INFO] Train: [ 14/99] Final Prec@1 57.7300%
07/03 08:20:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][50/391]	Step 11730	Loss 1.4608	Prec@(1,5) (57.7%, 86.7%)
07/03 08:20:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][100/391]	Step 11730	Loss 1.4674	Prec@(1,5) (57.9%, 87.1%)
07/03 08:20:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][150/391]	Step 11730	Loss 1.4704	Prec@(1,5) (58.1%, 86.8%)
07/03 08:20:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][200/391]	Step 11730	Loss 1.4725	Prec@(1,5) (57.9%, 86.8%)
07/03 08:21:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][250/391]	Step 11730	Loss 1.4818	Prec@(1,5) (57.7%, 86.6%)
07/03 08:21:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][300/391]	Step 11730	Loss 1.4864	Prec@(1,5) (57.6%, 86.5%)
07/03 08:21:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][350/391]	Step 11730	Loss 1.4883	Prec@(1,5) (57.6%, 86.5%)
07/03 08:21:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [14][390/391]	Step 11730	Loss 1.4847	Prec@(1,5) (57.6%, 86.6%)
07/03 08:21:11午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 14/99] Final Prec@1 57.6040%
07/03 08:21:11午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 57.6040%
07/03 08:21:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][50/781]	Step 11780	lr 0.02369	Loss 2.3657 (2.0582)	Prec@(1,5) (59.6%, 87.6%)	
07/03 08:21:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][100/781]	Step 11830	lr 0.02369	Loss 2.0489 (2.0843)	Prec@(1,5) (59.4%, 87.5%)	
07/03 08:21:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][150/781]	Step 11880	lr 0.02369	Loss 1.8605 (2.0660)	Prec@(1,5) (59.7%, 87.6%)	
07/03 08:21:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][200/781]	Step 11930	lr 0.02369	Loss 1.9957 (2.0762)	Prec@(1,5) (59.6%, 87.4%)	
07/03 08:22:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][250/781]	Step 11980	lr 0.02369	Loss 1.9684 (2.0826)	Prec@(1,5) (59.5%, 87.4%)	
07/03 08:22:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][300/781]	Step 12030	lr 0.02369	Loss 1.9139 (2.0823)	Prec@(1,5) (59.5%, 87.4%)	
07/03 08:22:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][350/781]	Step 12080	lr 0.02369	Loss 2.2637 (2.1033)	Prec@(1,5) (59.1%, 87.1%)	
07/03 08:22:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][400/781]	Step 12130	lr 0.02369	Loss 2.0882 (2.1103)	Prec@(1,5) (59.0%, 87.0%)	
07/03 08:22:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][450/781]	Step 12180	lr 0.02369	Loss 2.4480 (2.1234)	Prec@(1,5) (58.7%, 86.8%)	
07/03 08:23:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][500/781]	Step 12230	lr 0.02369	Loss 2.2982 (2.1319)	Prec@(1,5) (58.6%, 86.6%)	
07/03 08:23:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][550/781]	Step 12280	lr 0.02369	Loss 2.1773 (2.1369)	Prec@(1,5) (58.5%, 86.6%)	
07/03 08:23:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][600/781]	Step 12330	lr 0.02369	Loss 2.3871 (2.1359)	Prec@(1,5) (58.5%, 86.6%)	
07/03 08:23:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][650/781]	Step 12380	lr 0.02369	Loss 2.2588 (2.1377)	Prec@(1,5) (58.5%, 86.6%)	
07/03 08:23:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][700/781]	Step 12430	lr 0.02369	Loss 2.7269 (2.1344)	Prec@(1,5) (58.5%, 86.6%)	
07/03 08:24:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][750/781]	Step 12480	lr 0.02369	Loss 2.3986 (2.1381)	Prec@(1,5) (58.4%, 86.6%)	
07/03 08:24:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [15][781/781]	Step 12511	lr 0.02369	Loss 1.8431 (2.1398)	Prec@(1,5) (58.4%, 86.6%)	
07/03 08:24:09午後 evaluateCell_trainer.py:172 [INFO] Train: [ 15/99] Final Prec@1 58.4020%
07/03 08:24:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][50/391]	Step 12512	Loss 1.4851	Prec@(1,5) (57.7%, 87.0%)
07/03 08:24:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][100/391]	Step 12512	Loss 1.5298	Prec@(1,5) (56.8%, 85.9%)
07/03 08:24:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][150/391]	Step 12512	Loss 1.5304	Prec@(1,5) (56.4%, 86.0%)
07/03 08:24:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][200/391]	Step 12512	Loss 1.5243	Prec@(1,5) (56.4%, 86.1%)
07/03 08:24:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][250/391]	Step 12512	Loss 1.5163	Prec@(1,5) (56.7%, 86.1%)
07/03 08:24:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][300/391]	Step 12512	Loss 1.5238	Prec@(1,5) (56.7%, 85.9%)
07/03 08:24:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][350/391]	Step 12512	Loss 1.5181	Prec@(1,5) (56.8%, 86.1%)
07/03 08:24:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [15][390/391]	Step 12512	Loss 1.5171	Prec@(1,5) (57.0%, 86.0%)
07/03 08:24:33午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 15/99] Final Prec@1 56.9640%
07/03 08:24:33午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 57.6040%
07/03 08:24:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][50/781]	Step 12562	lr 0.02352	Loss 2.2143 (2.0170)	Prec@(1,5) (60.8%, 88.1%)	
07/03 08:24:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][100/781]	Step 12612	lr 0.02352	Loss 2.1237 (2.0332)	Prec@(1,5) (60.3%, 88.1%)	
07/03 08:25:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][150/781]	Step 12662	lr 0.02352	Loss 1.8456 (2.0440)	Prec@(1,5) (60.5%, 87.8%)	
07/03 08:25:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][200/781]	Step 12712	lr 0.02352	Loss 2.2079 (2.0576)	Prec@(1,5) (60.1%, 87.7%)	
07/03 08:25:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][250/781]	Step 12762	lr 0.02352	Loss 1.8371 (2.0738)	Prec@(1,5) (59.6%, 87.5%)	
07/03 08:25:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][300/781]	Step 12812	lr 0.02352	Loss 1.9385 (2.0763)	Prec@(1,5) (59.5%, 87.5%)	
07/03 08:25:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][350/781]	Step 12862	lr 0.02352	Loss 1.5904 (2.0709)	Prec@(1,5) (59.7%, 87.5%)	
07/03 08:26:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][400/781]	Step 12912	lr 0.02352	Loss 1.8944 (2.0760)	Prec@(1,5) (59.6%, 87.5%)	
07/03 08:26:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][450/781]	Step 12962	lr 0.02352	Loss 2.0598 (2.0775)	Prec@(1,5) (59.5%, 87.5%)	
07/03 08:26:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][500/781]	Step 13012	lr 0.02352	Loss 2.6276 (2.0906)	Prec@(1,5) (59.3%, 87.3%)	
07/03 08:26:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][550/781]	Step 13062	lr 0.02352	Loss 2.5097 (2.0935)	Prec@(1,5) (59.2%, 87.3%)	
07/03 08:26:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][600/781]	Step 13112	lr 0.02352	Loss 1.8870 (2.0903)	Prec@(1,5) (59.3%, 87.3%)	
07/03 08:27:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][650/781]	Step 13162	lr 0.02352	Loss 2.2201 (2.0928)	Prec@(1,5) (59.3%, 87.3%)	
07/03 08:27:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][700/781]	Step 13212	lr 0.02352	Loss 2.1071 (2.0965)	Prec@(1,5) (59.2%, 87.2%)	
07/03 08:27:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][750/781]	Step 13262	lr 0.02352	Loss 2.2323 (2.1002)	Prec@(1,5) (59.2%, 87.1%)	
07/03 08:27:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [16][781/781]	Step 13293	lr 0.02352	Loss 2.0607 (2.1012)	Prec@(1,5) (59.1%, 87.1%)	
07/03 08:27:32午後 evaluateCell_trainer.py:172 [INFO] Train: [ 16/99] Final Prec@1 59.1180%
07/03 08:27:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][50/391]	Step 13294	Loss 1.4136	Prec@(1,5) (59.7%, 88.0%)
07/03 08:27:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][100/391]	Step 13294	Loss 1.4474	Prec@(1,5) (59.2%, 87.1%)
07/03 08:27:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][150/391]	Step 13294	Loss 1.4551	Prec@(1,5) (58.6%, 87.0%)
07/03 08:27:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][200/391]	Step 13294	Loss 1.4495	Prec@(1,5) (58.9%, 87.1%)
07/03 08:27:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][250/391]	Step 13294	Loss 1.4450	Prec@(1,5) (58.7%, 87.1%)
07/03 08:27:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][300/391]	Step 13294	Loss 1.4414	Prec@(1,5) (58.9%, 87.1%)
07/03 08:27:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][350/391]	Step 13294	Loss 1.4415	Prec@(1,5) (58.9%, 87.1%)
07/03 08:27:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [16][390/391]	Step 13294	Loss 1.4396	Prec@(1,5) (58.9%, 87.2%)
07/03 08:27:57午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 16/99] Final Prec@1 58.9120%
07/03 08:27:57午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 58.9120%
07/03 08:28:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][50/781]	Step 13344	lr 0.02333	Loss 1.5828 (1.9442)	Prec@(1,5) (62.6%, 88.8%)	
07/03 08:28:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][100/781]	Step 13394	lr 0.02333	Loss 1.5688 (1.9206)	Prec@(1,5) (62.4%, 89.2%)	
07/03 08:28:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][150/781]	Step 13444	lr 0.02333	Loss 2.2338 (1.9710)	Prec@(1,5) (61.5%, 88.7%)	
07/03 08:28:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][200/781]	Step 13494	lr 0.02333	Loss 1.9736 (1.9988)	Prec@(1,5) (61.0%, 88.4%)	
07/03 08:28:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][250/781]	Step 13544	lr 0.02333	Loss 2.1745 (2.0137)	Prec@(1,5) (60.7%, 88.2%)	
07/03 08:29:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][300/781]	Step 13594	lr 0.02333	Loss 1.9220 (2.0224)	Prec@(1,5) (60.6%, 88.2%)	
07/03 08:29:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][350/781]	Step 13644	lr 0.02333	Loss 2.1704 (2.0198)	Prec@(1,5) (60.7%, 88.1%)	
07/03 08:29:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][400/781]	Step 13694	lr 0.02333	Loss 2.1487 (2.0228)	Prec@(1,5) (60.6%, 88.1%)	
07/03 08:29:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][450/781]	Step 13744	lr 0.02333	Loss 2.3401 (2.0322)	Prec@(1,5) (60.5%, 87.9%)	
07/03 08:29:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][500/781]	Step 13794	lr 0.02333	Loss 1.9246 (2.0343)	Prec@(1,5) (60.4%, 87.9%)	
07/03 08:30:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][550/781]	Step 13844	lr 0.02333	Loss 2.4768 (2.0415)	Prec@(1,5) (60.3%, 87.8%)	
07/03 08:30:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][600/781]	Step 13894	lr 0.02333	Loss 1.9030 (2.0424)	Prec@(1,5) (60.2%, 87.8%)	
07/03 08:30:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][650/781]	Step 13944	lr 0.02333	Loss 2.7064 (2.0449)	Prec@(1,5) (60.3%, 87.8%)	
07/03 08:30:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][700/781]	Step 13994	lr 0.02333	Loss 2.2678 (2.0448)	Prec@(1,5) (60.3%, 87.8%)	
07/03 08:30:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][750/781]	Step 14044	lr 0.02333	Loss 1.8659 (2.0492)	Prec@(1,5) (60.2%, 87.7%)	
07/03 08:30:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [17][781/781]	Step 14075	lr 0.02333	Loss 2.2967 (2.0522)	Prec@(1,5) (60.0%, 87.7%)	
07/03 08:30:54午後 evaluateCell_trainer.py:172 [INFO] Train: [ 17/99] Final Prec@1 60.0360%
07/03 08:30:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][50/391]	Step 14076	Loss 1.3831	Prec@(1,5) (60.4%, 88.0%)
07/03 08:31:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][100/391]	Step 14076	Loss 1.3932	Prec@(1,5) (60.7%, 87.5%)
07/03 08:31:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][150/391]	Step 14076	Loss 1.4106	Prec@(1,5) (60.1%, 87.3%)
07/03 08:31:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][200/391]	Step 14076	Loss 1.4121	Prec@(1,5) (60.0%, 87.4%)
07/03 08:31:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][250/391]	Step 14076	Loss 1.4205	Prec@(1,5) (60.0%, 87.3%)
07/03 08:31:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][300/391]	Step 14076	Loss 1.4143	Prec@(1,5) (60.1%, 87.3%)
07/03 08:31:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][350/391]	Step 14076	Loss 1.4157	Prec@(1,5) (60.1%, 87.4%)
07/03 08:31:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [17][390/391]	Step 14076	Loss 1.4152	Prec@(1,5) (60.1%, 87.4%)
07/03 08:31:19午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 17/99] Final Prec@1 60.0880%
07/03 08:31:20午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 60.0880%
07/03 08:31:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][50/781]	Step 14126	lr 0.02313	Loss 1.6457 (1.9165)	Prec@(1,5) (62.6%, 89.0%)	
07/03 08:31:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][100/781]	Step 14176	lr 0.02313	Loss 2.2566 (1.9675)	Prec@(1,5) (61.5%, 88.5%)	
07/03 08:31:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][150/781]	Step 14226	lr 0.02313	Loss 1.7582 (1.9680)	Prec@(1,5) (61.4%, 88.6%)	
07/03 08:32:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][200/781]	Step 14276	lr 0.02313	Loss 1.9751 (1.9761)	Prec@(1,5) (61.1%, 88.9%)	
07/03 08:32:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][250/781]	Step 14326	lr 0.02313	Loss 1.9827 (1.9780)	Prec@(1,5) (61.1%, 88.6%)	
07/03 08:32:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][300/781]	Step 14376	lr 0.02313	Loss 2.1923 (1.9791)	Prec@(1,5) (61.2%, 88.6%)	
07/03 08:32:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][350/781]	Step 14426	lr 0.02313	Loss 2.4685 (1.9942)	Prec@(1,5) (61.0%, 88.5%)	
07/03 08:32:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][400/781]	Step 14476	lr 0.02313	Loss 1.5720 (1.9987)	Prec@(1,5) (60.9%, 88.3%)	
07/03 08:33:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][450/781]	Step 14526	lr 0.02313	Loss 1.6175 (1.9949)	Prec@(1,5) (61.0%, 88.4%)	
07/03 08:33:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][500/781]	Step 14576	lr 0.02313	Loss 1.8314 (1.9963)	Prec@(1,5) (61.0%, 88.4%)	
07/03 08:33:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][550/781]	Step 14626	lr 0.02313	Loss 1.8932 (1.9999)	Prec@(1,5) (61.0%, 88.4%)	
07/03 08:33:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][600/781]	Step 14676	lr 0.02313	Loss 2.2832 (2.0007)	Prec@(1,5) (60.9%, 88.3%)	
07/03 08:33:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][650/781]	Step 14726	lr 0.02313	Loss 1.4219 (2.0042)	Prec@(1,5) (60.9%, 88.3%)	
07/03 08:33:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][700/781]	Step 14776	lr 0.02313	Loss 1.9084 (2.0068)	Prec@(1,5) (60.8%, 88.2%)	
07/03 08:34:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][750/781]	Step 14826	lr 0.02313	Loss 1.8676 (2.0079)	Prec@(1,5) (60.8%, 88.2%)	
07/03 08:34:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [18][781/781]	Step 14857	lr 0.02313	Loss 1.9087 (2.0128)	Prec@(1,5) (60.7%, 88.1%)	
07/03 08:34:17午後 evaluateCell_trainer.py:172 [INFO] Train: [ 18/99] Final Prec@1 60.6520%
07/03 08:34:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][50/391]	Step 14858	Loss 1.4275	Prec@(1,5) (58.5%, 87.9%)
07/03 08:34:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][100/391]	Step 14858	Loss 1.4273	Prec@(1,5) (58.8%, 87.2%)
07/03 08:34:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][150/391]	Step 14858	Loss 1.4297	Prec@(1,5) (59.0%, 87.4%)
07/03 08:34:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][200/391]	Step 14858	Loss 1.4284	Prec@(1,5) (59.0%, 87.3%)
07/03 08:34:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][250/391]	Step 14858	Loss 1.4192	Prec@(1,5) (59.3%, 87.5%)
07/03 08:34:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][300/391]	Step 14858	Loss 1.4181	Prec@(1,5) (59.3%, 87.5%)
07/03 08:34:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][350/391]	Step 14858	Loss 1.4188	Prec@(1,5) (59.3%, 87.5%)
07/03 08:34:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [18][390/391]	Step 14858	Loss 1.4144	Prec@(1,5) (59.4%, 87.6%)
07/03 08:34:42午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 18/99] Final Prec@1 59.3600%
07/03 08:34:42午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 60.0880%
07/03 08:34:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][50/781]	Step 14908	lr 0.02292	Loss 1.9354 (1.8455)	Prec@(1,5) (63.7%, 89.2%)	
07/03 08:35:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][100/781]	Step 14958	lr 0.02292	Loss 2.6497 (1.9010)	Prec@(1,5) (62.4%, 89.0%)	
07/03 08:35:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][150/781]	Step 15008	lr 0.02292	Loss 1.3823 (1.9053)	Prec@(1,5) (62.2%, 89.0%)	
07/03 08:35:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][200/781]	Step 15058	lr 0.02292	Loss 1.6286 (1.9128)	Prec@(1,5) (62.3%, 88.9%)	
07/03 08:35:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][250/781]	Step 15108	lr 0.02292	Loss 1.8769 (1.9269)	Prec@(1,5) (62.1%, 88.9%)	
07/03 08:35:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][300/781]	Step 15158	lr 0.02292	Loss 1.9802 (1.9344)	Prec@(1,5) (61.8%, 88.8%)	
07/03 08:36:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][350/781]	Step 15208	lr 0.02292	Loss 1.8979 (1.9449)	Prec@(1,5) (61.8%, 88.7%)	
07/03 08:36:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][400/781]	Step 15258	lr 0.02292	Loss 2.3677 (1.9557)	Prec@(1,5) (61.6%, 88.7%)	
07/03 08:36:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][450/781]	Step 15308	lr 0.02292	Loss 1.9514 (1.9608)	Prec@(1,5) (61.5%, 88.6%)	
07/03 08:36:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][500/781]	Step 15358	lr 0.02292	Loss 2.1909 (1.9668)	Prec@(1,5) (61.4%, 88.4%)	
07/03 08:36:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][550/781]	Step 15408	lr 0.02292	Loss 2.0956 (1.9690)	Prec@(1,5) (61.4%, 88.4%)	
07/03 08:36:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][600/781]	Step 15458	lr 0.02292	Loss 1.9022 (1.9735)	Prec@(1,5) (61.3%, 88.3%)	
07/03 08:37:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][650/781]	Step 15508	lr 0.02292	Loss 2.1306 (1.9798)	Prec@(1,5) (61.2%, 88.2%)	
07/03 08:37:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][700/781]	Step 15558	lr 0.02292	Loss 2.3918 (1.9795)	Prec@(1,5) (61.2%, 88.2%)	
07/03 08:37:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][750/781]	Step 15608	lr 0.02292	Loss 2.1288 (1.9826)	Prec@(1,5) (61.2%, 88.2%)	
07/03 08:37:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [19][781/781]	Step 15639	lr 0.02292	Loss 1.5374 (1.9818)	Prec@(1,5) (61.2%, 88.2%)	
07/03 08:37:40午後 evaluateCell_trainer.py:172 [INFO] Train: [ 19/99] Final Prec@1 61.2340%
07/03 08:37:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][50/391]	Step 15640	Loss 1.3501	Prec@(1,5) (61.7%, 88.6%)
07/03 08:37:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][100/391]	Step 15640	Loss 1.3547	Prec@(1,5) (61.8%, 88.1%)
07/03 08:37:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][150/391]	Step 15640	Loss 1.3570	Prec@(1,5) (61.5%, 88.2%)
07/03 08:37:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][200/391]	Step 15640	Loss 1.3600	Prec@(1,5) (61.7%, 88.2%)
07/03 08:37:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][250/391]	Step 15640	Loss 1.3646	Prec@(1,5) (61.4%, 88.0%)
07/03 08:37:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][300/391]	Step 15640	Loss 1.3670	Prec@(1,5) (61.5%, 87.8%)
07/03 08:38:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][350/391]	Step 15640	Loss 1.3666	Prec@(1,5) (61.3%, 87.9%)
07/03 08:38:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [19][390/391]	Step 15640	Loss 1.3652	Prec@(1,5) (61.3%, 88.0%)
07/03 08:38:05午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 19/99] Final Prec@1 61.2920%
07/03 08:38:05午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 61.2920%
07/03 08:38:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][50/781]	Step 15690	lr 0.02271	Loss 1.6479 (1.8375)	Prec@(1,5) (64.6%, 90.3%)	
07/03 08:38:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][100/781]	Step 15740	lr 0.02271	Loss 1.5843 (1.8752)	Prec@(1,5) (63.4%, 90.1%)	
07/03 08:38:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][150/781]	Step 15790	lr 0.02271	Loss 2.0683 (1.8972)	Prec@(1,5) (63.0%, 89.7%)	
07/03 08:38:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][200/781]	Step 15840	lr 0.02271	Loss 1.9955 (1.9040)	Prec@(1,5) (62.7%, 89.7%)	
07/03 08:39:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][250/781]	Step 15890	lr 0.02271	Loss 1.7879 (1.9113)	Prec@(1,5) (62.6%, 89.4%)	
07/03 08:39:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][300/781]	Step 15940	lr 0.02271	Loss 1.6809 (1.9187)	Prec@(1,5) (62.6%, 89.3%)	
07/03 08:39:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][350/781]	Step 15990	lr 0.02271	Loss 2.1843 (1.9246)	Prec@(1,5) (62.5%, 89.2%)	
07/03 08:39:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][400/781]	Step 16040	lr 0.02271	Loss 2.4225 (1.9267)	Prec@(1,5) (62.3%, 89.1%)	
07/03 08:39:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][450/781]	Step 16090	lr 0.02271	Loss 1.4822 (1.9351)	Prec@(1,5) (62.1%, 89.1%)	
07/03 08:39:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][500/781]	Step 16140	lr 0.02271	Loss 1.5266 (1.9362)	Prec@(1,5) (62.2%, 89.0%)	
07/03 08:40:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][550/781]	Step 16190	lr 0.02271	Loss 1.9895 (1.9369)	Prec@(1,5) (62.1%, 89.0%)	
07/03 08:40:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][600/781]	Step 16240	lr 0.02271	Loss 1.8321 (1.9395)	Prec@(1,5) (62.1%, 89.0%)	
07/03 08:40:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][650/781]	Step 16290	lr 0.02271	Loss 1.8836 (1.9425)	Prec@(1,5) (62.0%, 88.9%)	
07/03 08:40:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][700/781]	Step 16340	lr 0.02271	Loss 1.4291 (1.9448)	Prec@(1,5) (62.0%, 88.9%)	
07/03 08:40:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][750/781]	Step 16390	lr 0.02271	Loss 1.8386 (1.9506)	Prec@(1,5) (61.9%, 88.8%)	
07/03 08:41:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [20][781/781]	Step 16421	lr 0.02271	Loss 1.7889 (1.9517)	Prec@(1,5) (61.9%, 88.9%)	
07/03 08:41:03午後 evaluateCell_trainer.py:172 [INFO] Train: [ 20/99] Final Prec@1 61.8840%
07/03 08:41:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][50/391]	Step 16422	Loss 1.3313	Prec@(1,5) (61.0%, 88.5%)
07/03 08:41:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][100/391]	Step 16422	Loss 1.3390	Prec@(1,5) (60.8%, 88.5%)
07/03 08:41:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][150/391]	Step 16422	Loss 1.3595	Prec@(1,5) (60.4%, 88.3%)
07/03 08:41:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][200/391]	Step 16422	Loss 1.3609	Prec@(1,5) (60.4%, 88.2%)
07/03 08:41:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][250/391]	Step 16422	Loss 1.3629	Prec@(1,5) (60.5%, 88.2%)
07/03 08:41:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][300/391]	Step 16422	Loss 1.3596	Prec@(1,5) (60.8%, 88.3%)
07/03 08:41:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][350/391]	Step 16422	Loss 1.3616	Prec@(1,5) (60.8%, 88.3%)
07/03 08:41:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [20][390/391]	Step 16422	Loss 1.3585	Prec@(1,5) (60.9%, 88.3%)
07/03 08:41:28午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 20/99] Final Prec@1 60.9080%
07/03 08:41:28午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 61.2920%
07/03 08:41:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][50/781]	Step 16472	lr 0.02248	Loss 1.6497 (1.8759)	Prec@(1,5) (63.8%, 89.4%)	
07/03 08:41:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][100/781]	Step 16522	lr 0.02248	Loss 1.7664 (1.8922)	Prec@(1,5) (63.3%, 89.3%)	
07/03 08:42:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][150/781]	Step 16572	lr 0.02248	Loss 2.0170 (1.8961)	Prec@(1,5) (63.4%, 89.5%)	
07/03 08:42:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][200/781]	Step 16622	lr 0.02248	Loss 1.7069 (1.8840)	Prec@(1,5) (63.5%, 89.6%)	
07/03 08:42:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][250/781]	Step 16672	lr 0.02248	Loss 2.0340 (1.8906)	Prec@(1,5) (63.3%, 89.5%)	
07/03 08:42:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][300/781]	Step 16722	lr 0.02248	Loss 1.6808 (1.9026)	Prec@(1,5) (63.1%, 89.3%)	
07/03 08:42:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][350/781]	Step 16772	lr 0.02248	Loss 2.0577 (1.9029)	Prec@(1,5) (63.1%, 89.3%)	
07/03 08:42:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][400/781]	Step 16822	lr 0.02248	Loss 1.9975 (1.9065)	Prec@(1,5) (62.9%, 89.2%)	
07/03 08:43:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][450/781]	Step 16872	lr 0.02248	Loss 1.7793 (1.9062)	Prec@(1,5) (63.0%, 89.2%)	
07/03 08:43:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][500/781]	Step 16922	lr 0.02248	Loss 2.1519 (1.9068)	Prec@(1,5) (63.0%, 89.2%)	
07/03 08:43:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][550/781]	Step 16972	lr 0.02248	Loss 1.7708 (1.9085)	Prec@(1,5) (62.9%, 89.2%)	
07/03 08:43:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][600/781]	Step 17022	lr 0.02248	Loss 1.4801 (1.9112)	Prec@(1,5) (62.9%, 89.1%)	
07/03 08:43:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][650/781]	Step 17072	lr 0.02248	Loss 2.0551 (1.9093)	Prec@(1,5) (62.9%, 89.2%)	
07/03 08:44:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][700/781]	Step 17122	lr 0.02248	Loss 1.9421 (1.9159)	Prec@(1,5) (62.7%, 89.1%)	
07/03 08:44:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][750/781]	Step 17172	lr 0.02248	Loss 1.8193 (1.9197)	Prec@(1,5) (62.6%, 89.0%)	
07/03 08:44:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [21][781/781]	Step 17203	lr 0.02248	Loss 1.8359 (1.9203)	Prec@(1,5) (62.6%, 89.0%)	
07/03 08:44:26午後 evaluateCell_trainer.py:172 [INFO] Train: [ 21/99] Final Prec@1 62.6100%
07/03 08:44:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][50/391]	Step 17204	Loss 1.2962	Prec@(1,5) (62.7%, 89.3%)
07/03 08:44:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][100/391]	Step 17204	Loss 1.3000	Prec@(1,5) (62.5%, 89.3%)
07/03 08:44:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][150/391]	Step 17204	Loss 1.2944	Prec@(1,5) (62.8%, 89.1%)
07/03 08:44:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][200/391]	Step 17204	Loss 1.2888	Prec@(1,5) (62.7%, 89.2%)
07/03 08:44:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][250/391]	Step 17204	Loss 1.2894	Prec@(1,5) (62.6%, 89.3%)
07/03 08:44:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][300/391]	Step 17204	Loss 1.2895	Prec@(1,5) (62.7%, 89.3%)
07/03 08:44:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][350/391]	Step 17204	Loss 1.2880	Prec@(1,5) (62.9%, 89.3%)
07/03 08:44:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [21][390/391]	Step 17204	Loss 1.2890	Prec@(1,5) (62.9%, 89.3%)
07/03 08:44:51午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 21/99] Final Prec@1 62.9240%
07/03 08:44:51午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 62.9240%
07/03 08:45:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][50/781]	Step 17254	lr 0.02225	Loss 1.7334 (1.8951)	Prec@(1,5) (62.4%, 89.8%)	
07/03 08:45:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][100/781]	Step 17304	lr 0.02225	Loss 1.3544 (1.8638)	Prec@(1,5) (63.2%, 90.0%)	
07/03 08:45:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][150/781]	Step 17354	lr 0.02225	Loss 2.0328 (1.8580)	Prec@(1,5) (63.7%, 89.8%)	
07/03 08:45:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][200/781]	Step 17404	lr 0.02225	Loss 1.7271 (1.8621)	Prec@(1,5) (63.6%, 90.0%)	
07/03 08:45:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][250/781]	Step 17454	lr 0.02225	Loss 1.3862 (1.8640)	Prec@(1,5) (63.6%, 89.8%)	
07/03 08:46:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][300/781]	Step 17504	lr 0.02225	Loss 1.8067 (1.8736)	Prec@(1,5) (63.5%, 89.6%)	
07/03 08:46:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][350/781]	Step 17554	lr 0.02225	Loss 1.6877 (1.8704)	Prec@(1,5) (63.6%, 89.7%)	
07/03 08:46:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][400/781]	Step 17604	lr 0.02225	Loss 1.9717 (1.8717)	Prec@(1,5) (63.5%, 89.7%)	
07/03 08:46:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][450/781]	Step 17654	lr 0.02225	Loss 1.9045 (1.8777)	Prec@(1,5) (63.4%, 89.6%)	
07/03 08:46:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][500/781]	Step 17704	lr 0.02225	Loss 1.9685 (1.8765)	Prec@(1,5) (63.4%, 89.6%)	
07/03 08:46:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][550/781]	Step 17754	lr 0.02225	Loss 1.6823 (1.8807)	Prec@(1,5) (63.3%, 89.5%)	
07/03 08:47:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][600/781]	Step 17804	lr 0.02225	Loss 1.7167 (1.8850)	Prec@(1,5) (63.2%, 89.5%)	
07/03 08:47:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][650/781]	Step 17854	lr 0.02225	Loss 2.0779 (1.8825)	Prec@(1,5) (63.2%, 89.4%)	
07/03 08:47:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][700/781]	Step 17904	lr 0.02225	Loss 1.6960 (1.8814)	Prec@(1,5) (63.2%, 89.4%)	
07/03 08:47:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][750/781]	Step 17954	lr 0.02225	Loss 2.3236 (1.8827)	Prec@(1,5) (63.2%, 89.4%)	
07/03 08:47:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [22][781/781]	Step 17985	lr 0.02225	Loss 1.8495 (1.8865)	Prec@(1,5) (63.1%, 89.4%)	
07/03 08:47:49午後 evaluateCell_trainer.py:172 [INFO] Train: [ 22/99] Final Prec@1 63.1220%
07/03 08:47:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][50/391]	Step 17986	Loss 1.4158	Prec@(1,5) (59.1%, 88.2%)
07/03 08:47:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][100/391]	Step 17986	Loss 1.4172	Prec@(1,5) (59.0%, 87.8%)
07/03 08:47:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][150/391]	Step 17986	Loss 1.3975	Prec@(1,5) (59.3%, 88.0%)
07/03 08:48:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][200/391]	Step 17986	Loss 1.3954	Prec@(1,5) (59.5%, 88.0%)
07/03 08:48:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][250/391]	Step 17986	Loss 1.4003	Prec@(1,5) (59.6%, 87.9%)
07/03 08:48:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][300/391]	Step 17986	Loss 1.3931	Prec@(1,5) (59.6%, 88.1%)
07/03 08:48:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][350/391]	Step 17986	Loss 1.3889	Prec@(1,5) (59.7%, 88.1%)
07/03 08:48:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [22][390/391]	Step 17986	Loss 1.3875	Prec@(1,5) (59.8%, 88.1%)
07/03 08:48:14午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 22/99] Final Prec@1 59.8280%
07/03 08:48:14午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 62.9240%
07/03 08:48:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][50/781]	Step 18036	lr 0.022	Loss 1.7569 (1.8546)	Prec@(1,5) (64.6%, 89.2%)	
07/03 08:48:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][100/781]	Step 18086	lr 0.022	Loss 1.5252 (1.8457)	Prec@(1,5) (64.1%, 89.3%)	
07/03 08:48:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][150/781]	Step 18136	lr 0.022	Loss 1.2970 (1.8409)	Prec@(1,5) (64.2%, 89.5%)	
07/03 08:49:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][200/781]	Step 18186	lr 0.022	Loss 1.7428 (1.8288)	Prec@(1,5) (64.4%, 89.8%)	
07/03 08:49:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][250/781]	Step 18236	lr 0.022	Loss 1.9725 (1.8387)	Prec@(1,5) (64.1%, 89.8%)	
07/03 08:49:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][300/781]	Step 18286	lr 0.022	Loss 1.5600 (1.8370)	Prec@(1,5) (64.3%, 89.8%)	
07/03 08:49:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][350/781]	Step 18336	lr 0.022	Loss 1.8146 (1.8402)	Prec@(1,5) (64.2%, 89.8%)	
07/03 08:49:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][400/781]	Step 18386	lr 0.022	Loss 1.6348 (1.8402)	Prec@(1,5) (64.0%, 89.8%)	
07/03 08:49:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][450/781]	Step 18436	lr 0.022	Loss 1.3503 (1.8474)	Prec@(1,5) (63.9%, 89.7%)	
07/03 08:50:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][500/781]	Step 18486	lr 0.022	Loss 1.9868 (1.8509)	Prec@(1,5) (63.9%, 89.6%)	
07/03 08:50:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][550/781]	Step 18536	lr 0.022	Loss 2.0159 (1.8564)	Prec@(1,5) (63.9%, 89.6%)	
07/03 08:50:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][600/781]	Step 18586	lr 0.022	Loss 1.5004 (1.8574)	Prec@(1,5) (63.8%, 89.6%)	
07/03 08:50:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][650/781]	Step 18636	lr 0.022	Loss 1.8763 (1.8563)	Prec@(1,5) (63.8%, 89.6%)	
07/03 08:50:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][700/781]	Step 18686	lr 0.022	Loss 1.8162 (1.8595)	Prec@(1,5) (63.6%, 89.6%)	
07/03 08:51:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][750/781]	Step 18736	lr 0.022	Loss 2.0941 (1.8615)	Prec@(1,5) (63.5%, 89.6%)	
07/03 08:51:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [23][781/781]	Step 18767	lr 0.022	Loss 1.6918 (1.8614)	Prec@(1,5) (63.5%, 89.6%)	
07/03 08:51:11午後 evaluateCell_trainer.py:172 [INFO] Train: [ 23/99] Final Prec@1 63.5400%
07/03 08:51:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][50/391]	Step 18768	Loss 1.4733	Prec@(1,5) (58.2%, 86.7%)
07/03 08:51:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][100/391]	Step 18768	Loss 1.4703	Prec@(1,5) (58.7%, 87.0%)
07/03 08:51:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][150/391]	Step 18768	Loss 1.4635	Prec@(1,5) (58.8%, 87.2%)
07/03 08:51:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][200/391]	Step 18768	Loss 1.4578	Prec@(1,5) (58.9%, 87.3%)
07/03 08:51:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][250/391]	Step 18768	Loss 1.4617	Prec@(1,5) (58.8%, 87.2%)
07/03 08:51:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][300/391]	Step 18768	Loss 1.4607	Prec@(1,5) (58.9%, 87.2%)
07/03 08:51:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][350/391]	Step 18768	Loss 1.4664	Prec@(1,5) (58.7%, 87.0%)
07/03 08:51:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [23][390/391]	Step 18768	Loss 1.4643	Prec@(1,5) (58.8%, 87.1%)
07/03 08:51:37午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 23/99] Final Prec@1 58.7640%
07/03 08:51:37午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 62.9240%
07/03 08:51:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][50/781]	Step 18818	lr 0.02175	Loss 2.0226 (1.7652)	Prec@(1,5) (65.4%, 90.9%)	
07/03 08:52:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][100/781]	Step 18868	lr 0.02175	Loss 1.6231 (1.7843)	Prec@(1,5) (65.2%, 90.8%)	
07/03 08:52:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][150/781]	Step 18918	lr 0.02175	Loss 1.6817 (1.7797)	Prec@(1,5) (65.3%, 90.5%)	
07/03 08:52:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][200/781]	Step 18968	lr 0.02175	Loss 2.0654 (1.7838)	Prec@(1,5) (65.4%, 90.4%)	
07/03 08:52:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][250/781]	Step 19018	lr 0.02175	Loss 1.4133 (1.7876)	Prec@(1,5) (65.4%, 90.4%)	
07/03 08:52:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][300/781]	Step 19068	lr 0.02175	Loss 1.5865 (1.7985)	Prec@(1,5) (65.0%, 90.3%)	
07/03 08:52:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][350/781]	Step 19118	lr 0.02175	Loss 1.9532 (1.8091)	Prec@(1,5) (64.8%, 90.1%)	
07/03 08:53:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][400/781]	Step 19168	lr 0.02175	Loss 2.1255 (1.8148)	Prec@(1,5) (64.7%, 90.1%)	
07/03 08:53:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][450/781]	Step 19218	lr 0.02175	Loss 1.8719 (1.8215)	Prec@(1,5) (64.6%, 90.0%)	
07/03 08:53:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][500/781]	Step 19268	lr 0.02175	Loss 2.0640 (1.8250)	Prec@(1,5) (64.4%, 89.9%)	
07/03 08:53:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][550/781]	Step 19318	lr 0.02175	Loss 1.8244 (1.8309)	Prec@(1,5) (64.3%, 89.9%)	
07/03 08:53:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][600/781]	Step 19368	lr 0.02175	Loss 1.6151 (1.8336)	Prec@(1,5) (64.2%, 89.8%)	
07/03 08:54:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][650/781]	Step 19418	lr 0.02175	Loss 1.7659 (1.8358)	Prec@(1,5) (64.2%, 89.8%)	
07/03 08:54:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][700/781]	Step 19468	lr 0.02175	Loss 1.8316 (1.8361)	Prec@(1,5) (64.1%, 89.8%)	
07/03 08:54:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][750/781]	Step 19518	lr 0.02175	Loss 2.1614 (1.8362)	Prec@(1,5) (64.2%, 89.8%)	
07/03 08:54:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [24][781/781]	Step 19549	lr 0.02175	Loss 2.1917 (1.8347)	Prec@(1,5) (64.2%, 89.9%)	
07/03 08:54:35午後 evaluateCell_trainer.py:172 [INFO] Train: [ 24/99] Final Prec@1 64.1900%
07/03 08:54:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][50/391]	Step 19550	Loss 1.3473	Prec@(1,5) (62.0%, 88.7%)
07/03 08:54:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][100/391]	Step 19550	Loss 1.3430	Prec@(1,5) (61.9%, 88.4%)
07/03 08:54:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][150/391]	Step 19550	Loss 1.3326	Prec@(1,5) (62.3%, 88.4%)
07/03 08:54:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][200/391]	Step 19550	Loss 1.3238	Prec@(1,5) (62.4%, 88.5%)
07/03 08:54:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][250/391]	Step 19550	Loss 1.3225	Prec@(1,5) (62.5%, 88.5%)
07/03 08:54:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][300/391]	Step 19550	Loss 1.3266	Prec@(1,5) (62.4%, 88.5%)
07/03 08:54:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][350/391]	Step 19550	Loss 1.3210	Prec@(1,5) (62.6%, 88.6%)
07/03 08:55:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [24][390/391]	Step 19550	Loss 1.3225	Prec@(1,5) (62.5%, 88.6%)
07/03 08:55:00午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 24/99] Final Prec@1 62.4920%
07/03 08:55:00午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 62.9240%
07/03 08:55:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][50/781]	Step 19600	lr 0.02149	Loss 2.0510 (1.7571)	Prec@(1,5) (65.5%, 90.8%)	
07/03 08:55:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][100/781]	Step 19650	lr 0.02149	Loss 1.6247 (1.7638)	Prec@(1,5) (65.2%, 90.9%)	
07/03 08:55:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][150/781]	Step 19700	lr 0.02149	Loss 1.9406 (1.7653)	Prec@(1,5) (64.9%, 91.0%)	
07/03 08:55:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][200/781]	Step 19750	lr 0.02149	Loss 1.7826 (1.7722)	Prec@(1,5) (64.8%, 90.8%)	
07/03 08:55:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][250/781]	Step 19800	lr 0.02149	Loss 1.6730 (1.7711)	Prec@(1,5) (64.9%, 90.7%)	
07/03 08:56:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][300/781]	Step 19850	lr 0.02149	Loss 1.9870 (1.7754)	Prec@(1,5) (64.7%, 90.7%)	
07/03 08:56:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][350/781]	Step 19900	lr 0.02149	Loss 1.9933 (1.7784)	Prec@(1,5) (64.7%, 90.7%)	
07/03 08:56:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][400/781]	Step 19950	lr 0.02149	Loss 2.1369 (1.7784)	Prec@(1,5) (64.8%, 90.6%)	
07/03 08:56:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][450/781]	Step 20000	lr 0.02149	Loss 1.7802 (1.7796)	Prec@(1,5) (64.8%, 90.5%)	
07/03 08:56:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][500/781]	Step 20050	lr 0.02149	Loss 1.7661 (1.7875)	Prec@(1,5) (64.7%, 90.4%)	
07/03 08:57:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][550/781]	Step 20100	lr 0.02149	Loss 1.7443 (1.7917)	Prec@(1,5) (64.6%, 90.4%)	
07/03 08:57:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][600/781]	Step 20150	lr 0.02149	Loss 1.9170 (1.7904)	Prec@(1,5) (64.6%, 90.4%)	
07/03 08:57:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][650/781]	Step 20200	lr 0.02149	Loss 1.5676 (1.7958)	Prec@(1,5) (64.5%, 90.3%)	
07/03 08:57:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][700/781]	Step 20250	lr 0.02149	Loss 1.7924 (1.7981)	Prec@(1,5) (64.5%, 90.3%)	
07/03 08:57:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][750/781]	Step 20300	lr 0.02149	Loss 2.0730 (1.8063)	Prec@(1,5) (64.4%, 90.1%)	
07/03 08:57:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [25][781/781]	Step 20331	lr 0.02149	Loss 2.0002 (1.8044)	Prec@(1,5) (64.4%, 90.2%)	
07/03 08:57:58午後 evaluateCell_trainer.py:172 [INFO] Train: [ 25/99] Final Prec@1 64.3740%
07/03 08:58:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][50/391]	Step 20332	Loss 1.2342	Prec@(1,5) (64.7%, 90.4%)
07/03 08:58:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][100/391]	Step 20332	Loss 1.2310	Prec@(1,5) (64.5%, 90.4%)
07/03 08:58:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][150/391]	Step 20332	Loss 1.2370	Prec@(1,5) (64.4%, 90.1%)
07/03 08:58:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][200/391]	Step 20332	Loss 1.2267	Prec@(1,5) (64.7%, 90.1%)
07/03 08:58:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][250/391]	Step 20332	Loss 1.2278	Prec@(1,5) (64.5%, 90.1%)
07/03 08:58:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][300/391]	Step 20332	Loss 1.2280	Prec@(1,5) (64.5%, 90.0%)
07/03 08:58:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][350/391]	Step 20332	Loss 1.2304	Prec@(1,5) (64.5%, 90.0%)
07/03 08:58:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [25][390/391]	Step 20332	Loss 1.2254	Prec@(1,5) (64.6%, 90.0%)
07/03 08:58:24午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 25/99] Final Prec@1 64.5960%
07/03 08:58:24午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 64.5960%
07/03 08:58:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][50/781]	Step 20382	lr 0.02121	Loss 1.5306 (1.6978)	Prec@(1,5) (67.1%, 91.3%)	
07/03 08:58:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][100/781]	Step 20432	lr 0.02121	Loss 1.8628 (1.7265)	Prec@(1,5) (66.7%, 91.4%)	
07/03 08:58:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][150/781]	Step 20482	lr 0.02121	Loss 1.6000 (1.7229)	Prec@(1,5) (66.7%, 91.3%)	
07/03 08:59:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][200/781]	Step 20532	lr 0.02121	Loss 2.1935 (1.7151)	Prec@(1,5) (66.7%, 91.4%)	
07/03 08:59:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][250/781]	Step 20582	lr 0.02121	Loss 1.6751 (1.7238)	Prec@(1,5) (66.4%, 91.3%)	
07/03 08:59:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][300/781]	Step 20632	lr 0.02121	Loss 1.8096 (1.7278)	Prec@(1,5) (66.2%, 91.2%)	
07/03 08:59:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][350/781]	Step 20682	lr 0.02121	Loss 2.4353 (1.7388)	Prec@(1,5) (66.0%, 91.1%)	
07/03 08:59:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][400/781]	Step 20732	lr 0.02121	Loss 1.6777 (1.7500)	Prec@(1,5) (65.9%, 90.9%)	
07/03 09:00:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][450/781]	Step 20782	lr 0.02121	Loss 1.4723 (1.7560)	Prec@(1,5) (65.7%, 90.9%)	
07/03 09:00:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][500/781]	Step 20832	lr 0.02121	Loss 2.1834 (1.7592)	Prec@(1,5) (65.7%, 90.9%)	
07/03 09:00:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][550/781]	Step 20882	lr 0.02121	Loss 1.9390 (1.7624)	Prec@(1,5) (65.6%, 90.9%)	
07/03 09:00:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][600/781]	Step 20932	lr 0.02121	Loss 2.0861 (1.7670)	Prec@(1,5) (65.4%, 90.8%)	
07/03 09:00:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][650/781]	Step 20982	lr 0.02121	Loss 1.9781 (1.7725)	Prec@(1,5) (65.3%, 90.8%)	
07/03 09:01:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][700/781]	Step 21032	lr 0.02121	Loss 2.3705 (1.7772)	Prec@(1,5) (65.2%, 90.7%)	
07/03 09:01:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][750/781]	Step 21082	lr 0.02121	Loss 1.7323 (1.7807)	Prec@(1,5) (65.1%, 90.7%)	
07/03 09:01:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [26][781/781]	Step 21113	lr 0.02121	Loss 1.7828 (1.7854)	Prec@(1,5) (65.0%, 90.7%)	
07/03 09:01:21午後 evaluateCell_trainer.py:172 [INFO] Train: [ 26/99] Final Prec@1 64.9600%
07/03 09:01:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][50/391]	Step 21114	Loss 1.3260	Prec@(1,5) (61.8%, 88.7%)
07/03 09:01:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][100/391]	Step 21114	Loss 1.3094	Prec@(1,5) (62.6%, 88.6%)
07/03 09:01:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][150/391]	Step 21114	Loss 1.2955	Prec@(1,5) (63.1%, 88.8%)
07/03 09:01:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][200/391]	Step 21114	Loss 1.2969	Prec@(1,5) (63.2%, 89.0%)
07/03 09:01:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][250/391]	Step 21114	Loss 1.2911	Prec@(1,5) (63.1%, 89.2%)
07/03 09:01:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][300/391]	Step 21114	Loss 1.3034	Prec@(1,5) (62.8%, 88.9%)
07/03 09:01:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][350/391]	Step 21114	Loss 1.3171	Prec@(1,5) (62.5%, 88.7%)
07/03 09:01:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [26][390/391]	Step 21114	Loss 1.3246	Prec@(1,5) (62.4%, 88.6%)
07/03 09:01:47午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 26/99] Final Prec@1 62.3880%
07/03 09:01:47午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 64.5960%
07/03 09:01:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][50/781]	Step 21164	lr 0.02094	Loss 1.7614 (1.7263)	Prec@(1,5) (65.6%, 91.2%)	
07/03 09:02:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][100/781]	Step 21214	lr 0.02094	Loss 1.8112 (1.7212)	Prec@(1,5) (66.1%, 91.1%)	
07/03 09:02:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][150/781]	Step 21264	lr 0.02094	Loss 2.0116 (1.7098)	Prec@(1,5) (66.3%, 91.3%)	
07/03 09:02:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][200/781]	Step 21314	lr 0.02094	Loss 1.4918 (1.7148)	Prec@(1,5) (66.1%, 91.1%)	
07/03 09:02:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][250/781]	Step 21364	lr 0.02094	Loss 1.9355 (1.7093)	Prec@(1,5) (66.2%, 91.2%)	
07/03 09:02:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][300/781]	Step 21414	lr 0.02094	Loss 2.3152 (1.7136)	Prec@(1,5) (66.2%, 91.2%)	
07/03 09:03:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][350/781]	Step 21464	lr 0.02094	Loss 1.5179 (1.7164)	Prec@(1,5) (66.1%, 91.2%)	
07/03 09:03:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][400/781]	Step 21514	lr 0.02094	Loss 1.6097 (1.7181)	Prec@(1,5) (66.1%, 91.1%)	
07/03 09:03:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][450/781]	Step 21564	lr 0.02094	Loss 1.8305 (1.7239)	Prec@(1,5) (66.0%, 91.0%)	
07/03 09:03:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][500/781]	Step 21614	lr 0.02094	Loss 1.6371 (1.7286)	Prec@(1,5) (65.9%, 91.0%)	
07/03 09:03:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][550/781]	Step 21664	lr 0.02094	Loss 2.2472 (1.7345)	Prec@(1,5) (65.7%, 90.9%)	
07/03 09:04:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][600/781]	Step 21714	lr 0.02094	Loss 2.1664 (1.7339)	Prec@(1,5) (65.7%, 91.0%)	
07/03 09:04:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][650/781]	Step 21764	lr 0.02094	Loss 1.6876 (1.7390)	Prec@(1,5) (65.6%, 90.9%)	
07/03 09:04:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][700/781]	Step 21814	lr 0.02094	Loss 2.6167 (1.7465)	Prec@(1,5) (65.6%, 90.8%)	
07/03 09:04:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][750/781]	Step 21864	lr 0.02094	Loss 1.9418 (1.7506)	Prec@(1,5) (65.6%, 90.7%)	
07/03 09:04:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [27][781/781]	Step 21895	lr 0.02094	Loss 1.7631 (1.7552)	Prec@(1,5) (65.5%, 90.6%)	
07/03 09:04:45午後 evaluateCell_trainer.py:172 [INFO] Train: [ 27/99] Final Prec@1 65.5300%
07/03 09:04:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][50/391]	Step 21896	Loss 1.1312	Prec@(1,5) (65.4%, 92.2%)
07/03 09:04:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][100/391]	Step 21896	Loss 1.1404	Prec@(1,5) (65.4%, 91.8%)
07/03 09:04:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][150/391]	Step 21896	Loss 1.1413	Prec@(1,5) (65.8%, 91.9%)
07/03 09:04:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][200/391]	Step 21896	Loss 1.1469	Prec@(1,5) (65.5%, 91.8%)
07/03 09:05:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][250/391]	Step 21896	Loss 1.1501	Prec@(1,5) (65.4%, 91.8%)
07/03 09:05:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][300/391]	Step 21896	Loss 1.1540	Prec@(1,5) (65.5%, 91.6%)
07/03 09:05:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][350/391]	Step 21896	Loss 1.1592	Prec@(1,5) (65.7%, 91.6%)
07/03 09:05:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [27][390/391]	Step 21896	Loss 1.1569	Prec@(1,5) (65.6%, 91.7%)
07/03 09:05:10午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 27/99] Final Prec@1 65.6440%
07/03 09:05:10午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 65.6440%
07/03 09:05:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][50/781]	Step 21946	lr 0.02065	Loss 1.5912 (1.6573)	Prec@(1,5) (67.9%, 91.1%)	
07/03 09:05:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][100/781]	Step 21996	lr 0.02065	Loss 2.2018 (1.6749)	Prec@(1,5) (67.2%, 91.3%)	
07/03 09:05:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][150/781]	Step 22046	lr 0.02065	Loss 1.5532 (1.6836)	Prec@(1,5) (67.0%, 91.5%)	
07/03 09:05:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][200/781]	Step 22096	lr 0.02065	Loss 1.6169 (1.6829)	Prec@(1,5) (67.0%, 91.6%)	
07/03 09:06:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][250/781]	Step 22146	lr 0.02065	Loss 2.2176 (1.6896)	Prec@(1,5) (66.9%, 91.5%)	
07/03 09:06:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][300/781]	Step 22196	lr 0.02065	Loss 1.4973 (1.6965)	Prec@(1,5) (66.7%, 91.5%)	
07/03 09:06:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][350/781]	Step 22246	lr 0.02065	Loss 2.0280 (1.7050)	Prec@(1,5) (66.5%, 91.4%)	
07/03 09:06:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][400/781]	Step 22296	lr 0.02065	Loss 1.5852 (1.7078)	Prec@(1,5) (66.5%, 91.4%)	
07/03 09:06:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][450/781]	Step 22346	lr 0.02065	Loss 1.4659 (1.7102)	Prec@(1,5) (66.5%, 91.3%)	
07/03 09:07:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][500/781]	Step 22396	lr 0.02065	Loss 1.6051 (1.7153)	Prec@(1,5) (66.4%, 91.2%)	
07/03 09:07:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][550/781]	Step 22446	lr 0.02065	Loss 1.8074 (1.7234)	Prec@(1,5) (66.3%, 91.1%)	
07/03 09:07:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][600/781]	Step 22496	lr 0.02065	Loss 1.8897 (1.7263)	Prec@(1,5) (66.3%, 91.1%)	
07/03 09:07:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][650/781]	Step 22546	lr 0.02065	Loss 1.5626 (1.7258)	Prec@(1,5) (66.3%, 91.1%)	
07/03 09:07:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][700/781]	Step 22596	lr 0.02065	Loss 2.1104 (1.7297)	Prec@(1,5) (66.1%, 91.0%)	
07/03 09:08:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][750/781]	Step 22646	lr 0.02065	Loss 1.9221 (1.7300)	Prec@(1,5) (66.1%, 91.0%)	
07/03 09:08:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [28][781/781]	Step 22677	lr 0.02065	Loss 1.3459 (1.7314)	Prec@(1,5) (66.1%, 91.0%)	
07/03 09:08:08午後 evaluateCell_trainer.py:172 [INFO] Train: [ 28/99] Final Prec@1 66.0460%
07/03 09:08:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][50/391]	Step 22678	Loss 1.2469	Prec@(1,5) (64.3%, 89.5%)
07/03 09:08:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][100/391]	Step 22678	Loss 1.2624	Prec@(1,5) (64.1%, 89.3%)
07/03 09:08:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][150/391]	Step 22678	Loss 1.2489	Prec@(1,5) (64.2%, 89.6%)
07/03 09:08:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][200/391]	Step 22678	Loss 1.2542	Prec@(1,5) (64.3%, 89.6%)
07/03 09:08:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][250/391]	Step 22678	Loss 1.2511	Prec@(1,5) (64.2%, 89.7%)
07/03 09:08:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][300/391]	Step 22678	Loss 1.2498	Prec@(1,5) (64.2%, 89.7%)
07/03 09:08:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][350/391]	Step 22678	Loss 1.2473	Prec@(1,5) (64.1%, 89.8%)
07/03 09:08:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [28][390/391]	Step 22678	Loss 1.2507	Prec@(1,5) (64.0%, 89.7%)
07/03 09:08:33午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 28/99] Final Prec@1 64.0400%
07/03 09:08:33午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 65.6440%
07/03 09:08:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][50/781]	Step 22728	lr 0.02035	Loss 1.8349 (1.6836)	Prec@(1,5) (67.3%, 91.3%)	
07/03 09:08:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][100/781]	Step 22778	lr 0.02035	Loss 1.6016 (1.6289)	Prec@(1,5) (68.3%, 91.6%)	
07/03 09:09:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][150/781]	Step 22828	lr 0.02035	Loss 1.2441 (1.6392)	Prec@(1,5) (68.1%, 91.6%)	
07/03 09:09:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][200/781]	Step 22878	lr 0.02035	Loss 2.0091 (1.6553)	Prec@(1,5) (67.7%, 91.6%)	
07/03 09:09:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][250/781]	Step 22928	lr 0.02035	Loss 1.4145 (1.6520)	Prec@(1,5) (67.8%, 91.8%)	
07/03 09:09:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][300/781]	Step 22978	lr 0.02035	Loss 1.3593 (1.6660)	Prec@(1,5) (67.4%, 91.7%)	
07/03 09:09:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][350/781]	Step 23028	lr 0.02035	Loss 1.6104 (1.6789)	Prec@(1,5) (67.3%, 91.5%)	
07/03 09:10:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][400/781]	Step 23078	lr 0.02035	Loss 1.5367 (1.6847)	Prec@(1,5) (67.1%, 91.5%)	
07/03 09:10:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][450/781]	Step 23128	lr 0.02035	Loss 1.6668 (1.6905)	Prec@(1,5) (67.1%, 91.4%)	
07/03 09:10:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][500/781]	Step 23178	lr 0.02035	Loss 1.9046 (1.6940)	Prec@(1,5) (67.0%, 91.3%)	
07/03 09:10:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][550/781]	Step 23228	lr 0.02035	Loss 1.6096 (1.6973)	Prec@(1,5) (66.8%, 91.3%)	
07/03 09:10:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][600/781]	Step 23278	lr 0.02035	Loss 1.8062 (1.6996)	Prec@(1,5) (66.8%, 91.2%)	
07/03 09:11:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][650/781]	Step 23328	lr 0.02035	Loss 1.9779 (1.7039)	Prec@(1,5) (66.7%, 91.2%)	
07/03 09:11:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][700/781]	Step 23378	lr 0.02035	Loss 1.3527 (1.7050)	Prec@(1,5) (66.7%, 91.1%)	
07/03 09:11:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][750/781]	Step 23428	lr 0.02035	Loss 1.1803 (1.7053)	Prec@(1,5) (66.7%, 91.1%)	
07/03 09:11:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [29][781/781]	Step 23459	lr 0.02035	Loss 2.1672 (1.7088)	Prec@(1,5) (66.6%, 91.1%)	
07/03 09:11:32午後 evaluateCell_trainer.py:172 [INFO] Train: [ 29/99] Final Prec@1 66.5740%
07/03 09:11:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][50/391]	Step 23460	Loss 1.1864	Prec@(1,5) (65.9%, 90.6%)
07/03 09:11:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][100/391]	Step 23460	Loss 1.1853	Prec@(1,5) (65.8%, 90.8%)
07/03 09:11:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][150/391]	Step 23460	Loss 1.1805	Prec@(1,5) (66.2%, 90.7%)
07/03 09:11:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][200/391]	Step 23460	Loss 1.1796	Prec@(1,5) (66.5%, 90.7%)
07/03 09:11:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][250/391]	Step 23460	Loss 1.1859	Prec@(1,5) (66.2%, 90.6%)
07/03 09:11:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][300/391]	Step 23460	Loss 1.1784	Prec@(1,5) (66.2%, 90.7%)
07/03 09:11:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][350/391]	Step 23460	Loss 1.1782	Prec@(1,5) (66.0%, 90.8%)
07/03 09:11:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [29][390/391]	Step 23460	Loss 1.1782	Prec@(1,5) (66.0%, 90.8%)
07/03 09:11:57午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 29/99] Final Prec@1 66.0280%
07/03 09:11:57午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 66.0280%
07/03 09:12:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][50/781]	Step 23510	lr 0.02005	Loss 1.5972 (1.5975)	Prec@(1,5) (69.7%, 92.2%)	
07/03 09:12:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][100/781]	Step 23560	lr 0.02005	Loss 1.8428 (1.6055)	Prec@(1,5) (69.1%, 91.9%)	
07/03 09:12:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][150/781]	Step 23610	lr 0.02005	Loss 1.2512 (1.6110)	Prec@(1,5) (68.6%, 92.2%)	
07/03 09:12:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][200/781]	Step 23660	lr 0.02005	Loss 1.5655 (1.6225)	Prec@(1,5) (68.4%, 92.2%)	
07/03 09:12:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][250/781]	Step 23710	lr 0.02005	Loss 1.6880 (1.6366)	Prec@(1,5) (68.2%, 92.1%)	
07/03 09:13:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][300/781]	Step 23760	lr 0.02005	Loss 1.9421 (1.6454)	Prec@(1,5) (68.0%, 91.9%)	
07/03 09:13:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][350/781]	Step 23810	lr 0.02005	Loss 1.8236 (1.6518)	Prec@(1,5) (67.9%, 91.7%)	
07/03 09:13:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][400/781]	Step 23860	lr 0.02005	Loss 1.7951 (1.6592)	Prec@(1,5) (67.8%, 91.6%)	
07/03 09:13:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][450/781]	Step 23910	lr 0.02005	Loss 1.8957 (1.6694)	Prec@(1,5) (67.6%, 91.5%)	
07/03 09:13:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][500/781]	Step 23960	lr 0.02005	Loss 1.6142 (1.6684)	Prec@(1,5) (67.6%, 91.5%)	
07/03 09:14:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][550/781]	Step 24010	lr 0.02005	Loss 1.5817 (1.6660)	Prec@(1,5) (67.7%, 91.5%)	
07/03 09:14:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][600/781]	Step 24060	lr 0.02005	Loss 2.0447 (1.6694)	Prec@(1,5) (67.5%, 91.5%)	
07/03 09:14:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][650/781]	Step 24110	lr 0.02005	Loss 2.2148 (1.6788)	Prec@(1,5) (67.4%, 91.4%)	
07/03 09:14:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][700/781]	Step 24160	lr 0.02005	Loss 2.3434 (1.6821)	Prec@(1,5) (67.3%, 91.4%)	
07/03 09:14:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][750/781]	Step 24210	lr 0.02005	Loss 1.7350 (1.6849)	Prec@(1,5) (67.2%, 91.3%)	
07/03 09:14:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [30][781/781]	Step 24241	lr 0.02005	Loss 1.6320 (1.6888)	Prec@(1,5) (67.2%, 91.3%)	
07/03 09:14:55午後 evaluateCell_trainer.py:172 [INFO] Train: [ 30/99] Final Prec@1 67.1580%
07/03 09:14:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][50/391]	Step 24242	Loss 1.2522	Prec@(1,5) (63.8%, 90.0%)
07/03 09:15:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][100/391]	Step 24242	Loss 1.2394	Prec@(1,5) (64.6%, 90.3%)
07/03 09:15:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][150/391]	Step 24242	Loss 1.2261	Prec@(1,5) (65.1%, 90.3%)
07/03 09:15:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][200/391]	Step 24242	Loss 1.2198	Prec@(1,5) (65.2%, 90.2%)
07/03 09:15:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][250/391]	Step 24242	Loss 1.2260	Prec@(1,5) (65.1%, 90.2%)
07/03 09:15:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][300/391]	Step 24242	Loss 1.2236	Prec@(1,5) (65.1%, 90.3%)
07/03 09:15:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][350/391]	Step 24242	Loss 1.2228	Prec@(1,5) (65.0%, 90.3%)
07/03 09:15:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [30][390/391]	Step 24242	Loss 1.2274	Prec@(1,5) (64.7%, 90.3%)
07/03 09:15:20午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 30/99] Final Prec@1 64.7440%
07/03 09:15:20午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 66.0280%
07/03 09:15:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][50/781]	Step 24292	lr 0.01975	Loss 1.5626 (1.6090)	Prec@(1,5) (69.3%, 91.7%)	
07/03 09:15:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][100/781]	Step 24342	lr 0.01975	Loss 1.6252 (1.6286)	Prec@(1,5) (68.4%, 91.5%)	
07/03 09:15:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][150/781]	Step 24392	lr 0.01975	Loss 2.0118 (1.6354)	Prec@(1,5) (68.2%, 91.6%)	
07/03 09:16:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][200/781]	Step 24442	lr 0.01975	Loss 1.5430 (1.6139)	Prec@(1,5) (68.5%, 91.9%)	
07/03 09:16:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][250/781]	Step 24492	lr 0.01975	Loss 1.7351 (1.6274)	Prec@(1,5) (68.3%, 91.8%)	
07/03 09:16:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][300/781]	Step 24542	lr 0.01975	Loss 1.6558 (1.6346)	Prec@(1,5) (68.0%, 91.8%)	
07/03 09:16:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][350/781]	Step 24592	lr 0.01975	Loss 1.3321 (1.6323)	Prec@(1,5) (68.1%, 91.9%)	
07/03 09:16:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][400/781]	Step 24642	lr 0.01975	Loss 1.5617 (1.6396)	Prec@(1,5) (67.9%, 91.8%)	
07/03 09:17:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][450/781]	Step 24692	lr 0.01975	Loss 1.8419 (1.6380)	Prec@(1,5) (68.0%, 91.8%)	
07/03 09:17:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][500/781]	Step 24742	lr 0.01975	Loss 1.6840 (1.6444)	Prec@(1,5) (67.9%, 91.7%)	
07/03 09:17:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][550/781]	Step 24792	lr 0.01975	Loss 1.9549 (1.6498)	Prec@(1,5) (67.7%, 91.7%)	
07/03 09:17:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][600/781]	Step 24842	lr 0.01975	Loss 2.1327 (1.6527)	Prec@(1,5) (67.7%, 91.7%)	
07/03 09:17:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][650/781]	Step 24892	lr 0.01975	Loss 1.9186 (1.6585)	Prec@(1,5) (67.6%, 91.6%)	
07/03 09:18:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][700/781]	Step 24942	lr 0.01975	Loss 2.1835 (1.6634)	Prec@(1,5) (67.4%, 91.6%)	
07/03 09:18:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][750/781]	Step 24992	lr 0.01975	Loss 2.0648 (1.6657)	Prec@(1,5) (67.3%, 91.6%)	
07/03 09:18:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [31][781/781]	Step 25023	lr 0.01975	Loss 1.4030 (1.6694)	Prec@(1,5) (67.3%, 91.6%)	
07/03 09:18:18午後 evaluateCell_trainer.py:172 [INFO] Train: [ 31/99] Final Prec@1 67.2720%
07/03 09:18:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][50/391]	Step 25024	Loss 1.2925	Prec@(1,5) (63.1%, 89.2%)
07/03 09:18:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][100/391]	Step 25024	Loss 1.2389	Prec@(1,5) (64.0%, 90.0%)
07/03 09:18:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][150/391]	Step 25024	Loss 1.2464	Prec@(1,5) (63.9%, 90.0%)
07/03 09:18:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][200/391]	Step 25024	Loss 1.2507	Prec@(1,5) (63.7%, 89.8%)
07/03 09:18:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][250/391]	Step 25024	Loss 1.2547	Prec@(1,5) (63.5%, 89.8%)
07/03 09:18:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][300/391]	Step 25024	Loss 1.2552	Prec@(1,5) (63.5%, 89.8%)
07/03 09:18:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][350/391]	Step 25024	Loss 1.2565	Prec@(1,5) (63.6%, 89.7%)
07/03 09:18:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [31][390/391]	Step 25024	Loss 1.2622	Prec@(1,5) (63.5%, 89.7%)
07/03 09:18:44午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 31/99] Final Prec@1 63.4720%
07/03 09:18:44午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 66.0280%
07/03 09:18:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][50/781]	Step 25074	lr 0.01943	Loss 1.4693 (1.5763)	Prec@(1,5) (69.7%, 91.9%)	
07/03 09:19:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][100/781]	Step 25124	lr 0.01943	Loss 1.4413 (1.5410)	Prec@(1,5) (70.1%, 92.5%)	
07/03 09:19:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][150/781]	Step 25174	lr 0.01943	Loss 1.6559 (1.5548)	Prec@(1,5) (69.5%, 92.5%)	
07/03 09:19:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][200/781]	Step 25224	lr 0.01943	Loss 1.8933 (1.5590)	Prec@(1,5) (69.4%, 92.4%)	
07/03 09:19:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][250/781]	Step 25274	lr 0.01943	Loss 1.4678 (1.5629)	Prec@(1,5) (69.2%, 92.4%)	
07/03 09:19:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][300/781]	Step 25324	lr 0.01943	Loss 1.5257 (1.5869)	Prec@(1,5) (68.8%, 92.1%)	
07/03 09:20:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][350/781]	Step 25374	lr 0.01943	Loss 1.8300 (1.5981)	Prec@(1,5) (68.5%, 92.0%)	
07/03 09:20:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][400/781]	Step 25424	lr 0.01943	Loss 1.8276 (1.6032)	Prec@(1,5) (68.3%, 92.0%)	
07/03 09:20:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][450/781]	Step 25474	lr 0.01943	Loss 1.4839 (1.6096)	Prec@(1,5) (68.3%, 92.0%)	
07/03 09:20:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][500/781]	Step 25524	lr 0.01943	Loss 1.9700 (1.6195)	Prec@(1,5) (68.1%, 91.9%)	
07/03 09:20:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][550/781]	Step 25574	lr 0.01943	Loss 2.2223 (1.6213)	Prec@(1,5) (68.1%, 92.0%)	
07/03 09:21:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][600/781]	Step 25624	lr 0.01943	Loss 1.5283 (1.6284)	Prec@(1,5) (67.9%, 91.9%)	
07/03 09:21:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][650/781]	Step 25674	lr 0.01943	Loss 1.7573 (1.6314)	Prec@(1,5) (68.0%, 91.9%)	
07/03 09:21:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][700/781]	Step 25724	lr 0.01943	Loss 1.2268 (1.6410)	Prec@(1,5) (67.7%, 91.7%)	
07/03 09:21:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][750/781]	Step 25774	lr 0.01943	Loss 1.4724 (1.6445)	Prec@(1,5) (67.7%, 91.7%)	
07/03 09:21:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [32][781/781]	Step 25805	lr 0.01943	Loss 1.8732 (1.6474)	Prec@(1,5) (67.6%, 91.7%)	
07/03 09:21:41午後 evaluateCell_trainer.py:172 [INFO] Train: [ 32/99] Final Prec@1 67.6340%
07/03 09:21:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][50/391]	Step 25806	Loss 1.2163	Prec@(1,5) (64.4%, 90.0%)
07/03 09:21:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][100/391]	Step 25806	Loss 1.2114	Prec@(1,5) (64.9%, 89.9%)
07/03 09:21:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][150/391]	Step 25806	Loss 1.1853	Prec@(1,5) (65.5%, 90.4%)
07/03 09:21:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][200/391]	Step 25806	Loss 1.1901	Prec@(1,5) (65.5%, 90.2%)
07/03 09:21:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][250/391]	Step 25806	Loss 1.1757	Prec@(1,5) (65.9%, 90.5%)
07/03 09:22:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][300/391]	Step 25806	Loss 1.1717	Prec@(1,5) (65.9%, 90.6%)
07/03 09:22:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][350/391]	Step 25806	Loss 1.1701	Prec@(1,5) (66.2%, 90.6%)
07/03 09:22:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [32][390/391]	Step 25806	Loss 1.1661	Prec@(1,5) (66.3%, 90.7%)
07/03 09:22:07午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 32/99] Final Prec@1 66.2600%
07/03 09:22:07午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 66.2600%
07/03 09:22:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][50/781]	Step 25856	lr 0.01911	Loss 1.9582 (1.5711)	Prec@(1,5) (69.1%, 92.8%)	
07/03 09:22:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][100/781]	Step 25906	lr 0.01911	Loss 1.2050 (1.5985)	Prec@(1,5) (68.2%, 92.7%)	
07/03 09:22:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][150/781]	Step 25956	lr 0.01911	Loss 1.3823 (1.5866)	Prec@(1,5) (68.7%, 92.5%)	
07/03 09:22:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][200/781]	Step 26006	lr 0.01911	Loss 1.7026 (1.5804)	Prec@(1,5) (68.9%, 92.6%)	
07/03 09:23:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][250/781]	Step 26056	lr 0.01911	Loss 1.6958 (1.5824)	Prec@(1,5) (68.9%, 92.5%)	
07/03 09:23:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][300/781]	Step 26106	lr 0.01911	Loss 1.5244 (1.5871)	Prec@(1,5) (68.9%, 92.5%)	
07/03 09:23:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][350/781]	Step 26156	lr 0.01911	Loss 1.5735 (1.5880)	Prec@(1,5) (68.8%, 92.5%)	
07/03 09:23:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][400/781]	Step 26206	lr 0.01911	Loss 1.6949 (1.5917)	Prec@(1,5) (68.7%, 92.4%)	
07/03 09:23:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][450/781]	Step 26256	lr 0.01911	Loss 1.4521 (1.6048)	Prec@(1,5) (68.5%, 92.2%)	
07/03 09:24:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][500/781]	Step 26306	lr 0.01911	Loss 1.6099 (1.6061)	Prec@(1,5) (68.4%, 92.2%)	
07/03 09:24:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][550/781]	Step 26356	lr 0.01911	Loss 1.8090 (1.6105)	Prec@(1,5) (68.4%, 92.1%)	
07/03 09:24:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][600/781]	Step 26406	lr 0.01911	Loss 1.7569 (1.6127)	Prec@(1,5) (68.3%, 92.1%)	
07/03 09:24:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][650/781]	Step 26456	lr 0.01911	Loss 1.9431 (1.6143)	Prec@(1,5) (68.3%, 92.0%)	
07/03 09:24:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][700/781]	Step 26506	lr 0.01911	Loss 1.6430 (1.6209)	Prec@(1,5) (68.1%, 92.0%)	
07/03 09:24:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][750/781]	Step 26556	lr 0.01911	Loss 1.6599 (1.6211)	Prec@(1,5) (68.1%, 92.0%)	
07/03 09:25:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [33][781/781]	Step 26587	lr 0.01911	Loss 1.4918 (1.6176)	Prec@(1,5) (68.2%, 92.0%)	
07/03 09:25:06午後 evaluateCell_trainer.py:172 [INFO] Train: [ 33/99] Final Prec@1 68.1640%
07/03 09:25:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][50/391]	Step 26588	Loss 1.2269	Prec@(1,5) (65.2%, 90.4%)
07/03 09:25:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][100/391]	Step 26588	Loss 1.2381	Prec@(1,5) (64.6%, 90.1%)
07/03 09:25:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][150/391]	Step 26588	Loss 1.2279	Prec@(1,5) (65.0%, 90.1%)
07/03 09:25:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][200/391]	Step 26588	Loss 1.2309	Prec@(1,5) (64.8%, 90.1%)
07/03 09:25:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][250/391]	Step 26588	Loss 1.2258	Prec@(1,5) (65.0%, 90.2%)
07/03 09:25:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][300/391]	Step 26588	Loss 1.2199	Prec@(1,5) (65.0%, 90.4%)
07/03 09:25:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][350/391]	Step 26588	Loss 1.2305	Prec@(1,5) (64.7%, 90.2%)
07/03 09:25:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [33][390/391]	Step 26588	Loss 1.2318	Prec@(1,5) (64.7%, 90.1%)
07/03 09:25:31午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 33/99] Final Prec@1 64.7160%
07/03 09:25:31午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 66.2600%
07/03 09:25:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][50/781]	Step 26638	lr 0.01878	Loss 1.7627 (1.4795)	Prec@(1,5) (71.6%, 93.7%)	
07/03 09:25:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][100/781]	Step 26688	lr 0.01878	Loss 1.1690 (1.4858)	Prec@(1,5) (70.7%, 93.7%)	
07/03 09:26:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][150/781]	Step 26738	lr 0.01878	Loss 1.3960 (1.5029)	Prec@(1,5) (70.6%, 93.4%)	
07/03 09:26:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][200/781]	Step 26788	lr 0.01878	Loss 1.5040 (1.5334)	Prec@(1,5) (70.2%, 93.1%)	
07/03 09:26:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][250/781]	Step 26838	lr 0.01878	Loss 1.5143 (1.5393)	Prec@(1,5) (69.9%, 93.0%)	
07/03 09:26:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][300/781]	Step 26888	lr 0.01878	Loss 1.7631 (1.5550)	Prec@(1,5) (69.6%, 92.8%)	
07/03 09:26:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][350/781]	Step 26938	lr 0.01878	Loss 1.8500 (1.5642)	Prec@(1,5) (69.4%, 92.7%)	
07/03 09:27:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][400/781]	Step 26988	lr 0.01878	Loss 1.8099 (1.5653)	Prec@(1,5) (69.4%, 92.7%)	
07/03 09:27:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][450/781]	Step 27038	lr 0.01878	Loss 1.8421 (1.5770)	Prec@(1,5) (69.0%, 92.6%)	
07/03 09:27:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][500/781]	Step 27088	lr 0.01878	Loss 1.5625 (1.5763)	Prec@(1,5) (69.0%, 92.6%)	
07/03 09:27:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][550/781]	Step 27138	lr 0.01878	Loss 1.7107 (1.5824)	Prec@(1,5) (68.9%, 92.6%)	
07/03 09:27:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][600/781]	Step 27188	lr 0.01878	Loss 1.6120 (1.5872)	Prec@(1,5) (68.8%, 92.5%)	
07/03 09:27:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][650/781]	Step 27238	lr 0.01878	Loss 1.2965 (1.5905)	Prec@(1,5) (68.7%, 92.5%)	
07/03 09:28:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][700/781]	Step 27288	lr 0.01878	Loss 1.5908 (1.5932)	Prec@(1,5) (68.6%, 92.4%)	
07/03 09:28:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][750/781]	Step 27338	lr 0.01878	Loss 1.7282 (1.5967)	Prec@(1,5) (68.5%, 92.4%)	
07/03 09:28:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [34][781/781]	Step 27369	lr 0.01878	Loss 1.3980 (1.5990)	Prec@(1,5) (68.5%, 92.3%)	
07/03 09:28:29午後 evaluateCell_trainer.py:172 [INFO] Train: [ 34/99] Final Prec@1 68.4960%
07/03 09:28:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][50/391]	Step 27370	Loss 1.1718	Prec@(1,5) (64.9%, 91.8%)
07/03 09:28:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][100/391]	Step 27370	Loss 1.1819	Prec@(1,5) (65.5%, 91.2%)
07/03 09:28:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][150/391]	Step 27370	Loss 1.1951	Prec@(1,5) (65.1%, 91.0%)
07/03 09:28:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][200/391]	Step 27370	Loss 1.2025	Prec@(1,5) (65.1%, 90.7%)
07/03 09:28:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][250/391]	Step 27370	Loss 1.2104	Prec@(1,5) (65.0%, 90.5%)
07/03 09:28:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][300/391]	Step 27370	Loss 1.2080	Prec@(1,5) (65.0%, 90.4%)
07/03 09:28:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][350/391]	Step 27370	Loss 1.2006	Prec@(1,5) (65.2%, 90.4%)
07/03 09:28:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [34][390/391]	Step 27370	Loss 1.2050	Prec@(1,5) (65.2%, 90.4%)
07/03 09:28:54午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 34/99] Final Prec@1 65.2040%
07/03 09:28:54午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 66.2600%
07/03 09:29:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][50/781]	Step 27420	lr 0.01845	Loss 0.9747 (1.5052)	Prec@(1,5) (70.2%, 92.8%)	
07/03 09:29:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][100/781]	Step 27470	lr 0.01845	Loss 1.3271 (1.5025)	Prec@(1,5) (70.7%, 92.6%)	
07/03 09:29:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][150/781]	Step 27520	lr 0.01845	Loss 1.4489 (1.4855)	Prec@(1,5) (71.0%, 92.9%)	
07/03 09:29:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][200/781]	Step 27570	lr 0.01845	Loss 1.5507 (1.5172)	Prec@(1,5) (70.4%, 92.6%)	
07/03 09:29:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][250/781]	Step 27620	lr 0.01845	Loss 1.7722 (1.5221)	Prec@(1,5) (70.4%, 92.6%)	
07/03 09:30:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][300/781]	Step 27670	lr 0.01845	Loss 1.4802 (1.5271)	Prec@(1,5) (70.1%, 92.6%)	
07/03 09:30:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][350/781]	Step 27720	lr 0.01845	Loss 1.6319 (1.5276)	Prec@(1,5) (70.2%, 92.6%)	
07/03 09:30:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][400/781]	Step 27770	lr 0.01845	Loss 1.6072 (1.5375)	Prec@(1,5) (69.9%, 92.5%)	
07/03 09:30:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][450/781]	Step 27820	lr 0.01845	Loss 1.2893 (1.5405)	Prec@(1,5) (69.8%, 92.5%)	
07/03 09:30:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][500/781]	Step 27870	lr 0.01845	Loss 1.6824 (1.5480)	Prec@(1,5) (69.6%, 92.5%)	
07/03 09:30:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][550/781]	Step 27920	lr 0.01845	Loss 1.7142 (1.5579)	Prec@(1,5) (69.4%, 92.4%)	
07/03 09:31:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][600/781]	Step 27970	lr 0.01845	Loss 1.4782 (1.5647)	Prec@(1,5) (69.2%, 92.4%)	
07/03 09:31:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][650/781]	Step 28020	lr 0.01845	Loss 1.6494 (1.5676)	Prec@(1,5) (69.1%, 92.4%)	
07/03 09:31:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][700/781]	Step 28070	lr 0.01845	Loss 1.9097 (1.5707)	Prec@(1,5) (69.1%, 92.4%)	
07/03 09:31:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][750/781]	Step 28120	lr 0.01845	Loss 1.4051 (1.5729)	Prec@(1,5) (69.0%, 92.4%)	
07/03 09:31:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [35][781/781]	Step 28151	lr 0.01845	Loss 1.4411 (1.5740)	Prec@(1,5) (68.9%, 92.4%)	
07/03 09:31:52午後 evaluateCell_trainer.py:172 [INFO] Train: [ 35/99] Final Prec@1 68.9180%
07/03 09:31:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][50/391]	Step 28152	Loss 1.0990	Prec@(1,5) (68.5%, 91.8%)
07/03 09:31:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][100/391]	Step 28152	Loss 1.0917	Prec@(1,5) (68.2%, 91.9%)
07/03 09:32:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][150/391]	Step 28152	Loss 1.0922	Prec@(1,5) (68.1%, 92.1%)
07/03 09:32:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][200/391]	Step 28152	Loss 1.0838	Prec@(1,5) (68.3%, 92.1%)
07/03 09:32:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][250/391]	Step 28152	Loss 1.0883	Prec@(1,5) (68.0%, 92.0%)
07/03 09:32:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][300/391]	Step 28152	Loss 1.0916	Prec@(1,5) (68.0%, 92.0%)
07/03 09:32:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][350/391]	Step 28152	Loss 1.0973	Prec@(1,5) (67.9%, 92.0%)
07/03 09:32:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [35][390/391]	Step 28152	Loss 1.0891	Prec@(1,5) (68.2%, 92.0%)
07/03 09:32:17午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 35/99] Final Prec@1 68.1800%
07/03 09:32:17午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 68.1800%
07/03 09:32:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][50/781]	Step 28202	lr 0.01811	Loss 1.4043 (1.4379)	Prec@(1,5) (72.7%, 93.5%)	
07/03 09:32:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][100/781]	Step 28252	lr 0.01811	Loss 1.3685 (1.4981)	Prec@(1,5) (71.0%, 93.1%)	
07/03 09:32:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][150/781]	Step 28302	lr 0.01811	Loss 1.8114 (1.5062)	Prec@(1,5) (70.7%, 93.0%)	
07/03 09:33:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][200/781]	Step 28352	lr 0.01811	Loss 1.9124 (1.5164)	Prec@(1,5) (70.6%, 93.0%)	
07/03 09:33:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][250/781]	Step 28402	lr 0.01811	Loss 1.4343 (1.5227)	Prec@(1,5) (70.3%, 92.9%)	
07/03 09:33:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][300/781]	Step 28452	lr 0.01811	Loss 1.5038 (1.5264)	Prec@(1,5) (70.1%, 92.9%)	
07/03 09:33:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][350/781]	Step 28502	lr 0.01811	Loss 1.8048 (1.5297)	Prec@(1,5) (70.1%, 92.9%)	
07/03 09:33:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][400/781]	Step 28552	lr 0.01811	Loss 1.5588 (1.5347)	Prec@(1,5) (70.0%, 92.9%)	
07/03 09:34:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][450/781]	Step 28602	lr 0.01811	Loss 1.3116 (1.5348)	Prec@(1,5) (69.9%, 92.9%)	
07/03 09:34:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][500/781]	Step 28652	lr 0.01811	Loss 1.6024 (1.5388)	Prec@(1,5) (69.8%, 92.8%)	
07/03 09:34:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][550/781]	Step 28702	lr 0.01811	Loss 1.1916 (1.5412)	Prec@(1,5) (69.7%, 92.8%)	
07/03 09:34:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][600/781]	Step 28752	lr 0.01811	Loss 1.8650 (1.5429)	Prec@(1,5) (69.7%, 92.7%)	
07/03 09:34:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][650/781]	Step 28802	lr 0.01811	Loss 1.4956 (1.5445)	Prec@(1,5) (69.7%, 92.7%)	
07/03 09:34:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][700/781]	Step 28852	lr 0.01811	Loss 1.5861 (1.5465)	Prec@(1,5) (69.6%, 92.7%)	
07/03 09:35:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][750/781]	Step 28902	lr 0.01811	Loss 1.5375 (1.5512)	Prec@(1,5) (69.5%, 92.6%)	
07/03 09:35:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [36][781/781]	Step 28933	lr 0.01811	Loss 1.5636 (1.5530)	Prec@(1,5) (69.5%, 92.6%)	
07/03 09:35:15午後 evaluateCell_trainer.py:172 [INFO] Train: [ 36/99] Final Prec@1 69.5080%
07/03 09:35:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][50/391]	Step 28934	Loss 1.0987	Prec@(1,5) (68.1%, 92.2%)
07/03 09:35:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][100/391]	Step 28934	Loss 1.1129	Prec@(1,5) (67.7%, 91.8%)
07/03 09:35:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][150/391]	Step 28934	Loss 1.1145	Prec@(1,5) (67.8%, 91.7%)
07/03 09:35:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][200/391]	Step 28934	Loss 1.1209	Prec@(1,5) (67.8%, 91.5%)
07/03 09:35:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][250/391]	Step 28934	Loss 1.1296	Prec@(1,5) (67.7%, 91.4%)
07/03 09:35:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][300/391]	Step 28934	Loss 1.1236	Prec@(1,5) (67.5%, 91.5%)
07/03 09:35:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][350/391]	Step 28934	Loss 1.1204	Prec@(1,5) (67.7%, 91.5%)
07/03 09:35:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [36][390/391]	Step 28934	Loss 1.1161	Prec@(1,5) (67.7%, 91.6%)
07/03 09:35:41午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 36/99] Final Prec@1 67.7280%
07/03 09:35:41午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 68.1800%
07/03 09:35:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][50/781]	Step 28984	lr 0.01777	Loss 1.2952 (1.5089)	Prec@(1,5) (70.7%, 93.0%)	
07/03 09:36:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][100/781]	Step 29034	lr 0.01777	Loss 1.3601 (1.4737)	Prec@(1,5) (71.7%, 93.4%)	
07/03 09:36:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][150/781]	Step 29084	lr 0.01777	Loss 1.7173 (1.4636)	Prec@(1,5) (71.6%, 93.7%)	
07/03 09:36:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][200/781]	Step 29134	lr 0.01777	Loss 1.4691 (1.4661)	Prec@(1,5) (71.5%, 93.5%)	
07/03 09:36:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][250/781]	Step 29184	lr 0.01777	Loss 2.2989 (1.4801)	Prec@(1,5) (71.2%, 93.3%)	
07/03 09:36:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][300/781]	Step 29234	lr 0.01777	Loss 1.7958 (1.4925)	Prec@(1,5) (71.0%, 93.2%)	
07/03 09:37:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][350/781]	Step 29284	lr 0.01777	Loss 1.4255 (1.5049)	Prec@(1,5) (70.7%, 93.1%)	
07/03 09:37:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][400/781]	Step 29334	lr 0.01777	Loss 1.3279 (1.5098)	Prec@(1,5) (70.6%, 93.1%)	
07/03 09:37:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][450/781]	Step 29384	lr 0.01777	Loss 1.3657 (1.5146)	Prec@(1,5) (70.6%, 93.0%)	
07/03 09:37:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][500/781]	Step 29434	lr 0.01777	Loss 1.6997 (1.5133)	Prec@(1,5) (70.6%, 93.0%)	
07/03 09:37:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][550/781]	Step 29484	lr 0.01777	Loss 1.5269 (1.5235)	Prec@(1,5) (70.3%, 92.9%)	
07/03 09:37:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][600/781]	Step 29534	lr 0.01777	Loss 0.8087 (1.5238)	Prec@(1,5) (70.3%, 92.9%)	
07/03 09:38:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][650/781]	Step 29584	lr 0.01777	Loss 1.7401 (1.5260)	Prec@(1,5) (70.3%, 92.9%)	
07/03 09:38:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][700/781]	Step 29634	lr 0.01777	Loss 2.0251 (1.5294)	Prec@(1,5) (70.2%, 92.8%)	
07/03 09:38:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][750/781]	Step 29684	lr 0.01777	Loss 1.1838 (1.5323)	Prec@(1,5) (70.1%, 92.8%)	
07/03 09:38:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [37][781/781]	Step 29715	lr 0.01777	Loss 1.4321 (1.5360)	Prec@(1,5) (70.0%, 92.8%)	
07/03 09:38:40午後 evaluateCell_trainer.py:172 [INFO] Train: [ 37/99] Final Prec@1 69.9800%
07/03 09:38:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][50/391]	Step 29716	Loss 1.1253	Prec@(1,5) (66.5%, 91.9%)
07/03 09:38:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][100/391]	Step 29716	Loss 1.1111	Prec@(1,5) (67.2%, 91.7%)
07/03 09:38:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][150/391]	Step 29716	Loss 1.1135	Prec@(1,5) (67.3%, 91.9%)
07/03 09:38:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][200/391]	Step 29716	Loss 1.0989	Prec@(1,5) (67.6%, 92.1%)
07/03 09:38:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][250/391]	Step 29716	Loss 1.0962	Prec@(1,5) (67.7%, 92.2%)
07/03 09:38:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][300/391]	Step 29716	Loss 1.0922	Prec@(1,5) (67.9%, 92.3%)
07/03 09:39:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][350/391]	Step 29716	Loss 1.0937	Prec@(1,5) (67.8%, 92.2%)
07/03 09:39:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [37][390/391]	Step 29716	Loss 1.0985	Prec@(1,5) (67.6%, 92.1%)
07/03 09:39:05午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 37/99] Final Prec@1 67.6320%
07/03 09:39:05午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 68.1800%
07/03 09:39:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][50/781]	Step 29766	lr 0.01742	Loss 1.6991 (1.5407)	Prec@(1,5) (70.1%, 92.8%)	
07/03 09:39:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][100/781]	Step 29816	lr 0.01742	Loss 1.3927 (1.5265)	Prec@(1,5) (70.1%, 93.0%)	
07/03 09:39:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][150/781]	Step 29866	lr 0.01742	Loss 1.2540 (1.4968)	Prec@(1,5) (70.6%, 93.4%)	
07/03 09:39:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][200/781]	Step 29916	lr 0.01742	Loss 1.6993 (1.4745)	Prec@(1,5) (71.0%, 93.6%)	
07/03 09:40:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][250/781]	Step 29966	lr 0.01742	Loss 1.4975 (1.4798)	Prec@(1,5) (71.1%, 93.5%)	
07/03 09:40:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][300/781]	Step 30016	lr 0.01742	Loss 1.3140 (1.4820)	Prec@(1,5) (71.0%, 93.5%)	
07/03 09:40:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][350/781]	Step 30066	lr 0.01742	Loss 1.9524 (1.4961)	Prec@(1,5) (70.7%, 93.4%)	
07/03 09:40:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][400/781]	Step 30116	lr 0.01742	Loss 1.6679 (1.4961)	Prec@(1,5) (70.7%, 93.4%)	
07/03 09:40:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][450/781]	Step 30166	lr 0.01742	Loss 1.2421 (1.4944)	Prec@(1,5) (70.7%, 93.3%)	
07/03 09:40:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][500/781]	Step 30216	lr 0.01742	Loss 1.4489 (1.4915)	Prec@(1,5) (70.8%, 93.3%)	
07/03 09:41:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][550/781]	Step 30266	lr 0.01742	Loss 1.4994 (1.4930)	Prec@(1,5) (70.8%, 93.3%)	
07/03 09:41:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][600/781]	Step 30316	lr 0.01742	Loss 1.6222 (1.4979)	Prec@(1,5) (70.6%, 93.2%)	
07/03 09:41:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][650/781]	Step 30366	lr 0.01742	Loss 1.7002 (1.5008)	Prec@(1,5) (70.6%, 93.2%)	
07/03 09:41:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][700/781]	Step 30416	lr 0.01742	Loss 1.8677 (1.5043)	Prec@(1,5) (70.6%, 93.2%)	
07/03 09:41:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][750/781]	Step 30466	lr 0.01742	Loss 1.5532 (1.5116)	Prec@(1,5) (70.5%, 93.1%)	
07/03 09:42:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [38][781/781]	Step 30497	lr 0.01742	Loss 1.1523 (1.5126)	Prec@(1,5) (70.5%, 93.1%)	
07/03 09:42:03午後 evaluateCell_trainer.py:172 [INFO] Train: [ 38/99] Final Prec@1 70.5080%
07/03 09:42:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][50/391]	Step 30498	Loss 0.9967	Prec@(1,5) (70.2%, 93.3%)
07/03 09:42:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][100/391]	Step 30498	Loss 0.9740	Prec@(1,5) (71.0%, 93.6%)
07/03 09:42:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][150/391]	Step 30498	Loss 0.9658	Prec@(1,5) (71.0%, 93.8%)
07/03 09:42:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][200/391]	Step 30498	Loss 0.9695	Prec@(1,5) (70.7%, 93.7%)
07/03 09:42:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][250/391]	Step 30498	Loss 0.9639	Prec@(1,5) (71.1%, 93.7%)
07/03 09:42:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][300/391]	Step 30498	Loss 0.9754	Prec@(1,5) (70.9%, 93.6%)
07/03 09:42:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][350/391]	Step 30498	Loss 0.9822	Prec@(1,5) (70.7%, 93.5%)
07/03 09:42:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [38][390/391]	Step 30498	Loss 0.9800	Prec@(1,5) (70.6%, 93.5%)
07/03 09:42:28午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 38/99] Final Prec@1 70.6480%
07/03 09:42:29午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 70.6480%
07/03 09:42:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][50/781]	Step 30548	lr 0.01706	Loss 1.3877 (1.4310)	Prec@(1,5) (72.4%, 93.7%)	
07/03 09:42:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][100/781]	Step 30598	lr 0.01706	Loss 1.2633 (1.4372)	Prec@(1,5) (71.9%, 93.6%)	
07/03 09:43:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][150/781]	Step 30648	lr 0.01706	Loss 1.5515 (1.4266)	Prec@(1,5) (71.9%, 94.0%)	
07/03 09:43:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][200/781]	Step 30698	lr 0.01706	Loss 1.3709 (1.4307)	Prec@(1,5) (72.0%, 93.7%)	
07/03 09:43:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][250/781]	Step 30748	lr 0.01706	Loss 1.4240 (1.4341)	Prec@(1,5) (72.1%, 93.7%)	
07/03 09:43:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][300/781]	Step 30798	lr 0.01706	Loss 1.6784 (1.4433)	Prec@(1,5) (71.7%, 93.7%)	
07/03 09:43:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][350/781]	Step 30848	lr 0.01706	Loss 1.6465 (1.4499)	Prec@(1,5) (71.5%, 93.6%)	
07/03 09:43:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][400/781]	Step 30898	lr 0.01706	Loss 1.7875 (1.4561)	Prec@(1,5) (71.4%, 93.5%)	
07/03 09:44:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][450/781]	Step 30948	lr 0.01706	Loss 1.0442 (1.4578)	Prec@(1,5) (71.4%, 93.5%)	
07/03 09:44:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][500/781]	Step 30998	lr 0.01706	Loss 2.0904 (1.4659)	Prec@(1,5) (71.3%, 93.4%)	
07/03 09:44:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][550/781]	Step 31048	lr 0.01706	Loss 1.7240 (1.4773)	Prec@(1,5) (71.1%, 93.2%)	
07/03 09:44:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][600/781]	Step 31098	lr 0.01706	Loss 1.5759 (1.4796)	Prec@(1,5) (71.1%, 93.2%)	
07/03 09:44:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][650/781]	Step 31148	lr 0.01706	Loss 2.1213 (1.4838)	Prec@(1,5) (71.0%, 93.2%)	
07/03 09:45:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][700/781]	Step 31198	lr 0.01706	Loss 1.5893 (1.4914)	Prec@(1,5) (70.8%, 93.1%)	
07/03 09:45:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][750/781]	Step 31248	lr 0.01706	Loss 1.5807 (1.4892)	Prec@(1,5) (70.8%, 93.1%)	
07/03 09:45:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [39][781/781]	Step 31279	lr 0.01706	Loss 1.5519 (1.4938)	Prec@(1,5) (70.7%, 93.1%)	
07/03 09:45:27午後 evaluateCell_trainer.py:172 [INFO] Train: [ 39/99] Final Prec@1 70.7000%
07/03 09:45:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][50/391]	Step 31280	Loss 1.0612	Prec@(1,5) (68.6%, 92.7%)
07/03 09:45:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][100/391]	Step 31280	Loss 1.0744	Prec@(1,5) (68.4%, 92.5%)
07/03 09:45:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][150/391]	Step 31280	Loss 1.0678	Prec@(1,5) (68.7%, 92.5%)
07/03 09:45:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][200/391]	Step 31280	Loss 1.0672	Prec@(1,5) (68.6%, 92.6%)
07/03 09:45:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][250/391]	Step 31280	Loss 1.0629	Prec@(1,5) (68.7%, 92.6%)
07/03 09:45:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][300/391]	Step 31280	Loss 1.0587	Prec@(1,5) (68.8%, 92.6%)
07/03 09:45:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][350/391]	Step 31280	Loss 1.0652	Prec@(1,5) (68.6%, 92.5%)
07/03 09:45:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [39][390/391]	Step 31280	Loss 1.0655	Prec@(1,5) (68.6%, 92.5%)
07/03 09:45:52午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 39/99] Final Prec@1 68.5560%
07/03 09:45:52午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 70.6480%
07/03 09:46:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][50/781]	Step 31330	lr 0.01671	Loss 1.1161 (1.4126)	Prec@(1,5) (72.3%, 94.0%)	
07/03 09:46:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][100/781]	Step 31380	lr 0.01671	Loss 1.5243 (1.3827)	Prec@(1,5) (73.1%, 94.1%)	
07/03 09:46:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][150/781]	Step 31430	lr 0.01671	Loss 1.3697 (1.3883)	Prec@(1,5) (73.1%, 94.1%)	
07/03 09:46:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][200/781]	Step 31480	lr 0.01671	Loss 1.8779 (1.4149)	Prec@(1,5) (72.3%, 93.8%)	
07/03 09:46:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][250/781]	Step 31530	lr 0.01671	Loss 1.4585 (1.4240)	Prec@(1,5) (72.1%, 93.6%)	
07/03 09:47:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][300/781]	Step 31580	lr 0.01671	Loss 1.4744 (1.4284)	Prec@(1,5) (71.9%, 93.7%)	
07/03 09:47:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][350/781]	Step 31630	lr 0.01671	Loss 1.6127 (1.4285)	Prec@(1,5) (71.9%, 93.7%)	
07/03 09:47:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][400/781]	Step 31680	lr 0.01671	Loss 1.6561 (1.4360)	Prec@(1,5) (71.7%, 93.6%)	
07/03 09:47:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][450/781]	Step 31730	lr 0.01671	Loss 1.4328 (1.4448)	Prec@(1,5) (71.5%, 93.6%)	
07/03 09:47:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][500/781]	Step 31780	lr 0.01671	Loss 1.8170 (1.4557)	Prec@(1,5) (71.4%, 93.5%)	
07/03 09:47:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][550/781]	Step 31830	lr 0.01671	Loss 1.5866 (1.4538)	Prec@(1,5) (71.4%, 93.5%)	
07/03 09:48:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][600/781]	Step 31880	lr 0.01671	Loss 1.4120 (1.4568)	Prec@(1,5) (71.4%, 93.4%)	
07/03 09:48:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][650/781]	Step 31930	lr 0.01671	Loss 2.3331 (1.4612)	Prec@(1,5) (71.4%, 93.4%)	
07/03 09:48:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][700/781]	Step 31980	lr 0.01671	Loss 1.5507 (1.4661)	Prec@(1,5) (71.3%, 93.3%)	
07/03 09:48:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][750/781]	Step 32030	lr 0.01671	Loss 1.7611 (1.4743)	Prec@(1,5) (71.1%, 93.3%)	
07/03 09:48:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [40][781/781]	Step 32061	lr 0.01671	Loss 1.3403 (1.4730)	Prec@(1,5) (71.2%, 93.3%)	
07/03 09:48:50午後 evaluateCell_trainer.py:172 [INFO] Train: [ 40/99] Final Prec@1 71.1840%
07/03 09:48:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][50/391]	Step 32062	Loss 0.9686	Prec@(1,5) (70.7%, 93.6%)
07/03 09:48:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][100/391]	Step 32062	Loss 0.9678	Prec@(1,5) (71.0%, 93.4%)
07/03 09:48:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][150/391]	Step 32062	Loss 0.9699	Prec@(1,5) (70.9%, 93.4%)
07/03 09:49:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][200/391]	Step 32062	Loss 0.9648	Prec@(1,5) (71.1%, 93.5%)
07/03 09:49:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][250/391]	Step 32062	Loss 0.9716	Prec@(1,5) (71.1%, 93.4%)
07/03 09:49:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][300/391]	Step 32062	Loss 0.9707	Prec@(1,5) (71.0%, 93.4%)
07/03 09:49:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][350/391]	Step 32062	Loss 0.9691	Prec@(1,5) (71.1%, 93.5%)
07/03 09:49:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [40][390/391]	Step 32062	Loss 0.9666	Prec@(1,5) (71.2%, 93.5%)
07/03 09:49:15午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 40/99] Final Prec@1 71.1720%
07/03 09:49:15午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 71.1720%
07/03 09:49:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][50/781]	Step 32112	lr 0.01635	Loss 1.3170 (1.3901)	Prec@(1,5) (73.1%, 94.8%)	
07/03 09:49:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][100/781]	Step 32162	lr 0.01635	Loss 1.0861 (1.3904)	Prec@(1,5) (73.0%, 94.3%)	
07/03 09:49:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][150/781]	Step 32212	lr 0.01635	Loss 1.1008 (1.4038)	Prec@(1,5) (72.5%, 94.2%)	
07/03 09:50:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][200/781]	Step 32262	lr 0.01635	Loss 1.3485 (1.3996)	Prec@(1,5) (72.6%, 94.1%)	
07/03 09:50:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][250/781]	Step 32312	lr 0.01635	Loss 1.6151 (1.3989)	Prec@(1,5) (72.6%, 94.1%)	
07/03 09:50:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][300/781]	Step 32362	lr 0.01635	Loss 1.4951 (1.3993)	Prec@(1,5) (72.6%, 94.1%)	
07/03 09:50:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][350/781]	Step 32412	lr 0.01635	Loss 1.5422 (1.3958)	Prec@(1,5) (72.6%, 94.1%)	
07/03 09:50:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][400/781]	Step 32462	lr 0.01635	Loss 1.5207 (1.4052)	Prec@(1,5) (72.5%, 94.0%)	
07/03 09:50:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][450/781]	Step 32512	lr 0.01635	Loss 1.5960 (1.4148)	Prec@(1,5) (72.4%, 94.0%)	
07/03 09:51:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][500/781]	Step 32562	lr 0.01635	Loss 1.3431 (1.4253)	Prec@(1,5) (72.2%, 93.8%)	
07/03 09:51:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][550/781]	Step 32612	lr 0.01635	Loss 1.0746 (1.4310)	Prec@(1,5) (72.1%, 93.7%)	
07/03 09:51:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][600/781]	Step 32662	lr 0.01635	Loss 1.6993 (1.4302)	Prec@(1,5) (72.1%, 93.7%)	
07/03 09:51:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][650/781]	Step 32712	lr 0.01635	Loss 1.6965 (1.4326)	Prec@(1,5) (72.0%, 93.7%)	
07/03 09:51:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][700/781]	Step 32762	lr 0.01635	Loss 1.3738 (1.4370)	Prec@(1,5) (71.9%, 93.6%)	
07/03 09:52:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][750/781]	Step 32812	lr 0.01635	Loss 1.6517 (1.4433)	Prec@(1,5) (71.7%, 93.6%)	
07/03 09:52:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [41][781/781]	Step 32843	lr 0.01635	Loss 1.8071 (1.4453)	Prec@(1,5) (71.8%, 93.6%)	
07/03 09:52:14午後 evaluateCell_trainer.py:172 [INFO] Train: [ 41/99] Final Prec@1 71.7660%
07/03 09:52:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][50/391]	Step 32844	Loss 0.8917	Prec@(1,5) (74.0%, 94.3%)
07/03 09:52:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][100/391]	Step 32844	Loss 0.9026	Prec@(1,5) (73.4%, 94.2%)
07/03 09:52:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][150/391]	Step 32844	Loss 0.9106	Prec@(1,5) (73.4%, 94.0%)
07/03 09:52:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][200/391]	Step 32844	Loss 0.9136	Prec@(1,5) (73.3%, 94.0%)
07/03 09:52:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][250/391]	Step 32844	Loss 0.9178	Prec@(1,5) (73.2%, 93.9%)
07/03 09:52:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][300/391]	Step 32844	Loss 0.9210	Prec@(1,5) (73.2%, 93.8%)
07/03 09:52:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][350/391]	Step 32844	Loss 0.9234	Prec@(1,5) (73.2%, 93.7%)
07/03 09:52:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [41][390/391]	Step 32844	Loss 0.9254	Prec@(1,5) (73.1%, 93.7%)
07/03 09:52:38午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 41/99] Final Prec@1 73.1000%
07/03 09:52:39午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 73.1000%
07/03 09:52:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][50/781]	Step 32894	lr 0.01598	Loss 1.2651 (1.4019)	Prec@(1,5) (73.2%, 93.7%)	
07/03 09:53:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][100/781]	Step 32944	lr 0.01598	Loss 1.6401 (1.3981)	Prec@(1,5) (72.7%, 94.1%)	
07/03 09:53:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][150/781]	Step 32994	lr 0.01598	Loss 1.5305 (1.3774)	Prec@(1,5) (72.9%, 94.4%)	
07/03 09:53:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][200/781]	Step 33044	lr 0.01598	Loss 1.3823 (1.3792)	Prec@(1,5) (73.0%, 94.3%)	
07/03 09:53:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][250/781]	Step 33094	lr 0.01598	Loss 1.0480 (1.3895)	Prec@(1,5) (72.8%, 94.3%)	
07/03 09:53:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][300/781]	Step 33144	lr 0.01598	Loss 1.5307 (1.4002)	Prec@(1,5) (72.6%, 94.1%)	
07/03 09:53:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][350/781]	Step 33194	lr 0.01598	Loss 1.3895 (1.4020)	Prec@(1,5) (72.5%, 94.1%)	
07/03 09:54:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][400/781]	Step 33244	lr 0.01598	Loss 1.5349 (1.4049)	Prec@(1,5) (72.2%, 94.1%)	
07/03 09:54:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][450/781]	Step 33294	lr 0.01598	Loss 1.3129 (1.4081)	Prec@(1,5) (72.2%, 94.0%)	
07/03 09:54:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][500/781]	Step 33344	lr 0.01598	Loss 1.0912 (1.4135)	Prec@(1,5) (72.1%, 94.0%)	
07/03 09:54:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][550/781]	Step 33394	lr 0.01598	Loss 1.3579 (1.4180)	Prec@(1,5) (72.1%, 94.0%)	
07/03 09:54:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][600/781]	Step 33444	lr 0.01598	Loss 1.2301 (1.4232)	Prec@(1,5) (72.0%, 93.9%)	
07/03 09:55:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][650/781]	Step 33494	lr 0.01598	Loss 1.6124 (1.4261)	Prec@(1,5) (71.9%, 93.8%)	
07/03 09:55:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][700/781]	Step 33544	lr 0.01598	Loss 1.2813 (1.4288)	Prec@(1,5) (71.9%, 93.8%)	
07/03 09:55:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][750/781]	Step 33594	lr 0.01598	Loss 1.2819 (1.4334)	Prec@(1,5) (71.8%, 93.7%)	
07/03 09:55:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [42][781/781]	Step 33625	lr 0.01598	Loss 1.5327 (1.4342)	Prec@(1,5) (71.8%, 93.7%)	
07/03 09:55:37午後 evaluateCell_trainer.py:172 [INFO] Train: [ 42/99] Final Prec@1 71.7880%
07/03 09:55:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][50/391]	Step 33626	Loss 0.9879	Prec@(1,5) (71.2%, 92.7%)
07/03 09:55:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][100/391]	Step 33626	Loss 0.9956	Prec@(1,5) (70.7%, 93.1%)
07/03 09:55:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][150/391]	Step 33626	Loss 0.9968	Prec@(1,5) (70.3%, 93.3%)
07/03 09:55:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][200/391]	Step 33626	Loss 1.0032	Prec@(1,5) (70.2%, 93.1%)
07/03 09:55:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][250/391]	Step 33626	Loss 1.0032	Prec@(1,5) (70.1%, 93.2%)
07/03 09:55:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][300/391]	Step 33626	Loss 1.0025	Prec@(1,5) (70.0%, 93.2%)
07/03 09:56:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][350/391]	Step 33626	Loss 1.0051	Prec@(1,5) (70.1%, 93.1%)
07/03 09:56:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [42][390/391]	Step 33626	Loss 1.0036	Prec@(1,5) (70.2%, 93.1%)
07/03 09:56:03午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 42/99] Final Prec@1 70.1760%
07/03 09:56:03午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 73.1000%
07/03 09:56:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][50/781]	Step 33676	lr 0.01562	Loss 1.5867 (1.3106)	Prec@(1,5) (74.4%, 94.6%)	
07/03 09:56:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][100/781]	Step 33726	lr 0.01562	Loss 1.6855 (1.3286)	Prec@(1,5) (74.1%, 94.6%)	
07/03 09:56:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][150/781]	Step 33776	lr 0.01562	Loss 1.1227 (1.3334)	Prec@(1,5) (74.0%, 94.7%)	
07/03 09:56:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][200/781]	Step 33826	lr 0.01562	Loss 1.2287 (1.3269)	Prec@(1,5) (74.0%, 94.8%)	
07/03 09:57:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][250/781]	Step 33876	lr 0.01562	Loss 1.7273 (1.3387)	Prec@(1,5) (73.7%, 94.6%)	
07/03 09:57:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][300/781]	Step 33926	lr 0.01562	Loss 1.6439 (1.3517)	Prec@(1,5) (73.4%, 94.4%)	
07/03 09:57:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][350/781]	Step 33976	lr 0.01562	Loss 1.4589 (1.3673)	Prec@(1,5) (73.0%, 94.3%)	
07/03 09:57:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][400/781]	Step 34026	lr 0.01562	Loss 1.2333 (1.3809)	Prec@(1,5) (72.9%, 94.2%)	
07/03 09:57:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][450/781]	Step 34076	lr 0.01562	Loss 1.2496 (1.3846)	Prec@(1,5) (72.9%, 94.1%)	
07/03 09:57:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][500/781]	Step 34126	lr 0.01562	Loss 1.7158 (1.3902)	Prec@(1,5) (72.8%, 94.1%)	
07/03 09:58:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][550/781]	Step 34176	lr 0.01562	Loss 1.3891 (1.3975)	Prec@(1,5) (72.6%, 94.0%)	
07/03 09:58:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][600/781]	Step 34226	lr 0.01562	Loss 1.3711 (1.3962)	Prec@(1,5) (72.5%, 94.1%)	
07/03 09:58:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][650/781]	Step 34276	lr 0.01562	Loss 1.6747 (1.3956)	Prec@(1,5) (72.6%, 94.1%)	
07/03 09:58:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][700/781]	Step 34326	lr 0.01562	Loss 1.7576 (1.4016)	Prec@(1,5) (72.5%, 94.1%)	
07/03 09:58:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][750/781]	Step 34376	lr 0.01562	Loss 1.5414 (1.4033)	Prec@(1,5) (72.4%, 94.0%)	
07/03 09:59:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [43][781/781]	Step 34407	lr 0.01562	Loss 1.4792 (1.4021)	Prec@(1,5) (72.5%, 94.1%)	
07/03 09:59:00午後 evaluateCell_trainer.py:172 [INFO] Train: [ 43/99] Final Prec@1 72.4500%
07/03 09:59:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][50/391]	Step 34408	Loss 0.9177	Prec@(1,5) (73.7%, 94.1%)
07/03 09:59:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][100/391]	Step 34408	Loss 0.8784	Prec@(1,5) (74.5%, 94.4%)
07/03 09:59:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][150/391]	Step 34408	Loss 0.8665	Prec@(1,5) (74.5%, 94.6%)
07/03 09:59:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][200/391]	Step 34408	Loss 0.8718	Prec@(1,5) (74.4%, 94.4%)
07/03 09:59:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][250/391]	Step 34408	Loss 0.8727	Prec@(1,5) (74.2%, 94.5%)
07/03 09:59:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][300/391]	Step 34408	Loss 0.8725	Prec@(1,5) (74.2%, 94.5%)
07/03 09:59:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][350/391]	Step 34408	Loss 0.8678	Prec@(1,5) (74.3%, 94.5%)
07/03 09:59:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [43][390/391]	Step 34408	Loss 0.8678	Prec@(1,5) (74.3%, 94.6%)
07/03 09:59:26午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 43/99] Final Prec@1 74.3000%
07/03 09:59:26午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 74.3000%
07/03 09:59:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][50/781]	Step 34458	lr 0.01525	Loss 1.2319 (1.2684)	Prec@(1,5) (75.4%, 95.1%)	
07/03 09:59:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][100/781]	Step 34508	lr 0.01525	Loss 1.3620 (1.2664)	Prec@(1,5) (75.4%, 95.1%)	
07/03 10:00:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][150/781]	Step 34558	lr 0.01525	Loss 1.1330 (1.2767)	Prec@(1,5) (75.4%, 95.0%)	
07/03 10:00:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][200/781]	Step 34608	lr 0.01525	Loss 1.4574 (1.2885)	Prec@(1,5) (75.1%, 94.8%)	
07/03 10:00:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][250/781]	Step 34658	lr 0.01525	Loss 1.3387 (1.3186)	Prec@(1,5) (74.5%, 94.5%)	
07/03 10:00:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][300/781]	Step 34708	lr 0.01525	Loss 1.6269 (1.3210)	Prec@(1,5) (74.3%, 94.5%)	
07/03 10:00:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][350/781]	Step 34758	lr 0.01525	Loss 1.4968 (1.3319)	Prec@(1,5) (74.1%, 94.5%)	
07/03 10:00:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][400/781]	Step 34808	lr 0.01525	Loss 1.0125 (1.3393)	Prec@(1,5) (73.8%, 94.4%)	
07/03 10:01:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][450/781]	Step 34858	lr 0.01525	Loss 1.0079 (1.3461)	Prec@(1,5) (73.7%, 94.4%)	
07/03 10:01:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][500/781]	Step 34908	lr 0.01525	Loss 1.5423 (1.3522)	Prec@(1,5) (73.6%, 94.3%)	
07/03 10:01:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][550/781]	Step 34958	lr 0.01525	Loss 1.4482 (1.3596)	Prec@(1,5) (73.5%, 94.3%)	
07/03 10:01:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][600/781]	Step 35008	lr 0.01525	Loss 1.5858 (1.3597)	Prec@(1,5) (73.4%, 94.3%)	
07/03 10:01:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][650/781]	Step 35058	lr 0.01525	Loss 1.0625 (1.3634)	Prec@(1,5) (73.4%, 94.2%)	
07/03 10:02:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][700/781]	Step 35108	lr 0.01525	Loss 1.5321 (1.3706)	Prec@(1,5) (73.2%, 94.2%)	
07/03 10:02:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][750/781]	Step 35158	lr 0.01525	Loss 1.3594 (1.3790)	Prec@(1,5) (73.1%, 94.1%)	
07/03 10:02:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [44][781/781]	Step 35189	lr 0.01525	Loss 1.0617 (1.3818)	Prec@(1,5) (73.0%, 94.1%)	
07/03 10:02:24午後 evaluateCell_trainer.py:172 [INFO] Train: [ 44/99] Final Prec@1 72.9960%
07/03 10:02:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][50/391]	Step 35190	Loss 0.9318	Prec@(1,5) (72.2%, 94.0%)
07/03 10:02:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][100/391]	Step 35190	Loss 0.9524	Prec@(1,5) (71.4%, 93.6%)
07/03 10:02:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][150/391]	Step 35190	Loss 0.9608	Prec@(1,5) (71.3%, 93.7%)
07/03 10:02:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][200/391]	Step 35190	Loss 0.9531	Prec@(1,5) (71.6%, 93.9%)
07/03 10:02:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][250/391]	Step 35190	Loss 0.9486	Prec@(1,5) (71.6%, 93.8%)
07/03 10:02:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][300/391]	Step 35190	Loss 0.9399	Prec@(1,5) (71.9%, 93.9%)
07/03 10:02:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][350/391]	Step 35190	Loss 0.9375	Prec@(1,5) (72.0%, 93.9%)
07/03 10:02:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [44][390/391]	Step 35190	Loss 0.9393	Prec@(1,5) (72.1%, 93.9%)
07/03 10:02:49午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 44/99] Final Prec@1 72.0560%
07/03 10:02:49午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 74.3000%
07/03 10:03:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][50/781]	Step 35240	lr 0.01488	Loss 1.6938 (1.3409)	Prec@(1,5) (74.1%, 94.3%)	
07/03 10:03:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][100/781]	Step 35290	lr 0.01488	Loss 1.3626 (1.3242)	Prec@(1,5) (74.0%, 94.7%)	
07/03 10:03:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][150/781]	Step 35340	lr 0.01488	Loss 1.2819 (1.3040)	Prec@(1,5) (74.5%, 95.0%)	
07/03 10:03:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][200/781]	Step 35390	lr 0.01488	Loss 1.3777 (1.3140)	Prec@(1,5) (74.4%, 94.8%)	
07/03 10:03:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][250/781]	Step 35440	lr 0.01488	Loss 1.4966 (1.3138)	Prec@(1,5) (74.2%, 94.9%)	
07/03 10:03:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][300/781]	Step 35490	lr 0.01488	Loss 1.5769 (1.3217)	Prec@(1,5) (74.1%, 94.8%)	
07/03 10:04:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][350/781]	Step 35540	lr 0.01488	Loss 1.7096 (1.3300)	Prec@(1,5) (74.0%, 94.7%)	
07/03 10:04:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][400/781]	Step 35590	lr 0.01488	Loss 1.1745 (1.3313)	Prec@(1,5) (73.9%, 94.8%)	
07/03 10:04:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][450/781]	Step 35640	lr 0.01488	Loss 1.3611 (1.3394)	Prec@(1,5) (73.7%, 94.7%)	
07/03 10:04:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][500/781]	Step 35690	lr 0.01488	Loss 1.4318 (1.3386)	Prec@(1,5) (73.8%, 94.7%)	
07/03 10:04:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][550/781]	Step 35740	lr 0.01488	Loss 1.5211 (1.3455)	Prec@(1,5) (73.6%, 94.7%)	
07/03 10:05:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][600/781]	Step 35790	lr 0.01488	Loss 1.2849 (1.3444)	Prec@(1,5) (73.7%, 94.6%)	
07/03 10:05:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][650/781]	Step 35840	lr 0.01488	Loss 1.4572 (1.3479)	Prec@(1,5) (73.6%, 94.6%)	
07/03 10:05:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][700/781]	Step 35890	lr 0.01488	Loss 1.5123 (1.3502)	Prec@(1,5) (73.6%, 94.5%)	
07/03 10:05:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][750/781]	Step 35940	lr 0.01488	Loss 1.4347 (1.3571)	Prec@(1,5) (73.4%, 94.4%)	
07/03 10:05:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [45][781/781]	Step 35971	lr 0.01488	Loss 1.3215 (1.3611)	Prec@(1,5) (73.4%, 94.4%)	
07/03 10:05:48午後 evaluateCell_trainer.py:172 [INFO] Train: [ 45/99] Final Prec@1 73.3600%
07/03 10:05:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][50/391]	Step 35972	Loss 0.9538	Prec@(1,5) (71.8%, 93.6%)
07/03 10:05:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][100/391]	Step 35972	Loss 0.9511	Prec@(1,5) (72.2%, 93.6%)
07/03 10:05:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][150/391]	Step 35972	Loss 0.9510	Prec@(1,5) (72.4%, 93.5%)
07/03 10:06:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][200/391]	Step 35972	Loss 0.9395	Prec@(1,5) (72.5%, 93.7%)
07/03 10:06:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][250/391]	Step 35972	Loss 0.9426	Prec@(1,5) (72.5%, 93.7%)
07/03 10:06:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][300/391]	Step 35972	Loss 0.9374	Prec@(1,5) (72.6%, 93.7%)
07/03 10:06:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][350/391]	Step 35972	Loss 0.9300	Prec@(1,5) (72.8%, 93.8%)
07/03 10:06:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [45][390/391]	Step 35972	Loss 0.9322	Prec@(1,5) (72.7%, 93.8%)
07/03 10:06:13午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 45/99] Final Prec@1 72.6600%
07/03 10:06:13午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 74.3000%
07/03 10:06:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][50/781]	Step 36022	lr 0.0145	Loss 1.1842 (1.2702)	Prec@(1,5) (74.8%, 95.5%)	
07/03 10:06:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][100/781]	Step 36072	lr 0.0145	Loss 1.2887 (1.2744)	Prec@(1,5) (75.3%, 95.3%)	
07/03 10:06:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][150/781]	Step 36122	lr 0.0145	Loss 1.2205 (1.2990)	Prec@(1,5) (74.8%, 94.9%)	
07/03 10:06:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][200/781]	Step 36172	lr 0.0145	Loss 1.7059 (1.3113)	Prec@(1,5) (74.1%, 94.8%)	
07/03 10:07:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][250/781]	Step 36222	lr 0.0145	Loss 1.3183 (1.3113)	Prec@(1,5) (74.2%, 94.8%)	
07/03 10:07:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][300/781]	Step 36272	lr 0.0145	Loss 1.1332 (1.3093)	Prec@(1,5) (74.3%, 94.8%)	
07/03 10:07:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][350/781]	Step 36322	lr 0.0145	Loss 1.4683 (1.3187)	Prec@(1,5) (74.1%, 94.7%)	
07/03 10:07:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][400/781]	Step 36372	lr 0.0145	Loss 1.2360 (1.3209)	Prec@(1,5) (74.1%, 94.7%)	
07/03 10:07:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][450/781]	Step 36422	lr 0.0145	Loss 1.3612 (1.3242)	Prec@(1,5) (74.1%, 94.7%)	
07/03 10:08:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][500/781]	Step 36472	lr 0.0145	Loss 1.6069 (1.3291)	Prec@(1,5) (74.1%, 94.6%)	
07/03 10:08:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][550/781]	Step 36522	lr 0.0145	Loss 1.3755 (1.3313)	Prec@(1,5) (74.1%, 94.5%)	
07/03 10:08:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][600/781]	Step 36572	lr 0.0145	Loss 1.1344 (1.3305)	Prec@(1,5) (74.0%, 94.6%)	
07/03 10:08:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][650/781]	Step 36622	lr 0.0145	Loss 1.8415 (1.3346)	Prec@(1,5) (73.9%, 94.5%)	
07/03 10:08:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][700/781]	Step 36672	lr 0.0145	Loss 1.4177 (1.3378)	Prec@(1,5) (73.8%, 94.5%)	
07/03 10:09:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][750/781]	Step 36722	lr 0.0145	Loss 0.9945 (1.3371)	Prec@(1,5) (73.9%, 94.5%)	
07/03 10:09:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [46][781/781]	Step 36753	lr 0.0145	Loss 1.2948 (1.3387)	Prec@(1,5) (73.9%, 94.5%)	
07/03 10:09:12午後 evaluateCell_trainer.py:172 [INFO] Train: [ 46/99] Final Prec@1 73.8760%
07/03 10:09:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][50/391]	Step 36754	Loss 0.8980	Prec@(1,5) (73.3%, 94.6%)
07/03 10:09:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][100/391]	Step 36754	Loss 0.8935	Prec@(1,5) (73.5%, 94.3%)
07/03 10:09:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][150/391]	Step 36754	Loss 0.8890	Prec@(1,5) (73.3%, 94.4%)
07/03 10:09:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][200/391]	Step 36754	Loss 0.8897	Prec@(1,5) (73.4%, 94.2%)
07/03 10:09:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][250/391]	Step 36754	Loss 0.8861	Prec@(1,5) (73.5%, 94.3%)
07/03 10:09:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][300/391]	Step 36754	Loss 0.8959	Prec@(1,5) (73.1%, 94.3%)
07/03 10:09:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][350/391]	Step 36754	Loss 0.8939	Prec@(1,5) (73.2%, 94.3%)
07/03 10:09:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [46][390/391]	Step 36754	Loss 0.8994	Prec@(1,5) (73.1%, 94.2%)
07/03 10:09:37午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 46/99] Final Prec@1 73.0960%
07/03 10:09:37午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 74.3000%
07/03 10:09:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][50/781]	Step 36804	lr 0.01413	Loss 1.3914 (1.2634)	Prec@(1,5) (75.3%, 94.8%)	
07/03 10:10:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][100/781]	Step 36854	lr 0.01413	Loss 1.0666 (1.2353)	Prec@(1,5) (76.2%, 95.1%)	
07/03 10:10:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][150/781]	Step 36904	lr 0.01413	Loss 1.2618 (1.2530)	Prec@(1,5) (75.9%, 95.1%)	
07/03 10:10:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][200/781]	Step 36954	lr 0.01413	Loss 1.2386 (1.2522)	Prec@(1,5) (75.5%, 95.2%)	
07/03 10:10:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][250/781]	Step 37004	lr 0.01413	Loss 1.4099 (1.2624)	Prec@(1,5) (75.1%, 95.2%)	
07/03 10:10:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][300/781]	Step 37054	lr 0.01413	Loss 1.3762 (1.2639)	Prec@(1,5) (75.1%, 95.2%)	
07/03 10:10:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][350/781]	Step 37104	lr 0.01413	Loss 1.3450 (1.2723)	Prec@(1,5) (74.9%, 95.1%)	
07/03 10:11:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][400/781]	Step 37154	lr 0.01413	Loss 1.1049 (1.2732)	Prec@(1,5) (74.9%, 95.1%)	
07/03 10:11:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][450/781]	Step 37204	lr 0.01413	Loss 1.0863 (1.2723)	Prec@(1,5) (75.0%, 95.1%)	
07/03 10:11:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][500/781]	Step 37254	lr 0.01413	Loss 1.1573 (1.2726)	Prec@(1,5) (75.0%, 95.1%)	
07/03 10:11:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][550/781]	Step 37304	lr 0.01413	Loss 1.0854 (1.2793)	Prec@(1,5) (74.8%, 95.1%)	
07/03 10:11:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][600/781]	Step 37354	lr 0.01413	Loss 1.0953 (1.2833)	Prec@(1,5) (74.7%, 95.0%)	
07/03 10:12:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][650/781]	Step 37404	lr 0.01413	Loss 1.2755 (1.2938)	Prec@(1,5) (74.6%, 94.9%)	
07/03 10:12:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][700/781]	Step 37454	lr 0.01413	Loss 1.4188 (1.2995)	Prec@(1,5) (74.5%, 94.9%)	
07/03 10:12:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][750/781]	Step 37504	lr 0.01413	Loss 1.2361 (1.3029)	Prec@(1,5) (74.5%, 94.9%)	
07/03 10:12:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [47][781/781]	Step 37535	lr 0.01413	Loss 1.1663 (1.3050)	Prec@(1,5) (74.5%, 94.8%)	
07/03 10:12:35午後 evaluateCell_trainer.py:172 [INFO] Train: [ 47/99] Final Prec@1 74.4840%
07/03 10:12:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][50/391]	Step 37536	Loss 0.8617	Prec@(1,5) (74.2%, 94.9%)
07/03 10:12:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][100/391]	Step 37536	Loss 0.8757	Prec@(1,5) (74.0%, 94.6%)
07/03 10:12:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][150/391]	Step 37536	Loss 0.8626	Prec@(1,5) (74.3%, 94.6%)
07/03 10:12:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][200/391]	Step 37536	Loss 0.8693	Prec@(1,5) (73.9%, 94.7%)
07/03 10:12:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][250/391]	Step 37536	Loss 0.8646	Prec@(1,5) (73.9%, 94.8%)
07/03 10:12:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][300/391]	Step 37536	Loss 0.8577	Prec@(1,5) (74.1%, 94.9%)
07/03 10:12:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][350/391]	Step 37536	Loss 0.8555	Prec@(1,5) (74.2%, 94.8%)
07/03 10:13:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [47][390/391]	Step 37536	Loss 0.8545	Prec@(1,5) (74.2%, 94.8%)
07/03 10:13:00午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 47/99] Final Prec@1 74.2200%
07/03 10:13:00午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 74.3000%
07/03 10:13:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][50/781]	Step 37586	lr 0.01375	Loss 1.2068 (1.2073)	Prec@(1,5) (77.2%, 95.5%)	
07/03 10:13:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][100/781]	Step 37636	lr 0.01375	Loss 1.1399 (1.2173)	Prec@(1,5) (76.9%, 95.7%)	
07/03 10:13:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][150/781]	Step 37686	lr 0.01375	Loss 1.4090 (1.2266)	Prec@(1,5) (76.4%, 95.5%)	
07/03 10:13:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][200/781]	Step 37736	lr 0.01375	Loss 1.3119 (1.2338)	Prec@(1,5) (76.5%, 95.4%)	
07/03 10:13:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][250/781]	Step 37786	lr 0.01375	Loss 1.2397 (1.2410)	Prec@(1,5) (76.4%, 95.3%)	
07/03 10:14:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][300/781]	Step 37836	lr 0.01375	Loss 1.7935 (1.2447)	Prec@(1,5) (76.1%, 95.4%)	
07/03 10:14:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][350/781]	Step 37886	lr 0.01375	Loss 1.1844 (1.2472)	Prec@(1,5) (76.1%, 95.3%)	
07/03 10:14:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][400/781]	Step 37936	lr 0.01375	Loss 1.1573 (1.2527)	Prec@(1,5) (75.9%, 95.3%)	
07/03 10:14:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][450/781]	Step 37986	lr 0.01375	Loss 1.3936 (1.2601)	Prec@(1,5) (75.7%, 95.3%)	
07/03 10:14:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][500/781]	Step 38036	lr 0.01375	Loss 1.8677 (1.2703)	Prec@(1,5) (75.3%, 95.3%)	
07/03 10:15:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][550/781]	Step 38086	lr 0.01375	Loss 1.3254 (1.2771)	Prec@(1,5) (75.1%, 95.2%)	
07/03 10:15:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][600/781]	Step 38136	lr 0.01375	Loss 1.3267 (1.2836)	Prec@(1,5) (74.9%, 95.1%)	
07/03 10:15:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][650/781]	Step 38186	lr 0.01375	Loss 1.1459 (1.2864)	Prec@(1,5) (74.9%, 95.1%)	
07/03 10:15:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][700/781]	Step 38236	lr 0.01375	Loss 1.0707 (1.2875)	Prec@(1,5) (74.8%, 95.0%)	
07/03 10:15:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][750/781]	Step 38286	lr 0.01375	Loss 1.5345 (1.2904)	Prec@(1,5) (74.7%, 95.0%)	
07/03 10:15:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [48][781/781]	Step 38317	lr 0.01375	Loss 1.3650 (1.2937)	Prec@(1,5) (74.7%, 95.0%)	
07/03 10:15:58午後 evaluateCell_trainer.py:172 [INFO] Train: [ 48/99] Final Prec@1 74.6880%
07/03 10:16:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][50/391]	Step 38318	Loss 0.7855	Prec@(1,5) (77.3%, 95.5%)
07/03 10:16:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][100/391]	Step 38318	Loss 0.8047	Prec@(1,5) (76.6%, 95.3%)
07/03 10:16:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][150/391]	Step 38318	Loss 0.8077	Prec@(1,5) (76.1%, 95.4%)
07/03 10:16:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][200/391]	Step 38318	Loss 0.8042	Prec@(1,5) (76.1%, 95.4%)
07/03 10:16:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][250/391]	Step 38318	Loss 0.8034	Prec@(1,5) (76.1%, 95.4%)
07/03 10:16:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][300/391]	Step 38318	Loss 0.8112	Prec@(1,5) (75.9%, 95.3%)
07/03 10:16:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][350/391]	Step 38318	Loss 0.8061	Prec@(1,5) (76.0%, 95.4%)
07/03 10:16:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [48][390/391]	Step 38318	Loss 0.8077	Prec@(1,5) (76.0%, 95.3%)
07/03 10:16:23午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 48/99] Final Prec@1 75.9840%
07/03 10:16:23午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 75.9840%
07/03 10:16:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][50/781]	Step 38368	lr 0.01338	Loss 1.4418 (1.1668)	Prec@(1,5) (78.1%, 96.2%)	
07/03 10:16:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][100/781]	Step 38418	lr 0.01338	Loss 0.9834 (1.1781)	Prec@(1,5) (77.3%, 95.9%)	
07/03 10:16:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][150/781]	Step 38468	lr 0.01338	Loss 1.3450 (1.1944)	Prec@(1,5) (77.0%, 95.8%)	
07/03 10:17:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][200/781]	Step 38518	lr 0.01338	Loss 0.8747 (1.2093)	Prec@(1,5) (76.6%, 95.5%)	
07/03 10:17:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][250/781]	Step 38568	lr 0.01338	Loss 0.8558 (1.2067)	Prec@(1,5) (76.7%, 95.6%)	
07/03 10:17:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][300/781]	Step 38618	lr 0.01338	Loss 1.3725 (1.2139)	Prec@(1,5) (76.4%, 95.5%)	
07/03 10:17:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][350/781]	Step 38668	lr 0.01338	Loss 1.3995 (1.2282)	Prec@(1,5) (76.1%, 95.4%)	
07/03 10:17:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][400/781]	Step 38718	lr 0.01338	Loss 1.5268 (1.2422)	Prec@(1,5) (75.8%, 95.2%)	
07/03 10:18:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][450/781]	Step 38768	lr 0.01338	Loss 1.4512 (1.2480)	Prec@(1,5) (75.7%, 95.3%)	
07/03 10:18:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][500/781]	Step 38818	lr 0.01338	Loss 1.3960 (1.2492)	Prec@(1,5) (75.6%, 95.2%)	
07/03 10:18:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][550/781]	Step 38868	lr 0.01338	Loss 1.4228 (1.2558)	Prec@(1,5) (75.6%, 95.2%)	
07/03 10:18:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][600/781]	Step 38918	lr 0.01338	Loss 1.9197 (1.2564)	Prec@(1,5) (75.6%, 95.1%)	
07/03 10:18:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][650/781]	Step 38968	lr 0.01338	Loss 1.4180 (1.2579)	Prec@(1,5) (75.5%, 95.1%)	
07/03 10:19:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][700/781]	Step 39018	lr 0.01338	Loss 1.4746 (1.2622)	Prec@(1,5) (75.4%, 95.1%)	
07/03 10:19:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][750/781]	Step 39068	lr 0.01338	Loss 1.3275 (1.2671)	Prec@(1,5) (75.2%, 95.1%)	
07/03 10:19:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [49][781/781]	Step 39099	lr 0.01338	Loss 1.2073 (1.2684)	Prec@(1,5) (75.2%, 95.1%)	
07/03 10:19:21午後 evaluateCell_trainer.py:172 [INFO] Train: [ 49/99] Final Prec@1 75.2220%
07/03 10:19:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][50/391]	Step 39100	Loss 0.8777	Prec@(1,5) (73.0%, 94.7%)
07/03 10:19:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][100/391]	Step 39100	Loss 0.8942	Prec@(1,5) (73.0%, 94.6%)
07/03 10:19:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][150/391]	Step 39100	Loss 0.8886	Prec@(1,5) (73.3%, 94.6%)
07/03 10:19:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][200/391]	Step 39100	Loss 0.8862	Prec@(1,5) (73.3%, 94.7%)
07/03 10:19:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][250/391]	Step 39100	Loss 0.8813	Prec@(1,5) (73.5%, 94.6%)
07/03 10:19:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][300/391]	Step 39100	Loss 0.8846	Prec@(1,5) (73.5%, 94.6%)
07/03 10:19:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][350/391]	Step 39100	Loss 0.8876	Prec@(1,5) (73.4%, 94.6%)
07/03 10:19:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [49][390/391]	Step 39100	Loss 0.8869	Prec@(1,5) (73.5%, 94.6%)
07/03 10:19:47午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 49/99] Final Prec@1 73.5000%
07/03 10:19:47午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 75.9840%
07/03 10:19:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][50/781]	Step 39150	lr 0.013	Loss 1.5774 (1.1717)	Prec@(1,5) (77.7%, 95.7%)	
07/03 10:20:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][100/781]	Step 39200	lr 0.013	Loss 1.1768 (1.1868)	Prec@(1,5) (77.3%, 95.8%)	
07/03 10:20:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][150/781]	Step 39250	lr 0.013	Loss 1.1516 (1.1862)	Prec@(1,5) (77.2%, 95.9%)	
07/03 10:20:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][200/781]	Step 39300	lr 0.013	Loss 0.9311 (1.2034)	Prec@(1,5) (76.7%, 95.9%)	
07/03 10:20:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][250/781]	Step 39350	lr 0.013	Loss 1.0329 (1.2076)	Prec@(1,5) (76.6%, 95.8%)	
07/03 10:20:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][300/781]	Step 39400	lr 0.013	Loss 1.1828 (1.2127)	Prec@(1,5) (76.5%, 95.7%)	
07/03 10:21:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][350/781]	Step 39450	lr 0.013	Loss 1.4709 (1.2183)	Prec@(1,5) (76.3%, 95.7%)	
07/03 10:21:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][400/781]	Step 39500	lr 0.013	Loss 0.9975 (1.2196)	Prec@(1,5) (76.3%, 95.6%)	
07/03 10:21:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][450/781]	Step 39550	lr 0.013	Loss 1.3992 (1.2258)	Prec@(1,5) (76.2%, 95.5%)	
07/03 10:21:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][500/781]	Step 39600	lr 0.013	Loss 1.3175 (1.2294)	Prec@(1,5) (76.1%, 95.5%)	
07/03 10:21:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][550/781]	Step 39650	lr 0.013	Loss 1.3862 (1.2328)	Prec@(1,5) (76.1%, 95.4%)	
07/03 10:22:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][600/781]	Step 39700	lr 0.013	Loss 1.5682 (1.2350)	Prec@(1,5) (76.0%, 95.4%)	
07/03 10:22:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][650/781]	Step 39750	lr 0.013	Loss 1.2395 (1.2407)	Prec@(1,5) (75.9%, 95.4%)	
07/03 10:22:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][700/781]	Step 39800	lr 0.013	Loss 1.1027 (1.2469)	Prec@(1,5) (75.8%, 95.3%)	
07/03 10:22:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][750/781]	Step 39850	lr 0.013	Loss 1.6149 (1.2513)	Prec@(1,5) (75.7%, 95.3%)	
07/03 10:22:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [50][781/781]	Step 39881	lr 0.013	Loss 1.4406 (1.2536)	Prec@(1,5) (75.6%, 95.3%)	
07/03 10:22:45午後 evaluateCell_trainer.py:172 [INFO] Train: [ 50/99] Final Prec@1 75.6200%
07/03 10:22:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][50/391]	Step 39882	Loss 0.8585	Prec@(1,5) (74.7%, 94.9%)
07/03 10:22:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][100/391]	Step 39882	Loss 0.8596	Prec@(1,5) (74.1%, 94.7%)
07/03 10:22:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][150/391]	Step 39882	Loss 0.8475	Prec@(1,5) (74.5%, 94.9%)
07/03 10:22:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][200/391]	Step 39882	Loss 0.8502	Prec@(1,5) (74.4%, 94.9%)
07/03 10:23:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][250/391]	Step 39882	Loss 0.8499	Prec@(1,5) (74.6%, 94.9%)
07/03 10:23:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][300/391]	Step 39882	Loss 0.8498	Prec@(1,5) (74.6%, 95.0%)
07/03 10:23:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][350/391]	Step 39882	Loss 0.8500	Prec@(1,5) (74.4%, 95.0%)
07/03 10:23:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [50][390/391]	Step 39882	Loss 0.8486	Prec@(1,5) (74.5%, 95.0%)
07/03 10:23:11午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 50/99] Final Prec@1 74.4800%
07/03 10:23:11午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 75.9840%
07/03 10:23:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][50/781]	Step 39932	lr 0.01262	Loss 1.2810 (1.1407)	Prec@(1,5) (78.0%, 95.8%)	
07/03 10:23:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][100/781]	Step 39982	lr 0.01262	Loss 1.0943 (1.1394)	Prec@(1,5) (77.9%, 95.9%)	
07/03 10:23:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][150/781]	Step 40032	lr 0.01262	Loss 1.2989 (1.1515)	Prec@(1,5) (77.8%, 95.9%)	
07/03 10:23:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][200/781]	Step 40082	lr 0.01262	Loss 0.9626 (1.1625)	Prec@(1,5) (77.5%, 95.8%)	
07/03 10:24:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][250/781]	Step 40132	lr 0.01262	Loss 1.3828 (1.1623)	Prec@(1,5) (77.5%, 95.8%)	
07/03 10:24:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][300/781]	Step 40182	lr 0.01262	Loss 1.2267 (1.1714)	Prec@(1,5) (77.2%, 95.7%)	
07/03 10:24:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][350/781]	Step 40232	lr 0.01262	Loss 0.9251 (1.1787)	Prec@(1,5) (77.1%, 95.7%)	
07/03 10:24:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][400/781]	Step 40282	lr 0.01262	Loss 1.2111 (1.1865)	Prec@(1,5) (76.9%, 95.7%)	
07/03 10:24:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][450/781]	Step 40332	lr 0.01262	Loss 1.1570 (1.1929)	Prec@(1,5) (76.7%, 95.7%)	
07/03 10:25:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][500/781]	Step 40382	lr 0.01262	Loss 1.2059 (1.2055)	Prec@(1,5) (76.5%, 95.6%)	
07/03 10:25:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][550/781]	Step 40432	lr 0.01262	Loss 1.3287 (1.2086)	Prec@(1,5) (76.3%, 95.6%)	
07/03 10:25:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][600/781]	Step 40482	lr 0.01262	Loss 1.2640 (1.2149)	Prec@(1,5) (76.2%, 95.5%)	
07/03 10:25:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][650/781]	Step 40532	lr 0.01262	Loss 1.5869 (1.2174)	Prec@(1,5) (76.1%, 95.5%)	
07/03 10:25:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][700/781]	Step 40582	lr 0.01262	Loss 1.0064 (1.2195)	Prec@(1,5) (76.2%, 95.4%)	
07/03 10:26:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][750/781]	Step 40632	lr 0.01262	Loss 1.1412 (1.2237)	Prec@(1,5) (76.1%, 95.4%)	
07/03 10:26:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [51][781/781]	Step 40663	lr 0.01262	Loss 1.2211 (1.2241)	Prec@(1,5) (76.1%, 95.4%)	
07/03 10:26:09午後 evaluateCell_trainer.py:172 [INFO] Train: [ 51/99] Final Prec@1 76.0940%
07/03 10:26:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][50/391]	Step 40664	Loss 0.9233	Prec@(1,5) (72.1%, 94.1%)
07/03 10:26:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][100/391]	Step 40664	Loss 0.9181	Prec@(1,5) (72.4%, 94.1%)
07/03 10:26:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][150/391]	Step 40664	Loss 0.9111	Prec@(1,5) (72.7%, 94.2%)
07/03 10:26:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][200/391]	Step 40664	Loss 0.9037	Prec@(1,5) (72.9%, 94.5%)
07/03 10:26:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][250/391]	Step 40664	Loss 0.9053	Prec@(1,5) (72.9%, 94.4%)
07/03 10:26:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][300/391]	Step 40664	Loss 0.9004	Prec@(1,5) (73.1%, 94.5%)
07/03 10:26:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][350/391]	Step 40664	Loss 0.9059	Prec@(1,5) (73.0%, 94.4%)
07/03 10:26:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [51][390/391]	Step 40664	Loss 0.9035	Prec@(1,5) (73.1%, 94.4%)
07/03 10:26:34午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 51/99] Final Prec@1 73.0520%
07/03 10:26:34午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 75.9840%
07/03 10:26:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][50/781]	Step 40714	lr 0.01225	Loss 1.1945 (1.1147)	Prec@(1,5) (79.4%, 95.9%)	
07/03 10:26:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][100/781]	Step 40764	lr 0.01225	Loss 1.1281 (1.0914)	Prec@(1,5) (79.7%, 96.1%)	
07/03 10:27:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][150/781]	Step 40814	lr 0.01225	Loss 1.0631 (1.1075)	Prec@(1,5) (79.1%, 96.1%)	
07/03 10:27:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][200/781]	Step 40864	lr 0.01225	Loss 1.2263 (1.1329)	Prec@(1,5) (78.3%, 96.0%)	
07/03 10:27:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][250/781]	Step 40914	lr 0.01225	Loss 1.0641 (1.1387)	Prec@(1,5) (78.2%, 95.9%)	
07/03 10:27:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][300/781]	Step 40964	lr 0.01225	Loss 1.2236 (1.1544)	Prec@(1,5) (77.8%, 95.9%)	
07/03 10:27:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][350/781]	Step 41014	lr 0.01225	Loss 1.0813 (1.1621)	Prec@(1,5) (77.6%, 95.8%)	
07/03 10:28:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][400/781]	Step 41064	lr 0.01225	Loss 1.7014 (1.1674)	Prec@(1,5) (77.4%, 95.8%)	
07/03 10:28:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][450/781]	Step 41114	lr 0.01225	Loss 1.3638 (1.1805)	Prec@(1,5) (77.2%, 95.6%)	
07/03 10:28:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][500/781]	Step 41164	lr 0.01225	Loss 1.1368 (1.1865)	Prec@(1,5) (77.1%, 95.6%)	
07/03 10:28:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][550/781]	Step 41214	lr 0.01225	Loss 1.1801 (1.1905)	Prec@(1,5) (76.9%, 95.6%)	
07/03 10:28:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][600/781]	Step 41264	lr 0.01225	Loss 1.3103 (1.1948)	Prec@(1,5) (76.8%, 95.6%)	
07/03 10:29:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][650/781]	Step 41314	lr 0.01225	Loss 1.0611 (1.1998)	Prec@(1,5) (76.7%, 95.6%)	
07/03 10:29:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][700/781]	Step 41364	lr 0.01225	Loss 0.9142 (1.2019)	Prec@(1,5) (76.7%, 95.5%)	
07/03 10:29:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][750/781]	Step 41414	lr 0.01225	Loss 1.4709 (1.2033)	Prec@(1,5) (76.7%, 95.5%)	
07/03 10:29:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [52][781/781]	Step 41445	lr 0.01225	Loss 1.2759 (1.2055)	Prec@(1,5) (76.6%, 95.5%)	
07/03 10:29:32午後 evaluateCell_trainer.py:172 [INFO] Train: [ 52/99] Final Prec@1 76.6220%
07/03 10:29:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][50/391]	Step 41446	Loss 0.7814	Prec@(1,5) (76.4%, 95.4%)
07/03 10:29:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][100/391]	Step 41446	Loss 0.7768	Prec@(1,5) (76.5%, 95.5%)
07/03 10:29:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][150/391]	Step 41446	Loss 0.7726	Prec@(1,5) (76.8%, 95.5%)
07/03 10:29:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][200/391]	Step 41446	Loss 0.7768	Prec@(1,5) (76.5%, 95.6%)
07/03 10:29:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][250/391]	Step 41446	Loss 0.7723	Prec@(1,5) (76.5%, 95.6%)
07/03 10:29:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][300/391]	Step 41446	Loss 0.7697	Prec@(1,5) (76.5%, 95.6%)
07/03 10:29:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][350/391]	Step 41446	Loss 0.7685	Prec@(1,5) (76.5%, 95.7%)
07/03 10:29:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [52][390/391]	Step 41446	Loss 0.7683	Prec@(1,5) (76.5%, 95.6%)
07/03 10:29:57午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 52/99] Final Prec@1 76.4760%
07/03 10:29:57午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 76.4760%
07/03 10:30:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][50/781]	Step 41496	lr 0.01187	Loss 1.0592 (1.1562)	Prec@(1,5) (77.8%, 96.2%)	
07/03 10:30:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][100/781]	Step 41546	lr 0.01187	Loss 0.8374 (1.1467)	Prec@(1,5) (77.5%, 96.2%)	
07/03 10:30:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][150/781]	Step 41596	lr 0.01187	Loss 0.8971 (1.1496)	Prec@(1,5) (77.7%, 96.1%)	
07/03 10:30:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][200/781]	Step 41646	lr 0.01187	Loss 0.8240 (1.1437)	Prec@(1,5) (77.8%, 96.2%)	
07/03 10:30:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][250/781]	Step 41696	lr 0.01187	Loss 0.8174 (1.1487)	Prec@(1,5) (77.7%, 96.1%)	
07/03 10:31:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][300/781]	Step 41746	lr 0.01187	Loss 0.9523 (1.1501)	Prec@(1,5) (77.5%, 96.1%)	
07/03 10:31:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][350/781]	Step 41796	lr 0.01187	Loss 1.4667 (1.1507)	Prec@(1,5) (77.6%, 96.0%)	
07/03 10:31:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][400/781]	Step 41846	lr 0.01187	Loss 0.8028 (1.1534)	Prec@(1,5) (77.6%, 96.0%)	
07/03 10:31:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][450/781]	Step 41896	lr 0.01187	Loss 1.2459 (1.1630)	Prec@(1,5) (77.4%, 95.9%)	
07/03 10:31:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][500/781]	Step 41946	lr 0.01187	Loss 1.3983 (1.1711)	Prec@(1,5) (77.2%, 95.8%)	
07/03 10:32:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][550/781]	Step 41996	lr 0.01187	Loss 1.2332 (1.1737)	Prec@(1,5) (77.2%, 95.8%)	
07/03 10:32:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][600/781]	Step 42046	lr 0.01187	Loss 1.1012 (1.1779)	Prec@(1,5) (77.1%, 95.8%)	
07/03 10:32:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][650/781]	Step 42096	lr 0.01187	Loss 1.4959 (1.1847)	Prec@(1,5) (77.0%, 95.7%)	
07/03 10:32:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][700/781]	Step 42146	lr 0.01187	Loss 1.4361 (1.1914)	Prec@(1,5) (76.9%, 95.6%)	
07/03 10:32:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][750/781]	Step 42196	lr 0.01187	Loss 1.1716 (1.1889)	Prec@(1,5) (77.0%, 95.7%)	
07/03 10:32:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [53][781/781]	Step 42227	lr 0.01187	Loss 1.4162 (1.1922)	Prec@(1,5) (76.9%, 95.6%)	
07/03 10:32:55午後 evaluateCell_trainer.py:172 [INFO] Train: [ 53/99] Final Prec@1 76.8740%
07/03 10:32:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][50/391]	Step 42228	Loss 0.7945	Prec@(1,5) (75.2%, 95.6%)
07/03 10:33:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][100/391]	Step 42228	Loss 0.7830	Prec@(1,5) (76.0%, 95.6%)
07/03 10:33:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][150/391]	Step 42228	Loss 0.7905	Prec@(1,5) (75.7%, 95.6%)
07/03 10:33:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][200/391]	Step 42228	Loss 0.7790	Prec@(1,5) (76.0%, 95.7%)
07/03 10:33:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][250/391]	Step 42228	Loss 0.7800	Prec@(1,5) (76.1%, 95.6%)
07/03 10:33:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][300/391]	Step 42228	Loss 0.7798	Prec@(1,5) (76.1%, 95.6%)
07/03 10:33:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][350/391]	Step 42228	Loss 0.7781	Prec@(1,5) (76.1%, 95.7%)
07/03 10:33:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [53][390/391]	Step 42228	Loss 0.7743	Prec@(1,5) (76.3%, 95.7%)
07/03 10:33:21午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 53/99] Final Prec@1 76.2800%
07/03 10:33:21午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 76.4760%
07/03 10:33:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][50/781]	Step 42278	lr 0.0115	Loss 1.1489 (1.0969)	Prec@(1,5) (78.6%, 96.5%)	
07/03 10:33:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][100/781]	Step 42328	lr 0.0115	Loss 1.1626 (1.0993)	Prec@(1,5) (78.7%, 96.6%)	
07/03 10:33:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][150/781]	Step 42378	lr 0.0115	Loss 1.2827 (1.1065)	Prec@(1,5) (78.6%, 96.4%)	
07/03 10:34:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][200/781]	Step 42428	lr 0.0115	Loss 1.0575 (1.1163)	Prec@(1,5) (78.4%, 96.4%)	
07/03 10:34:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][250/781]	Step 42478	lr 0.0115	Loss 1.2383 (1.1232)	Prec@(1,5) (78.3%, 96.3%)	
07/03 10:34:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][300/781]	Step 42528	lr 0.0115	Loss 1.2877 (1.1334)	Prec@(1,5) (77.9%, 96.2%)	
07/03 10:34:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][350/781]	Step 42578	lr 0.0115	Loss 1.1834 (1.1276)	Prec@(1,5) (78.1%, 96.3%)	
07/03 10:34:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][400/781]	Step 42628	lr 0.0115	Loss 0.9050 (1.1229)	Prec@(1,5) (78.3%, 96.3%)	
07/03 10:35:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][450/781]	Step 42678	lr 0.0115	Loss 1.0481 (1.1235)	Prec@(1,5) (78.2%, 96.3%)	
07/03 10:35:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][500/781]	Step 42728	lr 0.0115	Loss 1.2429 (1.1302)	Prec@(1,5) (78.0%, 96.2%)	
07/03 10:35:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][550/781]	Step 42778	lr 0.0115	Loss 0.9110 (1.1333)	Prec@(1,5) (78.1%, 96.2%)	
07/03 10:35:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][600/781]	Step 42828	lr 0.0115	Loss 1.0206 (1.1323)	Prec@(1,5) (78.2%, 96.2%)	
07/03 10:35:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][650/781]	Step 42878	lr 0.0115	Loss 1.5612 (1.1368)	Prec@(1,5) (78.0%, 96.2%)	
07/03 10:36:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][700/781]	Step 42928	lr 0.0115	Loss 1.0980 (1.1412)	Prec@(1,5) (77.9%, 96.2%)	
07/03 10:36:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][750/781]	Step 42978	lr 0.0115	Loss 1.1439 (1.1453)	Prec@(1,5) (77.8%, 96.1%)	
07/03 10:36:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [54][781/781]	Step 43009	lr 0.0115	Loss 1.2592 (1.1477)	Prec@(1,5) (77.8%, 96.1%)	
07/03 10:36:19午後 evaluateCell_trainer.py:172 [INFO] Train: [ 54/99] Final Prec@1 77.8200%
07/03 10:36:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][50/391]	Step 43010	Loss 0.6962	Prec@(1,5) (79.0%, 96.7%)
07/03 10:36:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][100/391]	Step 43010	Loss 0.6989	Prec@(1,5) (79.0%, 96.5%)
07/03 10:36:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][150/391]	Step 43010	Loss 0.7002	Prec@(1,5) (78.7%, 96.6%)
07/03 10:36:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][200/391]	Step 43010	Loss 0.7140	Prec@(1,5) (78.3%, 96.4%)
07/03 10:36:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][250/391]	Step 43010	Loss 0.7089	Prec@(1,5) (78.2%, 96.5%)
07/03 10:36:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][300/391]	Step 43010	Loss 0.7090	Prec@(1,5) (78.2%, 96.4%)
07/03 10:36:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][350/391]	Step 43010	Loss 0.7075	Prec@(1,5) (78.3%, 96.4%)
07/03 10:36:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [54][390/391]	Step 43010	Loss 0.7049	Prec@(1,5) (78.3%, 96.4%)
07/03 10:36:45午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 54/99] Final Prec@1 78.2680%
07/03 10:36:45午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 78.2680%
07/03 10:36:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][50/781]	Step 43060	lr 0.01112	Loss 1.3615 (1.0658)	Prec@(1,5) (79.2%, 96.8%)	
07/03 10:37:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][100/781]	Step 43110	lr 0.01112	Loss 1.2580 (1.0561)	Prec@(1,5) (79.6%, 96.8%)	
07/03 10:37:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][150/781]	Step 43160	lr 0.01112	Loss 1.1313 (1.0736)	Prec@(1,5) (79.3%, 96.5%)	
07/03 10:37:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][200/781]	Step 43210	lr 0.01112	Loss 1.4206 (1.0928)	Prec@(1,5) (78.9%, 96.5%)	
07/03 10:37:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][250/781]	Step 43260	lr 0.01112	Loss 1.1008 (1.0893)	Prec@(1,5) (79.0%, 96.4%)	
07/03 10:37:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][300/781]	Step 43310	lr 0.01112	Loss 1.2379 (1.0882)	Prec@(1,5) (79.0%, 96.5%)	
07/03 10:38:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][350/781]	Step 43360	lr 0.01112	Loss 0.9086 (1.0913)	Prec@(1,5) (78.9%, 96.4%)	
07/03 10:38:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][400/781]	Step 43410	lr 0.01112	Loss 1.0417 (1.0989)	Prec@(1,5) (78.7%, 96.4%)	
07/03 10:38:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][450/781]	Step 43460	lr 0.01112	Loss 0.7559 (1.1054)	Prec@(1,5) (78.6%, 96.3%)	
07/03 10:38:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][500/781]	Step 43510	lr 0.01112	Loss 0.9503 (1.1106)	Prec@(1,5) (78.5%, 96.2%)	
07/03 10:38:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][550/781]	Step 43560	lr 0.01112	Loss 1.4131 (1.1130)	Prec@(1,5) (78.4%, 96.2%)	
07/03 10:39:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][600/781]	Step 43610	lr 0.01112	Loss 1.0067 (1.1186)	Prec@(1,5) (78.3%, 96.2%)	
07/03 10:39:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][650/781]	Step 43660	lr 0.01112	Loss 1.3619 (1.1243)	Prec@(1,5) (78.2%, 96.1%)	
07/03 10:39:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][700/781]	Step 43710	lr 0.01112	Loss 1.0366 (1.1245)	Prec@(1,5) (78.2%, 96.1%)	
07/03 10:39:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][750/781]	Step 43760	lr 0.01112	Loss 1.0984 (1.1259)	Prec@(1,5) (78.2%, 96.1%)	
07/03 10:39:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [55][781/781]	Step 43791	lr 0.01112	Loss 1.1204 (1.1304)	Prec@(1,5) (78.1%, 96.0%)	
07/03 10:39:43午後 evaluateCell_trainer.py:172 [INFO] Train: [ 55/99] Final Prec@1 78.1440%
07/03 10:39:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][50/391]	Step 43792	Loss 0.6891	Prec@(1,5) (78.9%, 96.6%)
07/03 10:39:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][100/391]	Step 43792	Loss 0.6939	Prec@(1,5) (78.6%, 96.7%)
07/03 10:39:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][150/391]	Step 43792	Loss 0.7036	Prec@(1,5) (78.3%, 96.5%)
07/03 10:39:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][200/391]	Step 43792	Loss 0.7003	Prec@(1,5) (78.4%, 96.4%)
07/03 10:39:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][250/391]	Step 43792	Loss 0.7014	Prec@(1,5) (78.6%, 96.3%)
07/03 10:40:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][300/391]	Step 43792	Loss 0.7037	Prec@(1,5) (78.5%, 96.2%)
07/03 10:40:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][350/391]	Step 43792	Loss 0.7078	Prec@(1,5) (78.4%, 96.2%)
07/03 10:40:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [55][390/391]	Step 43792	Loss 0.7091	Prec@(1,5) (78.3%, 96.1%)
07/03 10:40:08午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 55/99] Final Prec@1 78.3000%
07/03 10:40:08午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 78.3000%
07/03 10:40:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][50/781]	Step 43842	lr 0.01075	Loss 1.0714 (1.0570)	Prec@(1,5) (79.5%, 96.9%)	
07/03 10:40:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][100/781]	Step 43892	lr 0.01075	Loss 0.8785 (1.0431)	Prec@(1,5) (79.9%, 96.7%)	
07/03 10:40:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][150/781]	Step 43942	lr 0.01075	Loss 1.0153 (1.0523)	Prec@(1,5) (79.7%, 96.7%)	
07/03 10:40:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][200/781]	Step 43992	lr 0.01075	Loss 0.9310 (1.0558)	Prec@(1,5) (79.5%, 96.7%)	
07/03 10:41:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][250/781]	Step 44042	lr 0.01075	Loss 1.2232 (1.0616)	Prec@(1,5) (79.2%, 96.8%)	
07/03 10:41:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][300/781]	Step 44092	lr 0.01075	Loss 1.0507 (1.0773)	Prec@(1,5) (78.9%, 96.7%)	
07/03 10:41:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][350/781]	Step 44142	lr 0.01075	Loss 0.8408 (1.0835)	Prec@(1,5) (78.8%, 96.6%)	
07/03 10:41:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][400/781]	Step 44192	lr 0.01075	Loss 1.1440 (1.0849)	Prec@(1,5) (78.9%, 96.6%)	
07/03 10:41:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][450/781]	Step 44242	lr 0.01075	Loss 1.1434 (1.0906)	Prec@(1,5) (78.9%, 96.5%)	
07/03 10:42:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][500/781]	Step 44292	lr 0.01075	Loss 0.9374 (1.0935)	Prec@(1,5) (78.8%, 96.5%)	
07/03 10:42:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][550/781]	Step 44342	lr 0.01075	Loss 1.4602 (1.1034)	Prec@(1,5) (78.6%, 96.4%)	
07/03 10:42:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][600/781]	Step 44392	lr 0.01075	Loss 0.8081 (1.1066)	Prec@(1,5) (78.6%, 96.4%)	
07/03 10:42:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][650/781]	Step 44442	lr 0.01075	Loss 1.4390 (1.1131)	Prec@(1,5) (78.6%, 96.3%)	
07/03 10:42:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][700/781]	Step 44492	lr 0.01075	Loss 1.1123 (1.1108)	Prec@(1,5) (78.6%, 96.3%)	
07/03 10:42:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][750/781]	Step 44542	lr 0.01075	Loss 1.1990 (1.1116)	Prec@(1,5) (78.6%, 96.3%)	
07/03 10:43:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [56][781/781]	Step 44573	lr 0.01075	Loss 1.2081 (1.1148)	Prec@(1,5) (78.6%, 96.3%)	
07/03 10:43:06午後 evaluateCell_trainer.py:172 [INFO] Train: [ 56/99] Final Prec@1 78.5460%
07/03 10:43:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][50/391]	Step 44574	Loss 0.6702	Prec@(1,5) (80.0%, 96.6%)
07/03 10:43:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][100/391]	Step 44574	Loss 0.6823	Prec@(1,5) (79.6%, 96.4%)
07/03 10:43:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][150/391]	Step 44574	Loss 0.6898	Prec@(1,5) (79.3%, 96.3%)
07/03 10:43:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][200/391]	Step 44574	Loss 0.6877	Prec@(1,5) (79.3%, 96.4%)
07/03 10:43:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][250/391]	Step 44574	Loss 0.6821	Prec@(1,5) (79.3%, 96.5%)
07/03 10:43:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][300/391]	Step 44574	Loss 0.6796	Prec@(1,5) (79.2%, 96.6%)
07/03 10:43:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][350/391]	Step 44574	Loss 0.6717	Prec@(1,5) (79.5%, 96.7%)
07/03 10:43:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [56][390/391]	Step 44574	Loss 0.6763	Prec@(1,5) (79.4%, 96.6%)
07/03 10:43:31午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 56/99] Final Prec@1 79.3520%
07/03 10:43:32午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 79.3520%
07/03 10:43:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][50/781]	Step 44624	lr 0.01038	Loss 1.2456 (1.0231)	Prec@(1,5) (81.0%, 97.2%)	
07/03 10:43:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][100/781]	Step 44674	lr 0.01038	Loss 1.1744 (1.0313)	Prec@(1,5) (80.6%, 97.1%)	
07/03 10:44:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][150/781]	Step 44724	lr 0.01038	Loss 1.0422 (1.0544)	Prec@(1,5) (80.2%, 96.9%)	
07/03 10:44:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][200/781]	Step 44774	lr 0.01038	Loss 1.0470 (1.0561)	Prec@(1,5) (80.0%, 96.9%)	
07/03 10:44:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][250/781]	Step 44824	lr 0.01038	Loss 1.0312 (1.0581)	Prec@(1,5) (79.8%, 96.9%)	
07/03 10:44:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][300/781]	Step 44874	lr 0.01038	Loss 0.9348 (1.0549)	Prec@(1,5) (79.9%, 96.9%)	
07/03 10:44:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][350/781]	Step 44924	lr 0.01038	Loss 1.2803 (1.0581)	Prec@(1,5) (79.7%, 96.9%)	
07/03 10:45:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][400/781]	Step 44974	lr 0.01038	Loss 1.5369 (1.0623)	Prec@(1,5) (79.7%, 96.7%)	
07/03 10:45:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][450/781]	Step 45024	lr 0.01038	Loss 0.8998 (1.0683)	Prec@(1,5) (79.6%, 96.7%)	
07/03 10:45:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][500/781]	Step 45074	lr 0.01038	Loss 1.2838 (1.0756)	Prec@(1,5) (79.4%, 96.6%)	
07/03 10:45:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][550/781]	Step 45124	lr 0.01038	Loss 1.4465 (1.0788)	Prec@(1,5) (79.3%, 96.6%)	
07/03 10:45:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][600/781]	Step 45174	lr 0.01038	Loss 1.2514 (1.0854)	Prec@(1,5) (79.2%, 96.6%)	
07/03 10:45:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][650/781]	Step 45224	lr 0.01038	Loss 1.0673 (1.0913)	Prec@(1,5) (79.0%, 96.6%)	
07/03 10:46:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][700/781]	Step 45274	lr 0.01038	Loss 1.1454 (1.0942)	Prec@(1,5) (79.0%, 96.5%)	
07/03 10:46:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][750/781]	Step 45324	lr 0.01038	Loss 1.0148 (1.0953)	Prec@(1,5) (78.9%, 96.5%)	
07/03 10:46:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [57][781/781]	Step 45355	lr 0.01038	Loss 1.1670 (1.0978)	Prec@(1,5) (78.9%, 96.5%)	
07/03 10:46:29午後 evaluateCell_trainer.py:172 [INFO] Train: [ 57/99] Final Prec@1 78.8780%
07/03 10:46:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][50/391]	Step 45356	Loss 0.7017	Prec@(1,5) (77.8%, 96.4%)
07/03 10:46:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][100/391]	Step 45356	Loss 0.7047	Prec@(1,5) (78.2%, 96.5%)
07/03 10:46:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][150/391]	Step 45356	Loss 0.6976	Prec@(1,5) (78.4%, 96.5%)
07/03 10:46:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][200/391]	Step 45356	Loss 0.7012	Prec@(1,5) (78.2%, 96.6%)
07/03 10:46:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][250/391]	Step 45356	Loss 0.6916	Prec@(1,5) (78.6%, 96.6%)
07/03 10:46:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][300/391]	Step 45356	Loss 0.6926	Prec@(1,5) (78.7%, 96.6%)
07/03 10:46:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][350/391]	Step 45356	Loss 0.6931	Prec@(1,5) (78.8%, 96.5%)
07/03 10:46:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [57][390/391]	Step 45356	Loss 0.6929	Prec@(1,5) (78.8%, 96.5%)
07/03 10:46:55午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 57/99] Final Prec@1 78.8440%
07/03 10:46:55午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 79.3520%
07/03 10:47:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][50/781]	Step 45406	lr 0.01002	Loss 1.1614 (1.0539)	Prec@(1,5) (80.3%, 96.7%)	
07/03 10:47:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][100/781]	Step 45456	lr 0.01002	Loss 0.9659 (1.0566)	Prec@(1,5) (79.9%, 96.8%)	
07/03 10:47:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][150/781]	Step 45506	lr 0.01002	Loss 1.2159 (1.0519)	Prec@(1,5) (80.3%, 96.7%)	
07/03 10:47:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][200/781]	Step 45556	lr 0.01002	Loss 1.5629 (1.0523)	Prec@(1,5) (80.2%, 96.7%)	
07/03 10:47:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][250/781]	Step 45606	lr 0.01002	Loss 1.1342 (1.0594)	Prec@(1,5) (80.1%, 96.6%)	
07/03 10:48:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][300/781]	Step 45656	lr 0.01002	Loss 1.2700 (1.0590)	Prec@(1,5) (80.0%, 96.7%)	
07/03 10:48:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][350/781]	Step 45706	lr 0.01002	Loss 1.1086 (1.0579)	Prec@(1,5) (80.1%, 96.6%)	
07/03 10:48:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][400/781]	Step 45756	lr 0.01002	Loss 1.0890 (1.0575)	Prec@(1,5) (80.1%, 96.7%)	
07/03 10:48:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][450/781]	Step 45806	lr 0.01002	Loss 1.3588 (1.0553)	Prec@(1,5) (80.1%, 96.7%)	
07/03 10:48:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][500/781]	Step 45856	lr 0.01002	Loss 1.0604 (1.0545)	Prec@(1,5) (80.1%, 96.7%)	
07/03 10:49:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][550/781]	Step 45906	lr 0.01002	Loss 1.2335 (1.0611)	Prec@(1,5) (79.9%, 96.7%)	
07/03 10:49:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][600/781]	Step 45956	lr 0.01002	Loss 1.1117 (1.0620)	Prec@(1,5) (79.9%, 96.6%)	
07/03 10:49:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][650/781]	Step 46006	lr 0.01002	Loss 1.2258 (1.0624)	Prec@(1,5) (79.8%, 96.6%)	
07/03 10:49:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][700/781]	Step 46056	lr 0.01002	Loss 0.9166 (1.0649)	Prec@(1,5) (79.8%, 96.6%)	
07/03 10:49:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][750/781]	Step 46106	lr 0.01002	Loss 1.5422 (1.0665)	Prec@(1,5) (79.8%, 96.6%)	
07/03 10:49:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [58][781/781]	Step 46137	lr 0.01002	Loss 0.8196 (1.0689)	Prec@(1,5) (79.7%, 96.6%)	
07/03 10:49:53午後 evaluateCell_trainer.py:172 [INFO] Train: [ 58/99] Final Prec@1 79.6800%
07/03 10:49:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][50/391]	Step 46138	Loss 0.6363	Prec@(1,5) (80.2%, 97.0%)
07/03 10:50:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][100/391]	Step 46138	Loss 0.6253	Prec@(1,5) (80.4%, 97.0%)
07/03 10:50:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][150/391]	Step 46138	Loss 0.6231	Prec@(1,5) (80.6%, 97.0%)
07/03 10:50:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][200/391]	Step 46138	Loss 0.6262	Prec@(1,5) (80.4%, 97.0%)
07/03 10:50:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][250/391]	Step 46138	Loss 0.6377	Prec@(1,5) (80.1%, 96.9%)
07/03 10:50:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][300/391]	Step 46138	Loss 0.6346	Prec@(1,5) (80.3%, 96.9%)
07/03 10:50:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][350/391]	Step 46138	Loss 0.6344	Prec@(1,5) (80.2%, 96.9%)
07/03 10:50:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [58][390/391]	Step 46138	Loss 0.6312	Prec@(1,5) (80.4%, 96.9%)
07/03 10:50:18午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 58/99] Final Prec@1 80.4040%
07/03 10:50:18午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 80.4040%
07/03 10:50:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][50/781]	Step 46188	lr 0.00965	Loss 1.1107 (1.0010)	Prec@(1,5) (80.8%, 96.9%)	
07/03 10:50:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][100/781]	Step 46238	lr 0.00965	Loss 1.0785 (1.0213)	Prec@(1,5) (80.3%, 97.1%)	
07/03 10:50:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][150/781]	Step 46288	lr 0.00965	Loss 0.9685 (0.9969)	Prec@(1,5) (80.9%, 97.2%)	
07/03 10:51:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][200/781]	Step 46338	lr 0.00965	Loss 1.1963 (1.0128)	Prec@(1,5) (80.5%, 97.1%)	
07/03 10:51:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][250/781]	Step 46388	lr 0.00965	Loss 0.8666 (1.0149)	Prec@(1,5) (80.5%, 97.0%)	
07/03 10:51:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][300/781]	Step 46438	lr 0.00965	Loss 1.2142 (1.0206)	Prec@(1,5) (80.6%, 97.0%)	
07/03 10:51:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][350/781]	Step 46488	lr 0.00965	Loss 1.0430 (1.0171)	Prec@(1,5) (80.7%, 97.0%)	
07/03 10:51:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][400/781]	Step 46538	lr 0.00965	Loss 1.0709 (1.0168)	Prec@(1,5) (80.7%, 97.0%)	
07/03 10:52:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][450/781]	Step 46588	lr 0.00965	Loss 1.1828 (1.0199)	Prec@(1,5) (80.7%, 97.0%)	
07/03 10:52:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][500/781]	Step 46638	lr 0.00965	Loss 1.2477 (1.0294)	Prec@(1,5) (80.5%, 96.9%)	
07/03 10:52:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][550/781]	Step 46688	lr 0.00965	Loss 1.1598 (1.0284)	Prec@(1,5) (80.5%, 96.9%)	
07/03 10:52:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][600/781]	Step 46738	lr 0.00965	Loss 0.7865 (1.0313)	Prec@(1,5) (80.4%, 96.9%)	
07/03 10:52:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][650/781]	Step 46788	lr 0.00965	Loss 1.3158 (1.0364)	Prec@(1,5) (80.3%, 96.9%)	
07/03 10:52:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][700/781]	Step 46838	lr 0.00965	Loss 0.9294 (1.0373)	Prec@(1,5) (80.3%, 96.9%)	
07/03 10:53:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][750/781]	Step 46888	lr 0.00965	Loss 0.8798 (1.0405)	Prec@(1,5) (80.2%, 96.8%)	
07/03 10:53:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [59][781/781]	Step 46919	lr 0.00965	Loss 0.9624 (1.0414)	Prec@(1,5) (80.2%, 96.8%)	
07/03 10:53:17午後 evaluateCell_trainer.py:172 [INFO] Train: [ 59/99] Final Prec@1 80.2280%
07/03 10:53:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][50/391]	Step 46920	Loss 0.5931	Prec@(1,5) (81.3%, 97.3%)
07/03 10:53:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][100/391]	Step 46920	Loss 0.6032	Prec@(1,5) (81.3%, 97.3%)
07/03 10:53:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][150/391]	Step 46920	Loss 0.6176	Prec@(1,5) (80.9%, 97.1%)
07/03 10:53:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][200/391]	Step 46920	Loss 0.6140	Prec@(1,5) (81.0%, 97.1%)
07/03 10:53:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][250/391]	Step 46920	Loss 0.6118	Prec@(1,5) (81.2%, 97.1%)
07/03 10:53:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][300/391]	Step 46920	Loss 0.6132	Prec@(1,5) (81.0%, 97.1%)
07/03 10:53:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][350/391]	Step 46920	Loss 0.6134	Prec@(1,5) (81.0%, 97.1%)
07/03 10:53:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [59][390/391]	Step 46920	Loss 0.6096	Prec@(1,5) (81.1%, 97.1%)
07/03 10:53:42午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 59/99] Final Prec@1 81.0560%
07/03 10:53:42午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 81.0560%
07/03 10:53:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][50/781]	Step 46970	lr 0.00929	Loss 1.0919 (1.0181)	Prec@(1,5) (80.6%, 97.3%)	
07/03 10:54:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][100/781]	Step 47020	lr 0.00929	Loss 1.0312 (0.9770)	Prec@(1,5) (82.0%, 97.2%)	
07/03 10:54:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][150/781]	Step 47070	lr 0.00929	Loss 1.0833 (0.9910)	Prec@(1,5) (81.7%, 97.0%)	
07/03 10:54:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][200/781]	Step 47120	lr 0.00929	Loss 1.0486 (0.9940)	Prec@(1,5) (81.5%, 97.0%)	
07/03 10:54:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][250/781]	Step 47170	lr 0.00929	Loss 1.4144 (0.9955)	Prec@(1,5) (81.4%, 97.0%)	
07/03 10:54:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][300/781]	Step 47220	lr 0.00929	Loss 0.9883 (0.9982)	Prec@(1,5) (81.4%, 97.0%)	
07/03 10:55:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][350/781]	Step 47270	lr 0.00929	Loss 0.9536 (0.9908)	Prec@(1,5) (81.5%, 97.1%)	
07/03 10:55:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][400/781]	Step 47320	lr 0.00929	Loss 1.0448 (0.9973)	Prec@(1,5) (81.4%, 97.0%)	
07/03 10:55:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][450/781]	Step 47370	lr 0.00929	Loss 0.6655 (1.0013)	Prec@(1,5) (81.2%, 97.0%)	
07/03 10:55:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][500/781]	Step 47420	lr 0.00929	Loss 0.9398 (1.0017)	Prec@(1,5) (81.2%, 97.0%)	
07/03 10:55:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][550/781]	Step 47470	lr 0.00929	Loss 0.8718 (1.0094)	Prec@(1,5) (81.0%, 97.0%)	
07/03 10:55:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][600/781]	Step 47520	lr 0.00929	Loss 0.9977 (1.0130)	Prec@(1,5) (80.9%, 96.9%)	
07/03 10:56:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][650/781]	Step 47570	lr 0.00929	Loss 0.9101 (1.0174)	Prec@(1,5) (80.8%, 96.9%)	
07/03 10:56:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][700/781]	Step 47620	lr 0.00929	Loss 1.2110 (1.0184)	Prec@(1,5) (80.8%, 96.8%)	
07/03 10:56:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][750/781]	Step 47670	lr 0.00929	Loss 1.0817 (1.0224)	Prec@(1,5) (80.7%, 96.8%)	
07/03 10:56:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [60][781/781]	Step 47701	lr 0.00929	Loss 0.9653 (1.0217)	Prec@(1,5) (80.7%, 96.8%)	
07/03 10:56:40午後 evaluateCell_trainer.py:172 [INFO] Train: [ 60/99] Final Prec@1 80.7080%
07/03 10:56:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][50/391]	Step 47702	Loss 0.5778	Prec@(1,5) (81.8%, 97.2%)
07/03 10:56:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][100/391]	Step 47702	Loss 0.5651	Prec@(1,5) (82.4%, 97.2%)
07/03 10:56:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][150/391]	Step 47702	Loss 0.5787	Prec@(1,5) (82.1%, 97.2%)
07/03 10:56:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][200/391]	Step 47702	Loss 0.5806	Prec@(1,5) (81.8%, 97.3%)
07/03 10:56:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][250/391]	Step 47702	Loss 0.5772	Prec@(1,5) (82.0%, 97.2%)
07/03 10:57:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][300/391]	Step 47702	Loss 0.5731	Prec@(1,5) (82.2%, 97.3%)
07/03 10:57:03午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][350/391]	Step 47702	Loss 0.5745	Prec@(1,5) (82.0%, 97.3%)
07/03 10:57:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [60][390/391]	Step 47702	Loss 0.5743	Prec@(1,5) (82.0%, 97.3%)
07/03 10:57:06午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 60/99] Final Prec@1 81.9600%
07/03 10:57:06午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 81.9600%
07/03 10:57:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][50/781]	Step 47752	lr 0.00894	Loss 0.6519 (0.9394)	Prec@(1,5) (82.4%, 97.6%)	
07/03 10:57:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][100/781]	Step 47802	lr 0.00894	Loss 0.8334 (0.9541)	Prec@(1,5) (82.0%, 97.2%)	
07/03 10:57:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][150/781]	Step 47852	lr 0.00894	Loss 1.1883 (0.9587)	Prec@(1,5) (82.0%, 97.2%)	
07/03 10:57:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][200/781]	Step 47902	lr 0.00894	Loss 1.0862 (0.9610)	Prec@(1,5) (81.8%, 97.3%)	
07/03 10:58:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][250/781]	Step 47952	lr 0.00894	Loss 0.8828 (0.9688)	Prec@(1,5) (81.5%, 97.2%)	
07/03 10:58:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][300/781]	Step 48002	lr 0.00894	Loss 0.9116 (0.9662)	Prec@(1,5) (81.5%, 97.2%)	
07/03 10:58:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][350/781]	Step 48052	lr 0.00894	Loss 0.7741 (0.9668)	Prec@(1,5) (81.6%, 97.2%)	
07/03 10:58:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][400/781]	Step 48102	lr 0.00894	Loss 0.8274 (0.9725)	Prec@(1,5) (81.5%, 97.1%)	
07/03 10:58:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][450/781]	Step 48152	lr 0.00894	Loss 0.9652 (0.9738)	Prec@(1,5) (81.5%, 97.1%)	
07/03 10:59:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][500/781]	Step 48202	lr 0.00894	Loss 0.7466 (0.9747)	Prec@(1,5) (81.5%, 97.2%)	
07/03 10:59:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][550/781]	Step 48252	lr 0.00894	Loss 1.2102 (0.9805)	Prec@(1,5) (81.4%, 97.1%)	
07/03 10:59:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][600/781]	Step 48302	lr 0.00894	Loss 0.8162 (0.9814)	Prec@(1,5) (81.4%, 97.1%)	
07/03 10:59:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][650/781]	Step 48352	lr 0.00894	Loss 0.9642 (0.9851)	Prec@(1,5) (81.4%, 97.0%)	
07/03 10:59:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][700/781]	Step 48402	lr 0.00894	Loss 1.0046 (0.9901)	Prec@(1,5) (81.3%, 97.0%)	
07/03 10:59:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][750/781]	Step 48452	lr 0.00894	Loss 0.9372 (0.9923)	Prec@(1,5) (81.2%, 97.0%)	
07/03 11:00:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [61][781/781]	Step 48483	lr 0.00894	Loss 0.8469 (0.9948)	Prec@(1,5) (81.2%, 97.0%)	
07/03 11:00:04午後 evaluateCell_trainer.py:172 [INFO] Train: [ 61/99] Final Prec@1 81.1860%
07/03 11:00:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][50/391]	Step 48484	Loss 0.5903	Prec@(1,5) (81.3%, 97.1%)
07/03 11:00:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][100/391]	Step 48484	Loss 0.5877	Prec@(1,5) (81.7%, 97.2%)
07/03 11:00:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][150/391]	Step 48484	Loss 0.5845	Prec@(1,5) (81.9%, 97.4%)
07/03 11:00:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][200/391]	Step 48484	Loss 0.5860	Prec@(1,5) (81.8%, 97.3%)
07/03 11:00:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][250/391]	Step 48484	Loss 0.5845	Prec@(1,5) (81.9%, 97.3%)
07/03 11:00:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][300/391]	Step 48484	Loss 0.5849	Prec@(1,5) (81.7%, 97.3%)
07/03 11:00:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][350/391]	Step 48484	Loss 0.5855	Prec@(1,5) (81.8%, 97.3%)
07/03 11:00:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [61][390/391]	Step 48484	Loss 0.5864	Prec@(1,5) (81.8%, 97.3%)
07/03 11:00:29午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 61/99] Final Prec@1 81.8320%
07/03 11:00:29午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 81.9600%
07/03 11:00:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][50/781]	Step 48534	lr 0.00858	Loss 0.7010 (0.9273)	Prec@(1,5) (83.2%, 97.3%)	
07/03 11:00:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][100/781]	Step 48584	lr 0.00858	Loss 1.3348 (0.9321)	Prec@(1,5) (83.2%, 97.5%)	
07/03 11:01:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][150/781]	Step 48634	lr 0.00858	Loss 0.6587 (0.9288)	Prec@(1,5) (83.0%, 97.5%)	
07/03 11:01:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][200/781]	Step 48684	lr 0.00858	Loss 0.9307 (0.9379)	Prec@(1,5) (82.9%, 97.5%)	
07/03 11:01:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][250/781]	Step 48734	lr 0.00858	Loss 1.0740 (0.9446)	Prec@(1,5) (82.7%, 97.4%)	
07/03 11:01:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][300/781]	Step 48784	lr 0.00858	Loss 1.1665 (0.9571)	Prec@(1,5) (82.5%, 97.4%)	
07/03 11:01:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][350/781]	Step 48834	lr 0.00858	Loss 1.0959 (0.9567)	Prec@(1,5) (82.5%, 97.3%)	
07/03 11:02:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][400/781]	Step 48884	lr 0.00858	Loss 0.7583 (0.9542)	Prec@(1,5) (82.6%, 97.3%)	
07/03 11:02:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][450/781]	Step 48934	lr 0.00858	Loss 1.0761 (0.9537)	Prec@(1,5) (82.6%, 97.3%)	
07/03 11:02:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][500/781]	Step 48984	lr 0.00858	Loss 0.8302 (0.9620)	Prec@(1,5) (82.4%, 97.2%)	
07/03 11:02:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][550/781]	Step 49034	lr 0.00858	Loss 1.3256 (0.9609)	Prec@(1,5) (82.3%, 97.2%)	
07/03 11:02:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][600/781]	Step 49084	lr 0.00858	Loss 1.0275 (0.9658)	Prec@(1,5) (82.2%, 97.2%)	
07/03 11:02:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][650/781]	Step 49134	lr 0.00858	Loss 0.7683 (0.9663)	Prec@(1,5) (82.2%, 97.2%)	
07/03 11:03:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][700/781]	Step 49184	lr 0.00858	Loss 0.7368 (0.9673)	Prec@(1,5) (82.2%, 97.2%)	
07/03 11:03:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][750/781]	Step 49234	lr 0.00858	Loss 0.8864 (0.9695)	Prec@(1,5) (82.1%, 97.2%)	
07/03 11:03:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [62][781/781]	Step 49265	lr 0.00858	Loss 1.1256 (0.9705)	Prec@(1,5) (82.1%, 97.2%)	
07/03 11:03:27午後 evaluateCell_trainer.py:172 [INFO] Train: [ 62/99] Final Prec@1 82.0640%
07/03 11:03:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][50/391]	Step 49266	Loss 0.5592	Prec@(1,5) (82.4%, 97.8%)
07/03 11:03:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][100/391]	Step 49266	Loss 0.5532	Prec@(1,5) (82.8%, 97.7%)
07/03 11:03:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][150/391]	Step 49266	Loss 0.5424	Prec@(1,5) (83.1%, 97.7%)
07/03 11:03:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][200/391]	Step 49266	Loss 0.5404	Prec@(1,5) (83.1%, 97.8%)
07/03 11:03:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][250/391]	Step 49266	Loss 0.5369	Prec@(1,5) (83.3%, 97.8%)
07/03 11:03:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][300/391]	Step 49266	Loss 0.5406	Prec@(1,5) (83.1%, 97.7%)
07/03 11:03:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][350/391]	Step 49266	Loss 0.5448	Prec@(1,5) (83.0%, 97.7%)
07/03 11:03:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [62][390/391]	Step 49266	Loss 0.5467	Prec@(1,5) (83.0%, 97.7%)
07/03 11:03:52午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 62/99] Final Prec@1 82.9880%
07/03 11:03:53午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 82.9880%
07/03 11:04:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][50/781]	Step 49316	lr 0.00823	Loss 0.8392 (0.9096)	Prec@(1,5) (82.4%, 97.5%)	
07/03 11:04:16午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][100/781]	Step 49366	lr 0.00823	Loss 0.7820 (0.8959)	Prec@(1,5) (82.8%, 97.5%)	
07/03 11:04:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][150/781]	Step 49416	lr 0.00823	Loss 0.9527 (0.9004)	Prec@(1,5) (83.0%, 97.6%)	
07/03 11:04:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][200/781]	Step 49466	lr 0.00823	Loss 1.1088 (0.9095)	Prec@(1,5) (83.0%, 97.6%)	
07/03 11:04:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][250/781]	Step 49516	lr 0.00823	Loss 0.7834 (0.9134)	Prec@(1,5) (83.0%, 97.6%)	
07/03 11:05:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][300/781]	Step 49566	lr 0.00823	Loss 1.0114 (0.9234)	Prec@(1,5) (82.8%, 97.5%)	
07/03 11:05:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][350/781]	Step 49616	lr 0.00823	Loss 0.7611 (0.9239)	Prec@(1,5) (82.8%, 97.5%)	
07/03 11:05:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][400/781]	Step 49666	lr 0.00823	Loss 0.9918 (0.9274)	Prec@(1,5) (82.7%, 97.4%)	
07/03 11:05:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][450/781]	Step 49716	lr 0.00823	Loss 0.9782 (0.9337)	Prec@(1,5) (82.6%, 97.4%)	
07/03 11:05:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][500/781]	Step 49766	lr 0.00823	Loss 0.5476 (0.9388)	Prec@(1,5) (82.4%, 97.4%)	
07/03 11:05:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][550/781]	Step 49816	lr 0.00823	Loss 0.9915 (0.9402)	Prec@(1,5) (82.3%, 97.4%)	
07/03 11:06:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][600/781]	Step 49866	lr 0.00823	Loss 0.7502 (0.9401)	Prec@(1,5) (82.3%, 97.4%)	
07/03 11:06:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][650/781]	Step 49916	lr 0.00823	Loss 1.0199 (0.9439)	Prec@(1,5) (82.2%, 97.4%)	
07/03 11:06:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][700/781]	Step 49966	lr 0.00823	Loss 1.0532 (0.9443)	Prec@(1,5) (82.1%, 97.4%)	
07/03 11:06:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][750/781]	Step 50016	lr 0.00823	Loss 1.3396 (0.9460)	Prec@(1,5) (82.1%, 97.4%)	
07/03 11:06:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [63][781/781]	Step 50047	lr 0.00823	Loss 1.1359 (0.9502)	Prec@(1,5) (82.0%, 97.4%)	
07/03 11:06:51午後 evaluateCell_trainer.py:172 [INFO] Train: [ 63/99] Final Prec@1 82.0060%
07/03 11:06:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][50/391]	Step 50048	Loss 0.5229	Prec@(1,5) (84.0%, 98.1%)
07/03 11:06:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][100/391]	Step 50048	Loss 0.5191	Prec@(1,5) (84.1%, 98.0%)
07/03 11:07:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][150/391]	Step 50048	Loss 0.5204	Prec@(1,5) (84.0%, 98.0%)
07/03 11:07:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][200/391]	Step 50048	Loss 0.5228	Prec@(1,5) (83.9%, 97.9%)
07/03 11:07:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][250/391]	Step 50048	Loss 0.5285	Prec@(1,5) (83.7%, 97.9%)
07/03 11:07:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][300/391]	Step 50048	Loss 0.5307	Prec@(1,5) (83.6%, 97.8%)
07/03 11:07:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][350/391]	Step 50048	Loss 0.5317	Prec@(1,5) (83.6%, 97.8%)
07/03 11:07:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [63][390/391]	Step 50048	Loss 0.5319	Prec@(1,5) (83.6%, 97.8%)
07/03 11:07:17午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 63/99] Final Prec@1 83.6280%
07/03 11:07:17午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 83.6280%
07/03 11:07:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][50/781]	Step 50098	lr 0.00789	Loss 0.9831 (0.8914)	Prec@(1,5) (83.4%, 97.7%)	
07/03 11:07:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][100/781]	Step 50148	lr 0.00789	Loss 0.8744 (0.8689)	Prec@(1,5) (83.7%, 97.9%)	
07/03 11:07:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][150/781]	Step 50198	lr 0.00789	Loss 0.9928 (0.8731)	Prec@(1,5) (83.7%, 97.9%)	
07/03 11:08:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][200/781]	Step 50248	lr 0.00789	Loss 0.7614 (0.8792)	Prec@(1,5) (83.5%, 97.8%)	
07/03 11:08:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][250/781]	Step 50298	lr 0.00789	Loss 0.9542 (0.8903)	Prec@(1,5) (83.4%, 97.8%)	
07/03 11:08:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][300/781]	Step 50348	lr 0.00789	Loss 1.0328 (0.8864)	Prec@(1,5) (83.4%, 97.8%)	
07/03 11:08:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][350/781]	Step 50398	lr 0.00789	Loss 0.8968 (0.8916)	Prec@(1,5) (83.4%, 97.7%)	
07/03 11:08:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][400/781]	Step 50448	lr 0.00789	Loss 0.9717 (0.8930)	Prec@(1,5) (83.4%, 97.7%)	
07/03 11:08:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][450/781]	Step 50498	lr 0.00789	Loss 0.7444 (0.8992)	Prec@(1,5) (83.3%, 97.7%)	
07/03 11:09:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][500/781]	Step 50548	lr 0.00789	Loss 0.6536 (0.9022)	Prec@(1,5) (83.3%, 97.7%)	
07/03 11:09:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][550/781]	Step 50598	lr 0.00789	Loss 0.7047 (0.9061)	Prec@(1,5) (83.2%, 97.7%)	
07/03 11:09:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][600/781]	Step 50648	lr 0.00789	Loss 1.0112 (0.9107)	Prec@(1,5) (83.1%, 97.6%)	
07/03 11:09:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][650/781]	Step 50698	lr 0.00789	Loss 0.9402 (0.9135)	Prec@(1,5) (83.1%, 97.6%)	
07/03 11:09:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][700/781]	Step 50748	lr 0.00789	Loss 1.0121 (0.9153)	Prec@(1,5) (83.0%, 97.6%)	
07/03 11:10:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][750/781]	Step 50798	lr 0.00789	Loss 0.9555 (0.9178)	Prec@(1,5) (83.0%, 97.6%)	
07/03 11:10:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [64][781/781]	Step 50829	lr 0.00789	Loss 1.0763 (0.9203)	Prec@(1,5) (82.9%, 97.6%)	
07/03 11:10:15午後 evaluateCell_trainer.py:172 [INFO] Train: [ 64/99] Final Prec@1 82.9260%
07/03 11:10:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][50/391]	Step 50830	Loss 0.5531	Prec@(1,5) (82.7%, 97.6%)
07/03 11:10:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][100/391]	Step 50830	Loss 0.5589	Prec@(1,5) (82.4%, 97.5%)
07/03 11:10:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][150/391]	Step 50830	Loss 0.5600	Prec@(1,5) (81.9%, 97.7%)
07/03 11:10:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][200/391]	Step 50830	Loss 0.5710	Prec@(1,5) (81.6%, 97.5%)
07/03 11:10:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][250/391]	Step 50830	Loss 0.5759	Prec@(1,5) (81.6%, 97.5%)
07/03 11:10:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][300/391]	Step 50830	Loss 0.5744	Prec@(1,5) (81.7%, 97.6%)
07/03 11:10:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][350/391]	Step 50830	Loss 0.5750	Prec@(1,5) (81.8%, 97.6%)
07/03 11:10:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [64][390/391]	Step 50830	Loss 0.5774	Prec@(1,5) (81.6%, 97.5%)
07/03 11:10:40午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 64/99] Final Prec@1 81.6480%
07/03 11:10:40午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 83.6280%
07/03 11:10:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][50/781]	Step 50880	lr 0.00755	Loss 1.1301 (0.8328)	Prec@(1,5) (85.1%, 98.3%)	
07/03 11:11:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][100/781]	Step 50930	lr 0.00755	Loss 0.7170 (0.8267)	Prec@(1,5) (85.0%, 98.3%)	
07/03 11:11:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][150/781]	Step 50980	lr 0.00755	Loss 0.8652 (0.8388)	Prec@(1,5) (84.8%, 98.2%)	
07/03 11:11:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][200/781]	Step 51030	lr 0.00755	Loss 0.9072 (0.8489)	Prec@(1,5) (84.5%, 98.1%)	
07/03 11:11:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][250/781]	Step 51080	lr 0.00755	Loss 0.7877 (0.8456)	Prec@(1,5) (84.5%, 98.2%)	
07/03 11:11:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][300/781]	Step 51130	lr 0.00755	Loss 1.1014 (0.8437)	Prec@(1,5) (84.6%, 98.2%)	
07/03 11:12:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][350/781]	Step 51180	lr 0.00755	Loss 0.8085 (0.8479)	Prec@(1,5) (84.5%, 98.1%)	
07/03 11:12:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][400/781]	Step 51230	lr 0.00755	Loss 0.9415 (0.8530)	Prec@(1,5) (84.4%, 98.0%)	
07/03 11:12:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][450/781]	Step 51280	lr 0.00755	Loss 0.8500 (0.8569)	Prec@(1,5) (84.2%, 98.0%)	
07/03 11:12:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][500/781]	Step 51330	lr 0.00755	Loss 0.9167 (0.8661)	Prec@(1,5) (84.0%, 97.9%)	
07/03 11:12:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][550/781]	Step 51380	lr 0.00755	Loss 0.8145 (0.8675)	Prec@(1,5) (84.0%, 97.9%)	
07/03 11:12:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][600/781]	Step 51430	lr 0.00755	Loss 0.7444 (0.8699)	Prec@(1,5) (84.0%, 97.9%)	
07/03 11:13:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][650/781]	Step 51480	lr 0.00755	Loss 1.1926 (0.8746)	Prec@(1,5) (83.9%, 97.8%)	
07/03 11:13:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][700/781]	Step 51530	lr 0.00755	Loss 0.8320 (0.8817)	Prec@(1,5) (83.7%, 97.8%)	
07/03 11:13:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][750/781]	Step 51580	lr 0.00755	Loss 1.2858 (0.8906)	Prec@(1,5) (83.5%, 97.7%)	
07/03 11:13:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [65][781/781]	Step 51611	lr 0.00755	Loss 0.7841 (0.8946)	Prec@(1,5) (83.4%, 97.7%)	
07/03 11:13:38午後 evaluateCell_trainer.py:172 [INFO] Train: [ 65/99] Final Prec@1 83.4360%
07/03 11:13:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][50/391]	Step 51612	Loss 0.4583	Prec@(1,5) (86.0%, 98.5%)
07/03 11:13:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][100/391]	Step 51612	Loss 0.4788	Prec@(1,5) (85.3%, 98.3%)
07/03 11:13:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][150/391]	Step 51612	Loss 0.4755	Prec@(1,5) (85.1%, 98.4%)
07/03 11:13:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][200/391]	Step 51612	Loss 0.4749	Prec@(1,5) (85.3%, 98.4%)
07/03 11:13:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][250/391]	Step 51612	Loss 0.4711	Prec@(1,5) (85.4%, 98.4%)
07/03 11:13:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][300/391]	Step 51612	Loss 0.4707	Prec@(1,5) (85.4%, 98.4%)
07/03 11:14:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][350/391]	Step 51612	Loss 0.4701	Prec@(1,5) (85.4%, 98.4%)
07/03 11:14:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [65][390/391]	Step 51612	Loss 0.4708	Prec@(1,5) (85.4%, 98.4%)
07/03 11:14:04午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 65/99] Final Prec@1 85.4240%
07/03 11:14:04午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 85.4240%
07/03 11:14:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][50/781]	Step 51662	lr 0.00722	Loss 0.9979 (0.8858)	Prec@(1,5) (84.2%, 98.1%)	
07/03 11:14:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][100/781]	Step 51712	lr 0.00722	Loss 1.0850 (0.8650)	Prec@(1,5) (84.3%, 98.1%)	
07/03 11:14:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][150/781]	Step 51762	lr 0.00722	Loss 0.8860 (0.8642)	Prec@(1,5) (84.3%, 98.2%)	
07/03 11:14:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][200/781]	Step 51812	lr 0.00722	Loss 0.8344 (0.8581)	Prec@(1,5) (84.4%, 98.1%)	
07/03 11:15:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][250/781]	Step 51862	lr 0.00722	Loss 0.8819 (0.8606)	Prec@(1,5) (84.3%, 98.0%)	
07/03 11:15:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][300/781]	Step 51912	lr 0.00722	Loss 0.8579 (0.8678)	Prec@(1,5) (84.2%, 98.0%)	
07/03 11:15:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][350/781]	Step 51962	lr 0.00722	Loss 0.7996 (0.8730)	Prec@(1,5) (84.2%, 97.9%)	
07/03 11:15:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][400/781]	Step 52012	lr 0.00722	Loss 0.9845 (0.8715)	Prec@(1,5) (84.2%, 97.9%)	
07/03 11:15:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][450/781]	Step 52062	lr 0.00722	Loss 1.0931 (0.8740)	Prec@(1,5) (84.2%, 97.9%)	
07/03 11:15:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][500/781]	Step 52112	lr 0.00722	Loss 0.8494 (0.8707)	Prec@(1,5) (84.3%, 98.0%)	
07/03 11:16:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][550/781]	Step 52162	lr 0.00722	Loss 0.9586 (0.8710)	Prec@(1,5) (84.2%, 97.9%)	
07/03 11:16:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][600/781]	Step 52212	lr 0.00722	Loss 0.7188 (0.8685)	Prec@(1,5) (84.3%, 97.9%)	
07/03 11:16:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][650/781]	Step 52262	lr 0.00722	Loss 0.5367 (0.8695)	Prec@(1,5) (84.2%, 97.9%)	
07/03 11:16:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][700/781]	Step 52312	lr 0.00722	Loss 0.9918 (0.8700)	Prec@(1,5) (84.2%, 97.9%)	
07/03 11:16:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][750/781]	Step 52362	lr 0.00722	Loss 0.9142 (0.8747)	Prec@(1,5) (84.1%, 97.9%)	
07/03 11:17:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [66][781/781]	Step 52393	lr 0.00722	Loss 0.9246 (0.8762)	Prec@(1,5) (84.0%, 97.9%)	
07/03 11:17:02午後 evaluateCell_trainer.py:172 [INFO] Train: [ 66/99] Final Prec@1 84.0140%
07/03 11:17:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][50/391]	Step 52394	Loss 0.4665	Prec@(1,5) (85.6%, 98.2%)
07/03 11:17:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][100/391]	Step 52394	Loss 0.4510	Prec@(1,5) (86.0%, 98.3%)
07/03 11:17:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][150/391]	Step 52394	Loss 0.4547	Prec@(1,5) (85.9%, 98.2%)
07/03 11:17:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][200/391]	Step 52394	Loss 0.4562	Prec@(1,5) (85.9%, 98.3%)
07/03 11:17:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][250/391]	Step 52394	Loss 0.4575	Prec@(1,5) (85.8%, 98.2%)
07/03 11:17:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][300/391]	Step 52394	Loss 0.4554	Prec@(1,5) (86.0%, 98.2%)
07/03 11:17:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][350/391]	Step 52394	Loss 0.4549	Prec@(1,5) (86.0%, 98.3%)
07/03 11:17:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [66][390/391]	Step 52394	Loss 0.4540	Prec@(1,5) (86.1%, 98.3%)
07/03 11:17:27午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 66/99] Final Prec@1 86.1000%
07/03 11:17:27午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 86.1000%
07/03 11:17:39午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][50/781]	Step 52444	lr 0.00689	Loss 0.9321 (0.8243)	Prec@(1,5) (85.2%, 98.0%)	
07/03 11:17:51午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][100/781]	Step 52494	lr 0.00689	Loss 0.6394 (0.8123)	Prec@(1,5) (85.1%, 98.1%)	
07/03 11:18:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][150/781]	Step 52544	lr 0.00689	Loss 0.5403 (0.8074)	Prec@(1,5) (85.1%, 98.2%)	
07/03 11:18:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][200/781]	Step 52594	lr 0.00689	Loss 0.8123 (0.8267)	Prec@(1,5) (84.8%, 98.1%)	
07/03 11:18:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][250/781]	Step 52644	lr 0.00689	Loss 0.8698 (0.8289)	Prec@(1,5) (84.8%, 98.1%)	
07/03 11:18:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][300/781]	Step 52694	lr 0.00689	Loss 0.8406 (0.8381)	Prec@(1,5) (84.7%, 98.0%)	
07/03 11:18:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][350/781]	Step 52744	lr 0.00689	Loss 0.5960 (0.8410)	Prec@(1,5) (84.6%, 98.0%)	
07/03 11:18:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][400/781]	Step 52794	lr 0.00689	Loss 0.8122 (0.8371)	Prec@(1,5) (84.7%, 98.0%)	
07/03 11:19:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][450/781]	Step 52844	lr 0.00689	Loss 1.0221 (0.8391)	Prec@(1,5) (84.7%, 98.0%)	
07/03 11:19:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][500/781]	Step 52894	lr 0.00689	Loss 0.4966 (0.8393)	Prec@(1,5) (84.6%, 98.0%)	
07/03 11:19:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][550/781]	Step 52944	lr 0.00689	Loss 1.0787 (0.8410)	Prec@(1,5) (84.5%, 98.0%)	
07/03 11:19:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][600/781]	Step 52994	lr 0.00689	Loss 1.0786 (0.8412)	Prec@(1,5) (84.5%, 98.0%)	
07/03 11:19:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][650/781]	Step 53044	lr 0.00689	Loss 0.7641 (0.8423)	Prec@(1,5) (84.5%, 98.0%)	
07/03 11:20:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][700/781]	Step 53094	lr 0.00689	Loss 0.7124 (0.8435)	Prec@(1,5) (84.5%, 98.0%)	
07/03 11:20:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][750/781]	Step 53144	lr 0.00689	Loss 1.2375 (0.8465)	Prec@(1,5) (84.5%, 98.0%)	
07/03 11:20:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [67][781/781]	Step 53175	lr 0.00689	Loss 0.7809 (0.8476)	Prec@(1,5) (84.5%, 98.0%)	
07/03 11:20:26午後 evaluateCell_trainer.py:172 [INFO] Train: [ 67/99] Final Prec@1 84.4780%
07/03 11:20:30午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][50/391]	Step 53176	Loss 0.4723	Prec@(1,5) (85.6%, 98.4%)
07/03 11:20:33午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][100/391]	Step 53176	Loss 0.4806	Prec@(1,5) (85.2%, 98.3%)
07/03 11:20:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][150/391]	Step 53176	Loss 0.4858	Prec@(1,5) (85.1%, 98.2%)
07/03 11:20:39午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][200/391]	Step 53176	Loss 0.4912	Prec@(1,5) (84.8%, 98.2%)
07/03 11:20:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][250/391]	Step 53176	Loss 0.4863	Prec@(1,5) (84.8%, 98.3%)
07/03 11:20:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][300/391]	Step 53176	Loss 0.4851	Prec@(1,5) (84.8%, 98.3%)
07/03 11:20:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][350/391]	Step 53176	Loss 0.4843	Prec@(1,5) (84.7%, 98.3%)
07/03 11:20:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [67][390/391]	Step 53176	Loss 0.4831	Prec@(1,5) (84.8%, 98.3%)
07/03 11:20:51午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 67/99] Final Prec@1 84.7800%
07/03 11:20:51午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 86.1000%
07/03 11:21:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][50/781]	Step 53226	lr 0.00657	Loss 0.9989 (0.7803)	Prec@(1,5) (85.9%, 98.5%)	
07/03 11:21:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][100/781]	Step 53276	lr 0.00657	Loss 0.7574 (0.7831)	Prec@(1,5) (86.0%, 98.4%)	
07/03 11:21:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][150/781]	Step 53326	lr 0.00657	Loss 0.8077 (0.7875)	Prec@(1,5) (85.7%, 98.3%)	
07/03 11:21:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][200/781]	Step 53376	lr 0.00657	Loss 1.0182 (0.7911)	Prec@(1,5) (85.8%, 98.3%)	
07/03 11:21:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][250/781]	Step 53426	lr 0.00657	Loss 0.7152 (0.7998)	Prec@(1,5) (85.6%, 98.2%)	
07/03 11:21:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][300/781]	Step 53476	lr 0.00657	Loss 0.8258 (0.8014)	Prec@(1,5) (85.7%, 98.2%)	
07/03 11:22:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][350/781]	Step 53526	lr 0.00657	Loss 0.8840 (0.8062)	Prec@(1,5) (85.6%, 98.2%)	
07/03 11:22:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][400/781]	Step 53576	lr 0.00657	Loss 0.8387 (0.8057)	Prec@(1,5) (85.5%, 98.2%)	
07/03 11:22:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][450/781]	Step 53626	lr 0.00657	Loss 0.8534 (0.8094)	Prec@(1,5) (85.5%, 98.2%)	
07/03 11:22:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][500/781]	Step 53676	lr 0.00657	Loss 0.8266 (0.8119)	Prec@(1,5) (85.4%, 98.2%)	
07/03 11:22:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][550/781]	Step 53726	lr 0.00657	Loss 0.9234 (0.8149)	Prec@(1,5) (85.4%, 98.1%)	
07/03 11:23:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][600/781]	Step 53776	lr 0.00657	Loss 0.8181 (0.8183)	Prec@(1,5) (85.2%, 98.1%)	
07/03 11:23:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][650/781]	Step 53826	lr 0.00657	Loss 0.7615 (0.8219)	Prec@(1,5) (85.2%, 98.1%)	
07/03 11:23:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][700/781]	Step 53876	lr 0.00657	Loss 0.7558 (0.8229)	Prec@(1,5) (85.2%, 98.1%)	
07/03 11:23:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][750/781]	Step 53926	lr 0.00657	Loss 1.0366 (0.8263)	Prec@(1,5) (85.1%, 98.1%)	
07/03 11:23:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [68][781/781]	Step 53957	lr 0.00657	Loss 1.0535 (0.8274)	Prec@(1,5) (85.1%, 98.0%)	
07/03 11:23:49午後 evaluateCell_trainer.py:172 [INFO] Train: [ 68/99] Final Prec@1 85.0620%
07/03 11:23:53午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][50/391]	Step 53958	Loss 0.4468	Prec@(1,5) (85.8%, 98.3%)
07/03 11:23:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][100/391]	Step 53958	Loss 0.4491	Prec@(1,5) (85.9%, 98.4%)
07/03 11:23:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][150/391]	Step 53958	Loss 0.4511	Prec@(1,5) (85.9%, 98.4%)
07/03 11:24:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][200/391]	Step 53958	Loss 0.4529	Prec@(1,5) (85.9%, 98.3%)
07/03 11:24:06午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][250/391]	Step 53958	Loss 0.4490	Prec@(1,5) (86.0%, 98.4%)
07/03 11:24:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][300/391]	Step 53958	Loss 0.4458	Prec@(1,5) (86.0%, 98.4%)
07/03 11:24:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][350/391]	Step 53958	Loss 0.4517	Prec@(1,5) (85.9%, 98.3%)
07/03 11:24:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [68][390/391]	Step 53958	Loss 0.4523	Prec@(1,5) (85.9%, 98.3%)
07/03 11:24:15午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 68/99] Final Prec@1 85.8440%
07/03 11:24:15午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 86.1000%
07/03 11:24:26午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][50/781]	Step 54008	lr 0.00625	Loss 0.8440 (0.7949)	Prec@(1,5) (87.0%, 98.2%)	
07/03 11:24:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][100/781]	Step 54058	lr 0.00625	Loss 0.7155 (0.7738)	Prec@(1,5) (87.2%, 98.3%)	
07/03 11:24:49午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][150/781]	Step 54108	lr 0.00625	Loss 0.5814 (0.7609)	Prec@(1,5) (87.0%, 98.5%)	
07/03 11:25:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][200/781]	Step 54158	lr 0.00625	Loss 0.7527 (0.7597)	Prec@(1,5) (87.0%, 98.5%)	
07/03 11:25:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][250/781]	Step 54208	lr 0.00625	Loss 0.8191 (0.7602)	Prec@(1,5) (86.9%, 98.6%)	
07/03 11:25:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][300/781]	Step 54258	lr 0.00625	Loss 0.6841 (0.7611)	Prec@(1,5) (86.7%, 98.6%)	
07/03 11:25:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][350/781]	Step 54308	lr 0.00625	Loss 0.6070 (0.7636)	Prec@(1,5) (86.8%, 98.5%)	
07/03 11:25:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][400/781]	Step 54358	lr 0.00625	Loss 1.0379 (0.7649)	Prec@(1,5) (86.7%, 98.5%)	
07/03 11:25:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][450/781]	Step 54408	lr 0.00625	Loss 0.5578 (0.7671)	Prec@(1,5) (86.6%, 98.5%)	
07/03 11:26:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][500/781]	Step 54458	lr 0.00625	Loss 0.7600 (0.7696)	Prec@(1,5) (86.5%, 98.4%)	
07/03 11:26:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][550/781]	Step 54508	lr 0.00625	Loss 1.1244 (0.7790)	Prec@(1,5) (86.2%, 98.4%)	
07/03 11:26:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][600/781]	Step 54558	lr 0.00625	Loss 0.6897 (0.7803)	Prec@(1,5) (86.3%, 98.3%)	
07/03 11:26:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][650/781]	Step 54608	lr 0.00625	Loss 0.9934 (0.7847)	Prec@(1,5) (86.2%, 98.3%)	
07/03 11:26:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][700/781]	Step 54658	lr 0.00625	Loss 0.8820 (0.7887)	Prec@(1,5) (86.1%, 98.3%)	
07/03 11:27:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][750/781]	Step 54708	lr 0.00625	Loss 0.7177 (0.7896)	Prec@(1,5) (86.0%, 98.3%)	
07/03 11:27:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [69][781/781]	Step 54739	lr 0.00625	Loss 0.6736 (0.7897)	Prec@(1,5) (86.0%, 98.3%)	
07/03 11:27:13午後 evaluateCell_trainer.py:172 [INFO] Train: [ 69/99] Final Prec@1 86.0160%
07/03 11:27:16午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][50/391]	Step 54740	Loss 0.3765	Prec@(1,5) (88.1%, 99.0%)
07/03 11:27:19午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][100/391]	Step 54740	Loss 0.3942	Prec@(1,5) (87.7%, 98.7%)
07/03 11:27:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][150/391]	Step 54740	Loss 0.4062	Prec@(1,5) (87.3%, 98.6%)
07/03 11:27:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][200/391]	Step 54740	Loss 0.4002	Prec@(1,5) (87.5%, 98.7%)
07/03 11:27:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][250/391]	Step 54740	Loss 0.3950	Prec@(1,5) (87.6%, 98.7%)
07/03 11:27:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][300/391]	Step 54740	Loss 0.3969	Prec@(1,5) (87.6%, 98.7%)
07/03 11:27:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][350/391]	Step 54740	Loss 0.3958	Prec@(1,5) (87.7%, 98.7%)
07/03 11:27:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [69][390/391]	Step 54740	Loss 0.3944	Prec@(1,5) (87.7%, 98.8%)
07/03 11:27:38午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 69/99] Final Prec@1 87.7040%
07/03 11:27:38午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 87.7040%
07/03 11:27:50午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][50/781]	Step 54790	lr 0.00595	Loss 0.7813 (0.7211)	Prec@(1,5) (87.7%, 98.4%)	
07/03 11:28:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][100/781]	Step 54840	lr 0.00595	Loss 0.6976 (0.7390)	Prec@(1,5) (87.3%, 98.5%)	
07/03 11:28:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][150/781]	Step 54890	lr 0.00595	Loss 0.9017 (0.7561)	Prec@(1,5) (86.8%, 98.3%)	
07/03 11:28:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][200/781]	Step 54940	lr 0.00595	Loss 0.5968 (0.7620)	Prec@(1,5) (86.6%, 98.3%)	
07/03 11:28:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][250/781]	Step 54990	lr 0.00595	Loss 0.7363 (0.7681)	Prec@(1,5) (86.4%, 98.3%)	
07/03 11:28:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][300/781]	Step 55040	lr 0.00595	Loss 0.9405 (0.7707)	Prec@(1,5) (86.3%, 98.3%)	
07/03 11:28:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][350/781]	Step 55090	lr 0.00595	Loss 0.9066 (0.7703)	Prec@(1,5) (86.4%, 98.3%)	
07/03 11:29:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][400/781]	Step 55140	lr 0.00595	Loss 0.8096 (0.7711)	Prec@(1,5) (86.3%, 98.4%)	
07/03 11:29:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][450/781]	Step 55190	lr 0.00595	Loss 0.9487 (0.7720)	Prec@(1,5) (86.3%, 98.4%)	
07/03 11:29:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][500/781]	Step 55240	lr 0.00595	Loss 0.5071 (0.7739)	Prec@(1,5) (86.2%, 98.4%)	
07/03 11:29:44午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][550/781]	Step 55290	lr 0.00595	Loss 0.8421 (0.7766)	Prec@(1,5) (86.2%, 98.4%)	
07/03 11:29:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][600/781]	Step 55340	lr 0.00595	Loss 0.5709 (0.7775)	Prec@(1,5) (86.1%, 98.4%)	
07/03 11:30:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][650/781]	Step 55390	lr 0.00595	Loss 0.8142 (0.7817)	Prec@(1,5) (86.1%, 98.3%)	
07/03 11:30:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][700/781]	Step 55440	lr 0.00595	Loss 0.6527 (0.7833)	Prec@(1,5) (86.1%, 98.3%)	
07/03 11:30:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][750/781]	Step 55490	lr 0.00595	Loss 1.1255 (0.7831)	Prec@(1,5) (86.1%, 98.3%)	
07/03 11:30:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [70][781/781]	Step 55521	lr 0.00595	Loss 0.7192 (0.7837)	Prec@(1,5) (86.1%, 98.3%)	
07/03 11:30:36午後 evaluateCell_trainer.py:172 [INFO] Train: [ 70/99] Final Prec@1 86.0760%
07/03 11:30:40午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][50/391]	Step 55522	Loss 0.3836	Prec@(1,5) (88.4%, 98.7%)
07/03 11:30:43午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][100/391]	Step 55522	Loss 0.3879	Prec@(1,5) (88.3%, 98.7%)
07/03 11:30:46午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][150/391]	Step 55522	Loss 0.3851	Prec@(1,5) (88.5%, 98.7%)
07/03 11:30:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][200/391]	Step 55522	Loss 0.3843	Prec@(1,5) (88.4%, 98.7%)
07/03 11:30:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][250/391]	Step 55522	Loss 0.3817	Prec@(1,5) (88.5%, 98.8%)
07/03 11:30:56午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][300/391]	Step 55522	Loss 0.3791	Prec@(1,5) (88.5%, 98.8%)
07/03 11:30:59午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][350/391]	Step 55522	Loss 0.3818	Prec@(1,5) (88.3%, 98.7%)
07/03 11:31:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [70][390/391]	Step 55522	Loss 0.3817	Prec@(1,5) (88.3%, 98.8%)
07/03 11:31:01午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 70/99] Final Prec@1 88.2440%
07/03 11:31:02午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 88.2440%
07/03 11:31:14午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][50/781]	Step 55572	lr 0.00565	Loss 0.5953 (0.7123)	Prec@(1,5) (87.4%, 98.4%)	
07/03 11:31:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][100/781]	Step 55622	lr 0.00565	Loss 0.6993 (0.6982)	Prec@(1,5) (87.9%, 98.5%)	
07/03 11:31:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][150/781]	Step 55672	lr 0.00565	Loss 0.5435 (0.7117)	Prec@(1,5) (87.8%, 98.6%)	
07/03 11:31:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][200/781]	Step 55722	lr 0.00565	Loss 0.6525 (0.7127)	Prec@(1,5) (87.8%, 98.7%)	
07/03 11:31:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][250/781]	Step 55772	lr 0.00565	Loss 0.6583 (0.7141)	Prec@(1,5) (87.7%, 98.6%)	
07/03 11:32:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][300/781]	Step 55822	lr 0.00565	Loss 0.5049 (0.7193)	Prec@(1,5) (87.6%, 98.6%)	
07/03 11:32:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][350/781]	Step 55872	lr 0.00565	Loss 0.6874 (0.7220)	Prec@(1,5) (87.7%, 98.6%)	
07/03 11:32:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][400/781]	Step 55922	lr 0.00565	Loss 0.8619 (0.7269)	Prec@(1,5) (87.6%, 98.5%)	
07/03 11:32:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][450/781]	Step 55972	lr 0.00565	Loss 0.8489 (0.7319)	Prec@(1,5) (87.5%, 98.5%)	
07/03 11:32:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][500/781]	Step 56022	lr 0.00565	Loss 0.5693 (0.7330)	Prec@(1,5) (87.4%, 98.6%)	
07/03 11:33:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][550/781]	Step 56072	lr 0.00565	Loss 1.2854 (0.7365)	Prec@(1,5) (87.4%, 98.5%)	
07/03 11:33:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][600/781]	Step 56122	lr 0.00565	Loss 0.7666 (0.7410)	Prec@(1,5) (87.4%, 98.5%)	
07/03 11:33:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][650/781]	Step 56172	lr 0.00565	Loss 0.7554 (0.7427)	Prec@(1,5) (87.4%, 98.5%)	
07/03 11:33:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][700/781]	Step 56222	lr 0.00565	Loss 0.7066 (0.7431)	Prec@(1,5) (87.3%, 98.5%)	
07/03 11:33:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][750/781]	Step 56272	lr 0.00565	Loss 1.1907 (0.7477)	Prec@(1,5) (87.2%, 98.5%)	
07/03 11:34:01午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [71][781/781]	Step 56303	lr 0.00565	Loss 0.8883 (0.7494)	Prec@(1,5) (87.2%, 98.4%)	
07/03 11:34:01午後 evaluateCell_trainer.py:172 [INFO] Train: [ 71/99] Final Prec@1 87.1740%
07/03 11:34:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][50/391]	Step 56304	Loss 0.3732	Prec@(1,5) (87.7%, 98.8%)
07/03 11:34:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][100/391]	Step 56304	Loss 0.3715	Prec@(1,5) (88.4%, 98.7%)
07/03 11:34:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][150/391]	Step 56304	Loss 0.3655	Prec@(1,5) (88.6%, 98.8%)
07/03 11:34:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][200/391]	Step 56304	Loss 0.3704	Prec@(1,5) (88.4%, 98.7%)
07/03 11:34:17午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][250/391]	Step 56304	Loss 0.3698	Prec@(1,5) (88.5%, 98.8%)
07/03 11:34:20午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][300/391]	Step 56304	Loss 0.3672	Prec@(1,5) (88.6%, 98.8%)
07/03 11:34:23午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][350/391]	Step 56304	Loss 0.3648	Prec@(1,5) (88.7%, 98.8%)
07/03 11:34:26午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [71][390/391]	Step 56304	Loss 0.3657	Prec@(1,5) (88.7%, 98.8%)
07/03 11:34:26午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 71/99] Final Prec@1 88.6560%
07/03 11:34:26午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 88.6560%
07/03 11:34:37午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][50/781]	Step 56354	lr 0.00535	Loss 0.8636 (0.7105)	Prec@(1,5) (87.8%, 99.1%)	
07/03 11:34:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][100/781]	Step 56404	lr 0.00535	Loss 0.6946 (0.7122)	Prec@(1,5) (88.0%, 98.8%)	
07/03 11:35:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][150/781]	Step 56454	lr 0.00535	Loss 0.6434 (0.7032)	Prec@(1,5) (88.1%, 98.9%)	
07/03 11:35:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][200/781]	Step 56504	lr 0.00535	Loss 0.7797 (0.7069)	Prec@(1,5) (88.0%, 98.7%)	
07/03 11:35:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][250/781]	Step 56554	lr 0.00535	Loss 0.6329 (0.7097)	Prec@(1,5) (87.9%, 98.6%)	
07/03 11:35:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][300/781]	Step 56604	lr 0.00535	Loss 0.7300 (0.7084)	Prec@(1,5) (88.0%, 98.6%)	
07/03 11:35:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][350/781]	Step 56654	lr 0.00535	Loss 0.6702 (0.7155)	Prec@(1,5) (87.8%, 98.5%)	
07/03 11:35:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][400/781]	Step 56704	lr 0.00535	Loss 0.8807 (0.7219)	Prec@(1,5) (87.7%, 98.5%)	
07/03 11:36:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][450/781]	Step 56754	lr 0.00535	Loss 0.4797 (0.7244)	Prec@(1,5) (87.6%, 98.5%)	
07/03 11:36:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][500/781]	Step 56804	lr 0.00535	Loss 0.5822 (0.7282)	Prec@(1,5) (87.5%, 98.5%)	
07/03 11:36:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][550/781]	Step 56854	lr 0.00535	Loss 0.7598 (0.7290)	Prec@(1,5) (87.5%, 98.5%)	
07/03 11:36:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][600/781]	Step 56904	lr 0.00535	Loss 0.8480 (0.7294)	Prec@(1,5) (87.5%, 98.5%)	
07/03 11:36:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][650/781]	Step 56954	lr 0.00535	Loss 0.6659 (0.7305)	Prec@(1,5) (87.4%, 98.5%)	
07/03 11:37:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][700/781]	Step 57004	lr 0.00535	Loss 0.9286 (0.7327)	Prec@(1,5) (87.4%, 98.5%)	
07/03 11:37:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][750/781]	Step 57054	lr 0.00535	Loss 0.8340 (0.7353)	Prec@(1,5) (87.2%, 98.5%)	
07/03 11:37:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [72][781/781]	Step 57085	lr 0.00535	Loss 0.9546 (0.7358)	Prec@(1,5) (87.2%, 98.5%)	
07/03 11:37:24午後 evaluateCell_trainer.py:172 [INFO] Train: [ 72/99] Final Prec@1 87.2160%
07/03 11:37:28午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][50/391]	Step 57086	Loss 0.3630	Prec@(1,5) (89.2%, 98.6%)
07/03 11:37:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][100/391]	Step 57086	Loss 0.3526	Prec@(1,5) (89.1%, 98.9%)
07/03 11:37:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][150/391]	Step 57086	Loss 0.3518	Prec@(1,5) (88.9%, 99.0%)
07/03 11:37:37午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][200/391]	Step 57086	Loss 0.3552	Prec@(1,5) (88.8%, 98.9%)
07/03 11:37:41午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][250/391]	Step 57086	Loss 0.3598	Prec@(1,5) (88.7%, 98.9%)
07/03 11:37:44午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][300/391]	Step 57086	Loss 0.3580	Prec@(1,5) (88.8%, 98.9%)
07/03 11:37:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][350/391]	Step 57086	Loss 0.3576	Prec@(1,5) (88.8%, 98.9%)
07/03 11:37:50午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [72][390/391]	Step 57086	Loss 0.3547	Prec@(1,5) (89.0%, 98.9%)
07/03 11:37:50午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 72/99] Final Prec@1 88.9800%
07/03 11:37:50午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 88.9800%
07/03 11:38:02午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][50/781]	Step 57136	lr 0.00506	Loss 0.7364 (0.6951)	Prec@(1,5) (88.1%, 98.6%)	
07/03 11:38:13午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][100/781]	Step 57186	lr 0.00506	Loss 0.6993 (0.6773)	Prec@(1,5) (88.3%, 98.7%)	
07/03 11:38:24午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][150/781]	Step 57236	lr 0.00506	Loss 0.7726 (0.6921)	Prec@(1,5) (88.1%, 98.6%)	
07/03 11:38:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][200/781]	Step 57286	lr 0.00506	Loss 0.5004 (0.7007)	Prec@(1,5) (88.0%, 98.5%)	
07/03 11:38:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][250/781]	Step 57336	lr 0.00506	Loss 0.7320 (0.6909)	Prec@(1,5) (88.2%, 98.6%)	
07/03 11:38:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][300/781]	Step 57386	lr 0.00506	Loss 0.6458 (0.6960)	Prec@(1,5) (88.1%, 98.6%)	
07/03 11:39:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][350/781]	Step 57436	lr 0.00506	Loss 0.8250 (0.7017)	Prec@(1,5) (88.0%, 98.7%)	
07/03 11:39:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][400/781]	Step 57486	lr 0.00506	Loss 0.4840 (0.7044)	Prec@(1,5) (87.9%, 98.6%)	
07/03 11:39:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][450/781]	Step 57536	lr 0.00506	Loss 0.6441 (0.7059)	Prec@(1,5) (87.8%, 98.6%)	
07/03 11:39:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][500/781]	Step 57586	lr 0.00506	Loss 0.9030 (0.7055)	Prec@(1,5) (87.8%, 98.7%)	
07/03 11:39:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][550/781]	Step 57636	lr 0.00506	Loss 0.5941 (0.7065)	Prec@(1,5) (87.8%, 98.6%)	
07/03 11:40:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][600/781]	Step 57686	lr 0.00506	Loss 0.6894 (0.7074)	Prec@(1,5) (87.8%, 98.7%)	
07/03 11:40:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][650/781]	Step 57736	lr 0.00506	Loss 0.6767 (0.7082)	Prec@(1,5) (87.8%, 98.7%)	
07/03 11:40:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][700/781]	Step 57786	lr 0.00506	Loss 0.9108 (0.7101)	Prec@(1,5) (87.8%, 98.6%)	
07/03 11:40:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][750/781]	Step 57836	lr 0.00506	Loss 0.5438 (0.7118)	Prec@(1,5) (87.8%, 98.6%)	
07/03 11:40:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [73][781/781]	Step 57867	lr 0.00506	Loss 0.9343 (0.7122)	Prec@(1,5) (87.7%, 98.6%)	
07/03 11:40:48午後 evaluateCell_trainer.py:172 [INFO] Train: [ 73/99] Final Prec@1 87.7360%
07/03 11:40:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][50/391]	Step 57868	Loss 0.3182	Prec@(1,5) (89.9%, 99.2%)
07/03 11:40:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][100/391]	Step 57868	Loss 0.3142	Prec@(1,5) (89.9%, 99.2%)
07/03 11:40:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][150/391]	Step 57868	Loss 0.3151	Prec@(1,5) (90.0%, 99.1%)
07/03 11:41:01午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][200/391]	Step 57868	Loss 0.3185	Prec@(1,5) (90.0%, 99.1%)
07/03 11:41:04午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][250/391]	Step 57868	Loss 0.3198	Prec@(1,5) (90.0%, 99.1%)
07/03 11:41:07午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][300/391]	Step 57868	Loss 0.3233	Prec@(1,5) (89.9%, 99.1%)
07/03 11:41:10午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][350/391]	Step 57868	Loss 0.3228	Prec@(1,5) (89.9%, 99.1%)
07/03 11:41:13午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [73][390/391]	Step 57868	Loss 0.3234	Prec@(1,5) (90.0%, 99.1%)
07/03 11:41:13午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 73/99] Final Prec@1 89.9720%
07/03 11:41:13午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 89.9720%
07/03 11:41:25午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][50/781]	Step 57918	lr 0.00479	Loss 1.0168 (0.6450)	Prec@(1,5) (89.6%, 99.0%)	
07/03 11:41:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][100/781]	Step 57968	lr 0.00479	Loss 0.9215 (0.6357)	Prec@(1,5) (89.6%, 99.0%)	
07/03 11:41:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][150/781]	Step 58018	lr 0.00479	Loss 0.8339 (0.6509)	Prec@(1,5) (89.2%, 98.9%)	
07/03 11:41:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][200/781]	Step 58068	lr 0.00479	Loss 0.7596 (0.6594)	Prec@(1,5) (89.1%, 98.8%)	
07/03 11:42:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][250/781]	Step 58118	lr 0.00479	Loss 0.5182 (0.6708)	Prec@(1,5) (88.8%, 98.8%)	
07/03 11:42:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][300/781]	Step 58168	lr 0.00479	Loss 0.5486 (0.6746)	Prec@(1,5) (88.7%, 98.8%)	
07/03 11:42:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][350/781]	Step 58218	lr 0.00479	Loss 0.4285 (0.6726)	Prec@(1,5) (88.8%, 98.8%)	
07/03 11:42:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][400/781]	Step 58268	lr 0.00479	Loss 0.9881 (0.6742)	Prec@(1,5) (88.8%, 98.8%)	
07/03 11:42:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][450/781]	Step 58318	lr 0.00479	Loss 0.5362 (0.6779)	Prec@(1,5) (88.6%, 98.7%)	
07/03 11:43:07午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][500/781]	Step 58368	lr 0.00479	Loss 0.8145 (0.6801)	Prec@(1,5) (88.6%, 98.7%)	
07/03 11:43:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][550/781]	Step 58418	lr 0.00479	Loss 0.8362 (0.6796)	Prec@(1,5) (88.6%, 98.7%)	
07/03 11:43:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][600/781]	Step 58468	lr 0.00479	Loss 0.6897 (0.6816)	Prec@(1,5) (88.5%, 98.7%)	
07/03 11:43:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][650/781]	Step 58518	lr 0.00479	Loss 0.5531 (0.6818)	Prec@(1,5) (88.5%, 98.7%)	
07/03 11:43:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][700/781]	Step 58568	lr 0.00479	Loss 0.8171 (0.6833)	Prec@(1,5) (88.5%, 98.7%)	
07/03 11:44:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][750/781]	Step 58618	lr 0.00479	Loss 0.4958 (0.6832)	Prec@(1,5) (88.5%, 98.7%)	
07/03 11:44:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [74][781/781]	Step 58649	lr 0.00479	Loss 0.6226 (0.6841)	Prec@(1,5) (88.5%, 98.7%)	
07/03 11:44:11午後 evaluateCell_trainer.py:172 [INFO] Train: [ 74/99] Final Prec@1 88.4760%
07/03 11:44:14午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][50/391]	Step 58650	Loss 0.3139	Prec@(1,5) (90.5%, 99.1%)
07/03 11:44:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][100/391]	Step 58650	Loss 0.3109	Prec@(1,5) (90.9%, 99.2%)
07/03 11:44:21午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][150/391]	Step 58650	Loss 0.3113	Prec@(1,5) (90.8%, 99.1%)
07/03 11:44:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][200/391]	Step 58650	Loss 0.3089	Prec@(1,5) (90.8%, 99.1%)
07/03 11:44:27午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][250/391]	Step 58650	Loss 0.3110	Prec@(1,5) (90.6%, 99.1%)
07/03 11:44:31午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][300/391]	Step 58650	Loss 0.3139	Prec@(1,5) (90.6%, 99.1%)
07/03 11:44:34午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][350/391]	Step 58650	Loss 0.3162	Prec@(1,5) (90.5%, 99.1%)
07/03 11:44:36午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [74][390/391]	Step 58650	Loss 0.3174	Prec@(1,5) (90.5%, 99.1%)
07/03 11:44:37午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 74/99] Final Prec@1 90.4800%
07/03 11:44:37午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 90.4800%
07/03 11:44:48午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][50/781]	Step 58700	lr 0.00451	Loss 0.5586 (0.6489)	Prec@(1,5) (89.3%, 98.9%)	
07/03 11:45:00午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][100/781]	Step 58750	lr 0.00451	Loss 0.7589 (0.6452)	Prec@(1,5) (89.5%, 98.9%)	
07/03 11:45:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][150/781]	Step 58800	lr 0.00451	Loss 0.7479 (0.6385)	Prec@(1,5) (89.6%, 99.0%)	
07/03 11:45:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][200/781]	Step 58850	lr 0.00451	Loss 0.5900 (0.6405)	Prec@(1,5) (89.5%, 98.9%)	
07/03 11:45:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][250/781]	Step 58900	lr 0.00451	Loss 0.7529 (0.6456)	Prec@(1,5) (89.3%, 98.9%)	
07/03 11:45:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][300/781]	Step 58950	lr 0.00451	Loss 0.4582 (0.6442)	Prec@(1,5) (89.4%, 98.9%)	
07/03 11:45:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][350/781]	Step 59000	lr 0.00451	Loss 0.6310 (0.6463)	Prec@(1,5) (89.5%, 98.9%)	
07/03 11:46:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][400/781]	Step 59050	lr 0.00451	Loss 0.7202 (0.6484)	Prec@(1,5) (89.5%, 98.9%)	
07/03 11:46:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][450/781]	Step 59100	lr 0.00451	Loss 0.5697 (0.6565)	Prec@(1,5) (89.3%, 98.9%)	
07/03 11:46:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][500/781]	Step 59150	lr 0.00451	Loss 0.7373 (0.6576)	Prec@(1,5) (89.3%, 98.9%)	
07/03 11:46:42午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][550/781]	Step 59200	lr 0.00451	Loss 0.6378 (0.6613)	Prec@(1,5) (89.1%, 98.8%)	
07/03 11:46:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][600/781]	Step 59250	lr 0.00451	Loss 0.8415 (0.6678)	Prec@(1,5) (89.0%, 98.8%)	
07/03 11:47:05午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][650/781]	Step 59300	lr 0.00451	Loss 0.6265 (0.6712)	Prec@(1,5) (88.9%, 98.8%)	
07/03 11:47:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][700/781]	Step 59350	lr 0.00451	Loss 0.5473 (0.6706)	Prec@(1,5) (88.9%, 98.8%)	
07/03 11:47:28午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][750/781]	Step 59400	lr 0.00451	Loss 0.6874 (0.6709)	Prec@(1,5) (88.9%, 98.8%)	
07/03 11:47:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [75][781/781]	Step 59431	lr 0.00451	Loss 0.6894 (0.6715)	Prec@(1,5) (88.8%, 98.8%)	
07/03 11:47:35午後 evaluateCell_trainer.py:172 [INFO] Train: [ 75/99] Final Prec@1 88.8360%
07/03 11:47:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][50/391]	Step 59432	Loss 0.3020	Prec@(1,5) (91.0%, 99.3%)
07/03 11:47:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][100/391]	Step 59432	Loss 0.3007	Prec@(1,5) (91.1%, 99.1%)
07/03 11:47:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][150/391]	Step 59432	Loss 0.2964	Prec@(1,5) (90.9%, 99.2%)
07/03 11:47:48午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][200/391]	Step 59432	Loss 0.2943	Prec@(1,5) (90.9%, 99.2%)
07/03 11:47:51午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][250/391]	Step 59432	Loss 0.2922	Prec@(1,5) (91.0%, 99.2%)
07/03 11:47:54午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][300/391]	Step 59432	Loss 0.2903	Prec@(1,5) (91.0%, 99.2%)
07/03 11:47:57午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][350/391]	Step 59432	Loss 0.2929	Prec@(1,5) (91.0%, 99.2%)
07/03 11:48:00午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [75][390/391]	Step 59432	Loss 0.2915	Prec@(1,5) (91.0%, 99.2%)
07/03 11:48:00午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 75/99] Final Prec@1 91.0320%
07/03 11:48:00午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 91.0320%
07/03 11:48:12午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][50/781]	Step 59482	lr 0.00425	Loss 0.3903 (0.6249)	Prec@(1,5) (89.8%, 99.2%)	
07/03 11:48:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][100/781]	Step 59532	lr 0.00425	Loss 0.5625 (0.6146)	Prec@(1,5) (90.0%, 99.1%)	
07/03 11:48:35午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][150/781]	Step 59582	lr 0.00425	Loss 0.5579 (0.6015)	Prec@(1,5) (90.5%, 99.1%)	
07/03 11:48:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][200/781]	Step 59632	lr 0.00425	Loss 0.4450 (0.6062)	Prec@(1,5) (90.5%, 99.1%)	
07/03 11:48:58午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][250/781]	Step 59682	lr 0.00425	Loss 0.7689 (0.6114)	Prec@(1,5) (90.3%, 99.0%)	
07/03 11:49:09午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][300/781]	Step 59732	lr 0.00425	Loss 0.4453 (0.6207)	Prec@(1,5) (90.1%, 99.0%)	
07/03 11:49:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][350/781]	Step 59782	lr 0.00425	Loss 0.8404 (0.6218)	Prec@(1,5) (90.0%, 99.0%)	
07/03 11:49:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][400/781]	Step 59832	lr 0.00425	Loss 0.5132 (0.6245)	Prec@(1,5) (89.9%, 99.0%)	
07/03 11:49:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][450/781]	Step 59882	lr 0.00425	Loss 0.6970 (0.6289)	Prec@(1,5) (89.7%, 99.0%)	
07/03 11:49:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][500/781]	Step 59932	lr 0.00425	Loss 0.6166 (0.6297)	Prec@(1,5) (89.7%, 99.0%)	
07/03 11:50:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][550/781]	Step 59982	lr 0.00425	Loss 0.7262 (0.6336)	Prec@(1,5) (89.6%, 98.9%)	
07/03 11:50:17午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][600/781]	Step 60032	lr 0.00425	Loss 0.6438 (0.6369)	Prec@(1,5) (89.7%, 98.9%)	
07/03 11:50:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][650/781]	Step 60082	lr 0.00425	Loss 0.6434 (0.6410)	Prec@(1,5) (89.5%, 98.9%)	
07/03 11:50:40午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][700/781]	Step 60132	lr 0.00425	Loss 0.7826 (0.6442)	Prec@(1,5) (89.5%, 98.9%)	
07/03 11:50:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][750/781]	Step 60182	lr 0.00425	Loss 0.6838 (0.6450)	Prec@(1,5) (89.5%, 98.9%)	
07/03 11:50:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [76][781/781]	Step 60213	lr 0.00425	Loss 0.5960 (0.6448)	Prec@(1,5) (89.5%, 98.9%)	
07/03 11:50:59午後 evaluateCell_trainer.py:172 [INFO] Train: [ 76/99] Final Prec@1 89.4720%
07/03 11:51:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][50/391]	Step 60214	Loss 0.2696	Prec@(1,5) (91.8%, 99.3%)
07/03 11:51:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][100/391]	Step 60214	Loss 0.2829	Prec@(1,5) (91.4%, 99.2%)
07/03 11:51:09午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][150/391]	Step 60214	Loss 0.2807	Prec@(1,5) (91.4%, 99.3%)
07/03 11:51:12午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][200/391]	Step 60214	Loss 0.2776	Prec@(1,5) (91.5%, 99.3%)
07/03 11:51:15午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][250/391]	Step 60214	Loss 0.2783	Prec@(1,5) (91.5%, 99.3%)
07/03 11:51:18午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][300/391]	Step 60214	Loss 0.2812	Prec@(1,5) (91.4%, 99.3%)
07/03 11:51:22午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][350/391]	Step 60214	Loss 0.2807	Prec@(1,5) (91.5%, 99.2%)
07/03 11:51:24午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [76][390/391]	Step 60214	Loss 0.2816	Prec@(1,5) (91.5%, 99.2%)
07/03 11:51:24午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 76/99] Final Prec@1 91.5000%
07/03 11:51:24午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 91.5000%
07/03 11:51:36午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][50/781]	Step 60264	lr 0.004	Loss 0.5152 (0.5862)	Prec@(1,5) (91.3%, 98.8%)	
07/03 11:51:47午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][100/781]	Step 60314	lr 0.004	Loss 0.4502 (0.5993)	Prec@(1,5) (90.6%, 99.0%)	
07/03 11:51:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][150/781]	Step 60364	lr 0.004	Loss 0.5147 (0.6115)	Prec@(1,5) (90.3%, 99.0%)	
07/03 11:52:10午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][200/781]	Step 60414	lr 0.004	Loss 0.5179 (0.6100)	Prec@(1,5) (90.3%, 99.1%)	
07/03 11:52:21午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][250/781]	Step 60464	lr 0.004	Loss 0.5945 (0.6112)	Prec@(1,5) (90.1%, 99.0%)	
07/03 11:52:32午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][300/781]	Step 60514	lr 0.004	Loss 0.6741 (0.6128)	Prec@(1,5) (90.1%, 99.0%)	
07/03 11:52:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][350/781]	Step 60564	lr 0.004	Loss 0.6554 (0.6132)	Prec@(1,5) (90.2%, 99.0%)	
07/03 11:52:55午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][400/781]	Step 60614	lr 0.004	Loss 0.6171 (0.6150)	Prec@(1,5) (90.2%, 99.0%)	
07/03 11:53:06午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][450/781]	Step 60664	lr 0.004	Loss 0.5885 (0.6154)	Prec@(1,5) (90.1%, 99.0%)	
07/03 11:53:18午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][500/781]	Step 60714	lr 0.004	Loss 0.5336 (0.6156)	Prec@(1,5) (90.1%, 99.0%)	
07/03 11:53:29午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][550/781]	Step 60764	lr 0.004	Loss 0.8609 (0.6212)	Prec@(1,5) (89.9%, 99.0%)	
07/03 11:53:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][600/781]	Step 60814	lr 0.004	Loss 0.5729 (0.6217)	Prec@(1,5) (89.9%, 99.0%)	
07/03 11:53:52午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][650/781]	Step 60864	lr 0.004	Loss 0.6448 (0.6216)	Prec@(1,5) (89.9%, 99.0%)	
07/03 11:54:03午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][700/781]	Step 60914	lr 0.004	Loss 0.7295 (0.6251)	Prec@(1,5) (89.9%, 99.0%)	
07/03 11:54:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][750/781]	Step 60964	lr 0.004	Loss 0.5341 (0.6257)	Prec@(1,5) (89.8%, 99.0%)	
07/03 11:54:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [77][781/781]	Step 60995	lr 0.004	Loss 0.6953 (0.6266)	Prec@(1,5) (89.9%, 99.0%)	
07/03 11:54:22午後 evaluateCell_trainer.py:172 [INFO] Train: [ 77/99] Final Prec@1 89.8500%
07/03 11:54:25午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][50/391]	Step 60996	Loss 0.2672	Prec@(1,5) (91.5%, 99.4%)
07/03 11:54:29午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][100/391]	Step 60996	Loss 0.2733	Prec@(1,5) (91.7%, 99.4%)
07/03 11:54:32午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][150/391]	Step 60996	Loss 0.2742	Prec@(1,5) (91.6%, 99.4%)
07/03 11:54:35午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][200/391]	Step 60996	Loss 0.2756	Prec@(1,5) (91.7%, 99.3%)
07/03 11:54:38午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][250/391]	Step 60996	Loss 0.2778	Prec@(1,5) (91.7%, 99.3%)
07/03 11:54:42午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][300/391]	Step 60996	Loss 0.2785	Prec@(1,5) (91.6%, 99.3%)
07/03 11:54:45午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][350/391]	Step 60996	Loss 0.2794	Prec@(1,5) (91.6%, 99.2%)
07/03 11:54:47午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [77][390/391]	Step 60996	Loss 0.2755	Prec@(1,5) (91.8%, 99.3%)
07/03 11:54:47午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 77/99] Final Prec@1 91.7800%
07/03 11:54:48午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 91.7800%
07/03 11:54:59午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][50/781]	Step 61046	lr 0.00375	Loss 0.7531 (0.5581)	Prec@(1,5) (91.4%, 99.3%)	
07/03 11:55:11午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][100/781]	Step 61096	lr 0.00375	Loss 0.5936 (0.5737)	Prec@(1,5) (91.4%, 99.2%)	
07/03 11:55:22午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][150/781]	Step 61146	lr 0.00375	Loss 0.5447 (0.5783)	Prec@(1,5) (91.2%, 99.1%)	
07/03 11:55:33午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][200/781]	Step 61196	lr 0.00375	Loss 0.6417 (0.5826)	Prec@(1,5) (91.1%, 99.1%)	
07/03 11:55:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][250/781]	Step 61246	lr 0.00375	Loss 0.5360 (0.5813)	Prec@(1,5) (91.1%, 99.2%)	
07/03 11:55:56午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][300/781]	Step 61296	lr 0.00375	Loss 0.5516 (0.5834)	Prec@(1,5) (91.0%, 99.1%)	
07/03 11:56:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][350/781]	Step 61346	lr 0.00375	Loss 0.5320 (0.5852)	Prec@(1,5) (90.9%, 99.1%)	
07/03 11:56:19午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][400/781]	Step 61396	lr 0.00375	Loss 0.5990 (0.5942)	Prec@(1,5) (90.7%, 99.1%)	
07/03 11:56:30午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][450/781]	Step 61446	lr 0.00375	Loss 0.6610 (0.5951)	Prec@(1,5) (90.6%, 99.1%)	
07/03 11:56:41午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][500/781]	Step 61496	lr 0.00375	Loss 0.4795 (0.5951)	Prec@(1,5) (90.6%, 99.1%)	
07/03 11:56:53午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][550/781]	Step 61546	lr 0.00375	Loss 0.7964 (0.5973)	Prec@(1,5) (90.6%, 99.1%)	
07/03 11:57:04午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][600/781]	Step 61596	lr 0.00375	Loss 0.9209 (0.6014)	Prec@(1,5) (90.5%, 99.1%)	
07/03 11:57:15午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][650/781]	Step 61646	lr 0.00375	Loss 0.5597 (0.6030)	Prec@(1,5) (90.4%, 99.0%)	
07/03 11:57:27午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][700/781]	Step 61696	lr 0.00375	Loss 0.5773 (0.6046)	Prec@(1,5) (90.4%, 99.0%)	
07/03 11:57:38午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][750/781]	Step 61746	lr 0.00375	Loss 0.4627 (0.6062)	Prec@(1,5) (90.4%, 99.0%)	
07/03 11:57:45午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [78][781/781]	Step 61777	lr 0.00375	Loss 0.6344 (0.6074)	Prec@(1,5) (90.4%, 99.0%)	
07/03 11:57:45午後 evaluateCell_trainer.py:172 [INFO] Train: [ 78/99] Final Prec@1 90.3520%
07/03 11:57:49午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][50/391]	Step 61778	Loss 0.2620	Prec@(1,5) (91.9%, 99.3%)
07/03 11:57:52午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][100/391]	Step 61778	Loss 0.2668	Prec@(1,5) (91.9%, 99.3%)
07/03 11:57:55午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][150/391]	Step 61778	Loss 0.2621	Prec@(1,5) (92.0%, 99.3%)
07/03 11:57:58午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][200/391]	Step 61778	Loss 0.2674	Prec@(1,5) (91.9%, 99.2%)
07/03 11:58:02午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][250/391]	Step 61778	Loss 0.2716	Prec@(1,5) (91.7%, 99.2%)
07/03 11:58:05午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][300/391]	Step 61778	Loss 0.2719	Prec@(1,5) (91.7%, 99.1%)
07/03 11:58:08午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][350/391]	Step 61778	Loss 0.2733	Prec@(1,5) (91.6%, 99.2%)
07/03 11:58:11午後 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [78][390/391]	Step 61778	Loss 0.2736	Prec@(1,5) (91.6%, 99.2%)
07/03 11:58:11午後 evaluateCell_trainer.py:208 [INFO] Valid: [ 78/99] Final Prec@1 91.5800%
07/03 11:58:11午後 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 91.7800%
07/03 11:58:23午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][50/781]	Step 61828	lr 0.00352	Loss 0.5879 (0.5558)	Prec@(1,5) (91.8%, 99.2%)	
07/03 11:58:34午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][100/781]	Step 61878	lr 0.00352	Loss 0.5929 (0.5628)	Prec@(1,5) (91.3%, 99.2%)	
07/03 11:58:46午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][150/781]	Step 61928	lr 0.00352	Loss 0.5625 (0.5658)	Prec@(1,5) (91.2%, 99.2%)	
07/03 11:58:57午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][200/781]	Step 61978	lr 0.00352	Loss 0.5023 (0.5761)	Prec@(1,5) (91.0%, 99.1%)	
07/03 11:59:08午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][250/781]	Step 62028	lr 0.00352	Loss 0.3842 (0.5714)	Prec@(1,5) (91.1%, 99.2%)	
07/03 11:59:20午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][300/781]	Step 62078	lr 0.00352	Loss 0.7391 (0.5755)	Prec@(1,5) (91.0%, 99.2%)	
07/03 11:59:31午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][350/781]	Step 62128	lr 0.00352	Loss 0.6358 (0.5769)	Prec@(1,5) (90.9%, 99.2%)	
07/03 11:59:43午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][400/781]	Step 62178	lr 0.00352	Loss 0.5085 (0.5751)	Prec@(1,5) (90.9%, 99.2%)	
07/03 11:59:54午後 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][450/781]	Step 62228	lr 0.00352	Loss 0.6561 (0.5757)	Prec@(1,5) (90.9%, 99.2%)	
07/04 12:00:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][500/781]	Step 62278	lr 0.00352	Loss 0.7579 (0.5815)	Prec@(1,5) (90.8%, 99.2%)	
07/04 12:00:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][550/781]	Step 62328	lr 0.00352	Loss 0.6615 (0.5833)	Prec@(1,5) (90.7%, 99.2%)	
07/04 12:00:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][600/781]	Step 62378	lr 0.00352	Loss 0.5179 (0.5842)	Prec@(1,5) (90.7%, 99.2%)	
07/04 12:00:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][650/781]	Step 62428	lr 0.00352	Loss 0.5458 (0.5862)	Prec@(1,5) (90.7%, 99.2%)	
07/04 12:00:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][700/781]	Step 62478	lr 0.00352	Loss 0.5269 (0.5871)	Prec@(1,5) (90.7%, 99.2%)	
07/04 12:01:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][750/781]	Step 62528	lr 0.00352	Loss 0.7144 (0.5886)	Prec@(1,5) (90.6%, 99.2%)	
07/04 12:01:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [79][781/781]	Step 62559	lr 0.00352	Loss 0.5127 (0.5903)	Prec@(1,5) (90.6%, 99.2%)	
07/04 12:01:09午前 evaluateCell_trainer.py:172 [INFO] Train: [ 79/99] Final Prec@1 90.6320%
07/04 12:01:12午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][50/391]	Step 62560	Loss 0.2325	Prec@(1,5) (92.9%, 99.5%)
07/04 12:01:15午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][100/391]	Step 62560	Loss 0.2469	Prec@(1,5) (92.7%, 99.4%)
07/04 12:01:18午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][150/391]	Step 62560	Loss 0.2556	Prec@(1,5) (92.3%, 99.3%)
07/04 12:01:22午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][200/391]	Step 62560	Loss 0.2523	Prec@(1,5) (92.6%, 99.2%)
07/04 12:01:25午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][250/391]	Step 62560	Loss 0.2525	Prec@(1,5) (92.5%, 99.2%)
07/04 12:01:28午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][300/391]	Step 62560	Loss 0.2493	Prec@(1,5) (92.6%, 99.3%)
07/04 12:01:31午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][350/391]	Step 62560	Loss 0.2515	Prec@(1,5) (92.5%, 99.3%)
07/04 12:01:34午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [79][390/391]	Step 62560	Loss 0.2508	Prec@(1,5) (92.5%, 99.3%)
07/04 12:01:34午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 79/99] Final Prec@1 92.5200%
07/04 12:01:34午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 92.5200%
07/04 12:01:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][50/781]	Step 62610	lr 0.00329	Loss 0.5355 (0.5501)	Prec@(1,5) (92.3%, 99.0%)	
07/04 12:01:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][100/781]	Step 62660	lr 0.00329	Loss 0.6633 (0.5538)	Prec@(1,5) (92.2%, 99.2%)	
07/04 12:02:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][150/781]	Step 62710	lr 0.00329	Loss 0.3567 (0.5657)	Prec@(1,5) (91.8%, 99.1%)	
07/04 12:02:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][200/781]	Step 62760	lr 0.00329	Loss 0.5858 (0.5641)	Prec@(1,5) (91.8%, 99.1%)	
07/04 12:02:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][250/781]	Step 62810	lr 0.00329	Loss 0.4569 (0.5607)	Prec@(1,5) (91.9%, 99.2%)	
07/04 12:02:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][300/781]	Step 62860	lr 0.00329	Loss 0.4547 (0.5613)	Prec@(1,5) (91.8%, 99.2%)	
07/04 12:02:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][350/781]	Step 62910	lr 0.00329	Loss 0.5997 (0.5656)	Prec@(1,5) (91.7%, 99.2%)	
07/04 12:03:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][400/781]	Step 62960	lr 0.00329	Loss 0.4951 (0.5611)	Prec@(1,5) (91.8%, 99.2%)	
07/04 12:03:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][450/781]	Step 63010	lr 0.00329	Loss 0.4630 (0.5619)	Prec@(1,5) (91.7%, 99.2%)	
07/04 12:03:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][500/781]	Step 63060	lr 0.00329	Loss 0.7196 (0.5638)	Prec@(1,5) (91.6%, 99.2%)	
07/04 12:03:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][550/781]	Step 63110	lr 0.00329	Loss 0.5239 (0.5656)	Prec@(1,5) (91.5%, 99.2%)	
07/04 12:03:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][600/781]	Step 63160	lr 0.00329	Loss 0.6954 (0.5688)	Prec@(1,5) (91.4%, 99.2%)	
07/04 12:04:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][650/781]	Step 63210	lr 0.00329	Loss 0.4748 (0.5678)	Prec@(1,5) (91.5%, 99.2%)	
07/04 12:04:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][700/781]	Step 63260	lr 0.00329	Loss 0.5983 (0.5694)	Prec@(1,5) (91.4%, 99.2%)	
07/04 12:04:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][750/781]	Step 63310	lr 0.00329	Loss 0.7792 (0.5699)	Prec@(1,5) (91.4%, 99.2%)	
07/04 12:04:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [80][781/781]	Step 63341	lr 0.00329	Loss 0.7097 (0.5714)	Prec@(1,5) (91.4%, 99.2%)	
07/04 12:04:33午前 evaluateCell_trainer.py:172 [INFO] Train: [ 80/99] Final Prec@1 91.3720%
07/04 12:04:36午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][50/391]	Step 63342	Loss 0.2261	Prec@(1,5) (93.6%, 99.3%)
07/04 12:04:40午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][100/391]	Step 63342	Loss 0.2218	Prec@(1,5) (93.2%, 99.5%)
07/04 12:04:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][150/391]	Step 63342	Loss 0.2238	Prec@(1,5) (93.3%, 99.5%)
07/04 12:04:46午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][200/391]	Step 63342	Loss 0.2267	Prec@(1,5) (93.2%, 99.4%)
07/04 12:04:49午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][250/391]	Step 63342	Loss 0.2259	Prec@(1,5) (93.3%, 99.4%)
07/04 12:04:53午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][300/391]	Step 63342	Loss 0.2290	Prec@(1,5) (93.2%, 99.4%)
07/04 12:04:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][350/391]	Step 63342	Loss 0.2301	Prec@(1,5) (93.1%, 99.4%)
07/04 12:04:58午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [80][390/391]	Step 63342	Loss 0.2317	Prec@(1,5) (93.1%, 99.4%)
07/04 12:04:58午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 80/99] Final Prec@1 93.0800%
07/04 12:04:59午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 93.0800%
07/04 12:05:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][50/781]	Step 63392	lr 0.00308	Loss 0.4839 (0.5052)	Prec@(1,5) (92.7%, 99.3%)	
07/04 12:05:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][100/781]	Step 63442	lr 0.00308	Loss 0.7025 (0.5306)	Prec@(1,5) (91.7%, 99.2%)	
07/04 12:05:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][150/781]	Step 63492	lr 0.00308	Loss 0.3497 (0.5296)	Prec@(1,5) (91.8%, 99.2%)	
07/04 12:05:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][200/781]	Step 63542	lr 0.00308	Loss 0.5919 (0.5341)	Prec@(1,5) (91.9%, 99.2%)	
07/04 12:05:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][250/781]	Step 63592	lr 0.00308	Loss 0.6132 (0.5317)	Prec@(1,5) (92.0%, 99.2%)	
07/04 12:06:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][300/781]	Step 63642	lr 0.00308	Loss 0.6764 (0.5356)	Prec@(1,5) (91.9%, 99.2%)	
07/04 12:06:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][350/781]	Step 63692	lr 0.00308	Loss 0.3840 (0.5409)	Prec@(1,5) (91.8%, 99.2%)	
07/04 12:06:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][400/781]	Step 63742	lr 0.00308	Loss 0.6101 (0.5454)	Prec@(1,5) (91.7%, 99.2%)	
07/04 12:06:41午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][450/781]	Step 63792	lr 0.00308	Loss 0.6154 (0.5488)	Prec@(1,5) (91.6%, 99.2%)	
07/04 12:06:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][500/781]	Step 63842	lr 0.00308	Loss 0.6356 (0.5493)	Prec@(1,5) (91.5%, 99.2%)	
07/04 12:07:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][550/781]	Step 63892	lr 0.00308	Loss 0.5127 (0.5517)	Prec@(1,5) (91.5%, 99.2%)	
07/04 12:07:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][600/781]	Step 63942	lr 0.00308	Loss 0.5579 (0.5533)	Prec@(1,5) (91.5%, 99.2%)	
07/04 12:07:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][650/781]	Step 63992	lr 0.00308	Loss 0.4367 (0.5554)	Prec@(1,5) (91.5%, 99.2%)	
07/04 12:07:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][700/781]	Step 64042	lr 0.00308	Loss 0.5792 (0.5570)	Prec@(1,5) (91.5%, 99.2%)	
07/04 12:07:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][750/781]	Step 64092	lr 0.00308	Loss 0.4781 (0.5574)	Prec@(1,5) (91.5%, 99.2%)	
07/04 12:07:56午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [81][781/781]	Step 64123	lr 0.00308	Loss 0.5268 (0.5570)	Prec@(1,5) (91.5%, 99.2%)	
07/04 12:07:56午前 evaluateCell_trainer.py:172 [INFO] Train: [ 81/99] Final Prec@1 91.4560%
07/04 12:08:00午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][50/391]	Step 64124	Loss 0.2149	Prec@(1,5) (93.3%, 99.6%)
07/04 12:08:03午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][100/391]	Step 64124	Loss 0.2128	Prec@(1,5) (93.5%, 99.6%)
07/04 12:08:06午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][150/391]	Step 64124	Loss 0.2165	Prec@(1,5) (93.4%, 99.6%)
07/04 12:08:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][200/391]	Step 64124	Loss 0.2182	Prec@(1,5) (93.4%, 99.5%)
07/04 12:08:13午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][250/391]	Step 64124	Loss 0.2220	Prec@(1,5) (93.4%, 99.4%)
07/04 12:08:16午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][300/391]	Step 64124	Loss 0.2208	Prec@(1,5) (93.3%, 99.4%)
07/04 12:08:19午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][350/391]	Step 64124	Loss 0.2226	Prec@(1,5) (93.3%, 99.4%)
07/04 12:08:22午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [81][390/391]	Step 64124	Loss 0.2232	Prec@(1,5) (93.2%, 99.4%)
07/04 12:08:22午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 81/99] Final Prec@1 93.2520%
07/04 12:08:22午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 93.2520%
07/04 12:08:34午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][50/781]	Step 64174	lr 0.00287	Loss 0.9020 (0.5359)	Prec@(1,5) (92.4%, 99.0%)	
07/04 12:08:45午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][100/781]	Step 64224	lr 0.00287	Loss 0.7186 (0.5326)	Prec@(1,5) (92.5%, 99.2%)	
07/04 12:08:56午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][150/781]	Step 64274	lr 0.00287	Loss 0.5442 (0.5300)	Prec@(1,5) (92.7%, 99.2%)	
07/04 12:09:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][200/781]	Step 64324	lr 0.00287	Loss 0.5316 (0.5329)	Prec@(1,5) (92.5%, 99.3%)	
07/04 12:09:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][250/781]	Step 64374	lr 0.00287	Loss 0.6606 (0.5351)	Prec@(1,5) (92.3%, 99.2%)	
07/04 12:09:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][300/781]	Step 64424	lr 0.00287	Loss 0.5777 (0.5360)	Prec@(1,5) (92.2%, 99.2%)	
07/04 12:09:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][350/781]	Step 64474	lr 0.00287	Loss 0.7278 (0.5334)	Prec@(1,5) (92.2%, 99.3%)	
07/04 12:09:53午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][400/781]	Step 64524	lr 0.00287	Loss 0.5634 (0.5362)	Prec@(1,5) (92.0%, 99.2%)	
07/04 12:10:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][450/781]	Step 64574	lr 0.00287	Loss 0.5404 (0.5384)	Prec@(1,5) (92.0%, 99.2%)	
07/04 12:10:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][500/781]	Step 64624	lr 0.00287	Loss 0.8350 (0.5364)	Prec@(1,5) (92.1%, 99.2%)	
07/04 12:10:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][550/781]	Step 64674	lr 0.00287	Loss 0.4328 (0.5379)	Prec@(1,5) (92.0%, 99.2%)	
07/04 12:10:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][600/781]	Step 64724	lr 0.00287	Loss 0.9738 (0.5402)	Prec@(1,5) (92.0%, 99.2%)	
07/04 12:10:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][650/781]	Step 64774	lr 0.00287	Loss 0.4565 (0.5407)	Prec@(1,5) (92.0%, 99.2%)	
07/04 12:11:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][700/781]	Step 64824	lr 0.00287	Loss 0.6454 (0.5411)	Prec@(1,5) (92.0%, 99.2%)	
07/04 12:11:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][750/781]	Step 64874	lr 0.00287	Loss 0.7599 (0.5416)	Prec@(1,5) (92.0%, 99.2%)	
07/04 12:11:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [82][781/781]	Step 64905	lr 0.00287	Loss 1.0110 (0.5422)	Prec@(1,5) (92.0%, 99.2%)	
07/04 12:11:20午前 evaluateCell_trainer.py:172 [INFO] Train: [ 82/99] Final Prec@1 92.0260%
07/04 12:11:23午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][50/391]	Step 64906	Loss 0.2341	Prec@(1,5) (93.4%, 99.5%)
07/04 12:11:26午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][100/391]	Step 64906	Loss 0.2217	Prec@(1,5) (93.5%, 99.5%)
07/04 12:11:30午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][150/391]	Step 64906	Loss 0.2227	Prec@(1,5) (93.5%, 99.4%)
07/04 12:11:33午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][200/391]	Step 64906	Loss 0.2206	Prec@(1,5) (93.6%, 99.5%)
07/04 12:11:36午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][250/391]	Step 64906	Loss 0.2200	Prec@(1,5) (93.5%, 99.5%)
07/04 12:11:39午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][300/391]	Step 64906	Loss 0.2193	Prec@(1,5) (93.5%, 99.5%)
07/04 12:11:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][350/391]	Step 64906	Loss 0.2177	Prec@(1,5) (93.5%, 99.5%)
07/04 12:11:45午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [82][390/391]	Step 64906	Loss 0.2166	Prec@(1,5) (93.6%, 99.5%)
07/04 12:11:45午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 82/99] Final Prec@1 93.5600%
07/04 12:11:45午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 93.5600%
07/04 12:11:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][50/781]	Step 64956	lr 0.00267	Loss 0.4500 (0.5111)	Prec@(1,5) (92.2%, 99.5%)	
07/04 12:12:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][100/781]	Step 65006	lr 0.00267	Loss 0.4615 (0.5146)	Prec@(1,5) (91.9%, 99.4%)	
07/04 12:12:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][150/781]	Step 65056	lr 0.00267	Loss 0.5080 (0.5173)	Prec@(1,5) (92.0%, 99.3%)	
07/04 12:12:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][200/781]	Step 65106	lr 0.00267	Loss 0.4804 (0.5197)	Prec@(1,5) (92.2%, 99.4%)	
07/04 12:12:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][250/781]	Step 65156	lr 0.00267	Loss 0.4153 (0.5246)	Prec@(1,5) (92.2%, 99.3%)	
07/04 12:12:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][300/781]	Step 65206	lr 0.00267	Loss 0.4690 (0.5226)	Prec@(1,5) (92.3%, 99.3%)	
07/04 12:13:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][350/781]	Step 65256	lr 0.00267	Loss 0.4152 (0.5227)	Prec@(1,5) (92.3%, 99.3%)	
07/04 12:13:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][400/781]	Step 65306	lr 0.00267	Loss 0.5192 (0.5266)	Prec@(1,5) (92.2%, 99.3%)	
07/04 12:13:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][450/781]	Step 65356	lr 0.00267	Loss 0.3674 (0.5234)	Prec@(1,5) (92.2%, 99.3%)	
07/04 12:13:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][500/781]	Step 65406	lr 0.00267	Loss 0.5057 (0.5247)	Prec@(1,5) (92.2%, 99.3%)	
07/04 12:13:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][550/781]	Step 65456	lr 0.00267	Loss 0.4561 (0.5251)	Prec@(1,5) (92.2%, 99.3%)	
07/04 12:14:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][600/781]	Step 65506	lr 0.00267	Loss 0.4041 (0.5255)	Prec@(1,5) (92.2%, 99.3%)	
07/04 12:14:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][650/781]	Step 65556	lr 0.00267	Loss 0.4904 (0.5268)	Prec@(1,5) (92.2%, 99.3%)	
07/04 12:14:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][700/781]	Step 65606	lr 0.00267	Loss 0.5935 (0.5287)	Prec@(1,5) (92.1%, 99.3%)	
07/04 12:14:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][750/781]	Step 65656	lr 0.00267	Loss 0.4369 (0.5286)	Prec@(1,5) (92.1%, 99.3%)	
07/04 12:14:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [83][781/781]	Step 65687	lr 0.00267	Loss 0.4865 (0.5293)	Prec@(1,5) (92.1%, 99.3%)	
07/04 12:14:43午前 evaluateCell_trainer.py:172 [INFO] Train: [ 83/99] Final Prec@1 92.0860%
07/04 12:14:46午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][50/391]	Step 65688	Loss 0.1955	Prec@(1,5) (93.9%, 99.7%)
07/04 12:14:50午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][100/391]	Step 65688	Loss 0.1976	Prec@(1,5) (94.1%, 99.6%)
07/04 12:14:53午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][150/391]	Step 65688	Loss 0.1975	Prec@(1,5) (94.1%, 99.6%)
07/04 12:14:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][200/391]	Step 65688	Loss 0.1966	Prec@(1,5) (94.2%, 99.5%)
07/04 12:14:59午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][250/391]	Step 65688	Loss 0.2007	Prec@(1,5) (94.1%, 99.5%)
07/04 12:15:03午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][300/391]	Step 65688	Loss 0.2020	Prec@(1,5) (94.1%, 99.5%)
07/04 12:15:06午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][350/391]	Step 65688	Loss 0.2017	Prec@(1,5) (94.2%, 99.5%)
07/04 12:15:08午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [83][390/391]	Step 65688	Loss 0.2027	Prec@(1,5) (94.1%, 99.5%)
07/04 12:15:09午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 83/99] Final Prec@1 94.0800%
07/04 12:15:09午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 94.0800%
07/04 12:15:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][50/781]	Step 65738	lr 0.00248	Loss 0.4440 (0.4809)	Prec@(1,5) (93.3%, 99.4%)	
07/04 12:15:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][100/781]	Step 65788	lr 0.00248	Loss 0.4341 (0.4866)	Prec@(1,5) (93.3%, 99.3%)	
07/04 12:15:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][150/781]	Step 65838	lr 0.00248	Loss 0.4577 (0.5004)	Prec@(1,5) (93.0%, 99.2%)	
07/04 12:15:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][200/781]	Step 65888	lr 0.00248	Loss 0.3947 (0.5001)	Prec@(1,5) (92.9%, 99.3%)	
07/04 12:16:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][250/781]	Step 65938	lr 0.00248	Loss 0.2679 (0.4974)	Prec@(1,5) (92.9%, 99.2%)	
07/04 12:16:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][300/781]	Step 65988	lr 0.00248	Loss 0.3835 (0.4942)	Prec@(1,5) (93.0%, 99.3%)	
07/04 12:16:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][350/781]	Step 66038	lr 0.00248	Loss 0.6686 (0.5025)	Prec@(1,5) (92.8%, 99.3%)	
07/04 12:16:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][400/781]	Step 66088	lr 0.00248	Loss 0.4997 (0.5023)	Prec@(1,5) (92.8%, 99.3%)	
07/04 12:16:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][450/781]	Step 66138	lr 0.00248	Loss 0.5493 (0.5028)	Prec@(1,5) (92.7%, 99.3%)	
07/04 12:17:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][500/781]	Step 66188	lr 0.00248	Loss 0.8169 (0.5043)	Prec@(1,5) (92.7%, 99.3%)	
07/04 12:17:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][550/781]	Step 66238	lr 0.00248	Loss 0.8315 (0.5043)	Prec@(1,5) (92.8%, 99.3%)	
07/04 12:17:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][600/781]	Step 66288	lr 0.00248	Loss 0.4834 (0.5098)	Prec@(1,5) (92.7%, 99.3%)	
07/04 12:17:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][650/781]	Step 66338	lr 0.00248	Loss 0.4357 (0.5082)	Prec@(1,5) (92.7%, 99.3%)	
07/04 12:17:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][700/781]	Step 66388	lr 0.00248	Loss 0.4129 (0.5085)	Prec@(1,5) (92.7%, 99.3%)	
07/04 12:18:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][750/781]	Step 66438	lr 0.00248	Loss 0.6358 (0.5097)	Prec@(1,5) (92.7%, 99.3%)	
07/04 12:18:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [84][781/781]	Step 66469	lr 0.00248	Loss 0.4429 (0.5084)	Prec@(1,5) (92.7%, 99.3%)	
07/04 12:18:08午前 evaluateCell_trainer.py:172 [INFO] Train: [ 84/99] Final Prec@1 92.6700%
07/04 12:18:11午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][50/391]	Step 66470	Loss 0.2058	Prec@(1,5) (94.0%, 99.6%)
07/04 12:18:14午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][100/391]	Step 66470	Loss 0.1995	Prec@(1,5) (94.2%, 99.5%)
07/04 12:18:17午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][150/391]	Step 66470	Loss 0.1943	Prec@(1,5) (94.4%, 99.6%)
07/04 12:18:21午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][200/391]	Step 66470	Loss 0.1957	Prec@(1,5) (94.3%, 99.6%)
07/04 12:18:24午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][250/391]	Step 66470	Loss 0.1946	Prec@(1,5) (94.3%, 99.6%)
07/04 12:18:27午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][300/391]	Step 66470	Loss 0.1967	Prec@(1,5) (94.3%, 99.6%)
07/04 12:18:30午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][350/391]	Step 66470	Loss 0.1978	Prec@(1,5) (94.3%, 99.6%)
07/04 12:18:32午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [84][390/391]	Step 66470	Loss 0.1976	Prec@(1,5) (94.3%, 99.6%)
07/04 12:18:33午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 84/99] Final Prec@1 94.3000%
07/04 12:18:33午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 94.3000%
07/04 12:18:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][50/781]	Step 66520	lr 0.00231	Loss 0.3161 (0.4895)	Prec@(1,5) (92.8%, 99.6%)	
07/04 12:18:56午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][100/781]	Step 66570	lr 0.00231	Loss 0.6102 (0.4972)	Prec@(1,5) (92.7%, 99.4%)	
07/04 12:19:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][150/781]	Step 66620	lr 0.00231	Loss 0.5145 (0.4923)	Prec@(1,5) (92.9%, 99.4%)	
07/04 12:19:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][200/781]	Step 66670	lr 0.00231	Loss 0.5478 (0.4937)	Prec@(1,5) (93.0%, 99.4%)	
07/04 12:19:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][250/781]	Step 66720	lr 0.00231	Loss 0.4447 (0.4890)	Prec@(1,5) (93.0%, 99.4%)	
07/04 12:19:41午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][300/781]	Step 66770	lr 0.00231	Loss 0.3779 (0.4909)	Prec@(1,5) (93.0%, 99.4%)	
07/04 12:19:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][350/781]	Step 66820	lr 0.00231	Loss 0.6709 (0.4935)	Prec@(1,5) (92.9%, 99.4%)	
07/04 12:20:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][400/781]	Step 66870	lr 0.00231	Loss 0.4207 (0.4974)	Prec@(1,5) (92.9%, 99.4%)	
07/04 12:20:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][450/781]	Step 66920	lr 0.00231	Loss 0.5677 (0.4984)	Prec@(1,5) (92.8%, 99.4%)	
07/04 12:20:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][500/781]	Step 66970	lr 0.00231	Loss 0.4761 (0.4975)	Prec@(1,5) (92.9%, 99.4%)	
07/04 12:20:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][550/781]	Step 67020	lr 0.00231	Loss 0.5351 (0.4985)	Prec@(1,5) (92.9%, 99.4%)	
07/04 12:20:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][600/781]	Step 67070	lr 0.00231	Loss 0.5872 (0.5008)	Prec@(1,5) (92.8%, 99.4%)	
07/04 12:21:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][650/781]	Step 67120	lr 0.00231	Loss 0.5078 (0.5016)	Prec@(1,5) (92.8%, 99.4%)	
07/04 12:21:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][700/781]	Step 67170	lr 0.00231	Loss 0.4425 (0.5032)	Prec@(1,5) (92.8%, 99.4%)	
07/04 12:21:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][750/781]	Step 67220	lr 0.00231	Loss 0.5480 (0.5039)	Prec@(1,5) (92.8%, 99.4%)	
07/04 12:21:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [85][781/781]	Step 67251	lr 0.00231	Loss 0.3320 (0.5029)	Prec@(1,5) (92.8%, 99.4%)	
07/04 12:21:31午前 evaluateCell_trainer.py:172 [INFO] Train: [ 85/99] Final Prec@1 92.8040%
07/04 12:21:34午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][50/391]	Step 67252	Loss 0.1872	Prec@(1,5) (94.2%, 99.6%)
07/04 12:21:37午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][100/391]	Step 67252	Loss 0.1878	Prec@(1,5) (94.2%, 99.6%)
07/04 12:21:40午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][150/391]	Step 67252	Loss 0.1857	Prec@(1,5) (94.3%, 99.6%)
07/04 12:21:44午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][200/391]	Step 67252	Loss 0.1865	Prec@(1,5) (94.3%, 99.6%)
07/04 12:21:47午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][250/391]	Step 67252	Loss 0.1891	Prec@(1,5) (94.2%, 99.6%)
07/04 12:21:50午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][300/391]	Step 67252	Loss 0.1897	Prec@(1,5) (94.3%, 99.5%)
07/04 12:21:53午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][350/391]	Step 67252	Loss 0.1913	Prec@(1,5) (94.2%, 99.5%)
07/04 12:21:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [85][390/391]	Step 67252	Loss 0.1908	Prec@(1,5) (94.3%, 99.5%)
07/04 12:21:56午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 85/99] Final Prec@1 94.3080%
07/04 12:21:56午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 94.3080%
07/04 12:22:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][50/781]	Step 67302	lr 0.00214	Loss 0.5079 (0.4903)	Prec@(1,5) (92.9%, 99.3%)	
07/04 12:22:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][100/781]	Step 67352	lr 0.00214	Loss 0.5777 (0.4881)	Prec@(1,5) (93.0%, 99.3%)	
07/04 12:22:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][150/781]	Step 67402	lr 0.00214	Loss 0.6360 (0.4907)	Prec@(1,5) (93.0%, 99.4%)	
07/04 12:22:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][200/781]	Step 67452	lr 0.00214	Loss 0.4303 (0.4835)	Prec@(1,5) (93.2%, 99.5%)	
07/04 12:22:53午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][250/781]	Step 67502	lr 0.00214	Loss 0.4336 (0.4806)	Prec@(1,5) (93.4%, 99.5%)	
07/04 12:23:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][300/781]	Step 67552	lr 0.00214	Loss 0.3862 (0.4818)	Prec@(1,5) (93.3%, 99.5%)	
07/04 12:23:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][350/781]	Step 67602	lr 0.00214	Loss 0.3368 (0.4801)	Prec@(1,5) (93.4%, 99.5%)	
07/04 12:23:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][400/781]	Step 67652	lr 0.00214	Loss 0.6188 (0.4804)	Prec@(1,5) (93.4%, 99.5%)	
07/04 12:23:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][450/781]	Step 67702	lr 0.00214	Loss 0.3766 (0.4828)	Prec@(1,5) (93.3%, 99.4%)	
07/04 12:23:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][500/781]	Step 67752	lr 0.00214	Loss 0.6960 (0.4835)	Prec@(1,5) (93.3%, 99.4%)	
07/04 12:24:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][550/781]	Step 67802	lr 0.00214	Loss 0.5615 (0.4854)	Prec@(1,5) (93.2%, 99.5%)	
07/04 12:24:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][600/781]	Step 67852	lr 0.00214	Loss 0.3621 (0.4861)	Prec@(1,5) (93.2%, 99.5%)	
07/04 12:24:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][650/781]	Step 67902	lr 0.00214	Loss 0.8010 (0.4887)	Prec@(1,5) (93.1%, 99.5%)	
07/04 12:24:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][700/781]	Step 67952	lr 0.00214	Loss 0.4058 (0.4903)	Prec@(1,5) (93.1%, 99.5%)	
07/04 12:24:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][750/781]	Step 68002	lr 0.00214	Loss 0.6716 (0.4901)	Prec@(1,5) (93.1%, 99.4%)	
07/04 12:24:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [86][781/781]	Step 68033	lr 0.00214	Loss 0.5582 (0.4897)	Prec@(1,5) (93.1%, 99.5%)	
07/04 12:24:54午前 evaluateCell_trainer.py:172 [INFO] Train: [ 86/99] Final Prec@1 93.1200%
07/04 12:24:57午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][50/391]	Step 68034	Loss 0.1707	Prec@(1,5) (94.8%, 99.7%)
07/04 12:25:01午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][100/391]	Step 68034	Loss 0.1776	Prec@(1,5) (94.7%, 99.6%)
07/04 12:25:04午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][150/391]	Step 68034	Loss 0.1766	Prec@(1,5) (94.9%, 99.6%)
07/04 12:25:07午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][200/391]	Step 68034	Loss 0.1807	Prec@(1,5) (94.7%, 99.6%)
07/04 12:25:10午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][250/391]	Step 68034	Loss 0.1799	Prec@(1,5) (94.8%, 99.6%)
07/04 12:25:13午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][300/391]	Step 68034	Loss 0.1791	Prec@(1,5) (94.9%, 99.6%)
07/04 12:25:17午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][350/391]	Step 68034	Loss 0.1787	Prec@(1,5) (94.9%, 99.6%)
07/04 12:25:19午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [86][390/391]	Step 68034	Loss 0.1771	Prec@(1,5) (95.0%, 99.6%)
07/04 12:25:19午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 86/99] Final Prec@1 94.9680%
07/04 12:25:19午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 94.9680%
07/04 12:25:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][50/781]	Step 68084	lr 0.00199	Loss 0.4808 (0.4425)	Prec@(1,5) (94.1%, 99.5%)	
07/04 12:25:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][100/781]	Step 68134	lr 0.00199	Loss 0.4288 (0.4659)	Prec@(1,5) (93.7%, 99.4%)	
07/04 12:25:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][150/781]	Step 68184	lr 0.00199	Loss 0.4365 (0.4632)	Prec@(1,5) (93.9%, 99.4%)	
07/04 12:26:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][200/781]	Step 68234	lr 0.00199	Loss 0.3752 (0.4645)	Prec@(1,5) (93.8%, 99.4%)	
07/04 12:26:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][250/781]	Step 68284	lr 0.00199	Loss 0.7561 (0.4647)	Prec@(1,5) (93.8%, 99.4%)	
07/04 12:26:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][300/781]	Step 68334	lr 0.00199	Loss 0.3125 (0.4648)	Prec@(1,5) (93.7%, 99.4%)	
07/04 12:26:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][350/781]	Step 68384	lr 0.00199	Loss 0.4658 (0.4638)	Prec@(1,5) (93.8%, 99.5%)	
07/04 12:26:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][400/781]	Step 68434	lr 0.00199	Loss 0.3081 (0.4665)	Prec@(1,5) (93.7%, 99.4%)	
07/04 12:27:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][450/781]	Step 68484	lr 0.00199	Loss 0.5799 (0.4638)	Prec@(1,5) (93.8%, 99.5%)	
07/04 12:27:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][500/781]	Step 68534	lr 0.00199	Loss 0.5740 (0.4671)	Prec@(1,5) (93.7%, 99.5%)	
07/04 12:27:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][550/781]	Step 68584	lr 0.00199	Loss 0.7229 (0.4703)	Prec@(1,5) (93.6%, 99.5%)	
07/04 12:27:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][600/781]	Step 68634	lr 0.00199	Loss 0.5680 (0.4726)	Prec@(1,5) (93.6%, 99.5%)	
07/04 12:27:48午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][650/781]	Step 68684	lr 0.00199	Loss 0.4073 (0.4744)	Prec@(1,5) (93.6%, 99.5%)	
07/04 12:27:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][700/781]	Step 68734	lr 0.00199	Loss 0.6981 (0.4737)	Prec@(1,5) (93.6%, 99.5%)	
07/04 12:28:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][750/781]	Step 68784	lr 0.00199	Loss 0.5897 (0.4735)	Prec@(1,5) (93.6%, 99.5%)	
07/04 12:28:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [87][781/781]	Step 68815	lr 0.00199	Loss 0.5961 (0.4731)	Prec@(1,5) (93.6%, 99.5%)	
07/04 12:28:17午前 evaluateCell_trainer.py:172 [INFO] Train: [ 87/99] Final Prec@1 93.6240%
07/04 12:28:21午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][50/391]	Step 68816	Loss 0.1737	Prec@(1,5) (95.1%, 99.8%)
07/04 12:28:24午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][100/391]	Step 68816	Loss 0.1727	Prec@(1,5) (95.3%, 99.8%)
07/04 12:28:27午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][150/391]	Step 68816	Loss 0.1724	Prec@(1,5) (95.2%, 99.7%)
07/04 12:28:30午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][200/391]	Step 68816	Loss 0.1717	Prec@(1,5) (95.3%, 99.7%)
07/04 12:28:34午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][250/391]	Step 68816	Loss 0.1712	Prec@(1,5) (95.3%, 99.7%)
07/04 12:28:37午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][300/391]	Step 68816	Loss 0.1742	Prec@(1,5) (95.2%, 99.6%)
07/04 12:28:40午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][350/391]	Step 68816	Loss 0.1756	Prec@(1,5) (95.1%, 99.6%)
07/04 12:28:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [87][390/391]	Step 68816	Loss 0.1748	Prec@(1,5) (95.1%, 99.6%)
07/04 12:28:43午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 87/99] Final Prec@1 95.1160%
07/04 12:28:43午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 95.1160%
07/04 12:28:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][50/781]	Step 68866	lr 0.00184	Loss 0.5669 (0.4391)	Prec@(1,5) (94.2%, 99.7%)	
07/04 12:29:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][100/781]	Step 68916	lr 0.00184	Loss 0.4330 (0.4577)	Prec@(1,5) (93.8%, 99.6%)	
07/04 12:29:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][150/781]	Step 68966	lr 0.00184	Loss 0.3892 (0.4632)	Prec@(1,5) (93.7%, 99.6%)	
07/04 12:29:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][200/781]	Step 69016	lr 0.00184	Loss 0.4857 (0.4582)	Prec@(1,5) (93.9%, 99.6%)	
07/04 12:29:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][250/781]	Step 69066	lr 0.00184	Loss 0.6294 (0.4626)	Prec@(1,5) (93.8%, 99.5%)	
07/04 12:29:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][300/781]	Step 69116	lr 0.00184	Loss 0.3940 (0.4575)	Prec@(1,5) (94.0%, 99.5%)	
07/04 12:30:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][350/781]	Step 69166	lr 0.00184	Loss 0.6016 (0.4629)	Prec@(1,5) (93.9%, 99.5%)	
07/04 12:30:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][400/781]	Step 69216	lr 0.00184	Loss 0.4871 (0.4614)	Prec@(1,5) (94.0%, 99.5%)	
07/04 12:30:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][450/781]	Step 69266	lr 0.00184	Loss 0.4037 (0.4665)	Prec@(1,5) (93.9%, 99.5%)	
07/04 12:30:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][500/781]	Step 69316	lr 0.00184	Loss 0.4302 (0.4665)	Prec@(1,5) (93.8%, 99.4%)	
07/04 12:30:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][550/781]	Step 69366	lr 0.00184	Loss 0.4900 (0.4652)	Prec@(1,5) (93.8%, 99.5%)	
07/04 12:31:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][600/781]	Step 69416	lr 0.00184	Loss 0.3437 (0.4659)	Prec@(1,5) (93.8%, 99.5%)	
07/04 12:31:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][650/781]	Step 69466	lr 0.00184	Loss 0.3087 (0.4675)	Prec@(1,5) (93.7%, 99.4%)	
07/04 12:31:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][700/781]	Step 69516	lr 0.00184	Loss 0.6858 (0.4697)	Prec@(1,5) (93.7%, 99.4%)	
07/04 12:31:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][750/781]	Step 69566	lr 0.00184	Loss 0.4331 (0.4691)	Prec@(1,5) (93.7%, 99.4%)	
07/04 12:31:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [88][781/781]	Step 69597	lr 0.00184	Loss 0.5057 (0.4688)	Prec@(1,5) (93.7%, 99.5%)	
07/04 12:31:42午前 evaluateCell_trainer.py:172 [INFO] Train: [ 88/99] Final Prec@1 93.7240%
07/04 12:31:45午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][50/391]	Step 69598	Loss 0.1544	Prec@(1,5) (95.4%, 99.7%)
07/04 12:31:48午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][100/391]	Step 69598	Loss 0.1717	Prec@(1,5) (94.9%, 99.6%)
07/04 12:31:52午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][150/391]	Step 69598	Loss 0.1718	Prec@(1,5) (95.0%, 99.6%)
07/04 12:31:55午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][200/391]	Step 69598	Loss 0.1685	Prec@(1,5) (95.1%, 99.6%)
07/04 12:31:58午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][250/391]	Step 69598	Loss 0.1703	Prec@(1,5) (95.1%, 99.6%)
07/04 12:32:01午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][300/391]	Step 69598	Loss 0.1707	Prec@(1,5) (95.1%, 99.6%)
07/04 12:32:04午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][350/391]	Step 69598	Loss 0.1694	Prec@(1,5) (95.2%, 99.6%)
07/04 12:32:07午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [88][390/391]	Step 69598	Loss 0.1698	Prec@(1,5) (95.1%, 99.6%)
07/04 12:32:07午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 88/99] Final Prec@1 95.1040%
07/04 12:32:07午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 95.1160%
07/04 12:32:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][50/781]	Step 69648	lr 0.00171	Loss 0.4216 (0.4380)	Prec@(1,5) (94.4%, 99.6%)	
07/04 12:32:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][100/781]	Step 69698	lr 0.00171	Loss 0.7428 (0.4433)	Prec@(1,5) (94.3%, 99.5%)	
07/04 12:32:41午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][150/781]	Step 69748	lr 0.00171	Loss 0.3307 (0.4442)	Prec@(1,5) (94.4%, 99.5%)	
07/04 12:32:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][200/781]	Step 69798	lr 0.00171	Loss 0.4132 (0.4489)	Prec@(1,5) (94.1%, 99.5%)	
07/04 12:33:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][250/781]	Step 69848	lr 0.00171	Loss 0.5868 (0.4520)	Prec@(1,5) (94.0%, 99.5%)	
07/04 12:33:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][300/781]	Step 69898	lr 0.00171	Loss 0.4215 (0.4504)	Prec@(1,5) (93.9%, 99.5%)	
07/04 12:33:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][350/781]	Step 69948	lr 0.00171	Loss 0.3908 (0.4522)	Prec@(1,5) (93.9%, 99.5%)	
07/04 12:33:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][400/781]	Step 69998	lr 0.00171	Loss 0.5232 (0.4488)	Prec@(1,5) (93.9%, 99.5%)	
07/04 12:33:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][450/781]	Step 70048	lr 0.00171	Loss 0.3917 (0.4489)	Prec@(1,5) (93.9%, 99.5%)	
07/04 12:34:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][500/781]	Step 70098	lr 0.00171	Loss 0.4479 (0.4482)	Prec@(1,5) (94.0%, 99.5%)	
07/04 12:34:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][550/781]	Step 70148	lr 0.00171	Loss 0.7044 (0.4509)	Prec@(1,5) (93.9%, 99.5%)	
07/04 12:34:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][600/781]	Step 70198	lr 0.00171	Loss 0.3091 (0.4495)	Prec@(1,5) (94.0%, 99.5%)	
07/04 12:34:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][650/781]	Step 70248	lr 0.00171	Loss 0.3498 (0.4523)	Prec@(1,5) (94.0%, 99.5%)	
07/04 12:34:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][700/781]	Step 70298	lr 0.00171	Loss 0.4300 (0.4522)	Prec@(1,5) (94.0%, 99.5%)	
07/04 12:34:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][750/781]	Step 70348	lr 0.00171	Loss 0.5621 (0.4539)	Prec@(1,5) (94.0%, 99.5%)	
07/04 12:35:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [89][781/781]	Step 70379	lr 0.00171	Loss 0.5375 (0.4538)	Prec@(1,5) (94.0%, 99.5%)	
07/04 12:35:05午前 evaluateCell_trainer.py:172 [INFO] Train: [ 89/99] Final Prec@1 93.9560%
07/04 12:35:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][50/391]	Step 70380	Loss 0.1531	Prec@(1,5) (95.7%, 99.7%)
07/04 12:35:12午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][100/391]	Step 70380	Loss 0.1533	Prec@(1,5) (95.7%, 99.8%)
07/04 12:35:15午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][150/391]	Step 70380	Loss 0.1565	Prec@(1,5) (95.5%, 99.7%)
07/04 12:35:18午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][200/391]	Step 70380	Loss 0.1554	Prec@(1,5) (95.5%, 99.7%)
07/04 12:35:21午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][250/391]	Step 70380	Loss 0.1562	Prec@(1,5) (95.4%, 99.7%)
07/04 12:35:25午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][300/391]	Step 70380	Loss 0.1584	Prec@(1,5) (95.3%, 99.7%)
07/04 12:35:28午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][350/391]	Step 70380	Loss 0.1577	Prec@(1,5) (95.4%, 99.7%)
07/04 12:35:30午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [89][390/391]	Step 70380	Loss 0.1579	Prec@(1,5) (95.4%, 99.7%)
07/04 12:35:30午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 89/99] Final Prec@1 95.3680%
07/04 12:35:31午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 95.3680%
07/04 12:35:42午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][50/781]	Step 70430	lr 0.00159	Loss 0.5896 (0.4430)	Prec@(1,5) (94.2%, 99.5%)	
07/04 12:35:54午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][100/781]	Step 70480	lr 0.00159	Loss 0.3690 (0.4555)	Prec@(1,5) (94.1%, 99.4%)	
07/04 12:36:05午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][150/781]	Step 70530	lr 0.00159	Loss 0.4041 (0.4468)	Prec@(1,5) (94.4%, 99.4%)	
07/04 12:36:16午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][200/781]	Step 70580	lr 0.00159	Loss 0.4916 (0.4444)	Prec@(1,5) (94.4%, 99.5%)	
07/04 12:36:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][250/781]	Step 70630	lr 0.00159	Loss 0.5744 (0.4475)	Prec@(1,5) (94.3%, 99.4%)	
07/04 12:36:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][300/781]	Step 70680	lr 0.00159	Loss 0.6464 (0.4444)	Prec@(1,5) (94.3%, 99.5%)	
07/04 12:36:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][350/781]	Step 70730	lr 0.00159	Loss 0.4051 (0.4438)	Prec@(1,5) (94.2%, 99.5%)	
07/04 12:37:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][400/781]	Step 70780	lr 0.00159	Loss 0.3268 (0.4418)	Prec@(1,5) (94.2%, 99.5%)	
07/04 12:37:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][450/781]	Step 70830	lr 0.00159	Loss 0.5163 (0.4435)	Prec@(1,5) (94.2%, 99.4%)	
07/04 12:37:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][500/781]	Step 70880	lr 0.00159	Loss 0.3945 (0.4427)	Prec@(1,5) (94.3%, 99.4%)	
07/04 12:37:36午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][550/781]	Step 70930	lr 0.00159	Loss 0.3630 (0.4425)	Prec@(1,5) (94.3%, 99.4%)	
07/04 12:37:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][600/781]	Step 70980	lr 0.00159	Loss 0.6084 (0.4431)	Prec@(1,5) (94.3%, 99.4%)	
07/04 12:37:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][650/781]	Step 71030	lr 0.00159	Loss 0.3547 (0.4435)	Prec@(1,5) (94.3%, 99.4%)	
07/04 12:38:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][700/781]	Step 71080	lr 0.00159	Loss 0.4099 (0.4434)	Prec@(1,5) (94.2%, 99.5%)	
07/04 12:38:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][750/781]	Step 71130	lr 0.00159	Loss 0.3907 (0.4436)	Prec@(1,5) (94.3%, 99.5%)	
07/04 12:38:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [90][781/781]	Step 71161	lr 0.00159	Loss 0.5222 (0.4436)	Prec@(1,5) (94.3%, 99.5%)	
07/04 12:38:29午前 evaluateCell_trainer.py:172 [INFO] Train: [ 90/99] Final Prec@1 94.2720%
07/04 12:38:32午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][50/391]	Step 71162	Loss 0.1659	Prec@(1,5) (95.0%, 99.7%)
07/04 12:38:35午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][100/391]	Step 71162	Loss 0.1644	Prec@(1,5) (95.1%, 99.6%)
07/04 12:38:38午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][150/391]	Step 71162	Loss 0.1659	Prec@(1,5) (95.1%, 99.6%)
07/04 12:38:41午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][200/391]	Step 71162	Loss 0.1677	Prec@(1,5) (95.0%, 99.6%)
07/04 12:38:45午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][250/391]	Step 71162	Loss 0.1650	Prec@(1,5) (95.2%, 99.6%)
07/04 12:38:48午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][300/391]	Step 71162	Loss 0.1617	Prec@(1,5) (95.3%, 99.6%)
07/04 12:38:51午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][350/391]	Step 71162	Loss 0.1615	Prec@(1,5) (95.3%, 99.7%)
07/04 12:38:54午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [90][390/391]	Step 71162	Loss 0.1613	Prec@(1,5) (95.3%, 99.7%)
07/04 12:38:54午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 90/99] Final Prec@1 95.3360%
07/04 12:38:54午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 95.3680%
07/04 12:39:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][50/781]	Step 71212	lr 0.00148	Loss 0.2515 (0.4006)	Prec@(1,5) (95.4%, 99.6%)	
07/04 12:39:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][100/781]	Step 71262	lr 0.00148	Loss 0.4046 (0.4197)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:39:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][150/781]	Step 71312	lr 0.00148	Loss 0.5786 (0.4207)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:39:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][200/781]	Step 71362	lr 0.00148	Loss 0.3213 (0.4261)	Prec@(1,5) (94.5%, 99.6%)	
07/04 12:39:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][250/781]	Step 71412	lr 0.00148	Loss 0.5787 (0.4293)	Prec@(1,5) (94.4%, 99.6%)	
07/04 12:40:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][300/781]	Step 71462	lr 0.00148	Loss 0.2847 (0.4250)	Prec@(1,5) (94.5%, 99.6%)	
07/04 12:40:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][350/781]	Step 71512	lr 0.00148	Loss 0.3884 (0.4255)	Prec@(1,5) (94.5%, 99.6%)	
07/04 12:40:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][400/781]	Step 71562	lr 0.00148	Loss 0.5797 (0.4267)	Prec@(1,5) (94.5%, 99.6%)	
07/04 12:40:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][450/781]	Step 71612	lr 0.00148	Loss 0.4136 (0.4272)	Prec@(1,5) (94.5%, 99.6%)	
07/04 12:40:48午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][500/781]	Step 71662	lr 0.00148	Loss 0.5077 (0.4269)	Prec@(1,5) (94.5%, 99.6%)	
07/04 12:40:59午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][550/781]	Step 71712	lr 0.00148	Loss 0.7223 (0.4278)	Prec@(1,5) (94.4%, 99.6%)	
07/04 12:41:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][600/781]	Step 71762	lr 0.00148	Loss 0.3671 (0.4308)	Prec@(1,5) (94.4%, 99.6%)	
07/04 12:41:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][650/781]	Step 71812	lr 0.00148	Loss 0.4156 (0.4298)	Prec@(1,5) (94.4%, 99.6%)	
07/04 12:41:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][700/781]	Step 71862	lr 0.00148	Loss 0.3573 (0.4310)	Prec@(1,5) (94.4%, 99.6%)	
07/04 12:41:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][750/781]	Step 71912	lr 0.00148	Loss 0.4430 (0.4309)	Prec@(1,5) (94.4%, 99.6%)	
07/04 12:41:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [91][781/781]	Step 71943	lr 0.00148	Loss 0.3536 (0.4312)	Prec@(1,5) (94.4%, 99.6%)	
07/04 12:41:52午前 evaluateCell_trainer.py:172 [INFO] Train: [ 91/99] Final Prec@1 94.4000%
07/04 12:41:55午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][50/391]	Step 71944	Loss 0.1456	Prec@(1,5) (96.2%, 99.7%)
07/04 12:41:58午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][100/391]	Step 71944	Loss 0.1491	Prec@(1,5) (96.0%, 99.7%)
07/04 12:42:01午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][150/391]	Step 71944	Loss 0.1532	Prec@(1,5) (95.7%, 99.6%)
07/04 12:42:05午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][200/391]	Step 71944	Loss 0.1547	Prec@(1,5) (95.7%, 99.6%)
07/04 12:42:08午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][250/391]	Step 71944	Loss 0.1534	Prec@(1,5) (95.7%, 99.6%)
07/04 12:42:11午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][300/391]	Step 71944	Loss 0.1523	Prec@(1,5) (95.8%, 99.6%)
07/04 12:42:14午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][350/391]	Step 71944	Loss 0.1527	Prec@(1,5) (95.8%, 99.6%)
07/04 12:42:17午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [91][390/391]	Step 71944	Loss 0.1523	Prec@(1,5) (95.8%, 99.6%)
07/04 12:42:17午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 91/99] Final Prec@1 95.7960%
07/04 12:42:17午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 95.7960%
07/04 12:42:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][50/781]	Step 71994	lr 0.00138	Loss 0.3714 (0.4307)	Prec@(1,5) (94.3%, 99.5%)	
07/04 12:42:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][100/781]	Step 72044	lr 0.00138	Loss 0.4806 (0.4323)	Prec@(1,5) (94.4%, 99.6%)	
07/04 12:42:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][150/781]	Step 72094	lr 0.00138	Loss 0.3249 (0.4317)	Prec@(1,5) (94.6%, 99.5%)	
07/04 12:43:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][200/781]	Step 72144	lr 0.00138	Loss 0.4834 (0.4260)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:43:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][250/781]	Step 72194	lr 0.00138	Loss 0.2965 (0.4301)	Prec@(1,5) (94.5%, 99.6%)	
07/04 12:43:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][300/781]	Step 72244	lr 0.00138	Loss 0.5189 (0.4268)	Prec@(1,5) (94.6%, 99.6%)	
07/04 12:43:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][350/781]	Step 72294	lr 0.00138	Loss 0.4448 (0.4292)	Prec@(1,5) (94.5%, 99.6%)	
07/04 12:43:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][400/781]	Step 72344	lr 0.00138	Loss 0.4328 (0.4249)	Prec@(1,5) (94.6%, 99.6%)	
07/04 12:44:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][450/781]	Step 72394	lr 0.00138	Loss 0.4204 (0.4252)	Prec@(1,5) (94.6%, 99.6%)	
07/04 12:44:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][500/781]	Step 72444	lr 0.00138	Loss 0.4228 (0.4257)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:44:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][550/781]	Step 72494	lr 0.00138	Loss 0.5491 (0.4271)	Prec@(1,5) (94.6%, 99.6%)	
07/04 12:44:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][600/781]	Step 72544	lr 0.00138	Loss 0.3661 (0.4275)	Prec@(1,5) (94.6%, 99.6%)	
07/04 12:44:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][650/781]	Step 72594	lr 0.00138	Loss 0.2757 (0.4281)	Prec@(1,5) (94.6%, 99.6%)	
07/04 12:44:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][700/781]	Step 72644	lr 0.00138	Loss 0.3529 (0.4265)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:45:08午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][750/781]	Step 72694	lr 0.00138	Loss 0.3989 (0.4272)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:45:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [92][781/781]	Step 72725	lr 0.00138	Loss 0.3932 (0.4274)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:45:16午前 evaluateCell_trainer.py:172 [INFO] Train: [ 92/99] Final Prec@1 94.6580%
07/04 12:45:19午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][50/391]	Step 72726	Loss 0.1386	Prec@(1,5) (96.0%, 99.7%)
07/04 12:45:22午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][100/391]	Step 72726	Loss 0.1384	Prec@(1,5) (96.2%, 99.6%)
07/04 12:45:25午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][150/391]	Step 72726	Loss 0.1439	Prec@(1,5) (96.0%, 99.6%)
07/04 12:45:29午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][200/391]	Step 72726	Loss 0.1489	Prec@(1,5) (95.8%, 99.6%)
07/04 12:45:32午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][250/391]	Step 72726	Loss 0.1503	Prec@(1,5) (95.8%, 99.6%)
07/04 12:45:35午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][300/391]	Step 72726	Loss 0.1492	Prec@(1,5) (95.8%, 99.6%)
07/04 12:45:38午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][350/391]	Step 72726	Loss 0.1504	Prec@(1,5) (95.8%, 99.6%)
07/04 12:45:40午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [92][390/391]	Step 72726	Loss 0.1500	Prec@(1,5) (95.8%, 99.6%)
07/04 12:45:40午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 92/99] Final Prec@1 95.8080%
07/04 12:45:41午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 95.8080%
07/04 12:45:52午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][50/781]	Step 72776	lr 0.00129	Loss 0.4207 (0.4214)	Prec@(1,5) (94.8%, 99.6%)	
07/04 12:46:04午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][100/781]	Step 72826	lr 0.00129	Loss 0.2898 (0.4251)	Prec@(1,5) (94.8%, 99.6%)	
07/04 12:46:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][150/781]	Step 72876	lr 0.00129	Loss 0.3471 (0.4219)	Prec@(1,5) (94.8%, 99.5%)	
07/04 12:46:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][200/781]	Step 72926	lr 0.00129	Loss 0.3751 (0.4214)	Prec@(1,5) (94.8%, 99.5%)	
07/04 12:46:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][250/781]	Step 72976	lr 0.00129	Loss 0.5710 (0.4157)	Prec@(1,5) (94.9%, 99.6%)	
07/04 12:46:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][300/781]	Step 73026	lr 0.00129	Loss 0.5287 (0.4171)	Prec@(1,5) (94.8%, 99.6%)	
07/04 12:47:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][350/781]	Step 73076	lr 0.00129	Loss 0.5230 (0.4166)	Prec@(1,5) (94.8%, 99.6%)	
07/04 12:47:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][400/781]	Step 73126	lr 0.00129	Loss 0.3341 (0.4175)	Prec@(1,5) (94.8%, 99.6%)	
07/04 12:47:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][450/781]	Step 73176	lr 0.00129	Loss 0.7606 (0.4186)	Prec@(1,5) (94.8%, 99.6%)	
07/04 12:47:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][500/781]	Step 73226	lr 0.00129	Loss 0.3586 (0.4190)	Prec@(1,5) (94.8%, 99.6%)	
07/04 12:47:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][550/781]	Step 73276	lr 0.00129	Loss 0.3734 (0.4216)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:47:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][600/781]	Step 73326	lr 0.00129	Loss 0.4264 (0.4226)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:48:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][650/781]	Step 73376	lr 0.00129	Loss 0.2838 (0.4218)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:48:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][700/781]	Step 73426	lr 0.00129	Loss 0.2594 (0.4224)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:48:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][750/781]	Step 73476	lr 0.00129	Loss 0.2865 (0.4215)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:48:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [93][781/781]	Step 73507	lr 0.00129	Loss 0.4992 (0.4222)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:48:39午前 evaluateCell_trainer.py:172 [INFO] Train: [ 93/99] Final Prec@1 94.7080%
07/04 12:48:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][50/391]	Step 73508	Loss 0.1409	Prec@(1,5) (95.8%, 99.7%)
07/04 12:48:46午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][100/391]	Step 73508	Loss 0.1419	Prec@(1,5) (95.8%, 99.7%)
07/04 12:48:49午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][150/391]	Step 73508	Loss 0.1446	Prec@(1,5) (95.7%, 99.7%)
07/04 12:48:53午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][200/391]	Step 73508	Loss 0.1444	Prec@(1,5) (95.8%, 99.7%)
07/04 12:48:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][250/391]	Step 73508	Loss 0.1428	Prec@(1,5) (95.9%, 99.7%)
07/04 12:48:59午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][300/391]	Step 73508	Loss 0.1427	Prec@(1,5) (95.9%, 99.7%)
07/04 12:49:02午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][350/391]	Step 73508	Loss 0.1434	Prec@(1,5) (95.9%, 99.7%)
07/04 12:49:05午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [93][390/391]	Step 73508	Loss 0.1447	Prec@(1,5) (95.9%, 99.7%)
07/04 12:49:05午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 93/99] Final Prec@1 95.8520%
07/04 12:49:05午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 95.8520%
07/04 12:49:17午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][50/781]	Step 73558	lr 0.00121	Loss 0.3275 (0.4106)	Prec@(1,5) (95.2%, 99.6%)	
07/04 12:49:28午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][100/781]	Step 73608	lr 0.00121	Loss 0.4149 (0.4079)	Prec@(1,5) (95.1%, 99.6%)	
07/04 12:49:39午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][150/781]	Step 73658	lr 0.00121	Loss 0.4521 (0.4053)	Prec@(1,5) (95.0%, 99.7%)	
07/04 12:49:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][200/781]	Step 73708	lr 0.00121	Loss 0.4013 (0.4060)	Prec@(1,5) (94.8%, 99.7%)	
07/04 12:50:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][250/781]	Step 73758	lr 0.00121	Loss 0.4842 (0.4108)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:50:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][300/781]	Step 73808	lr 0.00121	Loss 0.2731 (0.4101)	Prec@(1,5) (94.7%, 99.6%)	
07/04 12:50:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][350/781]	Step 73858	lr 0.00121	Loss 0.3675 (0.4055)	Prec@(1,5) (94.8%, 99.6%)	
07/04 12:50:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][400/781]	Step 73908	lr 0.00121	Loss 0.3759 (0.4088)	Prec@(1,5) (94.8%, 99.6%)	
07/04 12:50:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][450/781]	Step 73958	lr 0.00121	Loss 0.3661 (0.4078)	Prec@(1,5) (94.9%, 99.6%)	
07/04 12:50:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][500/781]	Step 74008	lr 0.00121	Loss 0.5686 (0.4070)	Prec@(1,5) (94.9%, 99.6%)	
07/04 12:51:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][550/781]	Step 74058	lr 0.00121	Loss 0.4313 (0.4080)	Prec@(1,5) (94.9%, 99.6%)	
07/04 12:51:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][600/781]	Step 74108	lr 0.00121	Loss 0.2678 (0.4073)	Prec@(1,5) (94.9%, 99.6%)	
07/04 12:51:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][650/781]	Step 74158	lr 0.00121	Loss 0.5045 (0.4078)	Prec@(1,5) (94.9%, 99.6%)	
07/04 12:51:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][700/781]	Step 74208	lr 0.00121	Loss 0.4048 (0.4077)	Prec@(1,5) (94.9%, 99.6%)	
07/04 12:51:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][750/781]	Step 74258	lr 0.00121	Loss 0.4314 (0.4102)	Prec@(1,5) (94.9%, 99.6%)	
07/04 12:52:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [94][781/781]	Step 74289	lr 0.00121	Loss 0.3680 (0.4121)	Prec@(1,5) (94.8%, 99.6%)	
07/04 12:52:03午前 evaluateCell_trainer.py:172 [INFO] Train: [ 94/99] Final Prec@1 94.8260%
07/04 12:52:06午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][50/391]	Step 74290	Loss 0.1399	Prec@(1,5) (96.2%, 99.6%)
07/04 12:52:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][100/391]	Step 74290	Loss 0.1394	Prec@(1,5) (96.2%, 99.7%)
07/04 12:52:12午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][150/391]	Step 74290	Loss 0.1413	Prec@(1,5) (96.0%, 99.6%)
07/04 12:52:16午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][200/391]	Step 74290	Loss 0.1396	Prec@(1,5) (96.0%, 99.7%)
07/04 12:52:19午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][250/391]	Step 74290	Loss 0.1392	Prec@(1,5) (96.1%, 99.7%)
07/04 12:52:22午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][300/391]	Step 74290	Loss 0.1423	Prec@(1,5) (96.0%, 99.6%)
07/04 12:52:25午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][350/391]	Step 74290	Loss 0.1395	Prec@(1,5) (96.1%, 99.7%)
07/04 12:52:28午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [94][390/391]	Step 74290	Loss 0.1407	Prec@(1,5) (96.1%, 99.7%)
07/04 12:52:28午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 94/99] Final Prec@1 96.1160%
07/04 12:52:28午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.1160%
07/04 12:52:40午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][50/781]	Step 74340	lr 0.00115	Loss 0.4235 (0.3731)	Prec@(1,5) (95.4%, 99.7%)	
07/04 12:52:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][100/781]	Step 74390	lr 0.00115	Loss 0.2524 (0.3919)	Prec@(1,5) (94.8%, 99.7%)	
07/04 12:53:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][150/781]	Step 74440	lr 0.00115	Loss 0.3817 (0.3998)	Prec@(1,5) (94.8%, 99.6%)	
07/04 12:53:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][200/781]	Step 74490	lr 0.00115	Loss 0.6123 (0.4002)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:53:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][250/781]	Step 74540	lr 0.00115	Loss 0.3896 (0.3960)	Prec@(1,5) (95.1%, 99.6%)	
07/04 12:53:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][300/781]	Step 74590	lr 0.00115	Loss 0.5552 (0.3937)	Prec@(1,5) (95.1%, 99.6%)	
07/04 12:53:48午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][350/781]	Step 74640	lr 0.00115	Loss 0.5435 (0.3957)	Prec@(1,5) (94.9%, 99.6%)	
07/04 12:54:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][400/781]	Step 74690	lr 0.00115	Loss 0.4921 (0.3972)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:54:11午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][450/781]	Step 74740	lr 0.00115	Loss 0.2818 (0.3980)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:54:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][500/781]	Step 74790	lr 0.00115	Loss 0.5938 (0.3983)	Prec@(1,5) (95.1%, 99.6%)	
07/04 12:54:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][550/781]	Step 74840	lr 0.00115	Loss 0.6735 (0.4002)	Prec@(1,5) (95.1%, 99.6%)	
07/04 12:54:45午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][600/781]	Step 74890	lr 0.00115	Loss 0.5367 (0.4017)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:54:56午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][650/781]	Step 74940	lr 0.00115	Loss 0.3174 (0.4016)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:55:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][700/781]	Step 74990	lr 0.00115	Loss 0.6932 (0.4040)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:55:19午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][750/781]	Step 75040	lr 0.00115	Loss 0.4961 (0.4043)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:55:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [95][781/781]	Step 75071	lr 0.00115	Loss 0.3679 (0.4048)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:55:26午前 evaluateCell_trainer.py:172 [INFO] Train: [ 95/99] Final Prec@1 94.9680%
07/04 12:55:29午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][50/391]	Step 75072	Loss 0.1322	Prec@(1,5) (96.2%, 99.7%)
07/04 12:55:33午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][100/391]	Step 75072	Loss 0.1347	Prec@(1,5) (96.3%, 99.7%)
07/04 12:55:36午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][150/391]	Step 75072	Loss 0.1365	Prec@(1,5) (96.3%, 99.7%)
07/04 12:55:39午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][200/391]	Step 75072	Loss 0.1392	Prec@(1,5) (96.3%, 99.7%)
07/04 12:55:42午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][250/391]	Step 75072	Loss 0.1378	Prec@(1,5) (96.2%, 99.7%)
07/04 12:55:45午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][300/391]	Step 75072	Loss 0.1388	Prec@(1,5) (96.2%, 99.6%)
07/04 12:55:49午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][350/391]	Step 75072	Loss 0.1382	Prec@(1,5) (96.3%, 99.6%)
07/04 12:55:51午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [95][390/391]	Step 75072	Loss 0.1378	Prec@(1,5) (96.3%, 99.7%)
07/04 12:55:51午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 95/99] Final Prec@1 96.2800%
07/04 12:55:52午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.2800%
07/04 12:56:03午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][50/781]	Step 75122	lr 0.00109	Loss 0.3921 (0.3683)	Prec@(1,5) (96.0%, 99.7%)	
07/04 12:56:15午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][100/781]	Step 75172	lr 0.00109	Loss 0.5619 (0.3892)	Prec@(1,5) (95.3%, 99.7%)	
07/04 12:56:26午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][150/781]	Step 75222	lr 0.00109	Loss 0.4082 (0.3907)	Prec@(1,5) (95.2%, 99.7%)	
07/04 12:56:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][200/781]	Step 75272	lr 0.00109	Loss 0.2888 (0.3933)	Prec@(1,5) (95.2%, 99.7%)	
07/04 12:56:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][250/781]	Step 75322	lr 0.00109	Loss 0.5378 (0.3996)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:57:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][300/781]	Step 75372	lr 0.00109	Loss 0.3580 (0.4001)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:57:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][350/781]	Step 75422	lr 0.00109	Loss 0.3277 (0.4021)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:57:23午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][400/781]	Step 75472	lr 0.00109	Loss 0.5053 (0.4053)	Prec@(1,5) (94.9%, 99.6%)	
07/04 12:57:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][450/781]	Step 75522	lr 0.00109	Loss 0.5757 (0.4052)	Prec@(1,5) (94.9%, 99.6%)	
07/04 12:57:46午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][500/781]	Step 75572	lr 0.00109	Loss 0.3553 (0.4041)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:57:57午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][550/781]	Step 75622	lr 0.00109	Loss 0.3503 (0.4027)	Prec@(1,5) (95.1%, 99.6%)	
07/04 12:58:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][600/781]	Step 75672	lr 0.00109	Loss 0.4413 (0.4046)	Prec@(1,5) (95.0%, 99.6%)	
07/04 12:58:20午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][650/781]	Step 75722	lr 0.00109	Loss 0.2895 (0.4033)	Prec@(1,5) (95.1%, 99.6%)	
07/04 12:58:31午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][700/781]	Step 75772	lr 0.00109	Loss 0.3283 (0.4039)	Prec@(1,5) (95.1%, 99.6%)	
07/04 12:58:43午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][750/781]	Step 75822	lr 0.00109	Loss 0.5137 (0.4049)	Prec@(1,5) (95.1%, 99.6%)	
07/04 12:58:50午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [96][781/781]	Step 75853	lr 0.00109	Loss 0.5060 (0.4045)	Prec@(1,5) (95.1%, 99.6%)	
07/04 12:58:50午前 evaluateCell_trainer.py:172 [INFO] Train: [ 96/99] Final Prec@1 95.0580%
07/04 12:58:53午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][50/391]	Step 75854	Loss 0.1361	Prec@(1,5) (96.0%, 99.8%)
07/04 12:58:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][100/391]	Step 75854	Loss 0.1314	Prec@(1,5) (96.2%, 99.8%)
07/04 12:58:59午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][150/391]	Step 75854	Loss 0.1292	Prec@(1,5) (96.3%, 99.8%)
07/04 12:59:02午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][200/391]	Step 75854	Loss 0.1279	Prec@(1,5) (96.4%, 99.8%)
07/04 12:59:05午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][250/391]	Step 75854	Loss 0.1265	Prec@(1,5) (96.5%, 99.8%)
07/04 12:59:09午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][300/391]	Step 75854	Loss 0.1282	Prec@(1,5) (96.4%, 99.8%)
07/04 12:59:12午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][350/391]	Step 75854	Loss 0.1295	Prec@(1,5) (96.4%, 99.8%)
07/04 12:59:15午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [96][390/391]	Step 75854	Loss 0.1285	Prec@(1,5) (96.5%, 99.8%)
07/04 12:59:15午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 96/99] Final Prec@1 96.4680%
07/04 12:59:15午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.4680%
07/04 12:59:27午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][50/781]	Step 75904	lr 0.00105	Loss 0.3841 (0.3739)	Prec@(1,5) (95.7%, 99.6%)	
07/04 12:59:38午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][100/781]	Step 75954	lr 0.00105	Loss 0.3866 (0.3858)	Prec@(1,5) (95.6%, 99.5%)	
07/04 12:59:49午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][150/781]	Step 76004	lr 0.00105	Loss 0.3441 (0.3882)	Prec@(1,5) (95.5%, 99.6%)	
07/04 01:00:01午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][200/781]	Step 76054	lr 0.00105	Loss 0.2981 (0.3951)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:00:12午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][250/781]	Step 76104	lr 0.00105	Loss 0.3357 (0.3938)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:00:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][300/781]	Step 76154	lr 0.00105	Loss 0.3912 (0.3960)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:00:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][350/781]	Step 76204	lr 0.00105	Loss 0.3748 (0.3954)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:00:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][400/781]	Step 76254	lr 0.00105	Loss 0.3547 (0.3964)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:00:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][450/781]	Step 76304	lr 0.00105	Loss 0.4365 (0.3976)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:01:09午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][500/781]	Step 76354	lr 0.00105	Loss 0.3297 (0.3986)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:01:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][550/781]	Step 76404	lr 0.00105	Loss 0.3799 (0.3982)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:01:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][600/781]	Step 76454	lr 0.00105	Loss 0.6142 (0.4000)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:01:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][650/781]	Step 76504	lr 0.00105	Loss 0.3455 (0.3989)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:01:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][700/781]	Step 76554	lr 0.00105	Loss 0.2836 (0.3992)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:02:06午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][750/781]	Step 76604	lr 0.00105	Loss 0.2730 (0.3983)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:02:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [97][781/781]	Step 76635	lr 0.00105	Loss 0.4142 (0.3980)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:02:14午前 evaluateCell_trainer.py:172 [INFO] Train: [ 97/99] Final Prec@1 95.1640%
07/04 01:02:17午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][50/391]	Step 76636	Loss 0.1208	Prec@(1,5) (96.4%, 99.8%)
07/04 01:02:20午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][100/391]	Step 76636	Loss 0.1215	Prec@(1,5) (96.5%, 99.7%)
07/04 01:02:24午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][150/391]	Step 76636	Loss 0.1225	Prec@(1,5) (96.6%, 99.8%)
07/04 01:02:27午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][200/391]	Step 76636	Loss 0.1276	Prec@(1,5) (96.5%, 99.7%)
07/04 01:02:30午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][250/391]	Step 76636	Loss 0.1280	Prec@(1,5) (96.4%, 99.7%)
07/04 01:02:33午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][300/391]	Step 76636	Loss 0.1266	Prec@(1,5) (96.5%, 99.7%)
07/04 01:02:36午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][350/391]	Step 76636	Loss 0.1268	Prec@(1,5) (96.4%, 99.7%)
07/04 01:02:39午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [97][390/391]	Step 76636	Loss 0.1272	Prec@(1,5) (96.5%, 99.8%)
07/04 01:02:39午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 97/99] Final Prec@1 96.4600%
07/04 01:02:39午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.4680%
07/04 01:02:51午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][50/781]	Step 76686	lr 0.00102	Loss 0.4943 (0.3925)	Prec@(1,5) (95.5%, 99.6%)	
07/04 01:03:02午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][100/781]	Step 76736	lr 0.00102	Loss 0.4813 (0.3958)	Prec@(1,5) (95.1%, 99.5%)	
07/04 01:03:13午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][150/781]	Step 76786	lr 0.00102	Loss 0.3726 (0.3908)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:03:24午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][200/781]	Step 76836	lr 0.00102	Loss 0.3432 (0.3863)	Prec@(1,5) (95.4%, 99.6%)	
07/04 01:03:35午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][250/781]	Step 76886	lr 0.00102	Loss 0.4135 (0.3883)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:03:47午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][300/781]	Step 76936	lr 0.00102	Loss 0.4776 (0.3894)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:03:58午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][350/781]	Step 76986	lr 0.00102	Loss 0.4553 (0.3891)	Prec@(1,5) (95.3%, 99.7%)	
07/04 01:04:10午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][400/781]	Step 77036	lr 0.00102	Loss 0.3233 (0.3921)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:04:21午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][450/781]	Step 77086	lr 0.00102	Loss 0.2409 (0.3935)	Prec@(1,5) (95.1%, 99.6%)	
07/04 01:04:32午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][500/781]	Step 77136	lr 0.00102	Loss 0.2767 (0.3930)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:04:44午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][550/781]	Step 77186	lr 0.00102	Loss 0.4594 (0.3942)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:04:55午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][600/781]	Step 77236	lr 0.00102	Loss 0.5507 (0.3948)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:05:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][650/781]	Step 77286	lr 0.00102	Loss 0.2872 (0.3962)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:05:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][700/781]	Step 77336	lr 0.00102	Loss 0.2914 (0.3963)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:05:29午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][750/781]	Step 77386	lr 0.00102	Loss 0.3730 (0.3953)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:05:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [98][781/781]	Step 77417	lr 0.00102	Loss 0.4907 (0.3945)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:05:37午前 evaluateCell_trainer.py:172 [INFO] Train: [ 98/99] Final Prec@1 95.2580%
07/04 01:05:40午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][50/391]	Step 77418	Loss 0.1261	Prec@(1,5) (96.6%, 99.7%)
07/04 01:05:43午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][100/391]	Step 77418	Loss 0.1273	Prec@(1,5) (96.4%, 99.8%)
07/04 01:05:46午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][150/391]	Step 77418	Loss 0.1301	Prec@(1,5) (96.4%, 99.7%)
07/04 01:05:50午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][200/391]	Step 77418	Loss 0.1292	Prec@(1,5) (96.4%, 99.8%)
07/04 01:05:53午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][250/391]	Step 77418	Loss 0.1308	Prec@(1,5) (96.3%, 99.7%)
07/04 01:05:56午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][300/391]	Step 77418	Loss 0.1317	Prec@(1,5) (96.2%, 99.7%)
07/04 01:05:59午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][350/391]	Step 77418	Loss 0.1304	Prec@(1,5) (96.3%, 99.7%)
07/04 01:06:02午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [98][390/391]	Step 77418	Loss 0.1285	Prec@(1,5) (96.4%, 99.7%)
07/04 01:06:02午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 98/99] Final Prec@1 96.3560%
07/04 01:06:02午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.4680%
07/04 01:06:14午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][50/781]	Step 77468	lr 0.00101	Loss 0.2599 (0.3586)	Prec@(1,5) (96.1%, 99.8%)	
07/04 01:06:25午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][100/781]	Step 77518	lr 0.00101	Loss 0.3389 (0.3735)	Prec@(1,5) (95.5%, 99.7%)	
07/04 01:06:37午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][150/781]	Step 77568	lr 0.00101	Loss 0.3738 (0.3843)	Prec@(1,5) (95.4%, 99.6%)	
07/04 01:06:48午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][200/781]	Step 77618	lr 0.00101	Loss 0.4843 (0.3875)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:07:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][250/781]	Step 77668	lr 0.00101	Loss 0.4769 (0.3841)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:07:11午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][300/781]	Step 77718	lr 0.00101	Loss 0.4552 (0.3864)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:07:22午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][350/781]	Step 77768	lr 0.00101	Loss 0.4659 (0.3853)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:07:33午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][400/781]	Step 77818	lr 0.00101	Loss 0.3478 (0.3892)	Prec@(1,5) (95.2%, 99.6%)	
07/04 01:07:45午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][450/781]	Step 77868	lr 0.00101	Loss 0.2066 (0.3899)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:07:56午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][500/781]	Step 77918	lr 0.00101	Loss 0.5490 (0.3884)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:08:07午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][550/781]	Step 77968	lr 0.00101	Loss 0.3421 (0.3899)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:08:18午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][600/781]	Step 78018	lr 0.00101	Loss 0.4370 (0.3901)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:08:30午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][650/781]	Step 78068	lr 0.00101	Loss 0.3077 (0.3912)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:08:41午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][700/781]	Step 78118	lr 0.00101	Loss 0.4109 (0.3925)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:08:53午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][750/781]	Step 78168	lr 0.00101	Loss 0.3234 (0.3927)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:09:00午前 evaluateCell_trainer.py:162 [INFO] Train: Epoch: [99][781/781]	Step 78199	lr 0.00101	Loss 0.3210 (0.3925)	Prec@(1,5) (95.3%, 99.6%)	
07/04 01:09:00午前 evaluateCell_trainer.py:172 [INFO] Train: [ 99/99] Final Prec@1 95.2760%
07/04 01:09:03午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][50/391]	Step 78200	Loss 0.1206	Prec@(1,5) (96.1%, 99.9%)
07/04 01:09:06午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][100/391]	Step 78200	Loss 0.1214	Prec@(1,5) (96.4%, 99.8%)
07/04 01:09:10午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][150/391]	Step 78200	Loss 0.1285	Prec@(1,5) (96.3%, 99.8%)
07/04 01:09:13午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][200/391]	Step 78200	Loss 0.1295	Prec@(1,5) (96.3%, 99.7%)
07/04 01:09:16午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][250/391]	Step 78200	Loss 0.1285	Prec@(1,5) (96.4%, 99.8%)
07/04 01:09:19午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][300/391]	Step 78200	Loss 0.1325	Prec@(1,5) (96.2%, 99.7%)
07/04 01:09:23午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][350/391]	Step 78200	Loss 0.1324	Prec@(1,5) (96.2%, 99.7%)
07/04 01:09:25午前 evaluateCell_trainer.py:201 [INFO] Valid: Epoch: [99][390/391]	Step 78200	Loss 0.1307	Prec@(1,5) (96.3%, 99.7%)
07/04 01:09:25午前 evaluateCell_trainer.py:208 [INFO] Valid: [ 99/99] Final Prec@1 96.3200%
07/04 01:09:25午前 evaluateCell_main.py:62 [INFO] Until now, best Prec@1 = 96.4680%
07/04 01:09:25午前 evaluateCell_main.py:67 [INFO] Final best Prec@1 = 96.4680%
